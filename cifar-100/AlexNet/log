 BAD LEARNING RATES:


 python cifar10_train.py 
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GT 650M
major: 3 minor: 0 memoryClockRate (GHz) 0.835
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.72GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)
2016-07-14 16:29:32.416146: step 0, loss = 5.73 (8.6 examples/sec; 14.816 sec/batch)
2016-07-14 16:29:39.284766: step 10, loss = 6.91 (273.6 examples/sec; 0.468 sec/batch)
2016-07-14 16:29:43.930161: step 20, loss = 5.80 (278.9 examples/sec; 0.459 sec/batch)
2016-07-14 16:29:48.491987: step 30, loss = 6.03 (292.9 examples/sec; 0.437 sec/batch)
2016-07-14 16:29:52.918413: step 40, loss = 8.26 (293.3 examples/sec; 0.436 sec/batch)
2016-07-14 16:29:57.311117: step 50, loss = 6.20 (300.0 examples/sec; 0.427 sec/batch)
2016-07-14 16:30:01.691553: step 60, loss = 5.62 (291.5 examples/sec; 0.439 sec/batch)
2016-07-14 16:30:06.089414: step 70, loss = 5.71 (291.0 examples/sec; 0.440 sec/batch)
2016-07-14 16:30:10.511529: step 80, loss = 5.90 (293.7 examples/sec; 0.436 sec/batch)
2016-07-14 16:30:14.858337: step 90, loss = 6.12 (291.8 examples/sec; 0.439 sec/batch)
2016-07-14 16:30:19.186497: step 100, loss = 6.09 (298.2 examples/sec; 0.429 sec/batch)
2016-07-14 16:30:24.252831: step 110, loss = 6.66 (288.4 examples/sec; 0.444 sec/batch)
2016-07-14 16:30:28.591581: step 120, loss = 6.17 (294.0 examples/sec; 0.435 sec/batch)
2016-07-14 16:30:32.968738: step 130, loss = 6.25 (298.9 examples/sec; 0.428 sec/batch)
2016-07-14 16:30:37.330806: step 140, loss = 5.95 (290.8 examples/sec; 0.440 sec/batch)
2016-07-14 16:30:41.657457: step 150, loss = 7.52 (296.4 examples/sec; 0.432 sec/batch)
2016-07-14 16:30:45.991809: step 160, loss = 7.05 (295.7 examples/sec; 0.433 sec/batch)
2016-07-14 16:30:51.022838: step 170, loss = 6.97 (208.6 examples/sec; 0.614 sec/batch)
2016-07-14 16:30:55.914749: step 180, loss = 6.66 (266.9 examples/sec; 0.480 sec/batch)
2016-07-14 16:31:00.417931: step 190, loss = 6.83 (282.9 examples/sec; 0.452 sec/batch)
2016-07-14 16:31:04.847131: step 200, loss = 7.04 (277.4 examples/sec; 0.461 sec/batch)
2016-07-14 16:31:10.524820: step 210, loss = 7.15 (205.0 examples/sec; 0.624 sec/batch)
2016-07-14 16:31:15.535351: step 220, loss = 7.00 (290.6 examples/sec; 0.440 sec/batch)
2016-07-14 16:31:19.873312: step 230, loss = 7.17 (286.0 examples/sec; 0.448 sec/batch)
2016-07-14 16:31:24.250898: step 240, loss = 221.74 (280.3 examples/sec; 0.457 sec/batch)
2016-07-14 16:31:28.606107: step 250, loss = 10.11 (299.5 examples/sec; 0.427 sec/batch)
2016-07-14 16:31:34.060154: step 260, loss = 10.19 (277.6 examples/sec; 0.461 sec/batch)
2016-07-14 16:31:38.491649: step 270, loss = 10.44 (293.0 examples/sec; 0.437 sec/batch)
2016-07-14 16:31:42.974295: step 280, loss = 31.10 (277.7 examples/sec; 0.461 sec/batch)
2016-07-14 16:31:47.517839: step 290, loss = 15.09 (291.4 examples/sec; 0.439 sec/batch)
2016-07-14 16:31:51.965149: step 300, loss = 15.67 (291.5 examples/sec; 0.439 sec/batch)
2016-07-14 16:31:56.830811: step 310, loss = 15.33 (284.4 examples/sec; 0.450 sec/batch)
2016-07-14 16:32:01.472581: step 320, loss = 16.01 (217.6 examples/sec; 0.588 sec/batch)
2016-07-14 16:32:06.746976: step 330, loss = 15.42 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 16:32:11.214018: step 340, loss = 14.94 (297.1 examples/sec; 0.431 sec/batch)
2016-07-14 16:32:15.512413: step 350, loss = 15.21 (303.4 examples/sec; 0.422 sec/batch)
2016-07-14 16:32:19.875555: step 360, loss = 15.09 (303.0 examples/sec; 0.422 sec/batch)
2016-07-14 16:32:25.286082: step 370, loss = 14.92 (280.0 examples/sec; 0.457 sec/batch)
2016-07-14 16:32:29.837662: step 380, loss = 14.61 (293.3 examples/sec; 0.436 sec/batch)
2016-07-14 16:32:34.255569: step 390, loss = 27.68 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 16:32:38.731419: step 400, loss = 14.68 (292.9 examples/sec; 0.437 sec/batch)
2016-07-14 16:32:43.612201: step 410, loss = 14.99 (298.4 examples/sec; 0.429 sec/batch)
2016-07-14 16:32:47.956786: step 420, loss = 14.94 (299.1 examples/sec; 0.428 sec/batch)
2016-07-14 16:32:52.221135: step 430, loss = 14.85 (303.3 examples/sec; 0.422 sec/batch)
2016-07-14 16:32:56.498889: step 440, loss = 14.60 (295.5 examples/sec; 0.433 sec/batch)
2016-07-14 16:33:00.822920: step 450, loss = 14.82 (293.8 examples/sec; 0.436 sec/batch)
2016-07-14 16:33:05.361881: step 460, loss = 14.51 (214.1 examples/sec; 0.598 sec/batch)
2016-07-14 16:33:10.645941: step 470, loss = 14.50 (276.9 examples/sec; 0.462 sec/batch)
2016-07-14 16:33:16.038607: step 480, loss = 19.90 (204.3 examples/sec; 0.626 sec/batch)
2016-07-14 16:33:20.529124: step 490, loss = 14.40 (294.8 examples/sec; 0.434 sec/batch)
2016-07-14 16:33:24.947534: step 500, loss = 15.16 (291.2 examples/sec; 0.439 sec/batch)
2016-07-14 16:33:30.397547: step 510, loss = 18.03 (219.1 examples/sec; 0.584 sec/batch)
2016-07-14 16:33:35.483268: step 520, loss = 17.50 (285.9 examples/sec; 0.448 sec/batch)
2016-07-14 16:33:39.958021: step 530, loss = 17.17 (286.3 examples/sec; 0.447 sec/batch)
2016-07-14 16:33:44.481254: step 540, loss = 16.85 (280.3 examples/sec; 0.457 sec/batch)
2016-07-14 16:33:50.667253: step 550, loss = 17.46 (214.5 examples/sec; 0.597 sec/batch)
2016-07-14 16:33:55.718616: step 560, loss = 16.90 (282.3 examples/sec; 0.453 sec/batch)
2016-07-14 16:34:00.132397: step 570, loss = 16.87 (290.4 examples/sec; 0.441 sec/batch)
2016-07-14 16:34:04.667782: step 580, loss = 16.99 (276.5 examples/sec; 0.463 sec/batch)
2016-07-14 16:34:09.102512: step 590, loss = 16.70 (296.2 examples/sec; 0.432 sec/batch)
2016-07-14 16:34:13.480323: step 600, loss = 17.15 (296.8 examples/sec; 0.431 sec/batch)
2016-07-14 16:34:18.305274: step 610, loss = 16.49 (294.0 examples/sec; 0.435 sec/batch)
2016-07-14 16:34:23.419760: step 620, loss = 16.11 (211.2 examples/sec; 0.606 sec/batch)
2016-07-14 16:34:28.233644: step 630, loss = 16.70 (285.9 examples/sec; 0.448 sec/batch)
2016-07-14 16:34:32.661055: step 640, loss = 16.07 (289.3 examples/sec; 0.442 sec/batch)
2016-07-14 16:34:36.984539: step 650, loss = 17.27 (310.9 examples/sec; 0.412 sec/batch)
2016-07-14 16:34:41.584178: step 660, loss = 8525212.00 (270.9 examples/sec; 0.472 sec/batch)
2016-07-14 16:34:46.237356: step 670, loss = 8457269.00 (275.5 examples/sec; 0.465 sec/batch)
2016-07-14 16:34:51.672144: step 680, loss = 8389867.00 (191.3 examples/sec; 0.669 sec/batch)
2016-07-14 16:34:56.889988: step 690, loss = 8323002.00 (266.5 examples/sec; 0.480 sec/batch)
2016-07-14 16:35:01.577827: step 700, loss = 8256670.50 (267.1 examples/sec; 0.479 sec/batch)
2016-07-14 16:35:08.074147: step 710, loss = 8190868.00 (186.9 examples/sec; 0.685 sec/batch)
2016-07-14 16:35:14.292276: step 720, loss = 8125589.50 (241.9 examples/sec; 0.529 sec/batch)
2016-07-14 16:35:19.086042: step 730, loss = 8060831.00 (271.9 examples/sec; 0.471 sec/batch)
2016-07-14 16:35:23.907625: step 740, loss = 7996589.50 (251.7 examples/sec; 0.508 sec/batch)
2016-07-14 16:35:28.625506: step 750, loss = 7932859.50 (276.2 examples/sec; 0.463 sec/batch)
2016-07-14 16:35:33.679348: step 760, loss = 7869637.50 (190.9 examples/sec; 0.670 sec/batch)
2016-07-14 16:35:40.276418: step 770, loss = 7806919.50 (198.6 examples/sec; 0.645 sec/batch)
2016-07-14 16:35:45.281540: step 780, loss = 7744700.50 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 16:35:50.043210: step 790, loss = 7682977.00 (261.6 examples/sec; 0.489 sec/batch)
2016-07-14 16:35:54.844508: step 800, loss = 7621746.50 (275.7 examples/sec; 0.464 sec/batch)
2016-07-14 16:36:00.056612: step 810, loss = 7561005.00 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 16:36:04.781723: step 820, loss = 7500745.00 (260.8 examples/sec; 0.491 sec/batch)
2016-07-14 16:36:10.549141: step 830, loss = 7440967.50 (265.9 examples/sec; 0.481 sec/batch)
2016-07-14 16:36:16.419597: step 840, loss = 7381664.50 (196.4 examples/sec; 0.652 sec/batch)
2016-07-14 16:36:21.635536: step 850, loss = 7322835.00 (203.1 examples/sec; 0.630 sec/batch)
2016-07-14 16:36:27.421912: step 860, loss = 7264474.50 (256.1 examples/sec; 0.500 sec/batch)
2016-07-14 16:36:32.207815: step 870, loss = 7206579.50 (271.6 examples/sec; 0.471 sec/batch)
2016-07-14 16:36:37.233591: step 880, loss = 7149145.50 (227.7 examples/sec; 0.562 sec/batch)
2016-07-14 16:36:43.983728: step 890, loss = 7092168.50 (206.3 examples/sec; 0.620 sec/batch)
2016-07-14 16:36:49.115794: step 900, loss = 7035645.00 (250.8 examples/sec; 0.510 sec/batch)
2016-07-14 16:36:54.534082: step 910, loss = 6979574.00 (263.1 examples/sec; 0.486 sec/batch)
2016-07-14 16:36:59.331005: step 920, loss = 6923950.00 (277.2 examples/sec; 0.462 sec/batch)
2016-07-14 16:37:04.225026: step 930, loss = 6868768.00 (266.3 examples/sec; 0.481 sec/batch)
2016-07-14 16:37:08.925312: step 940, loss = 6814026.00 (278.8 examples/sec; 0.459 sec/batch)
2016-07-14 16:37:13.557550: step 950, loss = 6759720.00 (257.4 examples/sec; 0.497 sec/batch)
2016-07-14 16:37:18.728324: step 960, loss = 6705847.00 (203.3 examples/sec; 0.630 sec/batch)
2016-07-14 16:37:24.210573: step 970, loss = 6652404.00 (255.9 examples/sec; 0.500 sec/batch)
2016-07-14 16:37:29.034678: step 980, loss = 6599386.00 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 16:37:33.694603: step 990, loss = 6546792.00 (277.0 examples/sec; 0.462 sec/batch)
2016-07-14 16:37:38.440187: step 1000, loss = 6494615.00 (272.4 examples/sec; 0.470 sec/batch)
2016-07-14 16:37:44.117168: step 1010, loss = 6442855.50 (273.2 examples/sec; 0.469 sec/batch)
2016-07-14 16:37:49.954667: step 1020, loss = 6391507.00 (252.6 examples/sec; 0.507 sec/batch)
2016-07-14 16:37:54.778095: step 1030, loss = 6340569.00 (272.2 examples/sec; 0.470 sec/batch)
2016-07-14 16:37:59.571078: step 1040, loss = 6290038.50 (259.6 examples/sec; 0.493 sec/batch)
2016-07-14 16:38:04.291255: step 1050, loss = 6239909.00 (280.9 examples/sec; 0.456 sec/batch)
2016-07-14 16:38:09.142413: step 1060, loss = 6190178.50 (251.9 examples/sec; 0.508 sec/batch)
2016-07-14 16:38:15.836315: step 1070, loss = 6140844.00 (204.4 examples/sec; 0.626 sec/batch)
2016-07-14 16:38:21.171869: step 1080, loss = 6091904.50 (252.2 examples/sec; 0.508 sec/batch)
2016-07-14 16:38:27.002259: step 1090, loss = 6043354.00 (254.1 examples/sec; 0.504 sec/batch)
2016-07-14 16:38:31.882203: step 1100, loss = 5995190.50 (278.4 examples/sec; 0.460 sec/batch)
2016-07-14 16:38:37.414792: step 1110, loss = 5947411.50 (249.7 examples/sec; 0.513 sec/batch)
2016-07-14 16:38:42.328582: step 1120, loss = 5900011.00 (263.7 examples/sec; 0.485 sec/batch)
2016-07-14 16:38:48.082123: step 1130, loss = 5852991.00 (186.2 examples/sec; 0.687 sec/batch)
2016-07-14 16:38:54.276704: step 1140, loss = 5806344.00 (262.8 examples/sec; 0.487 sec/batch)
2016-07-14 16:38:59.869766: step 1150, loss = 5760068.50 (196.2 examples/sec; 0.652 sec/batch)
2016-07-14 16:39:05.030542: step 1160, loss = 5714162.50 (267.4 examples/sec; 0.479 sec/batch)
2016-07-14 16:39:09.775285: step 1170, loss = 5668623.50 (254.1 examples/sec; 0.504 sec/batch)
2016-07-14 16:39:14.693598: step 1180, loss = 5623446.00 (265.4 examples/sec; 0.482 sec/batch)
2016-07-14 16:39:19.328734: step 1190, loss = 5578629.50 (274.6 examples/sec; 0.466 sec/batch)
2016-07-14 16:39:24.542709: step 1200, loss = 5534170.00 (248.9 examples/sec; 0.514 sec/batch)
2016-07-14 16:39:31.148199: step 1210, loss = 5490063.50 (206.1 examples/sec; 0.621 sec/batch)
2016-07-14 16:39:35.885822: step 1220, loss = 5446310.50 (269.9 examples/sec; 0.474 sec/batch)
2016-07-14 16:39:40.742985: step 1230, loss = 5402905.00 (263.4 examples/sec; 0.486 sec/batch)
2016-07-14 16:39:47.221168: step 1240, loss = 5359846.00 (202.7 examples/sec; 0.632 sec/batch)
2016-07-14 16:39:52.656798: step 1250, loss = 5317128.50 (257.9 examples/sec; 0.496 sec/batch)
2016-07-14 16:39:57.557808: step 1260, loss = 5274753.50 (232.7 examples/sec; 0.550 sec/batch)
2016-07-14 16:40:02.360113: step 1270, loss = 5232715.00 (272.9 examples/sec; 0.469 sec/batch)
2016-07-14 16:40:06.995723: step 1280, loss = 5191012.50 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 16:40:11.845110: step 1290, loss = 5149642.00 (267.6 examples/sec; 0.478 sec/batch)
2016-07-14 16:40:17.550595: step 1300, loss = 5108601.00 (248.9 examples/sec; 0.514 sec/batch)
2016-07-14 16:40:23.045523: step 1310, loss = 5067886.00 (276.4 examples/sec; 0.463 sec/batch)
2016-07-14 16:40:27.799571: step 1320, loss = 5027498.00 (262.5 examples/sec; 0.488 sec/batch)
2016-07-14 16:40:34.000421: step 1330, loss = 4987429.50 (197.7 examples/sec; 0.648 sec/batch)
2016-07-14 16:40:39.676764: step 1340, loss = 4947682.50 (264.2 examples/sec; 0.484 sec/batch)
2016-07-14 16:40:44.781414: step 1350, loss = 4908251.00 (231.4 examples/sec; 0.553 sec/batch)
2016-07-14 16:40:50.037056: step 1360, loss = 4869133.00 (191.4 examples/sec; 0.669 sec/batch)
2016-07-14 16:40:56.689072: step 1370, loss = 4830327.50 (206.2 examples/sec; 0.621 sec/batch)
2016-07-14 16:41:01.688122: step 1380, loss = 4791832.50 (278.2 examples/sec; 0.460 sec/batch)
2016-07-14 16:41:06.489119: step 1390, loss = 4753643.50 (277.3 examples/sec; 0.462 sec/batch)
2016-07-14 16:41:11.087187: step 1400, loss = 4715757.00 (276.3 examples/sec; 0.463 sec/batch)
2016-07-14 16:41:16.444245: step 1410, loss = 4678174.50 (269.8 examples/sec; 0.474 sec/batch)
2016-07-14 16:41:22.333894: step 1420, loss = 4640891.00 (220.9 examples/sec; 0.579 sec/batch)
2016-07-14 16:41:27.420686: step 1430, loss = 4603905.00 (239.1 examples/sec; 0.535 sec/batch)
2016-07-14 16:41:32.577722: step 1440, loss = 4567213.00 (254.3 examples/sec; 0.503 sec/batch)
2016-07-14 16:41:39.992571: step 1450, loss = 4530814.50 (201.7 examples/sec; 0.635 sec/batch)
2016-07-14 16:41:45.718045: step 1460, loss = 4494704.50 (196.4 examples/sec; 0.652 sec/batch)
2016-07-14 16:41:51.391468: step 1470, loss = 4458883.50 (265.6 examples/sec; 0.482 sec/batch)
2016-07-14 16:41:57.593798: step 1480, loss = 4423348.00 (210.6 examples/sec; 0.608 sec/batch)
2016-07-14 16:42:02.474264: step 1490, loss = 4388095.00 (280.0 examples/sec; 0.457 sec/batch)
2016-07-14 16:42:07.273474: step 1500, loss = 4353123.50 (260.7 examples/sec; 0.491 sec/batch)
2016-07-14 16:42:12.628951: step 1510, loss = 4318430.00 (270.4 examples/sec; 0.473 sec/batch)
2016-07-14 16:42:17.582222: step 1520, loss = 4284013.50 (224.2 examples/sec; 0.571 sec/batch)
2016-07-14 16:42:24.208492: step 1530, loss = 4249872.00 (202.2 examples/sec; 0.633 sec/batch)
2016-07-14 16:42:29.353533: step 1540, loss = 4216002.00 (265.1 examples/sec; 0.483 sec/batch)
2016-07-14 16:42:35.333931: step 1550, loss = 4182401.50 (208.0 examples/sec; 0.616 sec/batch)
2016-07-14 16:42:41.471601: step 1560, loss = 4149070.00 (189.9 examples/sec; 0.674 sec/batch)
2016-07-14 16:42:46.672428: step 1570, loss = 4116002.50 (196.3 examples/sec; 0.652 sec/batch)
2016-07-14 16:42:52.125971: step 1580, loss = 4083199.25 (257.4 examples/sec; 0.497 sec/batch)
2016-07-14 16:42:57.901443: step 1590, loss = 4050658.00 (253.1 examples/sec; 0.506 sec/batch)
2016-07-14 16:43:02.715011: step 1600, loss = 4018375.50 (273.6 examples/sec; 0.468 sec/batch)
2016-07-14 16:43:08.001297: step 1610, loss = 3986350.00 (261.9 examples/sec; 0.489 sec/batch)
2016-07-14 16:43:14.190164: step 1620, loss = 3954579.75 (204.9 examples/sec; 0.625 sec/batch)
2016-07-14 16:43:19.804103: step 1630, loss = 3923063.50 (255.6 examples/sec; 0.501 sec/batch)
2016-07-14 16:43:24.481336: step 1640, loss = 3891797.50 (276.2 examples/sec; 0.463 sec/batch)
2016-07-14 16:43:29.352846: step 1650, loss = 3860781.25 (261.7 examples/sec; 0.489 sec/batch)
2016-07-14 16:43:34.062608: step 1660, loss = 3830011.50 (267.7 examples/sec; 0.478 sec/batch)
2016-07-14 16:43:39.528417: step 1670, loss = 3799488.25 (192.5 examples/sec; 0.665 sec/batch)
2016-07-14 16:43:45.773897: step 1680, loss = 3769208.75 (257.0 examples/sec; 0.498 sec/batch)
2016-07-14 16:43:50.615783: step 1690, loss = 3739168.25 (275.7 examples/sec; 0.464 sec/batch)
2016-07-14 16:43:55.449781: step 1700, loss = 3709368.50 (257.7 examples/sec; 0.497 sec/batch)
2016-07-14 16:44:02.625381: step 1710, loss = 3679806.25 (199.1 examples/sec; 0.643 sec/batch)
2016-07-14 16:44:08.279338: step 1720, loss = 3650479.00 (263.2 examples/sec; 0.486 sec/batch)
2016-07-14 16:44:13.136146: step 1730, loss = 3621386.50 (271.6 examples/sec; 0.471 sec/batch)
2016-07-14 16:44:18.424975: step 1740, loss = 3592524.25 (182.4 examples/sec; 0.702 sec/batch)
2016-07-14 16:44:25.030594: step 1750, loss = 3563892.75 (201.0 examples/sec; 0.637 sec/batch)
2016-07-14 16:44:30.166643: step 1760, loss = 3535490.50 (202.0 examples/sec; 0.634 sec/batch)
2016-07-14 16:44:35.736959: step 1770, loss = 3507313.25 (263.4 examples/sec; 0.486 sec/batch)
2016-07-14 16:44:40.474792: step 1780, loss = 3479361.25 (277.4 examples/sec; 0.461 sec/batch)
2016-07-14 16:44:45.102014: step 1790, loss = 3451631.75 (276.6 examples/sec; 0.463 sec/batch)
2016-07-14 16:44:49.786916: step 1800, loss = 3424123.50 (269.7 examples/sec; 0.475 sec/batch)
2016-07-14 16:44:54.911223: step 1810, loss = 3396834.75 (276.2 examples/sec; 0.463 sec/batch)
2016-07-14 16:45:00.371005: step 1820, loss = 3369763.00 (202.5 examples/sec; 0.632 sec/batch)
2016-07-14 16:45:05.475363: step 1830, loss = 3342906.25 (262.8 examples/sec; 0.487 sec/batch)
2016-07-14 16:45:11.219402: step 1840, loss = 3316265.25 (262.1 examples/sec; 0.488 sec/batch)
2016-07-14 16:45:16.855273: step 1850, loss = 3289836.00 (208.6 examples/sec; 0.614 sec/batch)
2016-07-14 16:45:21.876264: step 1860, loss = 3263616.50 (267.0 examples/sec; 0.479 sec/batch)
2016-07-14 16:45:26.599336: step 1870, loss = 3237606.00 (254.2 examples/sec; 0.504 sec/batch)
2016-07-14 16:45:32.208929: step 1880, loss = 3211804.00 (186.0 examples/sec; 0.688 sec/batch)
2016-07-14 16:45:38.289904: step 1890, loss = 3186207.00 (261.7 examples/sec; 0.489 sec/batch)
2016-07-14 16:45:43.632129: step 1900, loss = 3160814.00 (206.7 examples/sec; 0.619 sec/batch)
2016-07-14 16:45:49.585450: step 1910, loss = 3135624.25 (247.7 examples/sec; 0.517 sec/batch)
2016-07-14 16:45:55.253403: step 1920, loss = 3110633.25 (266.4 examples/sec; 0.480 sec/batch)
2016-07-14 16:46:00.008580: step 1930, loss = 3085843.00 (262.9 examples/sec; 0.487 sec/batch)
2016-07-14 16:46:04.624892: step 1940, loss = 3061250.00 (278.5 examples/sec; 0.460 sec/batch)
2016-07-14 16:46:09.267203: step 1950, loss = 3036852.00 (274.3 examples/sec; 0.467 sec/batch)
2016-07-14 16:46:15.038733: step 1960, loss = 3012650.00 (259.0 examples/sec; 0.494 sec/batch)
2016-07-14 16:46:19.808064: step 1970, loss = 2988640.25 (279.8 examples/sec; 0.457 sec/batch)
2016-07-14 16:46:24.653925: step 1980, loss = 2964822.00 (260.8 examples/sec; 0.491 sec/batch)
2016-07-14 16:46:30.925890: step 1990, loss = 2941193.25 (209.7 examples/sec; 0.610 sec/batch)
2016-07-14 16:46:36.358289: step 2000, loss = 2917752.75 (261.0 examples/sec; 0.491 sec/batch)
2016-07-14 16:46:42.187564: step 2010, loss = 2894499.25 (261.4 examples/sec; 0.490 sec/batch)
2016-07-14 16:46:47.672119: step 2020, loss = 2871431.25 (191.1 examples/sec; 0.670 sec/batch)
2016-07-14 16:46:53.979498: step 2030, loss = 2848546.75 (259.7 examples/sec; 0.493 sec/batch)
2016-07-14 16:46:58.832649: step 2040, loss = 2825845.00 (274.1 examples/sec; 0.467 sec/batch)
2016-07-14 16:47:03.614258: step 2050, loss = 2803323.75 (255.9 examples/sec; 0.500 sec/batch)
2016-07-14 16:47:08.357435: step 2060, loss = 2780982.00 (270.5 examples/sec; 0.473 sec/batch)
2016-07-14 16:47:13.205725: step 2070, loss = 2758818.50 (258.4 examples/sec; 0.495 sec/batch)
2016-07-14 16:47:18.103341: step 2080, loss = 2736832.25 (264.5 examples/sec; 0.484 sec/batch)
2016-07-14 16:47:22.908007: step 2090, loss = 2715020.25 (265.1 examples/sec; 0.483 sec/batch)
2016-07-14 16:47:27.646069: step 2100, loss = 2693382.50 (261.5 examples/sec; 0.490 sec/batch)
2016-07-14 16:47:34.400359: step 2110, loss = 2671916.75 (206.5 examples/sec; 0.620 sec/batch)
2016-07-14 16:47:39.973238: step 2120, loss = 2650623.25 (255.7 examples/sec; 0.501 sec/batch)
2016-07-14 16:47:44.723056: step 2130, loss = 2629498.00 (273.2 examples/sec; 0.469 sec/batch)
2016-07-14 16:47:49.574878: step 2140, loss = 2608542.25 (262.5 examples/sec; 0.488 sec/batch)
2016-07-14 16:47:54.313861: step 2150, loss = 2587753.00 (259.3 examples/sec; 0.494 sec/batch)
2016-07-14 16:47:59.806531: step 2160, loss = 2567129.25 (189.9 examples/sec; 0.674 sec/batch)
2016-07-14 16:48:06.073784: step 2170, loss = 2546669.75 (259.8 examples/sec; 0.493 sec/batch)
2016-07-14 16:48:10.853091: step 2180, loss = 2526374.00 (269.4 examples/sec; 0.475 sec/batch)
2016-07-14 16:48:15.630545: step 2190, loss = 2506239.50 (259.8 examples/sec; 0.493 sec/batch)
2016-07-14 16:48:20.372589: step 2200, loss = 2486266.25 (274.9 examples/sec; 0.466 sec/batch)
2016-07-14 16:48:25.766996: step 2210, loss = 2466451.25 (245.5 examples/sec; 0.521 sec/batch)
2016-07-14 16:48:32.281578: step 2220, loss = 2446794.25 (206.0 examples/sec; 0.621 sec/batch)
2016-07-14 16:48:37.450803: step 2230, loss = 2427294.00 (260.9 examples/sec; 0.491 sec/batch)
2016-07-14 16:48:42.143827: step 2240, loss = 2407949.50 (267.9 examples/sec; 0.478 sec/batch)
2016-07-14 16:48:46.987585: step 2250, loss = 2388758.75 (270.7 examples/sec; 0.473 sec/batch)
2016-07-14 16:48:51.803604: step 2260, loss = 2369721.25 (251.3 examples/sec; 0.509 sec/batch)
2016-07-14 16:48:56.632691: step 2270, loss = 2350835.25 (274.9 examples/sec; 0.466 sec/batch)
2016-07-14 16:49:01.300001: step 2280, loss = 2332100.50 (276.8 examples/sec; 0.462 sec/batch)
2016-07-14 16:49:06.310576: step 2290, loss = 2313514.25 (208.4 examples/sec; 0.614 sec/batch)
2016-07-14 16:49:11.914632: step 2300, loss = 2295076.50 (266.8 examples/sec; 0.480 sec/batch)
2016-07-14 16:49:17.276201: step 2310, loss = 2276785.00 (278.8 examples/sec; 0.459 sec/batch)
2016-07-14 16:49:22.141761: step 2320, loss = 2258639.25 (268.2 examples/sec; 0.477 sec/batch)
2016-07-14 16:49:26.910337: step 2330, loss = 2240639.00 (285.2 examples/sec; 0.449 sec/batch)
2016-07-14 16:49:31.536532: step 2340, loss = 2222782.25 (270.8 examples/sec; 0.473 sec/batch)
2016-07-14 16:49:37.063100: step 2350, loss = 2205067.75 (206.7 examples/sec; 0.619 sec/batch)
2016-07-14 16:49:42.053692: step 2360, loss = 2187493.75 (272.7 examples/sec; 0.469 sec/batch)
2016-07-14 16:49:46.802799: step 2370, loss = 2170060.00 (264.0 examples/sec; 0.485 sec/batch)
2016-07-14 16:49:52.426427: step 2380, loss = 2152765.25 (191.2 examples/sec; 0.669 sec/batch)
2016-07-14 16:49:58.945977: step 2390, loss = 2135608.75 (254.0 examples/sec; 0.504 sec/batch)
2016-07-14 16:50:03.737132: step 2400, loss = 2118589.00 (279.8 examples/sec; 0.458 sec/batch)
2016-07-14 16:50:09.484238: step 2410, loss = 2101704.50 (266.5 examples/sec; 0.480 sec/batch)
2016-07-14 16:50:15.932853: step 2420, loss = 2084953.88 (205.8 examples/sec; 0.622 sec/batch)
2016-07-14 16:50:21.346168: step 2430, loss = 2068337.38 (260.4 examples/sec; 0.492 sec/batch)
2016-07-14 16:50:27.098655: step 2440, loss = 2051853.75 (261.1 examples/sec; 0.490 sec/batch)
2016-07-14 16:50:31.844655: step 2450, loss = 2035501.12 (280.4 examples/sec; 0.457 sec/batch)
2016-07-14 16:50:36.422339: step 2460, loss = 2019278.88 (283.0 examples/sec; 0.452 sec/batch)
2016-07-14 16:50:41.083970: step 2470, loss = 2003186.12 (277.7 examples/sec; 0.461 sec/batch)
2016-07-14 16:50:46.880849: step 2480, loss = 1987221.12 (261.6 examples/sec; 0.489 sec/batch)
2016-07-14 16:50:51.660253: step 2490, loss = 1971383.25 (280.0 examples/sec; 0.457 sec/batch)
2016-07-14 16:50:56.394230: step 2500, loss = 1955672.38 (264.8 examples/sec; 0.483 sec/batch)
2016-07-14 16:51:03.275710: step 2510, loss = 1940086.25 (206.7 examples/sec; 0.619 sec/batch)
2016-07-14 16:51:08.690064: step 2520, loss = 1924624.25 (263.6 examples/sec; 0.486 sec/batch)
2016-07-14 16:51:13.400015: step 2530, loss = 1909285.75 (275.1 examples/sec; 0.465 sec/batch)
2016-07-14 16:51:18.242017: step 2540, loss = 1894070.12 (271.2 examples/sec; 0.472 sec/batch)
2016-07-14 16:51:22.958850: step 2550, loss = 1878974.88 (262.9 examples/sec; 0.487 sec/batch)
2016-07-14 16:51:28.699957: step 2560, loss = 1863999.62 (187.7 examples/sec; 0.682 sec/batch)
2016-07-14 16:51:34.800390: step 2570, loss = 1849144.00 (263.2 examples/sec; 0.486 sec/batch)
2016-07-14 16:51:39.544466: step 2580, loss = 1834407.00 (277.5 examples/sec; 0.461 sec/batch)
2016-07-14 16:51:44.292297: step 2590, loss = 1819787.12 (258.4 examples/sec; 0.495 sec/batch)
2016-07-14 16:51:49.000266: step 2600, loss = 1805284.12 (277.7 examples/sec; 0.461 sec/batch)
2016-07-14 16:51:54.572550: step 2610, loss = 1790896.88 (188.2 examples/sec; 0.680 sec/batch)
2016-07-14 16:52:01.075274: step 2620, loss = 1776623.88 (207.0 examples/sec; 0.618 sec/batch)
2016-07-14 16:52:06.155148: step 2630, loss = 1762465.00 (223.5 examples/sec; 0.573 sec/batch)
2016-07-14 16:52:11.848673: step 2640, loss = 1748418.62 (265.1 examples/sec; 0.483 sec/batch)
2016-07-14 16:52:16.647969: step 2650, loss = 1734484.25 (271.1 examples/sec; 0.472 sec/batch)
2016-07-14 16:52:21.867789: step 2660, loss = 1720660.88 (258.1 examples/sec; 0.496 sec/batch)
2016-07-14 16:52:26.593248: step 2670, loss = 1706947.75 (264.2 examples/sec; 0.484 sec/batch)
2016-07-14 16:52:32.305356: step 2680, loss = 1693344.00 (188.8 examples/sec; 0.678 sec/batch)
2016-07-14 16:52:38.643075: step 2690, loss = 1679849.25 (226.5 examples/sec; 0.565 sec/batch)
2016-07-14 16:52:43.977821: step 2700, loss = 1666461.12 (204.6 examples/sec; 0.626 sec/batch)
2016-07-14 16:52:49.910672: step 2710, loss = 1653179.62 (266.4 examples/sec; 0.480 sec/batch)
2016-07-14 16:52:54.560559: step 2720, loss = 1640004.38 (267.6 examples/sec; 0.478 sec/batch)
2016-07-14 16:53:00.005480: step 2730, loss = 1626933.62 (185.3 examples/sec; 0.691 sec/batch)
2016-07-14 16:53:06.374708: step 2740, loss = 1613967.25 (261.9 examples/sec; 0.489 sec/batch)
2016-07-14 16:53:11.139392: step 2750, loss = 1601104.88 (281.7 examples/sec; 0.454 sec/batch)
2016-07-14 16:53:15.846585: step 2760, loss = 1588345.38 (260.4 examples/sec; 0.492 sec/batch)
2016-07-14 16:53:20.561500: step 2770, loss = 1575685.75 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 16:53:25.379505: step 2780, loss = 1563128.62 (264.3 examples/sec; 0.484 sec/batch)
2016-07-14 16:53:29.997978: step 2790, loss = 1550671.12 (278.1 examples/sec; 0.460 sec/batch)
2016-07-14 16:53:35.265860: step 2800, loss = 1538312.38 (191.8 examples/sec; 0.667 sec/batch)
2016-07-14 16:53:42.463324: step 2810, loss = 1526052.75 (260.5 examples/sec; 0.491 sec/batch)
2016-07-14 16:53:47.214187: step 2820, loss = 1513890.50 (281.4 examples/sec; 0.455 sec/batch)
2016-07-14 16:53:51.968482: step 2830, loss = 1501825.88 (260.6 examples/sec; 0.491 sec/batch)
2016-07-14 16:53:56.665352: step 2840, loss = 1489856.75 (275.8 examples/sec; 0.464 sec/batch)
2016-07-14 16:54:01.437359: step 2850, loss = 1477982.75 (263.8 examples/sec; 0.485 sec/batch)
2016-07-14 16:54:06.152266: step 2860, loss = 1466204.12 (270.0 examples/sec; 0.474 sec/batch)
2016-07-14 16:54:10.975252: step 2870, loss = 1454518.75 (278.6 examples/sec; 0.460 sec/batch)
2016-07-14 16:54:15.608474: step 2880, loss = 1442926.62 (286.2 examples/sec; 0.447 sec/batch)
2016-07-14 16:54:20.204888: step 2890, loss = 1431426.62 (276.3 examples/sec; 0.463 sec/batch)
2016-07-14 16:54:24.864653: step 2900, loss = 1420019.38 (271.3 examples/sec; 0.472 sec/batch)
2016-07-14 16:54:31.132192: step 2910, loss = 1408702.12 (267.8 examples/sec; 0.478 sec/batch)
2016-07-14 16:54:35.914417: step 2920, loss = 1397475.25 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 16:54:40.676183: step 2930, loss = 1386337.50 (264.4 examples/sec; 0.484 sec/batch)
2016-07-14 16:54:46.867048: step 2940, loss = 1375288.62 (203.1 examples/sec; 0.630 sec/batch)
2016-07-14 16:54:52.390300: step 2950, loss = 1364328.50 (264.4 examples/sec; 0.484 sec/batch)
2016-07-14 16:54:57.048786: step 2960, loss = 1353454.88 (280.2 examples/sec; 0.457 sec/batch)
2016-07-14 16:55:01.664507: step 2970, loss = 1342669.00 (284.3 examples/sec; 0.450 sec/batch)
2016-07-14 16:55:06.326582: step 2980, loss = 1331968.25 (269.2 examples/sec; 0.475 sec/batch)
2016-07-14 16:55:10.908933: step 2990, loss = 1321352.88 (282.3 examples/sec; 0.453 sec/batch)
2016-07-14 16:55:16.079497: step 3000, loss = 1310822.12 (205.2 examples/sec; 0.624 sec/batch)
2016-07-14 16:55:22.718566: step 3010, loss = 1300375.12 (204.7 examples/sec; 0.625 sec/batch)
2016-07-14 16:55:28.261557: step 3020, loss = 1290010.88 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 16:55:34.035411: step 3030, loss = 1279730.38 (208.2 examples/sec; 0.615 sec/batch)
2016-07-14 16:55:38.821440: step 3040, loss = 1269531.25 (284.0 examples/sec; 0.451 sec/batch)
2016-07-14 16:55:43.514752: step 3050, loss = 1259413.88 (261.2 examples/sec; 0.490 sec/batch)
2016-07-14 16:55:48.202082: step 3060, loss = 1249376.62 (267.4 examples/sec; 0.479 sec/batch)
2016-07-14 16:55:53.079043: step 3070, loss = 1239419.25 (264.3 examples/sec; 0.484 sec/batch)
2016-07-14 16:55:57.764098: step 3080, loss = 1229541.25 (273.2 examples/sec; 0.469 sec/batch)
2016-07-14 16:56:02.818353: step 3090, loss = 1219742.62 (191.6 examples/sec; 0.668 sec/batch)
2016-07-14 16:56:09.272729: step 3100, loss = 1210021.88 (207.7 examples/sec; 0.616 sec/batch)
2016-07-14 16:56:14.862546: step 3110, loss = 1200378.50 (275.7 examples/sec; 0.464 sec/batch)
2016-07-14 16:56:19.621985: step 3120, loss = 1190811.38 (257.9 examples/sec; 0.496 sec/batch)
2016-07-14 16:56:24.360393: step 3130, loss = 1181321.25 (272.9 examples/sec; 0.469 sec/batch)
2016-07-14 16:56:28.931799: step 3140, loss = 1171906.38 (285.6 examples/sec; 0.448 sec/batch)
2016-07-14 16:56:33.554402: step 3150, loss = 1162567.00 (279.9 examples/sec; 0.457 sec/batch)
2016-07-14 16:56:38.160024: step 3160, loss = 1153301.62 (272.2 examples/sec; 0.470 sec/batch)
2016-07-14 16:56:43.007391: step 3170, loss = 1144110.38 (209.6 examples/sec; 0.611 sec/batch)
2016-07-14 16:56:48.540157: step 3180, loss = 1134991.88 (269.6 examples/sec; 0.475 sec/batch)
2016-07-14 16:56:53.293187: step 3190, loss = 1125946.38 (267.7 examples/sec; 0.478 sec/batch)
2016-07-14 16:56:58.131866: step 3200, loss = 1116973.38 (257.5 examples/sec; 0.497 sec/batch)
2016-07-14 16:57:05.502161: step 3210, loss = 1108071.12 (203.7 examples/sec; 0.628 sec/batch)
2016-07-14 16:57:10.464529: step 3220, loss = 1099240.12 (277.1 examples/sec; 0.462 sec/batch)
2016-07-14 16:57:15.106959: step 3230, loss = 1090480.00 (267.1 examples/sec; 0.479 sec/batch)
2016-07-14 16:57:19.827587: step 3240, loss = 1081789.12 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 16:57:24.545438: step 3250, loss = 1073167.50 (266.1 examples/sec; 0.481 sec/batch)
2016-07-14 16:57:29.241063: step 3260, loss = 1064614.25 (277.5 examples/sec; 0.461 sec/batch)
2016-07-14 16:57:34.046458: step 3270, loss = 1056130.62 (264.7 examples/sec; 0.484 sec/batch)
2016-07-14 16:57:38.699465: step 3280, loss = 1047712.94 (261.7 examples/sec; 0.489 sec/batch)
2016-07-14 16:57:43.477308: step 3290, loss = 1039363.19 (276.4 examples/sec; 0.463 sec/batch)
2016-07-14 16:57:48.267513: step 3300, loss = 1031079.69 (256.6 examples/sec; 0.499 sec/batch)
2016-07-14 16:57:53.532971: step 3310, loss = 1022862.12 (278.6 examples/sec; 0.460 sec/batch)
2016-07-14 16:57:58.070278: step 3320, loss = 1014710.50 (288.3 examples/sec; 0.444 sec/batch)
2016-07-14 16:58:02.679308: step 3330, loss = 1006623.81 (263.5 examples/sec; 0.486 sec/batch)
2016-07-14 16:58:07.776253: step 3340, loss = 998600.94 (198.8 examples/sec; 0.644 sec/batch)
2016-07-14 16:58:13.251047: step 3350, loss = 990642.56 (268.3 examples/sec; 0.477 sec/batch)
2016-07-14 16:58:18.937421: step 3360, loss = 982747.50 (267.9 examples/sec; 0.478 sec/batch)
2016-07-14 16:58:23.701740: step 3370, loss = 974915.50 (274.3 examples/sec; 0.467 sec/batch)
2016-07-14 16:58:28.329684: step 3380, loss = 967145.75 (281.9 examples/sec; 0.454 sec/batch)
2016-07-14 16:58:32.932847: step 3390, loss = 959438.12 (276.3 examples/sec; 0.463 sec/batch)
2016-07-14 16:58:37.558868: step 3400, loss = 951791.50 (279.3 examples/sec; 0.458 sec/batch)
2016-07-14 16:58:43.841479: step 3410, loss = 944206.88 (261.7 examples/sec; 0.489 sec/batch)
2016-07-14 16:58:49.418633: step 3420, loss = 936680.69 (207.1 examples/sec; 0.618 sec/batch)
2016-07-14 16:58:54.480058: step 3430, loss = 929216.19 (230.8 examples/sec; 0.555 sec/batch)
2016-07-14 16:59:00.098869: step 3440, loss = 921810.75 (267.0 examples/sec; 0.479 sec/batch)
2016-07-14 16:59:04.819711: step 3450, loss = 914464.00 (282.4 examples/sec; 0.453 sec/batch)
2016-07-14 16:59:09.406011: step 3460, loss = 907176.12 (278.8 examples/sec; 0.459 sec/batch)
2016-07-14 16:59:14.041624: step 3470, loss = 899946.06 (273.4 examples/sec; 0.468 sec/batch)
2016-07-14 16:59:19.791970: step 3480, loss = 892774.06 (255.2 examples/sec; 0.502 sec/batch)
2016-07-14 16:59:24.530565: step 3490, loss = 885659.06 (273.3 examples/sec; 0.468 sec/batch)
2016-07-14 16:59:29.318461: step 3500, loss = 878600.25 (256.4 examples/sec; 0.499 sec/batch)
2016-07-14 16:59:36.334868: step 3510, loss = 871598.00 (201.7 examples/sec; 0.635 sec/batch)
2016-07-14 16:59:41.621567: step 3520, loss = 864651.69 (262.5 examples/sec; 0.488 sec/batch)
2016-07-14 16:59:46.265868: step 3530, loss = 857761.12 (274.4 examples/sec; 0.466 sec/batch)
2016-07-14 16:59:51.050748: step 3540, loss = 850924.62 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 16:59:55.738149: step 3550, loss = 844143.19 (268.0 examples/sec; 0.478 sec/batch)
2016-07-14 17:00:00.447162: step 3560, loss = 837415.62 (277.2 examples/sec; 0.462 sec/batch)
2016-07-14 17:00:05.159502: step 3570, loss = 830741.62 (259.4 examples/sec; 0.493 sec/batch)
2016-07-14 17:00:09.855587: step 3580, loss = 824120.81 (275.3 examples/sec; 0.465 sec/batch)
2016-07-14 17:00:14.680365: step 3590, loss = 817553.19 (266.8 examples/sec; 0.480 sec/batch)
2016-07-14 17:00:19.353637: step 3600, loss = 811037.88 (270.4 examples/sec; 0.473 sec/batch)
2016-07-14 17:00:25.819988: step 3610, loss = 804573.69 (187.4 examples/sec; 0.683 sec/batch)
2016-07-14 17:00:31.680573: step 3620, loss = 798161.75 (264.8 examples/sec; 0.483 sec/batch)
2016-07-14 17:00:36.454291: step 3630, loss = 791800.25 (277.6 examples/sec; 0.461 sec/batch)
2016-07-14 17:00:41.208337: step 3640, loss = 785490.06 (266.9 examples/sec; 0.480 sec/batch)
2016-07-14 17:00:45.934898: step 3650, loss = 779230.00 (268.4 examples/sec; 0.477 sec/batch)
2016-07-14 17:00:50.723847: step 3660, loss = 773020.00 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 17:00:55.359349: step 3670, loss = 766859.31 (261.1 examples/sec; 0.490 sec/batch)
2016-07-14 17:01:00.099259: step 3680, loss = 760747.69 (271.3 examples/sec; 0.472 sec/batch)
2016-07-14 17:01:04.844763: step 3690, loss = 754684.94 (266.7 examples/sec; 0.480 sec/batch)
2016-07-14 17:01:09.547302: step 3700, loss = 748669.88 (281.7 examples/sec; 0.454 sec/batch)
2016-07-14 17:01:14.922020: step 3710, loss = 742703.69 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 17:01:19.636831: step 3720, loss = 736784.38 (268.1 examples/sec; 0.477 sec/batch)
2016-07-14 17:01:25.277371: step 3730, loss = 730912.44 (187.9 examples/sec; 0.681 sec/batch)
2016-07-14 17:01:31.305182: step 3740, loss = 725087.06 (265.2 examples/sec; 0.483 sec/batch)
2016-07-14 17:01:36.074427: step 3750, loss = 719308.75 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 17:01:40.807701: step 3760, loss = 713576.19 (260.5 examples/sec; 0.491 sec/batch)
2016-07-14 17:01:45.467452: step 3770, loss = 707889.06 (287.7 examples/sec; 0.445 sec/batch)
2016-07-14 17:01:50.294601: step 3780, loss = 702247.38 (270.6 examples/sec; 0.473 sec/batch)
2016-07-14 17:01:54.983721: step 3790, loss = 696650.69 (263.1 examples/sec; 0.487 sec/batch)
2016-07-14 17:01:59.773640: step 3800, loss = 691098.75 (278.5 examples/sec; 0.460 sec/batch)
2016-07-14 17:02:05.088881: step 3810, loss = 685590.81 (260.3 examples/sec; 0.492 sec/batch)
2016-07-14 17:02:09.782007: step 3820, loss = 680127.12 (285.7 examples/sec; 0.448 sec/batch)
2016-07-14 17:02:14.326739: step 3830, loss = 674706.69 (288.1 examples/sec; 0.444 sec/batch)
2016-07-14 17:02:18.867117: step 3840, loss = 669329.88 (280.2 examples/sec; 0.457 sec/batch)
2016-07-14 17:02:23.927433: step 3850, loss = 663995.44 (208.2 examples/sec; 0.615 sec/batch)
2016-07-14 17:02:29.349305: step 3860, loss = 658703.00 (273.2 examples/sec; 0.468 sec/batch)
2016-07-14 17:02:34.009874: step 3870, loss = 653453.88 (286.9 examples/sec; 0.446 sec/batch)
2016-07-14 17:02:38.844136: step 3880, loss = 648245.88 (279.8 examples/sec; 0.457 sec/batch)
2016-07-14 17:02:43.542587: step 3890, loss = 643079.69 (264.3 examples/sec; 0.484 sec/batch)
2016-07-14 17:02:49.224533: step 3900, loss = 637954.44 (189.5 examples/sec; 0.675 sec/batch)
2016-07-14 17:02:55.966319: step 3910, loss = 632870.31 (260.1 examples/sec; 0.492 sec/batch)
2016-07-14 17:03:00.670070: step 3920, loss = 627826.50 (281.1 examples/sec; 0.455 sec/batch)
2016-07-14 17:03:05.449287: step 3930, loss = 622823.00 (267.0 examples/sec; 0.479 sec/batch)
2016-07-14 17:03:10.143809: step 3940, loss = 617859.38 (269.4 examples/sec; 0.475 sec/batch)
2016-07-14 17:03:15.286610: step 3950, loss = 612935.25 (183.0 examples/sec; 0.700 sec/batch)
2016-07-14 17:03:22.007969: step 3960, loss = 608050.12 (199.9 examples/sec; 0.640 sec/batch)
2016-07-14 17:03:27.251124: step 3970, loss = 603204.38 (198.1 examples/sec; 0.646 sec/batch)
2016-07-14 17:03:32.900632: step 3980, loss = 598397.56 (265.9 examples/sec; 0.481 sec/batch)
2016-07-14 17:03:37.591444: step 3990, loss = 593627.62 (267.6 examples/sec; 0.478 sec/batch)
2016-07-14 17:03:42.360858: step 4000, loss = 588896.81 (269.7 examples/sec; 0.475 sec/batch)
2016-07-14 17:03:48.009214: step 4010, loss = 584203.75 (258.1 examples/sec; 0.496 sec/batch)
2016-07-14 17:03:52.705513: step 4020, loss = 579547.62 (283.7 examples/sec; 0.451 sec/batch)
2016-07-14 17:03:57.491063: step 4030, loss = 574929.00 (259.0 examples/sec; 0.494 sec/batch)
2016-07-14 17:04:02.171175: step 4040, loss = 570347.06 (274.4 examples/sec; 0.466 sec/batch)
2016-07-14 17:04:07.239325: step 4050, loss = 565801.38 (190.4 examples/sec; 0.672 sec/batch)
2016-07-14 17:04:13.776904: step 4060, loss = 561291.94 (199.9 examples/sec; 0.640 sec/batch)
2016-07-14 17:04:18.852996: step 4070, loss = 556819.31 (216.5 examples/sec; 0.591 sec/batch)
2016-07-14 17:04:24.520828: step 4080, loss = 552381.50 (272.1 examples/sec; 0.470 sec/batch)
2016-07-14 17:04:29.235515: step 4090, loss = 547978.94 (277.4 examples/sec; 0.461 sec/batch)
2016-07-14 17:04:34.096303: step 4100, loss = 543611.56 (268.2 examples/sec; 0.477 sec/batch)
2016-07-14 17:04:41.324715: step 4110, loss = 539279.25 (202.6 examples/sec; 0.632 sec/batch)
2016-07-14 17:04:46.465336: step 4120, loss = 534981.50 (260.0 examples/sec; 0.492 sec/batch)
2016-07-14 17:04:51.106572: step 4130, loss = 530718.25 (265.7 examples/sec; 0.482 sec/batch)
2016-07-14 17:04:55.909282: step 4140, loss = 526488.12 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 17:05:00.692066: step 4150, loss = 522292.62 (261.9 examples/sec; 0.489 sec/batch)
2016-07-14 17:05:05.437183: step 4160, loss = 518129.94 (282.3 examples/sec; 0.453 sec/batch)
2016-07-14 17:05:10.001046: step 4170, loss = 514000.66 (281.5 examples/sec; 0.455 sec/batch)
2016-07-14 17:05:14.601659: step 4180, loss = 509904.22 (274.9 examples/sec; 0.466 sec/batch)
2016-07-14 17:05:19.520311: step 4190, loss = 505840.22 (203.8 examples/sec; 0.628 sec/batch)
2016-07-14 17:05:25.099589: step 4200, loss = 501809.34 (260.1 examples/sec; 0.492 sec/batch)
2016-07-14 17:05:31.608711: step 4210, loss = 497809.94 (261.8 examples/sec; 0.489 sec/batch)
2016-07-14 17:05:36.370910: step 4220, loss = 493842.53 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 17:05:41.104346: step 4230, loss = 489906.69 (255.1 examples/sec; 0.502 sec/batch)
2016-07-14 17:05:45.806450: step 4240, loss = 486002.66 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 17:05:50.649311: step 4250, loss = 482128.91 (247.9 examples/sec; 0.516 sec/batch)
2016-07-14 17:05:57.251122: step 4260, loss = 478286.66 (205.7 examples/sec; 0.622 sec/batch)
2016-07-14 17:06:02.418357: step 4270, loss = 474475.00 (261.2 examples/sec; 0.490 sec/batch)
2016-07-14 17:06:07.066358: step 4280, loss = 470693.38 (261.0 examples/sec; 0.490 sec/batch)
2016-07-14 17:06:11.839582: step 4290, loss = 466942.28 (279.2 examples/sec; 0.458 sec/batch)
2016-07-14 17:06:16.560678: step 4300, loss = 463220.56 (259.6 examples/sec; 0.493 sec/batch)
2016-07-14 17:06:23.352496: step 4310, loss = 459529.38 (203.5 examples/sec; 0.629 sec/batch)
2016-07-14 17:06:28.945732: step 4320, loss = 455866.62 (264.7 examples/sec; 0.484 sec/batch)
2016-07-14 17:06:33.640760: step 4330, loss = 452233.72 (273.9 examples/sec; 0.467 sec/batch)
2016-07-14 17:06:38.250026: step 4340, loss = 448629.38 (279.2 examples/sec; 0.458 sec/batch)
2016-07-14 17:06:43.057469: step 4350, loss = 445054.09 (210.3 examples/sec; 0.609 sec/batch)
2016-07-14 17:06:48.747604: step 4360, loss = 441507.38 (257.2 examples/sec; 0.498 sec/batch)
2016-07-14 17:06:54.496370: step 4370, loss = 437988.72 (204.0 examples/sec; 0.627 sec/batch)
2016-07-14 17:06:59.574580: step 4380, loss = 434497.88 (203.9 examples/sec; 0.628 sec/batch)
2016-07-14 17:07:05.115217: step 4390, loss = 431035.47 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 17:07:09.803550: step 4400, loss = 427600.06 (277.9 examples/sec; 0.461 sec/batch)
2016-07-14 17:07:15.430504: step 4410, loss = 424192.34 (187.5 examples/sec; 0.683 sec/batch)
2016-07-14 17:07:21.873430: step 4420, loss = 420811.47 (207.0 examples/sec; 0.618 sec/batch)
2016-07-14 17:07:26.996821: step 4430, loss = 417458.22 (217.7 examples/sec; 0.588 sec/batch)
2016-07-14 17:07:32.714405: step 4440, loss = 414130.41 (256.3 examples/sec; 0.499 sec/batch)
2016-07-14 17:07:37.452348: step 4450, loss = 410830.62 (287.1 examples/sec; 0.446 sec/batch)
2016-07-14 17:07:42.257497: step 4460, loss = 407556.03 (267.3 examples/sec; 0.479 sec/batch)
2016-07-14 17:07:48.728273: step 4470, loss = 404308.38 (205.1 examples/sec; 0.624 sec/batch)
2016-07-14 17:07:54.017570: step 4480, loss = 401086.09 (256.0 examples/sec; 0.500 sec/batch)
2016-07-14 17:07:58.753785: step 4490, loss = 397889.22 (271.6 examples/sec; 0.471 sec/batch)
2016-07-14 17:08:03.655082: step 4500, loss = 394718.44 (269.7 examples/sec; 0.475 sec/batch)
2016-07-14 17:08:08.865447: step 4510, loss = 391572.50 (269.2 examples/sec; 0.476 sec/batch)
2016-07-14 17:08:13.387867: step 4520, loss = 388452.41 (274.0 examples/sec; 0.467 sec/batch)
2016-07-14 17:08:18.005260: step 4530, loss = 385356.22 (280.2 examples/sec; 0.457 sec/batch)
2016-07-14 17:08:23.775067: step 4540, loss = 382284.69 (268.3 examples/sec; 0.477 sec/batch)
2016-07-14 17:08:28.636165: step 4550, loss = 379238.25 (267.5 examples/sec; 0.478 sec/batch)
2016-07-14 17:08:33.274855: step 4560, loss = 376215.97 (286.1 examples/sec; 0.447 sec/batch)
2016-07-14 17:08:37.861776: step 4570, loss = 373217.66 (276.8 examples/sec; 0.462 sec/batch)
2016-07-14 17:08:42.492976: step 4580, loss = 370243.44 (273.0 examples/sec; 0.469 sec/batch)
2016-07-14 17:08:48.312979: step 4590, loss = 367292.56 (264.9 examples/sec; 0.483 sec/batch)
2016-07-14 17:08:53.899953: step 4600, loss = 364365.09 (208.6 examples/sec; 0.614 sec/batch)
2016-07-14 17:08:59.563325: step 4610, loss = 361461.72 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 17:09:04.363365: step 4620, loss = 358580.47 (237.5 examples/sec; 0.539 sec/batch)
2016-07-14 17:09:10.199079: step 4630, loss = 355722.97 (186.7 examples/sec; 0.686 sec/batch)
2016-07-14 17:09:16.138279: step 4640, loss = 352887.84 (262.6 examples/sec; 0.487 sec/batch)
2016-07-14 17:09:20.854084: step 4650, loss = 350075.16 (277.5 examples/sec; 0.461 sec/batch)
2016-07-14 17:09:25.514003: step 4660, loss = 347285.59 (281.3 examples/sec; 0.455 sec/batch)
2016-07-14 17:09:30.130937: step 4670, loss = 344517.50 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 17:09:34.713756: step 4680, loss = 341772.22 (278.9 examples/sec; 0.459 sec/batch)
2016-07-14 17:09:39.306602: step 4690, loss = 339048.19 (275.8 examples/sec; 0.464 sec/batch)
2016-07-14 17:09:44.001179: step 4700, loss = 336346.12 (280.4 examples/sec; 0.456 sec/batch)
2016-07-14 17:09:49.122763: step 4710, loss = 333666.03 (273.0 examples/sec; 0.469 sec/batch)
2016-07-14 17:09:54.810935: step 4720, loss = 331006.28 (197.1 examples/sec; 0.649 sec/batch)
2016-07-14 17:09:59.732018: step 4730, loss = 328368.62 (255.9 examples/sec; 0.500 sec/batch)
2016-07-14 17:10:04.512762: step 4740, loss = 325751.44 (264.5 examples/sec; 0.484 sec/batch)
2016-07-14 17:10:10.488924: step 4750, loss = 323155.25 (184.2 examples/sec; 0.695 sec/batch)
2016-07-14 17:10:16.356991: step 4760, loss = 320580.34 (269.8 examples/sec; 0.474 sec/batch)
2016-07-14 17:10:21.156840: step 4770, loss = 318024.91 (283.5 examples/sec; 0.451 sec/batch)
2016-07-14 17:10:25.974071: step 4780, loss = 315490.75 (264.2 examples/sec; 0.485 sec/batch)
2016-07-14 17:10:30.699731: step 4790, loss = 312976.00 (282.8 examples/sec; 0.453 sec/batch)
2016-07-14 17:10:35.285768: step 4800, loss = 310482.41 (279.7 examples/sec; 0.458 sec/batch)
2016-07-14 17:10:40.484858: step 4810, loss = 308007.75 (281.6 examples/sec; 0.455 sec/batch)
2016-07-14 17:10:45.102457: step 4820, loss = 305552.88 (272.2 examples/sec; 0.470 sec/batch)
2016-07-14 17:10:50.867486: step 4830, loss = 303117.62 (199.3 examples/sec; 0.642 sec/batch)
2016-07-14 17:10:55.697516: step 4840, loss = 300701.94 (273.5 examples/sec; 0.468 sec/batch)
2016-07-14 17:11:00.434815: step 4850, loss = 298305.44 (272.7 examples/sec; 0.469 sec/batch)
2016-07-14 17:11:05.258457: step 4860, loss = 295928.16 (274.6 examples/sec; 0.466 sec/batch)
2016-07-14 17:11:10.069290: step 4870, loss = 293569.50 (266.8 examples/sec; 0.480 sec/batch)
2016-07-14 17:11:16.524619: step 4880, loss = 291230.03 (202.9 examples/sec; 0.631 sec/batch)
2016-07-14 17:11:22.052485: step 4890, loss = 288909.47 (260.7 examples/sec; 0.491 sec/batch)
2016-07-14 17:11:26.683043: step 4900, loss = 286606.44 (270.3 examples/sec; 0.474 sec/batch)
2016-07-14 17:11:32.062230: step 4910, loss = 284322.38 (273.2 examples/sec; 0.469 sec/batch)
2016-07-14 17:11:36.645852: step 4920, loss = 282056.34 (279.4 examples/sec; 0.458 sec/batch)
2016-07-14 17:11:41.274588: step 4930, loss = 279808.19 (259.9 examples/sec; 0.492 sec/batch)
2016-07-14 17:11:45.896090: step 4940, loss = 277578.75 (267.0 examples/sec; 0.479 sec/batch)
2016-07-14 17:11:50.441138: step 4950, loss = 275366.56 (283.7 examples/sec; 0.451 sec/batch)
2016-07-14 17:11:55.154193: step 4960, loss = 273171.78 (277.7 examples/sec; 0.461 sec/batch)
2016-07-14 17:11:59.792750: step 4970, loss = 270994.78 (276.5 examples/sec; 0.463 sec/batch)
2016-07-14 17:12:04.387639: step 4980, loss = 268835.09 (281.9 examples/sec; 0.454 sec/batch)
2016-07-14 17:12:09.042038: step 4990, loss = 266692.50 (275.2 examples/sec; 0.465 sec/batch)
2016-07-14 17:12:13.566145: step 5000, loss = 264566.94 (292.1 examples/sec; 0.438 sec/batch)
2016-07-14 17:12:19.176970: step 5010, loss = 262458.34 (276.1 examples/sec; 0.464 sec/batch)
2016-07-14 17:12:23.805136: step 5020, loss = 260366.69 (279.8 examples/sec; 0.457 sec/batch)
2016-07-14 17:12:28.395289: step 5030, loss = 258291.83 (270.5 examples/sec; 0.473 sec/batch)
2016-07-14 17:12:33.089362: step 5040, loss = 256233.33 (280.7 examples/sec; 0.456 sec/batch)
2016-07-14 17:12:37.734261: step 5050, loss = 254191.23 (275.5 examples/sec; 0.465 sec/batch)
2016-07-14 17:12:43.541844: step 5060, loss = 252165.17 (268.2 examples/sec; 0.477 sec/batch)
2016-07-14 17:12:48.891676: step 5070, loss = 250155.75 (205.8 examples/sec; 0.622 sec/batch)
2016-07-14 17:12:54.268604: step 5080, loss = 248162.00 (262.1 examples/sec; 0.488 sec/batch)
2016-07-14 17:12:59.065226: step 5090, loss = 246184.45 (244.1 examples/sec; 0.524 sec/batch)
2016-07-14 17:13:03.656486: step 5100, loss = 244222.28 (275.5 examples/sec; 0.465 sec/batch)
2016-07-14 17:13:08.811891: step 5110, loss = 242276.27 (275.4 examples/sec; 0.465 sec/batch)
2016-07-14 17:13:14.528190: step 5120, loss = 240345.33 (202.4 examples/sec; 0.632 sec/batch)
2016-07-14 17:13:19.684428: step 5130, loss = 238429.70 (206.0 examples/sec; 0.621 sec/batch)
2016-07-14 17:13:25.330875: step 5140, loss = 236529.50 (271.6 examples/sec; 0.471 sec/batch)
2016-07-14 17:13:30.064311: step 5150, loss = 234644.58 (254.9 examples/sec; 0.502 sec/batch)
2016-07-14 17:13:34.950372: step 5160, loss = 232774.19 (259.8 examples/sec; 0.493 sec/batch)
2016-07-14 17:13:39.681315: step 5170, loss = 230919.64 (271.4 examples/sec; 0.472 sec/batch)
2016-07-14 17:13:44.499722: step 5180, loss = 229078.84 (289.0 examples/sec; 0.443 sec/batch)
2016-07-14 17:13:49.137856: step 5190, loss = 227253.52 (278.5 examples/sec; 0.460 sec/batch)
2016-07-14 17:13:53.781185: step 5200, loss = 225442.39 (269.9 examples/sec; 0.474 sec/batch)
2016-07-14 17:14:00.088959: step 5210, loss = 223645.25 (263.7 examples/sec; 0.485 sec/batch)
2016-07-14 17:14:05.614441: step 5220, loss = 221863.44 (203.1 examples/sec; 0.630 sec/batch)
2016-07-14 17:14:10.809115: step 5230, loss = 220094.86 (257.6 examples/sec; 0.497 sec/batch)
2016-07-14 17:14:15.548224: step 5240, loss = 218340.98 (262.8 examples/sec; 0.487 sec/batch)
2016-07-14 17:14:20.261201: step 5250, loss = 216600.89 (262.5 examples/sec; 0.488 sec/batch)
2016-07-14 17:14:24.945237: step 5260, loss = 214874.69 (271.0 examples/sec; 0.472 sec/batch)
2016-07-14 17:14:31.034087: step 5270, loss = 213162.11 (192.3 examples/sec; 0.666 sec/batch)
2016-07-14 17:14:36.834920: step 5280, loss = 211463.36 (274.1 examples/sec; 0.467 sec/batch)
2016-07-14 17:14:42.487071: step 5290, loss = 209777.88 (205.6 examples/sec; 0.623 sec/batch)
2016-07-14 17:14:47.445747: step 5300, loss = 208106.22 (280.8 examples/sec; 0.456 sec/batch)
2016-07-14 17:14:52.779430: step 5310, loss = 206447.67 (246.8 examples/sec; 0.519 sec/batch)
2016-07-14 17:14:57.533881: step 5320, loss = 204802.69 (265.7 examples/sec; 0.482 sec/batch)
2016-07-14 17:15:02.114287: step 5330, loss = 203169.83 (272.8 examples/sec; 0.469 sec/batch)
2016-07-14 17:15:06.747944: step 5340, loss = 201550.80 (257.2 examples/sec; 0.498 sec/batch)
2016-07-14 17:15:12.522459: step 5350, loss = 199944.66 (266.9 examples/sec; 0.480 sec/batch)
2016-07-14 17:15:17.240360: step 5360, loss = 198351.42 (280.0 examples/sec; 0.457 sec/batch)
2016-07-14 17:15:22.002419: step 5370, loss = 196770.30 (264.7 examples/sec; 0.484 sec/batch)
2016-07-14 17:15:26.719152: step 5380, loss = 195202.11 (267.7 examples/sec; 0.478 sec/batch)
2016-07-14 17:15:31.572793: step 5390, loss = 193646.16 (270.9 examples/sec; 0.473 sec/batch)
2016-07-14 17:15:36.249733: step 5400, loss = 192103.67 (267.1 examples/sec; 0.479 sec/batch)
2016-07-14 17:15:42.851151: step 5410, loss = 190572.55 (189.7 examples/sec; 0.675 sec/batch)
2016-07-14 17:15:48.724307: step 5420, loss = 189053.44 (258.5 examples/sec; 0.495 sec/batch)
2016-07-14 17:15:54.281066: step 5430, loss = 187547.02 (198.6 examples/sec; 0.645 sec/batch)
2016-07-14 17:15:59.366967: step 5440, loss = 186052.08 (276.9 examples/sec; 0.462 sec/batch)
2016-07-14 17:16:04.028634: step 5450, loss = 184569.66 (271.1 examples/sec; 0.472 sec/batch)
2016-07-14 17:16:08.753010: step 5460, loss = 183098.70 (283.6 examples/sec; 0.451 sec/batch)
2016-07-14 17:16:13.471700: step 5470, loss = 181639.00 (266.0 examples/sec; 0.481 sec/batch)
2016-07-14 17:16:18.154447: step 5480, loss = 180191.89 (280.4 examples/sec; 0.457 sec/batch)
2016-07-14 17:16:22.936909: step 5490, loss = 178755.56 (257.3 examples/sec; 0.497 sec/batch)
2016-07-14 17:16:27.565986: step 5500, loss = 177330.98 (260.7 examples/sec; 0.491 sec/batch)
2016-07-14 17:16:32.875803: step 5510, loss = 175917.61 (270.8 examples/sec; 0.473 sec/batch)
2016-07-14 17:16:37.541860: step 5520, loss = 174515.66 (271.8 examples/sec; 0.471 sec/batch)
2016-07-14 17:16:42.225625: step 5530, loss = 173124.89 (254.8 examples/sec; 0.502 sec/batch)
2016-07-14 17:16:47.036148: step 5540, loss = 171745.11 (256.3 examples/sec; 0.499 sec/batch)
2016-07-14 17:16:51.773527: step 5550, loss = 170376.03 (268.6 examples/sec; 0.477 sec/batch)
2016-07-14 17:16:56.514359: step 5560, loss = 169018.47 (282.3 examples/sec; 0.453 sec/batch)
2016-07-14 17:17:01.206987: step 5570, loss = 167671.81 (275.7 examples/sec; 0.464 sec/batch)
2016-07-14 17:17:05.870756: step 5580, loss = 166335.17 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 17:17:10.475362: step 5590, loss = 165009.80 (280.4 examples/sec; 0.457 sec/batch)
2016-07-14 17:17:16.224636: step 5600, loss = 163694.66 (262.5 examples/sec; 0.488 sec/batch)
2016-07-14 17:17:22.384448: step 5610, loss = 162389.94 (209.5 examples/sec; 0.611 sec/batch)
2016-07-14 17:17:27.323154: step 5620, loss = 161095.69 (277.1 examples/sec; 0.462 sec/batch)
2016-07-14 17:17:31.973375: step 5630, loss = 159811.78 (266.2 examples/sec; 0.481 sec/batch)
2016-07-14 17:17:36.741534: step 5640, loss = 158538.91 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 17:17:41.338050: step 5650, loss = 157275.00 (274.9 examples/sec; 0.466 sec/batch)
2016-07-14 17:17:46.378867: step 5660, loss = 156021.52 (245.0 examples/sec; 0.523 sec/batch)
2016-07-14 17:17:52.530671: step 5670, loss = 154778.25 (234.9 examples/sec; 0.545 sec/batch)
2016-07-14 17:17:57.757926: step 5680, loss = 153544.70 (247.2 examples/sec; 0.518 sec/batch)
2016-07-14 17:18:03.027379: step 5690, loss = 152321.09 (238.3 examples/sec; 0.537 sec/batch)
2016-07-14 17:18:08.171662: step 5700, loss = 151107.02 (249.0 examples/sec; 0.514 sec/batch)
2016-07-14 17:18:13.963910: step 5710, loss = 149902.48 (233.3 examples/sec; 0.549 sec/batch)
2016-07-14 17:18:19.117646: step 5720, loss = 148707.83 (249.9 examples/sec; 0.512 sec/batch)
2016-07-14 17:18:24.268928: step 5730, loss = 147523.12 (241.2 examples/sec; 0.531 sec/batch)
2016-07-14 17:18:29.461816: step 5740, loss = 146347.41 (247.1 examples/sec; 0.518 sec/batch)
2016-07-14 17:18:34.738501: step 5750, loss = 145181.11 (223.2 examples/sec; 0.573 sec/batch)
2016-07-14 17:18:40.013397: step 5760, loss = 144023.80 (183.4 examples/sec; 0.698 sec/batch)
2016-07-14 17:18:45.945372: step 5770, loss = 142875.81 (230.1 examples/sec; 0.556 sec/batch)
2016-07-14 17:18:51.250049: step 5780, loss = 141737.20 (247.7 examples/sec; 0.517 sec/batch)
2016-07-14 17:18:56.530975: step 5790, loss = 140607.59 (244.7 examples/sec; 0.523 sec/batch)
2016-07-14 17:19:01.548606: step 5800, loss = 139487.20 (231.0 examples/sec; 0.554 sec/batch)
2016-07-14 17:19:07.239606: step 5810, loss = 138375.45 (270.9 examples/sec; 0.473 sec/batch)
2016-07-14 17:19:12.555123: step 5820, loss = 137272.66 (266.7 examples/sec; 0.480 sec/batch)
2016-07-14 17:19:17.546868: step 5830, loss = 136178.77 (245.9 examples/sec; 0.521 sec/batch)
2016-07-14 17:19:22.750915: step 5840, loss = 135093.48 (226.8 examples/sec; 0.564 sec/batch)
2016-07-14 17:19:27.905028: step 5850, loss = 134016.62 (245.8 examples/sec; 0.521 sec/batch)
2016-07-14 17:19:33.112740: step 5860, loss = 132948.73 (239.4 examples/sec; 0.535 sec/batch)
2016-07-14 17:19:38.311743: step 5870, loss = 131889.39 (269.1 examples/sec; 0.476 sec/batch)
2016-07-14 17:19:43.541651: step 5880, loss = 130838.02 (254.1 examples/sec; 0.504 sec/batch)
2016-07-14 17:19:48.683484: step 5890, loss = 129795.47 (246.0 examples/sec; 0.520 sec/batch)
2016-07-14 17:19:53.808233: step 5900, loss = 128760.73 (250.6 examples/sec; 0.511 sec/batch)
2016-07-14 17:19:59.558574: step 5910, loss = 127735.12 (261.2 examples/sec; 0.490 sec/batch)
2016-07-14 17:20:04.877507: step 5920, loss = 126716.62 (249.5 examples/sec; 0.513 sec/batch)
2016-07-14 17:20:09.987545: step 5930, loss = 125707.01 (242.8 examples/sec; 0.527 sec/batch)
2016-07-14 17:20:15.110310: step 5940, loss = 124705.23 (252.4 examples/sec; 0.507 sec/batch)
2016-07-14 17:20:20.333098: step 5950, loss = 123711.20 (203.3 examples/sec; 0.630 sec/batch)
2016-07-14 17:20:25.441069: step 5960, loss = 122725.33 (236.9 examples/sec; 0.540 sec/batch)
2016-07-14 17:20:31.538950: step 5970, loss = 121747.01 (210.7 examples/sec; 0.608 sec/batch)
2016-07-14 17:20:36.801565: step 5980, loss = 120776.87 (246.8 examples/sec; 0.519 sec/batch)
2016-07-14 17:20:41.988537: step 5990, loss = 119814.65 (238.8 examples/sec; 0.536 sec/batch)
2016-07-14 17:20:47.127586: step 6000, loss = 118859.42 (251.3 examples/sec; 0.509 sec/batch)
2016-07-14 17:20:53.283594: step 6010, loss = 117912.60 (258.4 examples/sec; 0.495 sec/batch)
2016-07-14 17:20:58.464480: step 6020, loss = 116972.55 (250.1 examples/sec; 0.512 sec/batch)
2016-07-14 17:21:03.719870: step 6030, loss = 116040.33 (257.1 examples/sec; 0.498 sec/batch)
2016-07-14 17:21:08.346305: step 6040, loss = 115115.59 (272.8 examples/sec; 0.469 sec/batch)
2016-07-14 17:21:12.718059: step 6050, loss = 114198.34 (272.0 examples/sec; 0.471 sec/batch)
2016-07-14 17:21:17.138653: step 6060, loss = 113288.56 (292.5 examples/sec; 0.438 sec/batch)
2016-07-14 17:21:21.519906: step 6070, loss = 112385.17 (309.0 examples/sec; 0.414 sec/batch)
2016-07-14 17:21:25.873591: step 6080, loss = 111489.77 (305.0 examples/sec; 0.420 sec/batch)
2016-07-14 17:21:30.433638: step 6090, loss = 110601.14 (316.6 examples/sec; 0.404 sec/batch)
2016-07-14 17:21:34.976593: step 6100, loss = 109719.63 (285.8 examples/sec; 0.448 sec/batch)
2016-07-14 17:21:41.162425: step 6110, loss = 108845.41 (149.2 examples/sec; 0.858 sec/batch)
2016-07-14 17:21:45.562643: step 6120, loss = 107977.71 (284.9 examples/sec; 0.449 sec/batch)
2016-07-14 17:21:50.052955: step 6130, loss = 107117.34 (294.6 examples/sec; 0.435 sec/batch)
2016-07-14 17:21:54.716309: step 6140, loss = 106263.38 (280.3 examples/sec; 0.457 sec/batch)
2016-07-14 17:21:59.030965: step 6150, loss = 105416.43 (313.2 examples/sec; 0.409 sec/batch)
2016-07-14 17:22:03.601969: step 6160, loss = 104576.36 (281.5 examples/sec; 0.455 sec/batch)
2016-07-14 17:22:08.013736: step 6170, loss = 103743.02 (280.1 examples/sec; 0.457 sec/batch)
2016-07-14 17:22:13.363074: step 6180, loss = 102916.34 (218.3 examples/sec; 0.586 sec/batch)
Traceback (most recent call last):


I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)
2016-07-14 18:04:39.704628: step 0, loss = 118669.47 (9.5 examples/sec; 13.539 sec/batch)
2016-07-14 18:04:45.405397: step 10, loss = 117723.57 (277.8 examples/sec; 0.461 sec/batch)
2016-07-14 18:04:50.030425: step 20, loss = 116785.70 (279.8 examples/sec; 0.457 sec/batch)
2016-07-14 18:04:54.664585: step 30, loss = 115854.89 (269.9 examples/sec; 0.474 sec/batch)
2016-07-14 18:04:59.308881: step 40, loss = 114931.58 (279.0 examples/sec; 0.459 sec/batch)
2016-07-14 18:05:03.942733: step 50, loss = 114015.55 (277.6 examples/sec; 0.461 sec/batch)
2016-07-14 18:05:08.526681: step 60, loss = 113106.94 (282.0 examples/sec; 0.454 sec/batch)
2016-07-14 18:05:13.151616: step 70, loss = 112205.46 (276.0 examples/sec; 0.464 sec/batch)
2016-07-14 18:05:17.733045: step 80, loss = 111311.48 (280.2 examples/sec; 0.457 sec/batch)
2016-07-14 18:05:22.353681: step 90, loss = 110424.20 (273.1 examples/sec; 0.469 sec/batch)
2016-07-14 18:05:26.963658: step 100, loss = 109544.63 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 18:05:32.143534: step 110, loss = 108671.14 (279.0 examples/sec; 0.459 sec/batch)
2016-07-14 18:05:36.826843: step 120, loss = 107805.21 (272.0 examples/sec; 0.471 sec/batch)
2016-07-14 18:05:41.472163: step 130, loss = 106945.93 (278.4 examples/sec; 0.460 sec/batch)
2016-07-14 18:05:46.120558: step 140, loss = 106093.61 (279.0 examples/sec; 0.459 sec/batch)
2016-07-14 18:05:50.725267: step 150, loss = 105248.30 (273.5 examples/sec; 0.468 sec/batch)
2016-07-14 18:05:55.367824: step 160, loss = 104409.46 (276.7 examples/sec; 0.463 sec/batch)
2016-07-14 18:06:00.007941: step 170, loss = 103577.08 (276.8 examples/sec; 0.462 sec/batch)
2016-07-14 18:06:04.647929: step 180, loss = 102751.70 (279.7 examples/sec; 0.458 sec/batch)
2016-07-14 18:06:09.322904: step 190, loss = 101932.70 (240.8 examples/sec; 0.532 sec/batch)
2016-07-14 18:06:14.936157: step 200, loss = 101120.41 (277.4 examples/sec; 0.461 sec/batch)
2016-07-14 18:06:20.087794: step 210, loss = 100315.01 (277.1 examples/sec; 0.462 sec/batch)
2016-07-14 18:06:24.724317: step 220, loss = 99515.27 (276.4 examples/sec; 0.463 sec/batch)
2016-07-14 18:06:29.403585: step 230, loss = 98722.06 (270.4 examples/sec; 0.473 sec/batch)
2016-07-14 18:06:34.006039: step 240, loss = 97935.55 (272.0 examples/sec; 0.471 sec/batch)
2016-07-14 18:06:38.590725: step 250, loss = 97154.98 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 18:06:43.239459: step 260, loss = 96380.81 (279.3 examples/sec; 0.458 sec/batch)
2016-07-14 18:06:47.870192: step 270, loss = 95612.70 (282.7 examples/sec; 0.453 sec/batch)
2016-07-14 18:06:52.524823: step 280, loss = 94850.65 (277.3 examples/sec; 0.462 sec/batch)
2016-07-14 18:06:57.169052: step 290, loss = 94094.92 (274.7 examples/sec; 0.466 sec/batch)
2016-07-14 18:07:01.867675: step 300, loss = 93344.85 (278.7 examples/sec; 0.459 sec/batch)
2016-07-14 18:07:06.995161: step 310, loss = 92600.72 (277.2 examples/sec; 0.462 sec/batch)
2016-07-14 18:07:12.519962: step 320, loss = 91862.66 (205.8 examples/sec; 0.622 sec/batch)
2016-07-14 18:07:17.511676: step 330, loss = 91130.76 (276.4 examples/sec; 0.463 sec/batch)
2016-07-14 18:07:22.183269: step 340, loss = 90404.36 (273.9 examples/sec; 0.467 sec/batch)
2016-07-14 18:07:26.790280: step 350, loss = 89683.84 (276.4 examples/sec; 0.463 sec/batch)
2016-07-14 18:07:31.414222: step 360, loss = 88968.91 (268.3 examples/sec; 0.477 sec/batch)
2016-07-14 18:07:36.039474: step 370, loss = 88260.38 (283.6 examples/sec; 0.451 sec/batch)
2016-07-14 18:07:40.666013: step 380, loss = 87557.29 (275.9 examples/sec; 0.464 sec/batch)
2016-07-14 18:07:45.304723: step 390, loss = 86858.86 (276.2 examples/sec; 0.463 sec/batch)
2016-07-14 18:07:49.975913: step 400, loss = 86166.86 (281.9 examples/sec; 0.454 sec/batch)
2016-07-14 18:07:55.166312: step 410, loss = 85479.93 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 18:07:59.858462: step 420, loss = 84798.66 (273.8 examples/sec; 0.467 sec/batch)
2016-07-14 18:08:04.524699: step 430, loss = 84123.45 (284.6 examples/sec; 0.450 sec/batch)
2016-07-14 18:08:09.183874: step 440, loss = 83452.33 (279.6 examples/sec; 0.458 sec/batch)
2016-07-14 18:08:14.113697: step 450, loss = 82787.34 (207.0 examples/sec; 0.618 sec/batch)
2016-07-14 18:08:19.688192: step 460, loss = 82127.62 (263.3 examples/sec; 0.486 sec/batch)
2016-07-14 18:08:24.409260: step 470, loss = 81473.22 (275.4 examples/sec; 0.465 sec/batch)
2016-07-14 18:08:29.271249: step 480, loss = 80824.14 (269.4 examples/sec; 0.475 sec/batch)
2016-07-14 18:08:33.960816: step 490, loss = 80179.86 (273.8 examples/sec; 0.467 sec/batch)
2016-07-14 18:08:38.586377: step 500, loss = 79541.23 (273.7 examples/sec; 0.468 sec/batch)
2016-07-14 18:08:43.767896: step 510, loss = 78907.10 (277.2 examples/sec; 0.462 sec/batch)
2016-07-14 18:08:48.455840: step 520, loss = 78278.38 (285.7 examples/sec; 0.448 sec/batch)
2016-07-14 18:08:53.097013: step 530, loss = 77654.42 (260.9 examples/sec; 0.491 sec/batch)
2016-07-14 18:08:57.712482: step 540, loss = 77035.28 (279.2 examples/sec; 0.458 sec/batch)
2016-07-14 18:09:02.764075: step 550, loss = 76421.38 (207.9 examples/sec; 0.616 sec/batch)
2016-07-14 18:09:08.224864: step 560, loss = 75812.53 (261.6 examples/sec; 0.489 sec/batch)
2016-07-14 18:09:12.927861: step 570, loss = 75207.83 (271.8 examples/sec; 0.471 sec/batch)
2016-07-14 18:09:17.789246: step 580, loss = 74608.95 (274.1 examples/sec; 0.467 sec/batch)
2016-07-14 18:09:22.560106: step 590, loss = 74013.97 (259.9 examples/sec; 0.492 sec/batch)
2016-07-14 18:09:27.343730: step 600, loss = 73424.45 (274.8 examples/sec; 0.466 sec/batch)
2016-07-14 18:09:32.709546: step 610, loss = 72839.12 (267.7 examples/sec; 0.478 sec/batch)
2016-07-14 18:09:37.396163: step 620, loss = 72259.27 (276.4 examples/sec; 0.463 sec/batch)
2016-07-14 18:09:42.048787: step 630, loss = 71682.77 (271.2 examples/sec; 0.472 sec/batch)
2016-07-14 18:09:46.716210: step 640, loss = 71111.66 (266.0 examples/sec; 0.481 sec/batch)
2016-07-14 18:09:51.376338: step 650, loss = 70545.16 (283.8 examples/sec; 0.451 sec/batch)
2016-07-14 18:09:56.064875: step 660, loss = 69982.66 (259.1 examples/sec; 0.494 sec/batch)
2016-07-14 18:10:00.894060: step 670, loss = 69425.05 (269.7 examples/sec; 0.475 sec/batch)
2016-07-14 18:10:06.893341: step 680, loss = 68871.49 (250.0 examples/sec; 0.512 sec/batch)
2016-07-14 18:10:11.657028: step 690, loss = 68322.98 (267.3 examples/sec; 0.479 sec/batch)
2016-07-14 18:10:16.339013: step 700, loss = 67777.88 (277.1 examples/sec; 0.462 sec/batch)
2016-07-14 18:10:21.592315: step 710, loss = 67238.26 (276.8 examples/sec; 0.462 sec/batch)
2016-07-14 18:10:26.226389: step 720, loss = 66702.30 (271.9 examples/sec; 0.471 sec/batch)
2016-07-14 18:10:31.452703: step 730, loss = 66170.81 (203.9 examples/sec; 0.628 sec/batch)
2016-07-14 18:10:36.796134: step 740, loss = 65643.57 (269.5 examples/sec; 0.475 sec/batch)
2016-07-14 18:10:41.491610: step 750, loss = 65120.23 (262.4 examples/sec; 0.488 sec/batch)
2016-07-14 18:10:46.341750: step 760, loss = 64601.01 (271.5 examples/sec; 0.471 sec/batch)
2016-07-14 18:10:51.038691: step 770, loss = 64086.68 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 18:10:55.708388: step 780, loss = 63575.71 (265.5 examples/sec; 0.482 sec/batch)
2016-07-14 18:11:01.465619: step 790, loss = 63068.93 (207.4 examples/sec; 0.617 sec/batch)
2016-07-14 18:11:06.342311: step 800, loss = 62566.58 (272.3 examples/sec; 0.470 sec/batch)
2016-07-14 18:11:11.547625: step 810, loss = 62067.64 (276.0 examples/sec; 0.464 sec/batch)
2016-07-14 18:11:16.237872: step 820, loss = 61573.09 (272.4 examples/sec; 0.470 sec/batch)
2016-07-14 18:11:20.862703: step 830, loss = 61082.55 (280.8 examples/sec; 0.456 sec/batch)
2016-07-14 18:11:25.622651: step 840, loss = 60595.46 (233.8 examples/sec; 0.548 sec/batch)
2016-07-14 18:11:31.368179: step 850, loss = 60112.95 (256.6 examples/sec; 0.499 sec/batch)
2016-07-14 18:11:36.214061: step 860, loss = 59634.00 (276.9 examples/sec; 0.462 sec/batch)
2016-07-14 18:11:41.127251: step 870, loss = 59158.38 (253.0 examples/sec; 0.506 sec/batch)
2016-07-14 18:11:45.796328: step 880, loss = 58686.78 (267.8 examples/sec; 0.478 sec/batch)
2016-07-14 18:11:50.435214: step 890, loss = 58219.36 (268.2 examples/sec; 0.477 sec/batch)
2016-07-14 18:11:55.103301: step 900, loss = 57755.47 (272.2 examples/sec; 0.470 sec/batch)
2016-07-14 18:12:00.344496: step 910, loss = 57295.11 (271.5 examples/sec; 0.472 sec/batch)
2016-07-14 18:12:05.004356: step 920, loss = 56838.58 (267.9 examples/sec; 0.478 sec/batch)
2016-07-14 18:12:09.703033: step 930, loss = 56385.61 (271.0 examples/sec; 0.472 sec/batch)
2016-07-14 18:12:14.358614: step 940, loss = 55936.09 (270.4 examples/sec; 0.473 sec/batch)
2016-07-14 18:12:19.600351: step 950, loss = 55490.27 (202.1 examples/sec; 0.633 sec/batch)
2016-07-14 18:12:24.921951: step 960, loss = 55048.13 (258.3 examples/sec; 0.496 sec/batch)
2016-07-14 18:12:29.669807: step 970, loss = 54609.38 (271.0 examples/sec; 0.472 sec/batch)
2016-07-14 18:12:34.354952: step 980, loss = 54174.07 (266.5 examples/sec; 0.480 sec/batch)
2016-07-14 18:12:39.070604: step 990, loss = 53742.72 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 18:12:43.682775: step 1000, loss = 53314.23 (273.1 examples/sec; 0.469 sec/batch)
2016-07-14 18:12:49.305287: step 1010, loss = 52889.54 (275.3 examples/sec; 0.465 sec/batch)
2016-07-14 18:12:53.985997: step 1020, loss = 52467.90 (262.2 examples/sec; 0.488 sec/batch)
2016-07-14 18:12:58.644992: step 1030, loss = 52049.44 (280.3 examples/sec; 0.457 sec/batch)
2016-07-14 18:13:03.341780: step 1040, loss = 51635.04 (272.4 examples/sec; 0.470 sec/batch)
2016-07-14 18:13:08.922147: step 1050, loss = 51223.43 (201.5 examples/sec; 0.635 sec/batch)
2016-07-14 18:13:14.081169: step 1060, loss = 50815.29 (208.1 examples/sec; 0.615 sec/batch)
2016-07-14 18:13:19.726468: step 1070, loss = 50409.86 (265.9 examples/sec; 0.481 sec/batch)
2016-07-14 18:13:24.512281: step 1080, loss = 50008.86 (272.3 examples/sec; 0.470 sec/batch)
2016-07-14 18:13:29.381564: step 1090, loss = 49609.87 (258.9 examples/sec; 0.494 sec/batch)
2016-07-14 18:13:34.089404: step 1100, loss = 49214.42 (259.2 examples/sec; 0.494 sec/batch)
2016-07-14 18:13:39.497618: step 1110, loss = 48822.26 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 18:13:44.109346: step 1120, loss = 48433.02 (271.9 examples/sec; 0.471 sec/batch)
2016-07-14 18:13:48.753143: step 1130, loss = 48047.41 (273.0 examples/sec; 0.469 sec/batch)
2016-07-14 18:13:54.514606: step 1140, loss = 47664.43 (257.8 examples/sec; 0.497 sec/batch)
2016-07-14 18:13:59.310564: step 1150, loss = 47283.83 (277.5 examples/sec; 0.461 sec/batch)
2016-07-14 18:14:04.135270: step 1160, loss = 46907.46 (259.5 examples/sec; 0.493 sec/batch)
2016-07-14 18:14:08.868208: step 1170, loss = 46533.93 (280.8 examples/sec; 0.456 sec/batch)
2016-07-14 18:14:13.511501: step 1180, loss = 46163.27 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 18:14:18.518255: step 1190, loss = 45795.14 (208.0 examples/sec; 0.615 sec/batch)
2016-07-14 18:14:24.090798: step 1200, loss = 45430.15 (264.2 examples/sec; 0.484 sec/batch)
2016-07-14 18:14:29.356257: step 1210, loss = 45067.95 (276.9 examples/sec; 0.462 sec/batch)
2016-07-14 18:14:34.018024: step 1220, loss = 44709.18 (269.6 examples/sec; 0.475 sec/batch)
2016-07-14 18:14:39.161897: step 1230, loss = 44352.84 (200.4 examples/sec; 0.639 sec/batch)
2016-07-14 18:14:44.612063: step 1240, loss = 43999.05 (263.3 examples/sec; 0.486 sec/batch)
2016-07-14 18:14:49.324390: step 1250, loss = 43648.56 (271.7 examples/sec; 0.471 sec/batch)
2016-07-14 18:14:54.174719: step 1260, loss = 43300.61 (278.5 examples/sec; 0.460 sec/batch)
2016-07-14 18:14:58.835945: step 1270, loss = 42955.73 (276.2 examples/sec; 0.463 sec/batch)
2016-07-14 18:15:03.491448: step 1280, loss = 42613.16 (275.7 examples/sec; 0.464 sec/batch)
2016-07-14 18:15:08.144749: step 1290, loss = 42273.45 (279.0 examples/sec; 0.459 sec/batch)
2016-07-14 18:15:12.812812: step 1300, loss = 41936.87 (273.8 examples/sec; 0.468 sec/batch)
2016-07-14 18:15:19.125411: step 1310, loss = 41602.71 (266.3 examples/sec; 0.481 sec/batch)
2016-07-14 18:15:23.991122: step 1320, loss = 41270.99 (257.8 examples/sec; 0.496 sec/batch)
2016-07-14 18:15:28.663691: step 1330, loss = 40942.27 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 18:15:33.333064: step 1340, loss = 40616.06 (268.8 examples/sec; 0.476 sec/batch)
2016-07-14 18:15:39.168037: step 1350, loss = 40292.11 (263.0 examples/sec; 0.487 sec/batch)
2016-07-14 18:15:44.759247: step 1360, loss = 39971.02 (202.2 examples/sec; 0.633 sec/batch)
2016-07-14 18:15:49.883682: step 1370, loss = 39652.34 (224.3 examples/sec; 0.571 sec/batch)
2016-07-14 18:15:55.646589: step 1380, loss = 39336.70 (258.9 examples/sec; 0.494 sec/batch)
2016-07-14 18:16:00.418981: step 1390, loss = 39022.70 (277.2 examples/sec; 0.462 sec/batch)
2016-07-14 18:16:05.243922: step 1400, loss = 38712.34 (266.9 examples/sec; 0.480 sec/batch)
2016-07-14 18:16:11.749605: step 1410, loss = 38403.32 (276.2 examples/sec; 0.463 sec/batch)
2016-07-14 18:16:16.457859: step 1420, loss = 38097.41 (278.7 examples/sec; 0.459 sec/batch)
2016-07-14 18:16:21.123730: step 1430, loss = 37793.52 (279.2 examples/sec; 0.458 sec/batch)
2016-07-14 18:16:25.798041: step 1440, loss = 37492.95 (270.8 examples/sec; 0.473 sec/batch)
2016-07-14 18:16:30.423779: step 1450, loss = 37193.69 (279.0 examples/sec; 0.459 sec/batch)
2016-07-14 18:16:35.039110: step 1460, loss = 36897.70 (274.2 examples/sec; 0.467 sec/batch)
2016-07-14 18:16:40.385106: step 1470, loss = 36603.70 (199.2 examples/sec; 0.643 sec/batch)
2016-07-14 18:16:45.580069: step 1480, loss = 36311.88 (262.1 examples/sec; 0.488 sec/batch)
2016-07-14 18:16:50.278572: step 1490, loss = 36022.11 (264.5 examples/sec; 0.484 sec/batch)
2016-07-14 18:16:55.157974: step 1500, loss = 35735.51 (271.8 examples/sec; 0.471 sec/batch)
2016-07-14 18:17:00.379942: step 1510, loss = 35450.86 (263.6 examples/sec; 0.486 sec/batch)
2016-07-14 18:17:05.075784: step 1520, loss = 35167.88 (273.5 examples/sec; 0.468 sec/batch)
2016-07-14 18:17:09.739150: step 1530, loss = 34887.66 (279.0 examples/sec; 0.459 sec/batch)
2016-07-14 18:17:14.437682: step 1540, loss = 34609.49 (278.9 examples/sec; 0.459 sec/batch)
2016-07-14 18:17:19.050828: step 1550, loss = 34333.43 (278.4 examples/sec; 0.460 sec/batch)
2016-07-14 18:17:24.453364: step 1560, loss = 34060.38 (201.7 examples/sec; 0.635 sec/batch)
2016-07-14 18:17:29.718236: step 1570, loss = 33788.66 (247.6 examples/sec; 0.517 sec/batch)
2016-07-14 18:17:35.477253: step 1580, loss = 33519.76 (261.8 examples/sec; 0.489 sec/batch)
2016-07-14 18:17:40.248776: step 1590, loss = 33253.04 (267.8 examples/sec; 0.478 sec/batch)
2016-07-14 18:17:44.884243: step 1600, loss = 32987.73 (275.9 examples/sec; 0.464 sec/batch)
2016-07-14 18:17:50.079178: step 1610, loss = 32724.34 (279.0 examples/sec; 0.459 sec/batch)
2016-07-14 18:17:54.743046: step 1620, loss = 32463.55 (267.3 examples/sec; 0.479 sec/batch)
2016-07-14 18:17:59.380453: step 1630, loss = 32204.96 (282.3 examples/sec; 0.453 sec/batch)
2016-07-14 18:18:04.031601: step 1640, loss = 31948.30 (270.3 examples/sec; 0.474 sec/batch)
2016-07-14 18:18:09.767290: step 1650, loss = 31693.95 (209.9 examples/sec; 0.610 sec/batch)
2016-07-14 18:18:14.626955: step 1660, loss = 31440.99 (277.9 examples/sec; 0.461 sec/batch)
2016-07-14 18:18:19.418481: step 1670, loss = 31190.39 (252.4 examples/sec; 0.507 sec/batch)
2016-07-14 18:18:24.184991: step 1680, loss = 30942.33 (271.6 examples/sec; 0.471 sec/batch)
2016-07-14 18:18:28.872961: step 1690, loss = 30695.50 (280.7 examples/sec; 0.456 sec/batch)
2016-07-14 18:18:33.567511: step 1700, loss = 30451.19 (273.4 examples/sec; 0.468 sec/batch)
2016-07-14 18:18:39.959812: step 1710, loss = 30208.49 (256.4 examples/sec; 0.499 sec/batch)
2016-07-14 18:18:44.714154: step 1720, loss = 29967.79 (266.9 examples/sec; 0.480 sec/batch)
2016-07-14 18:18:49.336097: step 1730, loss = 29728.72 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 18:18:53.988170: step 1740, loss = 29491.78 (280.6 examples/sec; 0.456 sec/batch)
2016-07-14 18:18:58.581157: step 1750, loss = 29256.42 (273.9 examples/sec; 0.467 sec/batch)
2016-07-14 18:19:03.202755: step 1760, loss = 29023.79 (268.7 examples/sec; 0.476 sec/batch)
2016-07-14 18:19:07.874393: step 1770, loss = 28792.20 (266.7 examples/sec; 0.480 sec/batch)
2016-07-14 18:19:12.454985: step 1780, loss = 28562.85 (286.4 examples/sec; 0.447 sec/batch)
2016-07-14 18:19:17.126174: step 1790, loss = 28335.62 (274.3 examples/sec; 0.467 sec/batch)
2016-07-14 18:19:22.932687: step 1800, loss = 28109.28 (249.1 examples/sec; 0.514 sec/batch)
2016-07-14 18:19:28.237493: step 1810, loss = 27885.27 (274.4 examples/sec; 0.466 sec/batch)
2016-07-14 18:19:32.835356: step 1820, loss = 27663.26 (279.2 examples/sec; 0.459 sec/batch)
2016-07-14 18:19:37.502294: step 1830, loss = 27442.81 (273.1 examples/sec; 0.469 sec/batch)
2016-07-14 18:19:43.262653: step 1840, loss = 27224.22 (264.7 examples/sec; 0.484 sec/batch)
2016-07-14 18:19:48.021196: step 1850, loss = 27007.38 (267.9 examples/sec; 0.478 sec/batch)
2016-07-14 18:19:52.820290: step 1860, loss = 26791.88 (252.9 examples/sec; 0.506 sec/batch)
2016-07-14 18:19:57.527905: step 1870, loss = 26578.34 (283.6 examples/sec; 0.451 sec/batch)
2016-07-14 18:20:02.409442: step 1880, loss = 26366.70 (271.8 examples/sec; 0.471 sec/batch)
2016-07-14 18:20:07.122399: step 1890, loss = 26156.54 (254.9 examples/sec; 0.502 sec/batch)
2016-07-14 18:20:11.908987: step 1900, loss = 25948.10 (277.9 examples/sec; 0.461 sec/batch)
2016-07-14 18:20:17.233768: step 1910, loss = 25741.46 (266.8 examples/sec; 0.480 sec/batch)
2016-07-14 18:20:21.920803: step 1920, loss = 25536.30 (273.1 examples/sec; 0.469 sec/batch)
2016-07-14 18:20:26.750596: step 1930, loss = 25332.57 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 18:20:31.508095: step 1940, loss = 25130.52 (264.4 examples/sec; 0.484 sec/batch)
2016-07-14 18:20:36.259981: step 1950, loss = 24930.61 (272.7 examples/sec; 0.469 sec/batch)
2016-07-14 18:20:40.885041: step 1960, loss = 24731.69 (284.7 examples/sec; 0.450 sec/batch)
2016-07-14 18:20:45.478166: step 1970, loss = 24534.85 (281.4 examples/sec; 0.455 sec/batch)
2016-07-14 18:20:50.167830: step 1980, loss = 24339.43 (274.2 examples/sec; 0.467 sec/batch)
2016-07-14 18:20:54.754680: step 1990, loss = 24145.04 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 18:20:59.958407: step 2000, loss = 23952.84 (205.5 examples/sec; 0.623 sec/batch)
2016-07-14 18:21:06.417170: step 2010, loss = 23762.15 (281.5 examples/sec; 0.455 sec/batch)
2016-07-14 18:21:11.140741: step 2020, loss = 23572.67 (260.1 examples/sec; 0.492 sec/batch)
2016-07-14 18:21:15.946083: step 2030, loss = 23384.72 (276.8 examples/sec; 0.462 sec/batch)
2016-07-14 18:21:20.558427: step 2040, loss = 23198.40 (283.9 examples/sec; 0.451 sec/batch)
2016-07-14 18:21:25.235947: step 2050, loss = 23013.50 (268.3 examples/sec; 0.477 sec/batch)
2016-07-14 18:21:29.892910: step 2060, loss = 22830.04 (280.9 examples/sec; 0.456 sec/batch)
2016-07-14 18:21:34.833914: step 2070, loss = 22648.17 (204.2 examples/sec; 0.627 sec/batch)
2016-07-14 18:21:40.355572: step 2080, loss = 22467.72 (264.3 examples/sec; 0.484 sec/batch)
2016-07-14 18:21:45.167220: step 2090, loss = 22288.71 (265.5 examples/sec; 0.482 sec/batch)
2016-07-14 18:21:49.992747: step 2100, loss = 22111.13 (266.2 examples/sec; 0.481 sec/batch)
2016-07-14 18:21:55.148818: step 2110, loss = 21934.68 (279.4 examples/sec; 0.458 sec/batch)
2016-07-14 18:21:59.809234: step 2120, loss = 21759.96 (277.3 examples/sec; 0.462 sec/batch)
2016-07-14 18:22:04.445074: step 2130, loss = 21586.15 (284.8 examples/sec; 0.449 sec/batch)
2016-07-14 18:22:09.037279: step 2140, loss = 21414.70 (275.5 examples/sec; 0.465 sec/batch)
2016-07-14 18:22:13.717571: step 2150, loss = 21244.16 (266.6 examples/sec; 0.480 sec/batch)
2016-07-14 18:22:18.402860: step 2160, loss = 21074.54 (260.0 examples/sec; 0.492 sec/batch)
2016-07-14 18:22:23.448676: step 2170, loss = 20906.63 (205.0 examples/sec; 0.624 sec/batch)
2016-07-14 18:22:28.869550: step 2180, loss = 20740.10 (263.0 examples/sec; 0.487 sec/batch)
2016-07-14 18:22:33.621958: step 2190, loss = 20574.94 (275.3 examples/sec; 0.465 sec/batch)
2016-07-14 18:22:38.635799: step 2200, loss = 20410.50 (191.9 examples/sec; 0.667 sec/batch)
2016-07-14 18:22:46.026307: step 2210, loss = 20248.14 (205.1 examples/sec; 0.624 sec/batch)
2016-07-14 18:22:50.842392: step 2220, loss = 20086.93 (274.2 examples/sec; 0.467 sec/batch)
2016-07-14 18:22:55.572687: step 2230, loss = 19926.93 (257.2 examples/sec; 0.498 sec/batch)
2016-07-14 18:23:01.457361: step 2240, loss = 19767.92 (189.9 examples/sec; 0.674 sec/batch)
2016-07-14 18:23:07.285820: step 2250, loss = 19610.39 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 18:23:11.920924: step 2260, loss = 19454.29 (276.1 examples/sec; 0.464 sec/batch)
2016-07-14 18:23:16.516028: step 2270, loss = 19299.14 (283.9 examples/sec; 0.451 sec/batch)
2016-07-14 18:23:21.092363: step 2280, loss = 19145.61 (271.7 examples/sec; 0.471 sec/batch)
2016-07-14 18:23:26.790063: step 2290, loss = 18993.02 (268.8 examples/sec; 0.476 sec/batch)
2016-07-14 18:23:31.585123: step 2300, loss = 18841.38 (272.9 examples/sec; 0.469 sec/batch)
2016-07-14 18:23:36.759564: step 2310, loss = 18691.16 (272.1 examples/sec; 0.470 sec/batch)
2016-07-14 18:23:41.423247: step 2320, loss = 18542.21 (272.4 examples/sec; 0.470 sec/batch)
2016-07-14 18:23:46.044039: step 2330, loss = 18394.60 (277.1 examples/sec; 0.462 sec/batch)
2016-07-14 18:23:50.965147: step 2340, loss = 18247.86 (205.8 examples/sec; 0.622 sec/batch)
2016-07-14 18:23:56.534637: step 2350, loss = 18102.52 (258.6 examples/sec; 0.495 sec/batch)
2016-07-14 18:24:01.267646: step 2360, loss = 17958.26 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 18:24:06.117980: step 2370, loss = 17815.26 (272.5 examples/sec; 0.470 sec/batch)
2016-07-14 18:24:10.757719: step 2380, loss = 17673.19 (276.3 examples/sec; 0.463 sec/batch)
2016-07-14 18:24:15.358333: step 2390, loss = 17532.25 (274.8 examples/sec; 0.466 sec/batch)
2016-07-14 18:24:20.012747: step 2400, loss = 17392.64 (268.4 examples/sec; 0.477 sec/batch)
2016-07-14 18:24:25.212544: step 2410, loss = 17253.71 (275.5 examples/sec; 0.465 sec/batch)
2016-07-14 18:24:29.884507: step 2420, loss = 17116.62 (283.0 examples/sec; 0.452 sec/batch)
2016-07-14 18:24:34.478123: step 2430, loss = 16980.19 (284.5 examples/sec; 0.450 sec/batch)
2016-07-14 18:24:39.098908: step 2440, loss = 16844.76 (268.4 examples/sec; 0.477 sec/batch)
2016-07-14 18:24:43.717899: step 2450, loss = 16710.79 (267.1 examples/sec; 0.479 sec/batch)
2016-07-14 18:24:48.316564: step 2460, loss = 16577.59 (279.7 examples/sec; 0.458 sec/batch)
2016-07-14 18:24:52.984839: step 2470, loss = 16445.73 (274.9 examples/sec; 0.466 sec/batch)
2016-07-14 18:24:58.757953: step 2480, loss = 16314.14 (255.1 examples/sec; 0.502 sec/batch)
2016-07-14 18:25:03.554107: step 2490, loss = 16184.28 (275.8 examples/sec; 0.464 sec/batch)
2016-07-14 18:25:08.171675: step 2500, loss = 16055.57 (275.9 examples/sec; 0.464 sec/batch)
2016-07-14 18:25:13.316241: step 2510, loss = 15927.52 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 18:25:17.941125: step 2520, loss = 15800.28 (273.5 examples/sec; 0.468 sec/batch)
2016-07-14 18:25:22.549827: step 2530, loss = 15674.66 (271.2 examples/sec; 0.472 sec/batch)
2016-07-14 18:25:27.588611: step 2540, loss = 15549.82 (203.3 examples/sec; 0.630 sec/batch)
2016-07-14 18:25:33.031230: step 2550, loss = 15425.69 (262.3 examples/sec; 0.488 sec/batch)
2016-07-14 18:25:37.730615: step 2560, loss = 15302.63 (278.8 examples/sec; 0.459 sec/batch)
2016-07-14 18:25:42.822982: step 2570, loss = 15180.93 (189.6 examples/sec; 0.675 sec/batch)
2016-07-14 18:25:49.464313: step 2580, loss = 15059.99 (197.0 examples/sec; 0.650 sec/batch)
2016-07-14 18:25:54.430582: step 2590, loss = 14939.91 (275.0 examples/sec; 0.465 sec/batch)
2016-07-14 18:25:59.034292: step 2600, loss = 14821.08 (285.0 examples/sec; 0.449 sec/batch)
2016-07-14 18:26:04.184558: step 2610, loss = 14703.12 (274.7 examples/sec; 0.466 sec/batch)
2016-07-14 18:26:08.821411: step 2620, loss = 14585.53 (285.5 examples/sec; 0.448 sec/batch)
2016-07-14 18:26:13.486407: step 2630, loss = 14469.52 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 18:26:19.262795: step 2640, loss = 14354.14 (254.4 examples/sec; 0.503 sec/batch)
2016-07-14 18:26:24.695282: step 2650, loss = 14239.73 (201.9 examples/sec; 0.634 sec/batch)
2016-07-14 18:26:29.851300: step 2660, loss = 14125.86 (268.0 examples/sec; 0.478 sec/batch)
2016-07-14 18:26:34.475156: step 2670, loss = 14013.62 (268.5 examples/sec; 0.477 sec/batch)
2016-07-14 18:26:39.300213: step 2680, loss = 13901.91 (276.3 examples/sec; 0.463 sec/batch)
2016-07-14 18:26:43.912641: step 2690, loss = 13791.36 (270.8 examples/sec; 0.473 sec/batch)
2016-07-14 18:26:48.576368: step 2700, loss = 13681.41 (269.0 examples/sec; 0.476 sec/batch)
2016-07-14 18:26:53.714264: step 2710, loss = 13572.10 (275.1 examples/sec; 0.465 sec/batch)
2016-07-14 18:26:58.394585: step 2720, loss = 13464.13 (245.7 examples/sec; 0.521 sec/batch)
2016-07-14 18:27:04.092783: step 2730, loss = 13357.13 (260.0 examples/sec; 0.492 sec/batch)
2016-07-14 18:27:08.809670: step 2740, loss = 13250.44 (276.6 examples/sec; 0.463 sec/batch)
2016-07-14 18:27:13.616113: step 2750, loss = 13144.93 (272.0 examples/sec; 0.471 sec/batch)
2016-07-14 18:27:18.286708: step 2760, loss = 13040.05 (280.9 examples/sec; 0.456 sec/batch)
2016-07-14 18:27:22.921160: step 2770, loss = 12935.95 (280.9 examples/sec; 0.456 sec/batch)
2016-07-14 18:27:27.582706: step 2780, loss = 12833.38 (267.7 examples/sec; 0.478 sec/batch)
2016-07-14 18:27:32.217790: step 2790, loss = 12730.92 (270.5 examples/sec; 0.473 sec/batch)
2016-07-14 18:27:36.858849: step 2800, loss = 12629.73 (279.3 examples/sec; 0.458 sec/batch)
2016-07-14 18:27:42.018409: step 2810, loss = 12528.71 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 18:27:46.647971: step 2820, loss = 12429.46 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 18:27:51.201730: step 2830, loss = 12329.84 (280.6 examples/sec; 0.456 sec/batch)
2016-07-14 18:27:55.842082: step 2840, loss = 12231.47 (275.4 examples/sec; 0.465 sec/batch)
2016-07-14 18:28:00.461366: step 2850, loss = 12133.97 (276.3 examples/sec; 0.463 sec/batch)
2016-07-14 18:28:05.127221: step 2860, loss = 12037.61 (277.1 examples/sec; 0.462 sec/batch)
2016-07-14 18:28:10.842403: step 2870, loss = 11941.86 (269.7 examples/sec; 0.475 sec/batch)
2016-07-14 18:28:15.650592: step 2880, loss = 11846.21 (277.9 examples/sec; 0.461 sec/batch)
2016-07-14 18:28:20.374358: step 2890, loss = 11752.16 (262.4 examples/sec; 0.488 sec/batch)
2016-07-14 18:28:25.154954: step 2900, loss = 11658.65 (274.1 examples/sec; 0.467 sec/batch)
2016-07-14 18:28:30.523225: step 2910, loss = 11565.51 (267.6 examples/sec; 0.478 sec/batch)
2016-07-14 18:28:35.237454: step 2920, loss = 11473.41 (265.3 examples/sec; 0.482 sec/batch)
2016-07-14 18:28:40.028090: step 2930, loss = 11381.90 (272.7 examples/sec; 0.469 sec/batch)
2016-07-14 18:28:44.795719: step 2940, loss = 11291.16 (264.3 examples/sec; 0.484 sec/batch)
2016-07-14 18:28:49.492747: step 2950, loss = 11201.26 (281.0 examples/sec; 0.455 sec/batch)
2016-07-14 18:28:54.245611: step 2960, loss = 11111.83 (272.2 examples/sec; 0.470 sec/batch)
2016-07-14 18:28:58.946976: step 2970, loss = 11023.33 (257.1 examples/sec; 0.498 sec/batch)
2016-07-14 18:29:03.741229: step 2980, loss = 10935.63 (275.9 examples/sec; 0.464 sec/batch)
2016-07-14 18:29:08.468693: step 2990, loss = 10848.42 (266.1 examples/sec; 0.481 sec/batch)
2016-07-14 18:29:13.200716: step 3000, loss = 10762.37 (268.8 examples/sec; 0.476 sec/batch)
2016-07-14 18:29:18.840424: step 3010, loss = 10676.07 (273.5 examples/sec; 0.468 sec/batch)
2016-07-14 18:29:23.500619: step 3020, loss = 10591.22 (281.3 examples/sec; 0.455 sec/batch)
2016-07-14 18:29:28.074510: step 3030, loss = 10506.62 (282.5 examples/sec; 0.453 sec/batch)
2016-07-14 18:29:32.707597: step 3040, loss = 10423.10 (272.8 examples/sec; 0.469 sec/batch)
2016-07-14 18:29:38.385584: step 3050, loss = 10340.16 (209.5 examples/sec; 0.611 sec/batch)
2016-07-14 18:29:43.210986: step 3060, loss = 10257.69 (267.4 examples/sec; 0.479 sec/batch)
2016-07-14 18:29:47.924625: step 3070, loss = 10175.67 (263.9 examples/sec; 0.485 sec/batch)
2016-07-14 18:29:53.737843: step 3080, loss = 10094.78 (192.9 examples/sec; 0.664 sec/batch)
2016-07-14 18:29:59.607318: step 3090, loss = 10014.47 (264.6 examples/sec; 0.484 sec/batch)
2016-07-14 18:30:04.380843: step 3100, loss = 9934.46 (263.3 examples/sec; 0.486 sec/batch)
2016-07-14 18:30:09.546707: step 3110, loss = 9855.13 (284.9 examples/sec; 0.449 sec/batch)
2016-07-14 18:30:14.229838: step 3120, loss = 9776.96 (265.5 examples/sec; 0.482 sec/batch)
2016-07-14 18:30:18.798739: step 3130, loss = 9698.80 (277.5 examples/sec; 0.461 sec/batch)
2016-07-14 18:30:23.356223: step 3140, loss = 9621.69 (275.9 examples/sec; 0.464 sec/batch)
2016-07-14 18:30:28.007342: step 3150, loss = 9545.03 (284.6 examples/sec; 0.450 sec/batch)
2016-07-14 18:30:32.619425: step 3160, loss = 9469.38 (274.0 examples/sec; 0.467 sec/batch)
2016-07-14 18:30:37.251181: step 3170, loss = 9393.89 (272.9 examples/sec; 0.469 sec/batch)
2016-07-14 18:30:41.841440: step 3180, loss = 9318.60 (277.5 examples/sec; 0.461 sec/batch)
2016-07-14 18:30:46.434885: step 3190, loss = 9244.90 (281.4 examples/sec; 0.455 sec/batch)
2016-07-14 18:30:51.098777: step 3200, loss = 9170.78 (275.1 examples/sec; 0.465 sec/batch)
2016-07-14 18:30:56.146784: step 3210, loss = 9097.67 (285.9 examples/sec; 0.448 sec/batch)
2016-07-14 18:31:00.770777: step 3220, loss = 9025.06 (263.7 examples/sec; 0.485 sec/batch)
2016-07-14 18:31:06.475846: step 3230, loss = 8953.43 (210.9 examples/sec; 0.607 sec/batch)
2016-07-14 18:31:11.299504: step 3240, loss = 8882.09 (273.3 examples/sec; 0.468 sec/batch)
2016-07-14 18:31:16.032399: step 3250, loss = 8811.11 (257.9 examples/sec; 0.496 sec/batch)
2016-07-14 18:31:20.782899: step 3260, loss = 8740.79 (268.1 examples/sec; 0.477 sec/batch)
2016-07-14 18:31:25.545744: step 3270, loss = 8671.18 (266.1 examples/sec; 0.481 sec/batch)
2016-07-14 18:31:30.213509: step 3280, loss = 8602.30 (276.3 examples/sec; 0.463 sec/batch)
2016-07-14 18:31:34.763999: step 3290, loss = 8533.87 (275.1 examples/sec; 0.465 sec/batch)
2016-07-14 18:31:39.383202: step 3300, loss = 8465.84 (277.6 examples/sec; 0.461 sec/batch)
2016-07-14 18:31:44.520465: step 3310, loss = 8398.35 (275.9 examples/sec; 0.464 sec/batch)
2016-07-14 18:31:49.087322: step 3320, loss = 8331.40 (276.2 examples/sec; 0.464 sec/batch)
2016-07-14 18:31:53.716262: step 3330, loss = 8265.08 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 18:31:59.504926: step 3340, loss = 8198.69 (262.7 examples/sec; 0.487 sec/batch)
2016-07-14 18:32:04.302257: step 3350, loss = 8133.47 (285.7 examples/sec; 0.448 sec/batch)
2016-07-14 18:32:09.074752: step 3360, loss = 8069.02 (265.7 examples/sec; 0.482 sec/batch)
2016-07-14 18:32:13.737251: step 3370, loss = 8004.70 (275.8 examples/sec; 0.464 sec/batch)
2016-07-14 18:32:18.563311: step 3380, loss = 7940.66 (266.2 examples/sec; 0.481 sec/batch)
2016-07-14 18:32:23.194092: step 3390, loss = 7877.71 (272.4 examples/sec; 0.470 sec/batch)
2016-07-14 18:32:27.784583: step 3400, loss = 7814.63 (276.9 examples/sec; 0.462 sec/batch)
2016-07-14 18:32:32.977217: step 3410, loss = 7752.86 (280.1 examples/sec; 0.457 sec/batch)
2016-07-14 18:32:37.554008: step 3420, loss = 7690.92 (282.0 examples/sec; 0.454 sec/batch)
2016-07-14 18:32:42.174220: step 3430, loss = 7629.52 (274.2 examples/sec; 0.467 sec/batch)
2016-07-14 18:32:47.870744: step 3440, loss = 7568.78 (264.3 examples/sec; 0.484 sec/batch)
2016-07-14 18:32:52.609414: step 3450, loss = 7508.43 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 18:32:57.200351: step 3460, loss = 7448.54 (280.6 examples/sec; 0.456 sec/batch)
2016-07-14 18:33:01.772967: step 3470, loss = 7389.12 (273.1 examples/sec; 0.469 sec/batch)
2016-07-14 18:33:07.513707: step 3480, loss = 7330.24 (267.3 examples/sec; 0.479 sec/batch)
2016-07-14 18:33:12.307279: step 3490, loss = 7271.87 (282.6 examples/sec; 0.453 sec/batch)
2016-07-14 18:33:17.057799: step 3500, loss = 7213.97 (262.2 examples/sec; 0.488 sec/batch)
2016-07-14 18:33:23.905974: step 3510, loss = 7156.09 (205.4 examples/sec; 0.623 sec/batch)
2016-07-14 18:33:29.317859: step 3520, loss = 7099.65 (263.6 examples/sec; 0.486 sec/batch)
2016-07-14 18:33:33.948975: step 3530, loss = 7043.33 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 18:33:38.519146: step 3540, loss = 6986.35 (283.2 examples/sec; 0.452 sec/batch)
2016-07-14 18:33:43.374443: step 3550, loss = 6931.37 (204.3 examples/sec; 0.626 sec/batch)
2016-07-14 18:33:48.927273: step 3560, loss = 6875.97 (266.7 examples/sec; 0.480 sec/batch)
2016-07-14 18:33:53.613539: step 3570, loss = 6821.08 (279.9 examples/sec; 0.457 sec/batch)
2016-07-14 18:33:58.202270: step 3580, loss = 6767.02 (284.2 examples/sec; 0.450 sec/batch)
2016-07-14 18:34:02.809824: step 3590, loss = 6713.07 (274.4 examples/sec; 0.467 sec/batch)
2016-07-14 18:34:07.418672: step 3600, loss = 6659.29 (278.7 examples/sec; 0.459 sec/batch)
2016-07-14 18:34:12.538617: step 3610, loss = 6606.23 (276.2 examples/sec; 0.463 sec/batch)
2016-07-14 18:34:17.134288: step 3620, loss = 6553.82 (284.1 examples/sec; 0.450 sec/batch)
2016-07-14 18:34:21.671849: step 3630, loss = 6501.45 (282.4 examples/sec; 0.453 sec/batch)
2016-07-14 18:34:26.290360: step 3640, loss = 6449.69 (269.0 examples/sec; 0.476 sec/batch)
2016-07-14 18:34:32.052440: step 3650, loss = 6398.41 (263.8 examples/sec; 0.485 sec/batch)
2016-07-14 18:34:36.791313: step 3660, loss = 6347.31 (283.0 examples/sec; 0.452 sec/batch)
2016-07-14 18:34:41.562484: step 3670, loss = 6296.72 (269.4 examples/sec; 0.475 sec/batch)
2016-07-14 18:34:46.258996: step 3680, loss = 6246.59 (280.3 examples/sec; 0.457 sec/batch)
2016-07-14 18:34:51.337142: step 3690, loss = 6196.64 (186.9 examples/sec; 0.685 sec/batch)
2016-07-14 18:34:57.833977: step 3700, loss = 6147.84 (206.8 examples/sec; 0.619 sec/batch)
2016-07-14 18:35:03.481586: step 3710, loss = 6098.25 (278.0 examples/sec; 0.461 sec/batch)
2016-07-14 18:35:08.167392: step 3720, loss = 6050.02 (266.9 examples/sec; 0.480 sec/batch)
2016-07-14 18:35:12.914992: step 3730, loss = 6001.80 (277.4 examples/sec; 0.461 sec/batch)
2016-07-14 18:35:17.693574: step 3740, loss = 5953.50 (261.7 examples/sec; 0.489 sec/batch)
2016-07-14 18:35:22.450657: step 3750, loss = 5906.64 (271.9 examples/sec; 0.471 sec/batch)
2016-07-14 18:35:27.259420: step 3760, loss = 5859.37 (279.0 examples/sec; 0.459 sec/batch)
2016-07-14 18:35:31.907369: step 3770, loss = 5812.66 (283.1 examples/sec; 0.452 sec/batch)
2016-07-14 18:35:36.487409: step 3780, loss = 5766.74 (272.5 examples/sec; 0.470 sec/batch)
2016-07-14 18:35:42.026596: step 3790, loss = 5720.68 (202.1 examples/sec; 0.633 sec/batch)
2016-07-14 18:35:47.005675: step 3800, loss = 5674.72 (277.8 examples/sec; 0.461 sec/batch)
2016-07-14 18:35:52.183298: step 3810, loss = 5629.58 (265.0 examples/sec; 0.483 sec/batch)
2016-07-14 18:35:56.765168: step 3820, loss = 5584.47 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 18:36:01.373864: step 3830, loss = 5540.27 (285.1 examples/sec; 0.449 sec/batch)
2016-07-14 18:36:06.003605: step 3840, loss = 5496.38 (274.1 examples/sec; 0.467 sec/batch)
2016-07-14 18:36:10.598979: step 3850, loss = 5452.60 (291.3 examples/sec; 0.439 sec/batch)
2016-07-14 18:36:15.141781: step 3860, loss = 5408.83 (271.1 examples/sec; 0.472 sec/batch)
2016-07-14 18:36:20.153786: step 3870, loss = 5365.70 (201.7 examples/sec; 0.635 sec/batch)
2016-07-14 18:36:25.912369: step 3880, loss = 5323.31 (242.8 examples/sec; 0.527 sec/batch)
2016-07-14 18:36:31.766744: step 3890, loss = 5280.68 (266.1 examples/sec; 0.481 sec/batch)
2016-07-14 18:36:37.225828: step 3900, loss = 5238.51 (203.4 examples/sec; 0.629 sec/batch)
2016-07-14 18:36:43.008875: step 3910, loss = 5197.17 (275.5 examples/sec; 0.465 sec/batch)
2016-07-14 18:36:47.635816: step 3920, loss = 5155.74 (266.6 examples/sec; 0.480 sec/batch)
2016-07-14 18:36:52.427533: step 3930, loss = 5114.33 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 18:36:57.177285: step 3940, loss = 5073.70 (262.8 examples/sec; 0.487 sec/batch)
2016-07-14 18:37:01.945406: step 3950, loss = 5033.20 (277.9 examples/sec; 0.461 sec/batch)
2016-07-14 18:37:06.697449: step 3960, loss = 4993.07 (270.0 examples/sec; 0.474 sec/batch)
2016-07-14 18:37:11.361135: step 3970, loss = 4953.21 (265.8 examples/sec; 0.482 sec/batch)
2016-07-14 18:37:16.944963: step 3980, loss = 4913.58 (189.7 examples/sec; 0.675 sec/batch)
2016-07-14 18:37:23.253031: step 3990, loss = 4874.71 (268.1 examples/sec; 0.478 sec/batch)
2016-07-14 18:37:28.136270: step 4000, loss = 4836.02 (272.3 examples/sec; 0.470 sec/batch)
2016-07-14 18:37:33.919483: step 4010, loss = 4797.49 (256.7 examples/sec; 0.499 sec/batch)
2016-07-14 18:37:38.625967: step 4020, loss = 4758.91 (268.0 examples/sec; 0.478 sec/batch)
2016-07-14 18:37:43.177386: step 4030, loss = 4721.43 (276.3 examples/sec; 0.463 sec/batch)
2016-07-14 18:37:47.744566: step 4040, loss = 4683.53 (284.6 examples/sec; 0.450 sec/batch)
2016-07-14 18:37:52.290948: step 4050, loss = 4646.37 (286.3 examples/sec; 0.447 sec/batch)
2016-07-14 18:37:56.891405: step 4060, loss = 4609.43 (279.6 examples/sec; 0.458 sec/batch)
2016-07-14 18:38:01.518847: step 4070, loss = 4572.60 (279.2 examples/sec; 0.458 sec/batch)
2016-07-14 18:38:07.226723: step 4080, loss = 4536.35 (260.2 examples/sec; 0.492 sec/batch)
2016-07-14 18:38:12.025992: step 4090, loss = 4500.10 (271.3 examples/sec; 0.472 sec/batch)
2016-07-14 18:38:16.816841: step 4100, loss = 4464.15 (270.4 examples/sec; 0.473 sec/batch)
2016-07-14 18:38:23.532517: step 4110, loss = 4429.02 (205.2 examples/sec; 0.624 sec/batch)
2016-07-14 18:38:29.112974: step 4120, loss = 4393.59 (260.2 examples/sec; 0.492 sec/batch)
2016-07-14 18:38:33.846235: step 4130, loss = 4358.42 (270.8 examples/sec; 0.473 sec/batch)
2016-07-14 18:38:38.713988: step 4140, loss = 4323.34 (258.2 examples/sec; 0.496 sec/batch)
2016-07-14 18:38:43.387496: step 4150, loss = 4289.40 (261.2 examples/sec; 0.490 sec/batch)
2016-07-14 18:38:48.861976: step 4160, loss = 4255.10 (185.9 examples/sec; 0.689 sec/batch)
2016-07-14 18:38:55.158758: step 4170, loss = 4221.01 (260.4 examples/sec; 0.491 sec/batch)
2016-07-14 18:39:00.335092: step 4180, loss = 4187.56 (209.1 examples/sec; 0.612 sec/batch)
2016-07-14 18:39:05.801952: step 4190, loss = 4154.00 (262.6 examples/sec; 0.487 sec/batch)
2016-07-14 18:39:10.438379: step 4200, loss = 4121.03 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 18:39:15.854683: step 4210, loss = 4088.03 (259.6 examples/sec; 0.493 sec/batch)
2016-07-14 18:39:20.460686: step 4220, loss = 4055.71 (282.9 examples/sec; 0.453 sec/batch)
2016-07-14 18:39:24.982414: step 4230, loss = 4023.41 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 18:39:29.619541: step 4240, loss = 3991.52 (267.1 examples/sec; 0.479 sec/batch)
2016-07-14 18:39:35.339192: step 4250, loss = 3959.67 (256.3 examples/sec; 0.499 sec/batch)
2016-07-14 18:39:40.113038: step 4260, loss = 3927.66 (275.8 examples/sec; 0.464 sec/batch)
2016-07-14 18:39:44.809487: step 4270, loss = 3896.84 (265.9 examples/sec; 0.481 sec/batch)
2016-07-14 18:39:49.508193: step 4280, loss = 3865.48 (287.5 examples/sec; 0.445 sec/batch)
2016-07-14 18:39:54.351884: step 4290, loss = 3834.86 (255.9 examples/sec; 0.500 sec/batch)
2016-07-14 18:39:59.026717: step 4300, loss = 3804.29 (273.1 examples/sec; 0.469 sec/batch)
2016-07-14 18:40:04.380768: step 4310, loss = 3773.91 (272.6 examples/sec; 0.470 sec/batch)
2016-07-14 18:40:09.011601: step 4320, loss = 3743.92 (284.7 examples/sec; 0.450 sec/batch)
2016-07-14 18:40:13.592629: step 4330, loss = 3714.02 (274.3 examples/sec; 0.467 sec/batch)
2016-07-14 18:40:18.192148: step 4340, loss = 3684.79 (276.1 examples/sec; 0.464 sec/batch)
2016-07-14 18:40:23.916676: step 4350, loss = 3654.87 (276.1 examples/sec; 0.464 sec/batch)
2016-07-14 18:40:28.744377: step 4360, loss = 3625.80 (284.0 examples/sec; 0.451 sec/batch)
2016-07-14 18:40:33.561086: step 4370, loss = 3597.45 (262.8 examples/sec; 0.487 sec/batch)
2016-07-14 18:40:38.304929: step 4380, loss = 3568.44 (267.7 examples/sec; 0.478 sec/batch)
2016-07-14 18:40:42.907593: step 4390, loss = 3540.19 (274.6 examples/sec; 0.466 sec/batch)
2016-07-14 18:40:47.718640: step 4400, loss = 3512.17 (206.6 examples/sec; 0.620 sec/batch)
2016-07-14 18:40:53.902892: step 4410, loss = 3483.90 (259.1 examples/sec; 0.494 sec/batch)
2016-07-14 18:40:58.561472: step 4420, loss = 3455.90 (282.4 examples/sec; 0.453 sec/batch)
2016-07-14 18:41:03.393890: step 4430, loss = 3428.49 (284.3 examples/sec; 0.450 sec/batch)
2016-07-14 18:41:08.044001: step 4440, loss = 3401.57 (271.7 examples/sec; 0.471 sec/batch)
2016-07-14 18:41:13.689971: step 4450, loss = 3400.13 (188.8 examples/sec; 0.678 sec/batch)
2016-07-14 18:41:19.775006: step 4460, loss = 3373.33 (262.1 examples/sec; 0.488 sec/batch)
2016-07-14 18:41:25.119822: step 4470, loss = 3347.28 (206.3 examples/sec; 0.621 sec/batch)
2016-07-14 18:41:30.420775: step 4480, loss = 3320.38 (264.8 examples/sec; 0.483 sec/batch)
2016-07-14 18:41:35.112372: step 4490, loss = 3293.72 (273.6 examples/sec; 0.468 sec/batch)
2016-07-14 18:41:39.977043: step 4500, loss = 3267.28 (268.6 examples/sec; 0.477 sec/batch)
2016-07-14 18:41:45.132606: step 4510, loss = 3241.24 (276.9 examples/sec; 0.462 sec/batch)
2016-07-14 18:41:49.663200: step 4520, loss = 3215.50 (280.8 examples/sec; 0.456 sec/batch)
2016-07-14 18:41:54.441432: step 4530, loss = 3189.85 (266.7 examples/sec; 0.480 sec/batch)
2016-07-14 18:42:00.419249: step 4540, loss = 3164.49 (262.2 examples/sec; 0.488 sec/batch)
2016-07-14 18:42:05.244104: step 4550, loss = 3139.20 (259.5 examples/sec; 0.493 sec/batch)
2016-07-14 18:42:10.099315: step 4560, loss = 3114.27 (260.9 examples/sec; 0.491 sec/batch)
2016-07-14 18:42:14.788049: step 4570, loss = 3089.51 (276.4 examples/sec; 0.463 sec/batch)
2016-07-14 18:42:19.624830: step 4580, loss = 3064.97 (272.6 examples/sec; 0.470 sec/batch)
2016-07-14 18:42:24.270493: step 4590, loss = 3040.77 (280.6 examples/sec; 0.456 sec/batch)
2016-07-14 18:42:28.879114: step 4600, loss = 3016.02 (272.2 examples/sec; 0.470 sec/batch)
2016-07-14 18:42:35.212886: step 4610, loss = 2992.58 (227.7 examples/sec; 0.562 sec/batch)
2016-07-14 18:42:40.106020: step 4620, loss = 2968.24 (279.7 examples/sec; 0.458 sec/batch)
2016-07-14 18:42:44.773243: step 4630, loss = 2944.77 (277.0 examples/sec; 0.462 sec/batch)
2016-07-14 18:42:49.477603: step 4640, loss = 2921.57 (275.3 examples/sec; 0.465 sec/batch)
2016-07-14 18:42:54.099413: step 4650, loss = 2897.74 (280.1 examples/sec; 0.457 sec/batch)
2016-07-14 18:42:58.783652: step 4660, loss = 2875.01 (276.7 examples/sec; 0.463 sec/batch)
2016-07-14 18:43:04.578999: step 4670, loss = 2851.87 (264.3 examples/sec; 0.484 sec/batch)
2016-07-14 18:43:09.319229: step 4680, loss = 2829.03 (274.7 examples/sec; 0.466 sec/batch)
2016-07-14 18:43:14.134045: step 4690, loss = 2806.87 (262.1 examples/sec; 0.488 sec/batch)
2016-07-14 18:43:18.922012: step 4700, loss = 2784.32 (267.8 examples/sec; 0.478 sec/batch)
2016-07-14 18:43:24.027955: step 4710, loss = 2761.98 (265.8 examples/sec; 0.482 sec/batch)
2016-07-14 18:43:28.655321: step 4720, loss = 2740.33 (284.3 examples/sec; 0.450 sec/batch)
2016-07-14 18:43:33.297798: step 4730, loss = 2718.37 (279.3 examples/sec; 0.458 sec/batch)
2016-07-14 18:43:37.962400: step 4740, loss = 2697.01 (268.0 examples/sec; 0.478 sec/batch)
2016-07-14 18:43:42.607970: step 4750, loss = 2674.85 (274.8 examples/sec; 0.466 sec/batch)
2016-07-14 18:43:47.296320: step 4760, loss = 2653.84 (233.2 examples/sec; 0.549 sec/batch)
2016-07-14 18:43:53.048224: step 4770, loss = 2632.84 (265.0 examples/sec; 0.483 sec/batch)
2016-07-14 18:43:57.740551: step 4780, loss = 2611.40 (269.5 examples/sec; 0.475 sec/batch)
2016-07-14 18:44:02.523382: step 4790, loss = 2591.00 (264.2 examples/sec; 0.485 sec/batch)
2016-07-14 18:44:07.204643: step 4800, loss = 2570.44 (270.6 examples/sec; 0.473 sec/batch)
2016-07-14 18:44:13.369845: step 4810, loss = 2550.07 (182.8 examples/sec; 0.700 sec/batch)
2016-07-14 18:44:19.674113: step 4820, loss = 2529.88 (259.9 examples/sec; 0.493 sec/batch)
2016-07-14 18:44:24.437968: step 4830, loss = 2509.19 (281.9 examples/sec; 0.454 sec/batch)
2016-07-14 18:44:29.229097: step 4840, loss = 2489.31 (262.0 examples/sec; 0.489 sec/batch)
2016-07-14 18:44:35.286815: step 4850, loss = 2469.41 (192.5 examples/sec; 0.665 sec/batch)
2016-07-14 18:44:41.022565: step 4860, loss = 2450.29 (256.0 examples/sec; 0.500 sec/batch)
2016-07-14 18:44:45.721195: step 4870, loss = 2430.42 (269.0 examples/sec; 0.476 sec/batch)
2016-07-14 18:44:50.562521: step 4880, loss = 2411.39 (263.2 examples/sec; 0.486 sec/batch)
2016-07-14 18:44:55.285094: step 4890, loss = 2391.96 (276.1 examples/sec; 0.464 sec/batch)
2016-07-14 18:45:00.631044: step 4900, loss = 2372.81 (188.1 examples/sec; 0.681 sec/batch)
2016-07-14 18:45:07.794686: step 4910, loss = 2353.99 (263.1 examples/sec; 0.486 sec/batch)
2016-07-14 18:45:12.609077: step 4920, loss = 2335.06 (282.7 examples/sec; 0.453 sec/batch)
2016-07-14 18:45:17.204242: step 4930, loss = 2316.86 (279.3 examples/sec; 0.458 sec/batch)
2016-07-14 18:45:21.809992: step 4940, loss = 2298.20 (277.2 examples/sec; 0.462 sec/batch)
2016-07-14 18:45:26.434476: step 4950, loss = 2280.07 (284.6 examples/sec; 0.450 sec/batch)
2016-07-14 18:45:32.270167: step 4960, loss = 2261.70 (258.4 examples/sec; 0.495 sec/batch)
2016-07-14 18:45:37.735948: step 4970, loss = 2244.10 (199.8 examples/sec; 0.641 sec/batch)
2016-07-14 18:45:42.950152: step 4980, loss = 2226.00 (261.1 examples/sec; 0.490 sec/batch)
2016-07-14 18:45:47.707128: step 4990, loss = 2208.18 (261.7 examples/sec; 0.489 sec/batch)
2016-07-14 18:45:52.481346: step 5000, loss = 2190.85 (282.4 examples/sec; 0.453 sec/batch)
2016-07-14 18:45:58.139012: step 5010, loss = 2172.93 (273.9 examples/sec; 0.467 sec/batch)
2016-07-14 18:46:02.763586: step 5020, loss = 2155.88 (269.3 examples/sec; 0.475 sec/batch)
2016-07-14 18:46:07.452324: step 5030, loss = 2138.79 (277.1 examples/sec; 0.462 sec/batch)
2016-07-14 18:46:12.172839: step 5040, loss = 2121.62 (265.1 examples/sec; 0.483 sec/batch)
2016-07-14 18:46:16.936154: step 5050, loss = 2104.75 (269.0 examples/sec; 0.476 sec/batch)
2016-07-14 18:46:21.633416: step 5060, loss = 2088.22 (273.0 examples/sec; 0.469 sec/batch)
2016-07-14 18:46:26.256177: step 5070, loss = 2071.55 (285.1 examples/sec; 0.449 sec/batch)
2016-07-14 18:46:30.892401: step 5080, loss = 2054.96 (290.0 examples/sec; 0.441 sec/batch)
2016-07-14 18:46:35.612730: step 5090, loss = 2038.45 (252.3 examples/sec; 0.507 sec/batch)
2016-07-14 18:46:40.192716: step 5100, loss = 2022.61 (282.1 examples/sec; 0.454 sec/batch)
2016-07-14 18:46:45.380359: step 5110, loss = 2006.37 (273.1 examples/sec; 0.469 sec/batch)
2016-07-14 18:46:49.995891: step 5120, loss = 1990.24 (284.4 examples/sec; 0.450 sec/batch)
2016-07-14 18:46:55.813300: step 5130, loss = 1974.46 (257.4 examples/sec; 0.497 sec/batch)
2016-07-14 18:47:01.117798: step 5140, loss = 1958.89 (205.7 examples/sec; 0.622 sec/batch)
2016-07-14 18:47:06.507920: step 5150, loss = 1943.15 (260.9 examples/sec; 0.491 sec/batch)
2016-07-14 18:47:11.288489: step 5160, loss = 1927.65 (267.1 examples/sec; 0.479 sec/batch)
2016-07-14 18:47:16.831558: step 5170, loss = 1912.49 (180.7 examples/sec; 0.708 sec/batch)
2016-07-14 18:47:23.474197: step 5180, loss = 1896.97 (229.8 examples/sec; 0.557 sec/batch)
2016-07-14 18:47:28.319898: step 5190, loss = 1882.01 (280.1 examples/sec; 0.457 sec/batch)
2016-07-14 18:47:32.990499: step 5200, loss = 1866.75 (275.7 examples/sec; 0.464 sec/batch)
2016-07-14 18:47:38.117622: step 5210, loss = 1851.98 (283.3 examples/sec; 0.452 sec/batch)
2016-07-14 18:47:43.878338: step 5220, loss = 1837.09 (265.5 examples/sec; 0.482 sec/batch)
2016-07-14 18:47:49.241475: step 5230, loss = 1823.00 (205.2 examples/sec; 0.624 sec/batch)
2016-07-14 18:47:54.649396: step 5240, loss = 1808.39 (243.2 examples/sec; 0.526 sec/batch)
2016-07-14 18:47:59.374641: step 5250, loss = 1793.91 (265.0 examples/sec; 0.483 sec/batch)
2016-07-14 18:48:04.283325: step 5260, loss = 1779.17 (272.8 examples/sec; 0.469 sec/batch)
2016-07-14 18:48:08.933927: step 5270, loss = 1765.18 (259.0 examples/sec; 0.494 sec/batch)
2016-07-14 18:48:14.959095: step 5280, loss = 1751.28 (179.3 examples/sec; 0.714 sec/batch)
2016-07-14 18:48:20.870352: step 5290, loss = 1737.44 (256.6 examples/sec; 0.499 sec/batch)
2016-07-14 18:48:25.585217: step 5300, loss = 1723.54 (275.7 examples/sec; 0.464 sec/batch)
2016-07-14 18:48:30.610973: step 5310, loss = 1709.64 (277.8 examples/sec; 0.461 sec/batch)
2016-07-14 18:48:35.189663: step 5320, loss = 1696.35 (289.2 examples/sec; 0.443 sec/batch)
2016-07-14 18:48:40.961406: step 5330, loss = 1682.70 (260.2 examples/sec; 0.492 sec/batch)
2016-07-14 18:48:46.596675: step 5340, loss = 1669.20 (206.0 examples/sec; 0.621 sec/batch)
2016-07-14 18:48:51.692048: step 5350, loss = 1656.02 (260.3 examples/sec; 0.492 sec/batch)
2016-07-14 18:48:56.364154: step 5360, loss = 1642.87 (259.2 examples/sec; 0.494 sec/batch)
2016-07-14 18:49:01.160675: step 5370, loss = 1629.93 (260.6 examples/sec; 0.491 sec/batch)
2016-07-14 18:49:05.973593: step 5380, loss = 1616.82 (256.9 examples/sec; 0.498 sec/batch)
2016-07-14 18:49:10.688087: step 5390, loss = 1603.92 (284.8 examples/sec; 0.449 sec/batch)
2016-07-14 18:49:15.304924: step 5400, loss = 1590.83 (283.9 examples/sec; 0.451 sec/batch)
2016-07-14 18:49:20.379501: step 5410, loss = 1578.42 (269.3 examples/sec; 0.475 sec/batch)
2016-07-14 18:49:25.757719: step 5420, loss = 1565.89 (205.0 examples/sec; 0.624 sec/batch)
2016-07-14 18:49:31.105109: step 5430, loss = 1553.31 (250.5 examples/sec; 0.511 sec/batch)
2016-07-14 18:49:36.920441: step 5440, loss = 1541.02 (254.2 examples/sec; 0.504 sec/batch)
2016-07-14 18:49:41.643164: step 5450, loss = 1528.65 (280.9 examples/sec; 0.456 sec/batch)
2016-07-14 18:49:46.203169: step 5460, loss = 1516.26 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 18:49:50.824588: step 5470, loss = 1504.90 (274.5 examples/sec; 0.466 sec/batch)
2016-07-14 18:49:56.571188: step 5480, loss = 1492.51 (268.2 examples/sec; 0.477 sec/batch)
2016-07-14 18:50:01.351979: step 5490, loss = 1480.76 (264.7 examples/sec; 0.484 sec/batch)
2016-07-14 18:50:06.023443: step 5500, loss = 1468.94 (267.9 examples/sec; 0.478 sec/batch)
2016-07-14 18:50:11.200600: step 5510, loss = 1457.58 (259.5 examples/sec; 0.493 sec/batch)
2016-07-14 18:50:15.722661: step 5520, loss = 1445.79 (281.6 examples/sec; 0.455 sec/batch)
2016-07-14 18:50:20.645208: step 5530, loss = 1434.36 (213.8 examples/sec; 0.599 sec/batch)
2016-07-14 18:50:26.087707: step 5540, loss = 1422.71 (269.1 examples/sec; 0.476 sec/batch)
2016-07-14 18:50:30.880152: step 5550, loss = 1411.48 (270.3 examples/sec; 0.474 sec/batch)
2016-07-14 18:50:35.475894: step 5560, loss = 1400.15 (275.3 examples/sec; 0.465 sec/batch)
2016-07-14 18:50:40.124481: step 5570, loss = 1388.96 (279.6 examples/sec; 0.458 sec/batch)
2016-07-14 18:50:44.708987: step 5580, loss = 1378.11 (275.7 examples/sec; 0.464 sec/batch)
2016-07-14 18:50:50.076654: step 5590, loss = 1366.81 (201.1 examples/sec; 0.637 sec/batch)
2016-07-14 18:50:55.236514: step 5600, loss = 1355.86 (272.2 examples/sec; 0.470 sec/batch)
2016-07-14 18:51:00.475129: step 5610, loss = 1345.46 (263.1 examples/sec; 0.486 sec/batch)
2016-07-14 18:51:05.257248: step 5620, loss = 1335.00 (268.7 examples/sec; 0.476 sec/batch)
2016-07-14 18:51:10.093113: step 5630, loss = 1324.12 (266.4 examples/sec; 0.480 sec/batch)
2016-07-14 18:51:16.391317: step 5640, loss = 1313.45 (194.1 examples/sec; 0.660 sec/batch)
2016-07-14 18:51:21.924904: step 5650, loss = 1302.95 (261.9 examples/sec; 0.489 sec/batch)
2016-07-14 18:51:26.915193: step 5660, loss = 1292.74 (255.4 examples/sec; 0.501 sec/batch)
2016-07-14 18:51:32.022419: step 5670, loss = 1282.20 (252.1 examples/sec; 0.508 sec/batch)
2016-07-14 18:51:37.847821: step 5680, loss = 1272.20 (185.1 examples/sec; 0.692 sec/batch)
2016-07-14 18:51:43.524798: step 5690, loss = 1261.90 (245.7 examples/sec; 0.521 sec/batch)
2016-07-14 18:51:49.713270: step 5700, loss = 1252.00 (229.8 examples/sec; 0.557 sec/batch)
2016-07-14 18:51:55.425523: step 5710, loss = 1241.86 (250.1 examples/sec; 0.512 sec/batch)
2016-07-14 18:52:00.582611: step 5720, loss = 1232.30 (239.3 examples/sec; 0.535 sec/batch)
2016-07-14 18:52:05.753026: step 5730, loss = 1222.58 (247.0 examples/sec; 0.518 sec/batch)
2016-07-14 18:52:10.833345: step 5740, loss = 1212.69 (280.6 examples/sec; 0.456 sec/batch)
2016-07-14 18:52:15.924303: step 5750, loss = 1202.87 (269.9 examples/sec; 0.474 sec/batch)
2016-07-14 18:52:21.871713: step 5760, loss = 1193.26 (195.1 examples/sec; 0.656 sec/batch)
2016-07-14 18:52:27.589575: step 5770, loss = 1184.04 (234.3 examples/sec; 0.546 sec/batch)
2016-07-14 18:52:33.743779: step 5780, loss = 1174.66 (253.6 examples/sec; 0.505 sec/batch)
2016-07-14 18:52:39.049971: step 5790, loss = 1165.25 (223.2 examples/sec; 0.573 sec/batch)
2016-07-14 18:52:44.256816: step 5800, loss = 1155.82 (243.0 examples/sec; 0.527 sec/batch)
2016-07-14 18:52:50.026756: step 5810, loss = 1146.70 (263.5 examples/sec; 0.486 sec/batch)
2016-07-14 18:52:55.221057: step 5820, loss = 1137.50 (253.8 examples/sec; 0.504 sec/batch)
2016-07-14 18:53:01.399142: step 5830, loss = 1128.67 (249.5 examples/sec; 0.513 sec/batch)
2016-07-14 18:53:06.552628: step 5840, loss = 1119.58 (261.1 examples/sec; 0.490 sec/batch)
2016-07-14 18:53:12.006739: step 5850, loss = 1110.39 (231.3 examples/sec; 0.553 sec/batch)
2016-07-14 18:53:17.195849: step 5860, loss = 1102.02 (255.7 examples/sec; 0.501 sec/batch)
2016-07-14 18:53:22.296609: step 5870, loss = 1092.82 (246.2 examples/sec; 0.520 sec/batch)
2016-07-14 18:53:27.405550: step 5880, loss = 1084.79 (268.0 examples/sec; 0.478 sec/batch)
2016-07-14 18:53:32.518281: step 5890, loss = 1075.99 (256.9 examples/sec; 0.498 sec/batch)
2016-07-14 18:53:37.722533: step 5900, loss = 1067.32 (247.5 examples/sec; 0.517 sec/batch)
2016-07-14 18:53:43.505532: step 5910, loss = 1058.94 (263.5 examples/sec; 0.486 sec/batch)
2016-07-14 18:53:48.625607: step 5920, loss = 1050.51 (247.7 examples/sec; 0.517 sec/batch)
2016-07-14 18:53:54.564436: step 5930, loss = 1042.15 (189.6 examples/sec; 0.675 sec/batch)
2016-07-14 18:54:00.007207: step 5940, loss = 1033.94 (263.6 examples/sec; 0.486 sec/batch)
2016-07-14 18:54:05.339187: step 5950, loss = 1025.68 (231.5 examples/sec; 0.553 sec/batch)
2016-07-14 18:54:10.501485: step 5960, loss = 1017.45 (241.9 examples/sec; 0.529 sec/batch)
2016-07-14 18:54:15.891639: step 5970, loss = 1009.25 (231.0 examples/sec; 0.554 sec/batch)
2016-07-14 18:54:21.004357: step 5980, loss = 1000.93 (252.7 examples/sec; 0.507 sec/batch)
2016-07-14 18:54:26.154807: step 5990, loss = 993.25 (270.3 examples/sec; 0.474 sec/batch)
2016-07-14 18:54:31.295566: step 6000, loss = 985.32 (249.1 examples/sec; 0.514 sec/batch)
2016-07-14 18:54:38.620126: step 6010, loss = 977.67 (235.1 examples/sec; 0.544 sec/batch)
2016-07-14 18:54:43.888210: step 6020, loss = 969.64 (236.2 examples/sec; 0.542 sec/batch)
2016-07-14 18:54:48.994413: step 6030, loss = 961.84 (256.3 examples/sec; 0.499 sec/batch)
2016-07-14 18:54:53.601712: step 6040, loss = 954.59 (309.4 examples/sec; 0.414 sec/batch)
2016-07-14 18:54:58.129027: step 6050, loss = 946.77 (294.5 examples/sec; 0.435 sec/batch)
2016-07-14 18:55:02.506367: step 6060, loss = 939.46 (309.0 examples/sec; 0.414 sec/batch)
2016-07-14 18:55:06.893668: step 6070, loss = 931.84 (307.8 examples/sec; 0.416 sec/batch)
2016-07-14 18:55:11.371951: step 6080, loss = 924.66 (275.0 examples/sec; 0.465 sec/batch)
2016-07-14 18:55:15.761885: step 6090, loss = 916.97 (284.7 examples/sec; 0.450 sec/batch)
2016-07-14 18:55:20.133996: step 6100, loss = 909.86 (265.9 examples/sec; 0.481 sec/batch)
2016-07-14 18:55:24.913555: step 6110, loss = 902.80 (313.2 examples/sec; 0.409 sec/batch)
2016-07-14 18:55:29.216567: step 6120, loss = 895.66 (303.2 examples/sec; 0.422 sec/batch)
2016-07-14 18:55:33.710912: step 6130, loss = 888.47 (274.7 examples/sec; 0.466 sec/batch)
2016-07-14 18:55:38.080649: step 6140, loss = 881.01 (282.9 examples/sec; 0.452 sec/batch)
2016-07-14 18:55:42.399388: step 6150, loss = 874.15 (317.8 examples/sec; 0.403 sec/batch)
2016-07-14 18:55:46.784230: step 6160, loss = 867.23 (289.6 examples/sec; 0.442 sec/batch)
2016-07-14 18:55:51.109830: step 6170, loss = 860.31 (298.0 examples/sec; 0.430 sec/batch)
2016-07-14 18:55:55.670156: step 6180, loss = 853.67 (262.6 examples/sec; 0.487 sec/batch)
Traceback (most recent call last):

LEARNING RATE: 1.0 for FAST CONVERGENCE

python cifar10_train.py 
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GT 650M
major: 3 minor: 0 memoryClockRate (GHz) 0.835
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.85GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)
2016-07-14 22:20:16.485072: step 0, loss = 49006.59 (41.2 examples/sec; 3.110 sec/batch)
2016-07-14 22:20:22.442931: step 10, loss = 45231.25 (279.9 examples/sec; 0.457 sec/batch)
2016-07-14 22:20:27.115850: step 20, loss = 41746.72 (279.4 examples/sec; 0.458 sec/batch)
2016-07-14 22:20:31.695471: step 30, loss = 38531.41 (278.2 examples/sec; 0.460 sec/batch)
2016-07-14 22:20:36.316652: step 40, loss = 35563.75 (264.1 examples/sec; 0.485 sec/batch)
2016-07-14 22:20:40.932571: step 50, loss = 32824.42 (280.5 examples/sec; 0.456 sec/batch)
2016-07-14 22:20:45.604352: step 60, loss = 30295.27 (272.2 examples/sec; 0.470 sec/batch)
2016-07-14 22:20:51.425631: step 70, loss = 27961.80 (287.5 examples/sec; 0.445 sec/batch)
2016-07-14 22:20:56.023308: step 80, loss = 25808.90 (285.6 examples/sec; 0.448 sec/batch)
2016-07-14 22:21:00.718380: step 90, loss = 23820.47 (270.1 examples/sec; 0.474 sec/batch)
2016-07-14 22:21:06.598380: step 100, loss = 21985.69 (272.4 examples/sec; 0.470 sec/batch)
2016-07-14 22:21:12.375698: step 110, loss = 20292.22 (275.4 examples/sec; 0.465 sec/batch)
2016-07-14 22:21:17.076834: step 120, loss = 18729.61 (268.8 examples/sec; 0.476 sec/batch)
2016-07-14 22:21:21.781730: step 130, loss = 17286.51 (285.1 examples/sec; 0.449 sec/batch)
2016-07-14 22:21:27.614637: step 140, loss = 15955.01 (283.3 examples/sec; 0.452 sec/batch)
2016-07-14 22:21:32.225078: step 150, loss = 14725.91 (271.6 examples/sec; 0.471 sec/batch)
2016-07-14 22:21:38.109713: step 160, loss = 13591.71 (215.5 examples/sec; 0.594 sec/batch)
2016-07-14 22:21:42.994922: step 170, loss = 12545.64 (271.2 examples/sec; 0.472 sec/batch)
2016-07-14 22:21:47.867985: step 180, loss = 11578.79 (262.2 examples/sec; 0.488 sec/batch)
2016-07-14 22:21:52.752235: step 190, loss = 10687.40 (270.7 examples/sec; 0.473 sec/batch)
2016-07-14 22:21:57.670954: step 200, loss = 9864.27 (260.8 examples/sec; 0.491 sec/batch)
2016-07-14 22:22:06.346219: step 210, loss = 9104.16 (276.8 examples/sec; 0.462 sec/batch)

LEARNING RATE: 0.1

python cifar10_train.py 
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GT 650M
major: 3 minor: 0 memoryClockRate (GHz) 0.835
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.83GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)
2016-07-14 22:24:16.586564: step 0, loss = 9785.56 (141.5 examples/sec; 0.904 sec/batch)
2016-07-14 22:24:22.374534: step 10, loss = 9707.15 (273.8 examples/sec; 0.467 sec/batch)
2016-07-14 22:24:27.083745: step 20, loss = 9629.34 (262.2 examples/sec; 0.488 sec/batch)
2016-07-14 22:24:31.735262: step 30, loss = 9553.05 (267.1 examples/sec; 0.479 sec/batch)
2016-07-14 22:24:37.206574: step 40, loss = 9477.26 (196.8 examples/sec; 0.650 sec/batch)
2016-07-14 22:24:42.279212: step 50, loss = 9401.76 (276.8 examples/sec; 0.462 sec/batch)
2016-07-14 22:24:47.213681: step 60, loss = 9326.09 (191.9 examples/sec; 0.667 sec/batch)
2016-07-14 22:24:52.841057: step 70, loss = 9251.92 (280.5 examples/sec; 0.456 sec/batch)
2016-07-14 22:24:57.549302: step 80, loss = 9179.05 (275.6 examples/sec; 0.464 sec/batch)
2016-07-14 22:25:03.481611: step 90, loss = 9105.42 (252.9 examples/sec; 0.506 sec/batch)
2016-07-14 22:25:08.349794: step 100, loss = 9032.87 (267.3 examples/sec; 0.479 sec/batch)
2016-07-14 22:25:14.345348: step 110, loss = 8960.90 (256.2 examples/sec; 0.500 sec/batch)
2016-07-14 22:25:19.105524: step 120, loss = 8889.92 (255.3 examples/sec; 0.501 sec/batch)
2016-07-14 22:25:24.038502: step 130, loss = 8818.56 (265.3 examples/sec; 0.482 sec/batch)
2016-07-14 22:25:28.910096: step 140, loss = 8748.22 (246.9 examples/sec; 0.518 sec/batch)
2016-07-14 22:25:33.740563: step 150, loss = 8678.36 (267.2 examples/sec; 0.479 sec/batch)
2016-07-14 22:25:38.659510: step 160, loss = 8609.22 (248.2 examples/sec; 0.516 sec/batch)
2016-07-14 22:25:43.469483: step 170, loss = 8541.48 (249.5 examples/sec; 0.513 sec/batch)
2016-07-14 22:25:48.395049: step 180, loss = 8472.74 (257.6 examples/sec; 0.497 sec/batch)
2016-07-14 22:25:53.262568: step 190, loss = 8405.56 (250.7 examples/sec; 0.511 sec/batch)
2016-07-14 22:25:59.438864: step 200, loss = 8338.63 (261.5 examples/sec; 0.489 sec/batch)
2016-07-14 22:26:05.306636: step 210, loss = 8271.81 (263.7 examples/sec; 0.485 sec/batch)
2016-07-14 22:26:11.174900: step 220, loss = 8205.92 (255.4 examples/sec; 0.501 sec/batch)
2016-07-14 22:26:16.056066: step 230, loss = 8140.92 (272.9 examples/sec; 0.469 sec/batch)
2016-07-14 22:26:20.871018: step 240, loss = 8076.50 (250.3 examples/sec; 0.511 sec/batch)
2016-07-14 22:26:25.716314: step 250, loss = 8011.10 (262.4 examples/sec; 0.488 sec/batch)
2016-07-14 22:26:30.379675: step 260, loss = 7947.41 (270.3 examples/sec; 0.473 sec/batch)
2016-07-14 22:26:35.320229: step 270, loss = 7884.30 (181.4 examples/sec; 0.706 sec/batch)
2016-07-14 22:26:41.155314: step 280, loss = 7821.29 (254.7 examples/sec; 0.503 sec/batch)
2016-07-14 22:26:45.896838: step 290, loss = 7759.09 (273.0 examples/sec; 0.469 sec/batch)
2016-07-14 22:26:50.847877: step 300, loss = 7697.15 (234.2 examples/sec; 0.547 sec/batch)
2016-07-14 22:26:58.126957: step 310, loss = 7636.99 (278.7 examples/sec; 0.459 sec/batch)
2016-07-14 22:27:02.802844: step 320, loss = 7574.88 (262.4 examples/sec; 0.488 sec/batch)
2016-07-14 22:27:08.666328: step 330, loss = 7514.56 (266.1 examples/sec; 0.481 sec/batch)
2016-07-14 22:27:13.436265: step 340, loss = 7454.47 (283.3 examples/sec; 0.452 sec/batch)
2016-07-14 22:27:18.190590: step 350, loss = 7395.43 (267.7 examples/sec; 0.478 sec/batch)
2016-07-14 22:27:22.839321: step 360, loss = 7337.04 (266.0 examples/sec; 0.481 sec/batch)
2016-07-14 22:27:27.612149: step 370, loss = 7277.98 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 22:27:32.290251: step 380, loss = 7220.74 (264.4 examples/sec; 0.484 sec/batch)
2016-07-14 22:27:37.065467: step 390, loss = 7163.03 (273.5 examples/sec; 0.468 sec/batch)
2016-07-14 22:27:41.874103: step 400, loss = 7105.45 (257.9 examples/sec; 0.496 sec/batch)
2016-07-14 22:27:47.521061: step 410, loss = 7049.63 (272.0 examples/sec; 0.471 sec/batch)
2016-07-14 22:27:52.278792: step 420, loss = 6993.33 (281.8 examples/sec; 0.454 sec/batch)
2016-07-14 22:27:57.045808: step 430, loss = 6936.33 (263.0 examples/sec; 0.487 sec/batch)
2016-07-14 22:28:03.107053: step 440, loss = 6881.97 (201.9 examples/sec; 0.634 sec/batch)
2016-07-14 22:28:08.758031: step 450, loss = 6826.73 (263.7 examples/sec; 0.485 sec/batch)
2016-07-14 22:28:13.444273: step 460, loss = 6773.16 (285.7 examples/sec; 0.448 sec/batch)
2016-07-14 22:28:18.256520: step 470, loss = 6718.43 (266.5 examples/sec; 0.480 sec/batch)
2016-07-14 22:28:22.900369: step 480, loss = 6664.64 (275.3 examples/sec; 0.465 sec/batch)
2016-07-14 22:28:27.675363: step 490, loss = 6612.18 (270.5 examples/sec; 0.473 sec/batch)
2016-07-14 22:28:32.365075: step 500, loss = 6559.45 (254.7 examples/sec; 0.503 sec/batch)
2016-07-14 22:28:38.029514: step 510, loss = 6507.08 (280.7 examples/sec; 0.456 sec/batch)
2016-07-14 22:28:42.785731: step 520, loss = 6454.88 (268.2 examples/sec; 0.477 sec/batch)
2016-07-14 22:28:47.457807: step 530, loss = 6404.50 (253.8 examples/sec; 0.504 sec/batch)
2016-07-14 22:28:52.888713: step 540, loss = 6352.75 (183.1 examples/sec; 0.699 sec/batch)
2016-07-14 22:28:59.114128: step 550, loss = 6301.60 (244.2 examples/sec; 0.524 sec/batch)
2016-07-14 22:29:03.916585: step 560, loss = 6252.04 (277.8 examples/sec; 0.461 sec/batch)
2016-07-14 22:29:08.625933: step 570, loss = 6201.82 (263.2 examples/sec; 0.486 sec/batch)
2016-07-14 22:29:13.355482: step 580, loss = 6153.54 (281.6 examples/sec; 0.455 sec/batch)
2016-07-14 22:29:18.133965: step 590, loss = 6103.15 (269.6 examples/sec; 0.475 sec/batch)
2016-07-14 22:29:22.877247: step 600, loss = 6054.85 (257.6 examples/sec; 0.497 sec/batch)
2016-07-14 22:29:30.475278: step 610, loss = 6006.85 (245.8 examples/sec; 0.521 sec/batch)
2016-07-14 22:29:36.154306: step 620, loss = 5958.76 (282.2 examples/sec; 0.454 sec/batch)
2016-07-14 22:29:40.741656: step 630, loss = 5911.51 (272.0 examples/sec; 0.471 sec/batch)
2016-07-14 22:29:46.222831: step 640, loss = 5864.53 (205.8 examples/sec; 0.622 sec/batch)
2016-07-14 22:29:51.193515: step 650, loss = 5818.53 (276.1 examples/sec; 0.464 sec/batch)
2016-07-14 22:29:55.778447: step 660, loss = 5771.19 (279.3 examples/sec; 0.458 sec/batch)
2016-07-14 22:30:00.353641: step 670, loss = 5725.04 (280.9 examples/sec; 0.456 sec/batch)
2016-07-14 22:30:05.636832: step 680, loss = 5679.50 (212.2 examples/sec; 0.603 sec/batch)
2016-07-14 22:30:10.775067: step 690, loss = 5634.45 (269.4 examples/sec; 0.475 sec/batch)
2016-07-14 22:30:15.413197: step 700, loss = 5589.98 (269.2 examples/sec; 0.475 sec/batch)
2016-07-14 22:30:21.192144: step 710, loss = 5544.47 (280.3 examples/sec; 0.457 sec/batch)
2016-07-14 22:30:25.960130: step 720, loss = 5501.81 (264.2 examples/sec; 0.485 sec/batch)
2016-07-14 22:30:30.616616: step 730, loss = 5457.27 (273.8 examples/sec; 0.468 sec/batch)
2016-07-14 22:30:35.713605: step 740, loss = 5412.89 (192.3 examples/sec; 0.666 sec/batch)
2016-07-14 22:30:42.212790: step 750, loss = 5370.19 (205.8 examples/sec; 0.622 sec/batch)
2016-07-14 22:30:46.901781: step 760, loss = 5327.51 (273.8 examples/sec; 0.468 sec/batch)
2016-07-14 22:30:51.467127: step 770, loss = 5285.94 (279.3 examples/sec; 0.458 sec/batch)
2016-07-14 22:30:56.095356: step 780, loss = 5243.03 (276.6 examples/sec; 0.463 sec/batch)
2016-07-14 22:31:00.674136: step 790, loss = 5201.70 (283.0 examples/sec; 0.452 sec/batch)
2016-07-14 22:31:06.116664: step 800, loss = 5160.15 (200.1 examples/sec; 0.640 sec/batch)
2016-07-14 22:31:12.162692: step 810, loss = 5118.62 (280.0 examples/sec; 0.457 sec/batch)
2016-07-14 22:31:16.733108: step 820, loss = 5078.12 (282.4 examples/sec; 0.453 sec/batch)
2016-07-14 22:31:21.350479: step 830, loss = 5037.69 (275.4 examples/sec; 0.465 sec/batch)
2016-07-14 22:31:27.031764: step 840, loss = 4997.10 (204.3 examples/sec; 0.627 sec/batch)
2016-07-14 22:31:31.816286: step 850, loss = 4957.86 (279.4 examples/sec; 0.458 sec/batch)
2016-07-14 22:31:36.490739: step 860, loss = 4917.90 (270.3 examples/sec; 0.474 sec/batch)
2016-07-14 22:31:41.262388: step 870, loss = 4879.77 (276.5 examples/sec; 0.463 sec/batch)
2016-07-14 22:31:45.896577: step 880, loss = 4839.74 (278.8 examples/sec; 0.459 sec/batch)
2016-07-14 22:31:50.564317: step 890, loss = 4801.25 (287.8 examples/sec; 0.445 sec/batch)
2016-07-14 22:31:56.252176: step 900, loss = 4763.00 (259.2 examples/sec; 0.494 sec/batch)
2016-07-14 22:32:03.018548: step 910, loss = 4725.23 (207.0 examples/sec; 0.618 sec/batch)
2016-07-14 22:32:07.796970: step 920, loss = 4688.26 (279.8 examples/sec; 0.457 sec/batch)
2016-07-14 22:32:12.507437: step 930, loss = 4649.96 (261.1 examples/sec; 0.490 sec/batch)
2016-07-14 22:32:17.202470: step 940, loss = 4613.85 (277.2 examples/sec; 0.462 sec/batch)
2016-07-14 22:32:21.970716: step 950, loss = 4576.55 (256.6 examples/sec; 0.499 sec/batch)
2016-07-14 22:32:26.645842: step 960, loss = 4539.70 (278.5 examples/sec; 0.460 sec/batch)
2016-07-14 22:32:31.457085: step 970, loss = 4503.93 (271.0 examples/sec; 0.472 sec/batch)
2016-07-14 22:32:36.135858: step 980, loss = 4467.87 (267.6 examples/sec; 0.478 sec/batch)
2016-07-14 22:32:40.901503: step 990, loss = 4433.22 (278.5 examples/sec; 0.460 sec/batch)
2016-07-14 22:32:45.623599: step 1000, loss = 4396.82 (270.2 examples/sec; 0.474 sec/batch)
2016-07-14 22:32:52.931461: step 1010, loss = 4361.98 (211.4 examples/sec; 0.605 sec/batch)
2016-07-14 22:32:58.285925: step 1020, loss = 4327.37 (257.2 examples/sec; 0.498 sec/batch)
2016-07-14 22:33:03.984837: step 1030, loss = 4292.68 (265.5 examples/sec; 0.482 sec/batch)
2016-07-14 22:33:08.734516: step 1040, loss = 4258.52 (277.4 examples/sec; 0.461 sec/batch)
2016-07-14 22:33:13.280648: step 1050, loss = 4224.45 (280.3 examples/sec; 0.457 sec/batch)
2016-07-14 22:33:17.872774: step 1060, loss = 4191.69 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 22:33:23.571148: step 1070, loss = 4157.69 (260.8 examples/sec; 0.491 sec/batch)
2016-07-14 22:33:28.314021: step 1080, loss = 4124.08 (283.2 examples/sec; 0.452 sec/batch)
2016-07-14 22:33:33.077245: step 1090, loss = 4091.23 (267.4 examples/sec; 0.479 sec/batch)
2016-07-14 22:33:37.749519: step 1100, loss = 4058.77 (284.0 examples/sec; 0.451 sec/batch)
2016-07-14 22:33:43.485363: step 1110, loss = 4027.47 (286.4 examples/sec; 0.447 sec/batch)
2016-07-14 22:33:48.159691: step 1120, loss = 3994.43 (260.9 examples/sec; 0.491 sec/batch)
2016-07-14 22:33:52.910508: step 1130, loss = 3963.23 (282.0 examples/sec; 0.454 sec/batch)
2016-07-14 22:33:57.748595: step 1140, loss = 3931.15 (264.0 examples/sec; 0.485 sec/batch)
2016-07-14 22:34:03.891083: step 1150, loss = 3899.57 (201.4 examples/sec; 0.636 sec/batch)
2016-07-14 22:34:09.291115: step 1160, loss = 3869.39 (284.1 examples/sec; 0.451 sec/batch)
2016-07-14 22:34:13.909804: step 1170, loss = 3837.77 (276.0 examples/sec; 0.464 sec/batch)
2016-07-14 22:34:19.581946: step 1180, loss = 3807.26 (270.0 examples/sec; 0.474 sec/batch)
2016-07-14 22:34:24.318300: step 1190, loss = 3777.44 (278.6 examples/sec; 0.459 sec/batch)
2016-07-14 22:34:28.893929: step 1200, loss = 3746.96 (289.4 examples/sec; 0.442 sec/batch)
2016-07-14 22:34:34.575316: step 1210, loss = 3717.69 (266.1 examples/sec; 0.481 sec/batch)
2016-07-14 22:34:39.328540: step 1220, loss = 3687.31 (263.1 examples/sec; 0.487 sec/batch)
2016-07-14 22:34:45.183228: step 1230, loss = 3657.84 (188.3 examples/sec; 0.680 sec/batch)
2016-07-14 22:34:50.578157: step 1240, loss = 3629.51 (259.6 examples/sec; 0.493 sec/batch)
2016-07-14 22:34:55.224198: step 1250, loss = 3600.42 (275.0 examples/sec; 0.465 sec/batch)
2016-07-14 22:34:59.989270: step 1260, loss = 3571.68 (277.6 examples/sec; 0.461 sec/batch)
2016-07-14 22:35:04.644773: step 1270, loss = 3542.83 (269.3 examples/sec; 0.475 sec/batch)
2016-07-14 22:35:09.342721: step 1280, loss = 3515.51 (276.0 examples/sec; 0.464 sec/batch)
2016-07-14 22:35:14.100194: step 1290, loss = 3487.08 (257.8 examples/sec; 0.496 sec/batch)
2016-07-14 22:35:18.735613: step 1300, loss = 3459.17 (283.6 examples/sec; 0.451 sec/batch)
2016-07-14 22:35:24.540866: step 1310, loss = 3431.39 (274.8 examples/sec; 0.466 sec/batch)
2016-07-14 22:35:29.299834: step 1320, loss = 3403.89 (249.9 examples/sec; 0.512 sec/batch)
2016-07-14 22:35:35.274080: step 1330, loss = 3377.70 (192.5 examples/sec; 0.665 sec/batch)
2016-07-14 22:35:40.928610: step 1340, loss = 3350.11 (265.1 examples/sec; 0.483 sec/batch)
2016-07-14 22:35:45.643987: step 1350, loss = 3323.51 (279.6 examples/sec; 0.458 sec/batch)
2016-07-14 22:35:50.431557: step 1360, loss = 3297.47 (265.7 examples/sec; 0.482 sec/batch)
2016-07-14 22:35:56.778266: step 1370, loss = 3270.65 (203.3 examples/sec; 0.630 sec/batch)
2016-07-14 22:36:02.137923: step 1380, loss = 3245.15 (264.4 examples/sec; 0.484 sec/batch)
2016-07-14 22:36:06.784923: step 1390, loss = 3218.42 (282.2 examples/sec; 0.454 sec/batch)
2016-07-14 22:36:11.555406: step 1400, loss = 3194.06 (282.2 examples/sec; 0.454 sec/batch)
2016-07-14 22:36:17.167834: step 1410, loss = 3167.40 (266.9 examples/sec; 0.479 sec/batch)
2016-07-14 22:36:21.857877: step 1420, loss = 3142.43 (278.5 examples/sec; 0.460 sec/batch)
2016-07-14 22:36:26.613073: step 1430, loss = 3117.89 (269.6 examples/sec; 0.475 sec/batch)
2016-07-14 22:36:31.252159: step 1440, loss = 3092.69 (272.6 examples/sec; 0.470 sec/batch)
2016-07-14 22:36:36.049112: step 1450, loss = 3067.97 (281.2 examples/sec; 0.455 sec/batch)
2016-07-14 22:36:40.757098: step 1460, loss = 3043.50 (264.3 examples/sec; 0.484 sec/batch)
2016-07-14 22:36:45.467247: step 1470, loss = 3020.13 (281.1 examples/sec; 0.455 sec/batch)
2016-07-14 22:36:50.234942: step 1480, loss = 2995.41 (270.2 examples/sec; 0.474 sec/batch)
2016-07-14 22:36:54.870023: step 1490, loss = 2971.23 (277.3 examples/sec; 0.462 sec/batch)
2016-07-14 22:36:59.642063: step 1500, loss = 2948.19 (274.9 examples/sec; 0.466 sec/batch)
2016-07-14 22:37:05.285846: step 1510, loss = 2924.24 (264.9 examples/sec; 0.483 sec/batch)
2016-07-14 22:37:10.023590: step 1520, loss = 2901.59 (278.4 examples/sec; 0.460 sec/batch)
2016-07-14 22:37:14.778791: step 1530, loss = 2877.66 (266.3 examples/sec; 0.481 sec/batch)
2016-07-14 22:37:19.450658: step 1540, loss = 2855.43 (262.9 examples/sec; 0.487 sec/batch)
2016-07-14 22:37:24.227089: step 1550, loss = 2832.58 (288.7 examples/sec; 0.443 sec/batch)
2016-07-14 22:37:28.892130: step 1560, loss = 2809.58 (273.4 examples/sec; 0.468 sec/batch)
2016-07-14 22:37:34.627458: step 1570, loss = 2787.36 (193.1 examples/sec; 0.663 sec/batch)
2016-07-14 22:37:40.501171: step 1580, loss = 2765.01 (268.7 examples/sec; 0.476 sec/batch)
2016-07-14 22:37:45.870101: step 1590, loss = 2742.84 (208.0 examples/sec; 0.615 sec/batch)
2016-07-14 22:37:50.974999: step 1600, loss = 2721.61 (260.4 examples/sec; 0.492 sec/batch)
2016-07-14 22:37:57.765243: step 1610, loss = 2699.22 (265.2 examples/sec; 0.483 sec/batch)
2016-07-14 22:38:02.428634: step 1620, loss = 2678.76 (278.4 examples/sec; 0.460 sec/batch)
2016-07-14 22:38:07.202694: step 1630, loss = 2656.59 (260.4 examples/sec; 0.492 sec/batch)
2016-07-14 22:38:11.800211: step 1640, loss = 2635.10 (268.6 examples/sec; 0.477 sec/batch)
2016-07-14 22:38:16.562929: step 1650, loss = 2614.86 (273.2 examples/sec; 0.469 sec/batch)
2016-07-14 22:38:21.273180: step 1660, loss = 2594.06 (264.6 examples/sec; 0.484 sec/batch)
2016-07-14 22:38:25.913386: step 1670, loss = 2573.61 (283.6 examples/sec; 0.451 sec/batch)
2016-07-14 22:38:30.684648: step 1680, loss = 2552.16 (261.6 examples/sec; 0.489 sec/batch)
2016-07-14 22:38:35.297554: step 1690, loss = 2532.86 (279.3 examples/sec; 0.458 sec/batch)
2016-07-14 22:38:39.834160: step 1700, loss = 2512.02 (276.5 examples/sec; 0.463 sec/batch)
2016-07-14 22:38:46.162004: step 1710, loss = 2491.69 (203.8 examples/sec; 0.628 sec/batch)
2016-07-14 22:38:51.276477: step 1720, loss = 2472.61 (261.2 examples/sec; 0.490 sec/batch)
2016-07-14 22:38:55.892102: step 1730, loss = 2452.19 (263.4 examples/sec; 0.486 sec/batch)
2016-07-14 22:39:00.625851: step 1740, loss = 2433.82 (283.8 examples/sec; 0.451 sec/batch)
2016-07-14 22:39:05.343371: step 1750, loss = 2413.43 (265.9 examples/sec; 0.481 sec/batch)
2016-07-14 22:39:10.064041: step 1760, loss = 2394.32 (279.8 examples/sec; 0.457 sec/batch)
2016-07-14 22:39:14.823157: step 1770, loss = 2375.79 (268.4 examples/sec; 0.477 sec/batch)
2016-07-14 22:39:19.434438: step 1780, loss = 2356.43 (275.1 examples/sec; 0.465 sec/batch)
2016-07-14 22:39:24.202384: step 1790, loss = 2337.65 (277.5 examples/sec; 0.461 sec/batch)
2016-07-14 22:39:28.846848: step 1800, loss = 2319.39 (266.4 examples/sec; 0.480 sec/batch)
2016-07-14 22:39:34.542237: step 1810, loss = 2301.75 (273.2 examples/sec; 0.469 sec/batch)
2016-07-14 22:39:39.293461: step 1820, loss = 2282.15 (261.3 examples/sec; 0.490 sec/batch)
2016-07-14 22:39:43.930329: step 1830, loss = 2264.05 (266.6 examples/sec; 0.480 sec/batch)
2016-07-14 22:39:48.702482: step 1840, loss = 2246.20 (271.7 examples/sec; 0.471 sec/batch)
2016-07-14 22:39:53.409673: step 1850, loss = 2228.19 (265.1 examples/sec; 0.483 sec/batch)
2016-07-14 22:39:58.099539: step 1860, loss = 2211.12 (284.5 examples/sec; 0.450 sec/batch)
2016-07-14 22:40:02.909029: step 1870, loss = 2192.65 (267.1 examples/sec; 0.479 sec/batch)
2016-07-14 22:40:07.502964: step 1880, loss = 2175.63 (279.5 examples/sec; 0.458 sec/batch)
2016-07-14 22:40:12.258649: step 1890, loss = 2158.43 (276.2 examples/sec; 0.463 sec/batch)
2016-07-14 22:40:17.004174: step 1900, loss = 2140.85 (264.7 examples/sec; 0.484 sec/batch)
2016-07-14 22:40:22.603707: step 1910, loss = 2123.78 (280.7 examples/sec; 0.456 sec/batch)
2016-07-14 22:40:27.364501: step 1920, loss = 2107.02 (271.5 examples/sec; 0.471 sec/batch)
2016-07-14 22:40:32.035298: step 1930, loss = 2092.42 (259.5 examples/sec; 0.493 sec/batch)
2016-07-14 22:40:37.606128: step 1940, loss = 2074.83 (190.4 examples/sec; 0.672 sec/batch)
2016-07-14 22:40:43.809265: step 1950, loss = 2058.85 (248.8 examples/sec; 0.515 sec/batch)
2016-07-14 22:40:48.537474: step 1960, loss = 2042.43 (284.6 examples/sec; 0.450 sec/batch)
2016-07-14 22:40:53.098952: step 1970, loss = 2025.72 (283.9 examples/sec; 0.451 sec/batch)
2016-07-14 22:40:57.624960: step 1980, loss = 2009.74 (278.9 examples/sec; 0.459 sec/batch)
2016-07-14 22:41:02.226412: step 1990, loss = 1993.82 (276.0 examples/sec; 0.464 sec/batch)
2016-07-14 22:41:07.828367: step 2000, loss = 1977.65 (263.7 examples/sec; 0.485 sec/batch)
2016-07-14 22:41:13.526885: step 2010, loss = 1962.62 (275.2 examples/sec; 0.465 sec/batch)
2016-07-14 22:41:18.071282: step 2020, loss = 1946.31 (286.3 examples/sec; 0.447 sec/batch)
2016-07-14 22:41:22.603163: step 2030, loss = 1931.68 (282.1 examples/sec; 0.454 sec/batch)
2016-07-14 22:41:27.208884: step 2040, loss = 1915.44 (277.4 examples/sec; 0.461 sec/batch)
2016-07-14 22:41:32.808689: step 2050, loss = 1900.01 (282.9 examples/sec; 0.452 sec/batch)
2016-07-14 22:41:37.349501: step 2060, loss = 1884.92 (286.5 examples/sec; 0.447 sec/batch)
2016-07-14 22:41:42.794453: step 2070, loss = 1870.05 (209.9 examples/sec; 0.610 sec/batch)
2016-07-14 22:41:47.756070: step 2080, loss = 1855.77 (275.4 examples/sec; 0.465 sec/batch)
2016-07-14 22:41:52.321370: step 2090, loss = 1839.81 (274.4 examples/sec; 0.466 sec/batch)
2016-07-14 22:41:56.873878: step 2100, loss = 1825.60 (282.8 examples/sec; 0.453 sec/batch)
2016-07-14 22:42:03.436190: step 2110, loss = 1811.51 (209.3 examples/sec; 0.612 sec/batch)
2016-07-14 22:42:08.460481: step 2120, loss = 1796.52 (207.2 examples/sec; 0.618 sec/batch)
2016-07-14 22:42:13.956070: step 2130, loss = 1782.69 (263.5 examples/sec; 0.486 sec/batch)
2016-07-14 22:42:18.616718: step 2140, loss = 1768.42 (272.3 examples/sec; 0.470 sec/batch)
2016-07-14 22:42:23.120213: step 2150, loss = 1755.29 (285.0 examples/sec; 0.449 sec/batch)
2016-07-14 22:42:27.869618: step 2160, loss = 1740.52 (231.5 examples/sec; 0.553 sec/batch)
2016-07-14 22:42:33.460408: step 2170, loss = 1726.45 (273.0 examples/sec; 0.469 sec/batch)
2016-07-14 22:42:38.174266: step 2180, loss = 1712.95 (275.4 examples/sec; 0.465 sec/batch)
2016-07-14 22:42:42.751816: step 2190, loss = 1699.35 (280.1 examples/sec; 0.457 sec/batch)
2016-07-14 22:42:47.392207: step 2200, loss = 1686.11 (280.7 examples/sec; 0.456 sec/batch)
2016-07-14 22:42:54.150489: step 2210, loss = 1671.97 (269.5 examples/sec; 0.475 sec/batch)
2016-07-14 22:42:58.818111: step 2220, loss = 1659.63 (277.8 examples/sec; 0.461 sec/batch)
2016-07-14 22:43:03.599923: step 2230, loss = 1645.88 (260.4 examples/sec; 0.492 sec/batch)
2016-07-14 22:43:08.253749: step 2240, loss = 1632.35 (268.7 examples/sec; 0.476 sec/batch)
2016-07-14 22:43:12.985786: step 2250, loss = 1619.64 (279.1 examples/sec; 0.459 sec/batch)
2016-07-14 22:43:17.697194: step 2260, loss = 1606.53 (272.0 examples/sec; 0.471 sec/batch)
2016-07-14 22:43:23.639713: step 2270, loss = 1594.71 (193.7 examples/sec; 0.661 sec/batch)
2016-07-14 22:43:29.306793: step 2280, loss = 1581.25 (268.1 examples/sec; 0.477 sec/batch)
2016-07-14 22:43:34.008745: step 2290, loss = 1568.43 (275.9 examples/sec; 0.464 sec/batch)
2016-07-14 22:43:38.746337: step 2300, loss = 1556.50 (269.6 examples/sec; 0.475 sec/batch)
2016-07-14 22:43:44.364516: step 2310, loss = 1543.43 (258.9 examples/sec; 0.494 sec/batch)
2016-07-14 22:43:49.825433: step 2320, loss = 1531.84 (188.1 examples/sec; 0.680 sec/batch)
2016-07-14 22:43:55.986566: step 2330, loss = 1519.18 (260.7 examples/sec; 0.491 sec/batch)
2016-07-14 22:44:00.779697: step 2340, loss = 1506.97 (273.2 examples/sec; 0.468 sec/batch)
2016-07-14 22:44:05.331608: step 2350, loss = 1495.60 (289.6 examples/sec; 0.442 sec/batch)
2016-07-14 22:44:09.806879: step 2360, loss = 1483.18 (285.7 examples/sec; 0.448 sec/batch)
2016-07-14 22:44:14.391639: step 2370, loss = 1472.38 (275.4 examples/sec; 0.465 sec/batch)
2016-07-14 22:44:20.041401: step 2380, loss = 1459.91 (258.9 examples/sec; 0.494 sec/batch)
2016-07-14 22:44:25.223748: step 2390, loss = 1447.90 (205.7 examples/sec; 0.622 sec/batch)
2016-07-14 22:44:30.606669: step 2400, loss = 1436.29 (258.1 examples/sec; 0.496 sec/batch)
2016-07-14 22:44:36.227449: step 2410, loss = 1425.61 (268.7 examples/sec; 0.476 sec/batch)

EFFICIENT LEARNING RATE: 0.001 - Actual Model:

python cifar10_train.py 
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GT 650M
major: 3 minor: 0 memoryClockRate (GHz) 0.835
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.85GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)
2016-07-15 07:29:50.763138: step 0, loss = 8.68 (38.2 examples/sec; 3.351 sec/batch)
2016-07-15 07:29:56.671771: step 10, loss = 8.69 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 07:30:01.575967: step 20, loss = 8.68 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 07:30:06.565942: step 30, loss = 8.68 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 07:30:11.388661: step 40, loss = 8.69 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 07:30:16.167700: step 50, loss = 8.68 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 07:30:20.952754: step 60, loss = 8.68 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 07:30:25.763890: step 70, loss = 8.68 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 07:30:30.545073: step 80, loss = 8.68 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 07:30:35.330556: step 90, loss = 8.69 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 07:30:40.116371: step 100, loss = 8.68 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 07:30:45.953774: step 110, loss = 8.68 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 07:30:50.805512: step 120, loss = 8.68 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 07:30:55.599289: step 130, loss = 8.68 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 07:31:00.334783: step 140, loss = 8.68 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 07:31:05.149448: step 150, loss = 8.69 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 07:31:09.926295: step 160, loss = 8.68 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 07:31:15.777578: step 170, loss = 8.68 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 07:31:20.659963: step 180, loss = 8.67 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 07:31:26.102559: step 190, loss = 8.68 (183.0 examples/sec; 0.699 sec/batch)
2016-07-15 07:31:31.577643: step 200, loss = 8.68 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 07:31:37.501353: step 210, loss = 8.68 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 07:31:42.277027: step 220, loss = 8.68 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 07:31:47.029596: step 230, loss = 8.67 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 07:31:51.726905: step 240, loss = 8.68 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 07:31:56.393783: step 250, loss = 8.68 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 07:32:01.178453: step 260, loss = 8.67 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 07:32:05.948696: step 270, loss = 8.68 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 07:32:11.852521: step 280, loss = 8.67 (190.6 examples/sec; 0.671 sec/batch)
2016-07-15 07:32:16.740282: step 290, loss = 8.67 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 07:32:22.057165: step 300, loss = 8.68 (194.9 examples/sec; 0.657 sec/batch)
2016-07-15 07:32:29.048388: step 310, loss = 8.68 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 07:32:33.888988: step 320, loss = 8.67 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 07:32:38.651942: step 330, loss = 8.67 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 07:32:44.345421: step 340, loss = 8.67 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 07:32:48.944989: step 350, loss = 8.67 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 07:32:53.563292: step 360, loss = 8.67 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 07:32:58.139177: step 370, loss = 8.67 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 07:33:02.778490: step 380, loss = 8.67 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 07:33:08.513390: step 390, loss = 8.67 (222.2 examples/sec; 0.576 sec/batch)
2016-07-15 07:33:13.109900: step 400, loss = 8.66 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 07:33:19.540985: step 410, loss = 8.67 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 07:33:24.342198: step 420, loss = 8.67 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 07:33:28.984598: step 430, loss = 8.67 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 07:33:33.654713: step 440, loss = 8.66 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 07:33:38.344485: step 450, loss = 8.66 (270.9 examples/sec; 0.473 sec/batch)
2016-07-15 07:33:43.024706: step 460, loss = 8.66 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 07:33:47.677190: step 470, loss = 8.67 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 07:33:53.462408: step 480, loss = 8.67 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 07:33:58.281600: step 490, loss = 8.67 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 07:34:03.031018: step 500, loss = 8.67 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 07:34:08.733938: step 510, loss = 8.66 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 07:34:13.368531: step 520, loss = 8.67 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 07:34:18.644796: step 530, loss = 8.66 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 07:34:23.979311: step 540, loss = 8.67 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 07:34:29.748450: step 550, loss = 8.66 (253.0 examples/sec; 0.506 sec/batch)
2016-07-15 07:34:34.550060: step 560, loss = 8.66 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 07:34:39.164607: step 570, loss = 8.66 (283.2 examples/sec; 0.452 sec/batch)
2016-07-15 07:34:43.850191: step 580, loss = 8.66 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 07:34:48.498579: step 590, loss = 8.66 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 07:34:53.513128: step 600, loss = 8.66 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 07:35:00.277081: step 610, loss = 8.66 (249.6 examples/sec; 0.513 sec/batch)
2016-07-15 07:35:05.000254: step 620, loss = 8.66 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 07:35:09.773891: step 630, loss = 8.66 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 07:35:14.726105: step 640, loss = 8.66 (249.8 examples/sec; 0.512 sec/batch)
2016-07-15 07:35:19.638087: step 650, loss = 8.66 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 07:35:24.821532: step 660, loss = 8.66 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 07:35:29.955982: step 670, loss = 8.65 (198.6 examples/sec; 0.645 sec/batch)
2016-07-15 07:35:35.236793: step 680, loss = 8.65 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 07:35:39.992761: step 690, loss = 8.66 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 07:35:44.659907: step 700, loss = 8.66 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 07:35:50.270582: step 710, loss = 8.65 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 07:35:54.899653: step 720, loss = 8.66 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 07:36:00.734160: step 730, loss = 8.65 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 07:36:05.598570: step 740, loss = 8.66 (281.6 examples/sec; 0.454 sec/batch)
2016-07-15 07:36:10.277623: step 750, loss = 8.65 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 07:36:14.969909: step 760, loss = 8.65 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 07:36:20.663511: step 770, loss = 8.65 (220.2 examples/sec; 0.581 sec/batch)
2016-07-15 07:36:25.291207: step 780, loss = 8.65 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 07:36:30.434076: step 790, loss = 8.65 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 07:36:35.749484: step 800, loss = 8.65 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 07:36:41.458882: step 810, loss = 8.65 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 07:36:46.105410: step 820, loss = 8.66 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 07:36:50.808025: step 830, loss = 8.65 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 07:36:55.565622: step 840, loss = 8.65 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 07:37:00.241633: step 850, loss = 8.65 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 07:37:04.994639: step 860, loss = 8.65 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 07:37:09.661563: step 870, loss = 8.65 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 07:37:14.296086: step 880, loss = 8.65 (279.8 examples/sec; 0.458 sec/batch)
2016-07-15 07:37:18.925082: step 890, loss = 8.65 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 07:37:24.673077: step 900, loss = 8.64 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 07:37:30.567780: step 910, loss = 8.65 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 07:37:35.373841: step 920, loss = 8.64 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 07:37:41.639991: step 930, loss = 8.64 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 07:37:47.175204: step 940, loss = 8.65 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 07:37:51.924896: step 950, loss = 8.64 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 07:37:56.767109: step 960, loss = 8.65 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 07:38:01.471153: step 970, loss = 8.64 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 07:38:06.286384: step 980, loss = 8.64 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 07:38:11.071995: step 990, loss = 8.65 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 07:38:15.850510: step 1000, loss = 8.64 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 07:38:21.637818: step 1010, loss = 8.64 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 07:38:26.385843: step 1020, loss = 8.64 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 07:38:32.305513: step 1030, loss = 8.64 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 07:38:38.184038: step 1040, loss = 8.64 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 07:38:42.999820: step 1050, loss = 8.64 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 07:38:47.852837: step 1060, loss = 8.64 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 07:38:54.260076: step 1070, loss = 8.64 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 07:38:59.689284: step 1080, loss = 8.64 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 07:39:04.414643: step 1090, loss = 8.64 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 07:39:09.253835: step 1100, loss = 8.63 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 07:39:14.896863: step 1110, loss = 8.64 (283.2 examples/sec; 0.452 sec/batch)
2016-07-15 07:39:19.581312: step 1120, loss = 8.63 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 07:39:24.248754: step 1130, loss = 8.63 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 07:39:29.037132: step 1140, loss = 8.63 (221.9 examples/sec; 0.577 sec/batch)
2016-07-15 07:39:34.739044: step 1150, loss = 8.63 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 07:39:39.508311: step 1160, loss = 8.63 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 07:39:44.348476: step 1170, loss = 8.63 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 07:39:49.096295: step 1180, loss = 8.63 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 07:39:53.734772: step 1190, loss = 8.63 (280.4 examples/sec; 0.457 sec/batch)
2016-07-15 07:39:59.068644: step 1200, loss = 8.63 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 07:40:05.350255: step 1210, loss = 8.63 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 07:40:10.064552: step 1220, loss = 8.63 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 07:40:14.727337: step 1230, loss = 8.63 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 07:40:20.372822: step 1240, loss = 8.63 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 07:40:25.504142: step 1250, loss = 8.63 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 07:40:30.981797: step 1260, loss = 8.63 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 07:40:35.637612: step 1270, loss = 8.62 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 07:40:41.381182: step 1280, loss = 8.63 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 07:40:46.716592: step 1290, loss = 8.63 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 07:40:52.094410: step 1300, loss = 8.63 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 07:40:57.786522: step 1310, loss = 8.63 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 07:41:02.608487: step 1320, loss = 8.63 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 07:41:07.440687: step 1330, loss = 8.63 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 07:41:12.203898: step 1340, loss = 8.62 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 07:41:16.830631: step 1350, loss = 8.62 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 07:41:21.532288: step 1360, loss = 8.63 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 07:41:26.157674: step 1370, loss = 8.62 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 07:41:31.529067: step 1380, loss = 8.63 (199.5 examples/sec; 0.642 sec/batch)
2016-07-15 07:41:36.702066: step 1390, loss = 8.62 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 07:41:41.457090: step 1400, loss = 8.62 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 07:41:47.300632: step 1410, loss = 8.62 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 07:41:51.942067: step 1420, loss = 8.62 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 07:41:56.610261: step 1430, loss = 8.63 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 07:42:02.354434: step 1440, loss = 8.62 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 07:42:07.032168: step 1450, loss = 8.62 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 07:42:12.691412: step 1460, loss = 8.62 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 07:42:17.840653: step 1470, loss = 8.61 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 07:42:23.394272: step 1480, loss = 8.61 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 07:42:28.101331: step 1490, loss = 8.61 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 07:42:32.996337: step 1500, loss = 8.62 (241.0 examples/sec; 0.531 sec/batch)
2016-07-15 07:42:40.058771: step 1510, loss = 8.61 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 07:42:44.719764: step 1520, loss = 8.62 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 07:42:49.311612: step 1530, loss = 8.61 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 07:42:54.475048: step 1540, loss = 8.61 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 07:42:59.874518: step 1550, loss = 8.61 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 07:43:04.592116: step 1560, loss = 8.61 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 07:43:09.686012: step 1570, loss = 8.62 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 07:43:15.384152: step 1580, loss = 8.61 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 07:43:20.094443: step 1590, loss = 8.61 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 07:43:25.819716: step 1600, loss = 8.61 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 07:43:31.610884: step 1610, loss = 8.60 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 07:43:36.429178: step 1620, loss = 8.60 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 07:43:41.103739: step 1630, loss = 8.61 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 07:43:46.474342: step 1640, loss = 8.61 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 07:43:52.959587: step 1650, loss = 8.61 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 07:43:57.783558: step 1660, loss = 8.61 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 07:44:02.557884: step 1670, loss = 8.61 (243.2 examples/sec; 0.526 sec/batch)
2016-07-15 07:44:08.451847: step 1680, loss = 8.61 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 07:44:13.329751: step 1690, loss = 8.61 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 07:44:18.006184: step 1700, loss = 8.60 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 07:44:23.586893: step 1710, loss = 8.60 (286.7 examples/sec; 0.447 sec/batch)
2016-07-15 07:44:28.285201: step 1720, loss = 8.60 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 07:44:34.063257: step 1730, loss = 8.60 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 07:44:38.920034: step 1740, loss = 8.61 (250.7 examples/sec; 0.510 sec/batch)
2016-07-15 07:44:43.717941: step 1750, loss = 8.60 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 07:44:49.934706: step 1760, loss = 8.60 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 07:44:55.507302: step 1770, loss = 8.60 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 07:45:00.244883: step 1780, loss = 8.60 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 07:45:05.146011: step 1790, loss = 8.60 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 07:45:09.841935: step 1800, loss = 8.60 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 07:45:15.634149: step 1810, loss = 8.60 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 07:45:20.430328: step 1820, loss = 8.59 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 07:45:25.140912: step 1830, loss = 8.60 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 07:45:29.710353: step 1840, loss = 8.60 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 07:45:34.355716: step 1850, loss = 8.60 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 07:45:38.950612: step 1860, loss = 8.61 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 07:45:44.550224: step 1870, loss = 8.60 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 07:45:49.501398: step 1880, loss = 8.59 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 07:45:54.229468: step 1890, loss = 8.60 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 07:45:59.040276: step 1900, loss = 8.60 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 07:46:04.604865: step 1910, loss = 8.59 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 07:46:09.406634: step 1920, loss = 8.60 (218.6 examples/sec; 0.585 sec/batch)
2016-07-15 07:46:15.277790: step 1930, loss = 8.59 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 07:46:21.186047: step 1940, loss = 8.58 (218.6 examples/sec; 0.586 sec/batch)
2016-07-15 07:46:26.021665: step 1950, loss = 8.59 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 07:46:30.834958: step 1960, loss = 8.59 (241.9 examples/sec; 0.529 sec/batch)
2016-07-15 07:46:35.566895: step 1970, loss = 8.59 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 07:46:40.364281: step 1980, loss = 8.59 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 07:46:45.092775: step 1990, loss = 8.58 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 07:46:49.911633: step 2000, loss = 8.59 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 07:46:55.661223: step 2010, loss = 8.58 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 07:47:00.393070: step 2020, loss = 8.58 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 07:47:05.287128: step 2030, loss = 8.58 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 07:47:09.971478: step 2040, loss = 8.58 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 07:47:14.636259: step 2050, loss = 8.59 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 07:47:19.252502: step 2060, loss = 8.58 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 07:47:23.841209: step 2070, loss = 8.57 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 07:47:28.512536: step 2080, loss = 8.58 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 07:47:34.245634: step 2090, loss = 8.58 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 07:47:39.741545: step 2100, loss = 8.59 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 07:47:45.828597: step 2110, loss = 8.58 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 07:47:50.469538: step 2120, loss = 8.59 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 07:47:55.140818: step 2130, loss = 8.58 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 07:48:00.761509: step 2140, loss = 8.57 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 07:48:05.948027: step 2150, loss = 8.59 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 07:48:11.382269: step 2160, loss = 8.58 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 07:48:16.050666: step 2170, loss = 8.58 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 07:48:20.679512: step 2180, loss = 8.57 (284.1 examples/sec; 0.451 sec/batch)
2016-07-15 07:48:25.372649: step 2190, loss = 8.57 (247.2 examples/sec; 0.518 sec/batch)
2016-07-15 07:48:31.077447: step 2200, loss = 8.57 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 07:48:36.905714: step 2210, loss = 8.58 (252.3 examples/sec; 0.507 sec/batch)
2016-07-15 07:48:41.769557: step 2220, loss = 8.58 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 07:48:46.463378: step 2230, loss = 8.57 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 07:48:51.086881: step 2240, loss = 8.57 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 07:48:56.607852: step 2250, loss = 8.57 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 07:49:01.581080: step 2260, loss = 8.57 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 07:49:06.300005: step 2270, loss = 8.58 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 07:49:11.955168: step 2280, loss = 8.55 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 07:49:18.045629: step 2290, loss = 8.56 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 07:49:22.852546: step 2300, loss = 8.57 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 07:49:28.411230: step 2310, loss = 8.55 (286.0 examples/sec; 0.448 sec/batch)
2016-07-15 07:49:33.100035: step 2320, loss = 8.57 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 07:49:38.852185: step 2330, loss = 8.55 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 07:49:44.380272: step 2340, loss = 8.56 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 07:49:49.407932: step 2350, loss = 8.57 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 07:49:54.143838: step 2360, loss = 8.55 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 07:49:58.826773: step 2370, loss = 8.58 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 07:50:04.248094: step 2380, loss = 8.56 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 07:50:09.354197: step 2390, loss = 8.56 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 07:50:14.054118: step 2400, loss = 8.56 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 07:50:19.631240: step 2410, loss = 8.55 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 07:50:25.338337: step 2420, loss = 8.56 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 07:50:30.200854: step 2430, loss = 8.55 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 07:50:35.046143: step 2440, loss = 8.56 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 07:50:39.839776: step 2450, loss = 8.55 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 07:50:44.491828: step 2460, loss = 8.55 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 07:50:49.124349: step 2470, loss = 8.55 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 07:50:53.761838: step 2480, loss = 8.55 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 07:50:58.795821: step 2490, loss = 8.54 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 07:51:04.267153: step 2500, loss = 8.56 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 07:51:09.945776: step 2510, loss = 8.54 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 07:51:14.765169: step 2520, loss = 8.55 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 07:51:19.538520: step 2530, loss = 8.54 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 07:51:24.292252: step 2540, loss = 8.56 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 07:51:28.933969: step 2550, loss = 8.54 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 07:51:33.712215: step 2560, loss = 8.54 (220.7 examples/sec; 0.580 sec/batch)
2016-07-15 07:51:39.404511: step 2570, loss = 8.54 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 07:51:44.153475: step 2580, loss = 8.55 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 07:51:49.044026: step 2590, loss = 8.55 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 07:51:55.476762: step 2600, loss = 8.54 (209.2 examples/sec; 0.612 sec/batch)
2016-07-15 07:52:01.976402: step 2610, loss = 8.53 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 07:52:06.693805: step 2620, loss = 8.54 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 07:52:12.501951: step 2630, loss = 8.54 (183.9 examples/sec; 0.696 sec/batch)
2016-07-15 07:52:17.510935: step 2640, loss = 8.53 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 07:52:23.064346: step 2650, loss = 8.53 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 07:52:28.449737: step 2660, loss = 8.54 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 07:52:34.205472: step 2670, loss = 8.54 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 07:52:39.025030: step 2680, loss = 8.54 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 07:52:43.679763: step 2690, loss = 8.52 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 07:52:48.376100: step 2700, loss = 8.55 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 07:52:53.949170: step 2710, loss = 8.53 (281.7 examples/sec; 0.454 sec/batch)
2016-07-15 07:52:59.030192: step 2720, loss = 8.52 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 07:53:04.435392: step 2730, loss = 8.54 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 07:53:09.143709: step 2740, loss = 8.53 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 07:53:14.336484: step 2750, loss = 8.53 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 07:53:20.904957: step 2760, loss = 8.52 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 07:53:26.018464: step 2770, loss = 8.53 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 07:53:30.673430: step 2780, loss = 8.53 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 07:53:35.276509: step 2790, loss = 8.52 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 07:53:39.917501: step 2800, loss = 8.51 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 07:53:45.533310: step 2810, loss = 8.53 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 07:53:50.206675: step 2820, loss = 8.53 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 07:53:54.854197: step 2830, loss = 8.52 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 07:53:59.461529: step 2840, loss = 8.52 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 07:54:05.069127: step 2850, loss = 8.52 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 07:54:10.260016: step 2860, loss = 8.52 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 07:54:15.703858: step 2870, loss = 8.52 (282.7 examples/sec; 0.453 sec/batch)
2016-07-15 07:54:20.349701: step 2880, loss = 8.52 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 07:54:26.104053: step 2890, loss = 8.53 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 07:54:30.921091: step 2900, loss = 8.50 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 07:54:36.702294: step 2910, loss = 8.50 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 07:54:41.427686: step 2920, loss = 8.49 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 07:54:46.083947: step 2930, loss = 8.52 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 07:54:50.731096: step 2940, loss = 8.50 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 07:54:55.306853: step 2950, loss = 8.50 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 07:54:59.931307: step 2960, loss = 8.50 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 07:55:05.643505: step 2970, loss = 8.50 (221.9 examples/sec; 0.577 sec/batch)
2016-07-15 07:55:10.474484: step 2980, loss = 8.49 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 07:55:15.202453: step 2990, loss = 8.50 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 07:55:21.089827: step 3000, loss = 8.50 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 07:55:28.213280: step 3010, loss = 8.51 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 07:55:32.940618: step 3020, loss = 8.52 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 07:55:37.565832: step 3030, loss = 8.51 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 07:55:42.450105: step 3040, loss = 8.48 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 07:55:47.899831: step 3050, loss = 8.51 (283.4 examples/sec; 0.452 sec/batch)
2016-07-15 07:55:52.519425: step 3060, loss = 8.51 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 07:55:57.177894: step 3070, loss = 8.47 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 07:56:01.909950: step 3080, loss = 8.51 (228.5 examples/sec; 0.560 sec/batch)
2016-07-15 07:56:07.619188: step 3090, loss = 8.48 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 07:56:12.361937: step 3100, loss = 8.47 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 07:56:18.362340: step 3110, loss = 8.47 (188.4 examples/sec; 0.679 sec/batch)
2016-07-15 07:56:24.906404: step 3120, loss = 8.48 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 07:56:29.663919: step 3130, loss = 8.48 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 07:56:34.636503: step 3140, loss = 8.46 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 07:56:40.192557: step 3150, loss = 8.48 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 07:56:46.175129: step 3160, loss = 8.49 (255.2 examples/sec; 0.501 sec/batch)
2016-07-15 07:56:51.160402: step 3170, loss = 8.48 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 07:56:55.796460: step 3180, loss = 8.49 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 07:57:00.537209: step 3190, loss = 8.47 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 07:57:06.212843: step 3200, loss = 8.47 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 07:57:11.810229: step 3210, loss = 8.47 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 07:57:17.410336: step 3220, loss = 8.48 (205.6 examples/sec; 0.622 sec/batch)
2016-07-15 07:57:22.519619: step 3230, loss = 8.45 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 07:57:27.986509: step 3240, loss = 8.47 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 07:57:32.620045: step 3250, loss = 8.45 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 07:57:37.236011: step 3260, loss = 8.50 (283.5 examples/sec; 0.451 sec/batch)
2016-07-15 07:57:41.962933: step 3270, loss = 8.44 (234.5 examples/sec; 0.546 sec/batch)
2016-07-15 07:57:47.694612: step 3280, loss = 8.45 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 07:57:52.473942: step 3290, loss = 8.45 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 07:57:57.258996: step 3300, loss = 8.45 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 07:58:03.015901: step 3310, loss = 8.47 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 07:58:07.644672: step 3320, loss = 8.47 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 07:58:12.261167: step 3330, loss = 8.47 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 07:58:17.004347: step 3340, loss = 8.43 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 07:58:22.740540: step 3350, loss = 8.44 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 07:58:27.501856: step 3360, loss = 8.44 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 07:58:32.281828: step 3370, loss = 8.45 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 07:58:38.415524: step 3380, loss = 8.41 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 07:58:43.843750: step 3390, loss = 8.46 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 07:58:48.407640: step 3400, loss = 8.45 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 07:58:54.062778: step 3410, loss = 8.41 (229.1 examples/sec; 0.559 sec/batch)
2016-07-15 07:58:59.769630: step 3420, loss = 8.43 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 07:59:04.514534: step 3430, loss = 8.46 (270.9 examples/sec; 0.473 sec/batch)
2016-07-15 07:59:09.288209: step 3440, loss = 8.38 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 07:59:13.989187: step 3450, loss = 8.41 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 07:59:18.783747: step 3460, loss = 8.43 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 07:59:23.514726: step 3470, loss = 8.41 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 07:59:29.436285: step 3480, loss = 8.44 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 07:59:35.246487: step 3490, loss = 8.40 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 07:59:40.086321: step 3500, loss = 8.42 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 07:59:45.922403: step 3510, loss = 8.44 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 07:59:50.605838: step 3520, loss = 8.38 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 07:59:55.527060: step 3530, loss = 8.45 (246.8 examples/sec; 0.519 sec/batch)
2016-07-15 08:00:00.277968: step 3540, loss = 8.40 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 08:00:04.988817: step 3550, loss = 8.44 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 08:00:09.790840: step 3560, loss = 8.42 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 08:00:14.481471: step 3570, loss = 8.38 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 08:00:19.288914: step 3580, loss = 8.37 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 08:00:24.073013: step 3590, loss = 8.42 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 08:00:30.220563: step 3600, loss = 8.37 (197.5 examples/sec; 0.648 sec/batch)
2016-07-15 08:00:36.902524: step 3610, loss = 8.38 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 08:00:41.558831: step 3620, loss = 8.38 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 08:00:46.202116: step 3630, loss = 8.39 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 08:00:50.783508: step 3640, loss = 8.36 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 08:00:56.113730: step 3650, loss = 8.37 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 08:01:01.369563: step 3660, loss = 8.42 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 08:01:06.014063: step 3670, loss = 8.40 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 08:01:11.292054: step 3680, loss = 8.40 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 08:01:17.735953: step 3690, loss = 8.34 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 08:01:22.338569: step 3700, loss = 8.41 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 08:01:28.633253: step 3710, loss = 8.31 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 08:01:33.853026: step 3720, loss = 8.34 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 08:01:38.576096: step 3730, loss = 8.38 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 08:01:43.408656: step 3740, loss = 8.40 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 08:01:48.225671: step 3750, loss = 8.41 (247.7 examples/sec; 0.517 sec/batch)
2016-07-15 08:01:53.144289: step 3760, loss = 8.34 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 08:01:58.584080: step 3770, loss = 8.37 (133.8 examples/sec; 0.956 sec/batch)
2016-07-15 08:02:04.365348: step 3780, loss = 8.37 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 08:02:09.032910: step 3790, loss = 8.32 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 08:02:14.733539: step 3800, loss = 8.30 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 08:02:20.567256: step 3810, loss = 8.32 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 08:02:25.385892: step 3820, loss = 8.37 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 08:02:30.159292: step 3830, loss = 8.39 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 08:02:34.980113: step 3840, loss = 8.37 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 08:02:39.607690: step 3850, loss = 8.30 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 08:02:44.233093: step 3860, loss = 8.36 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 08:02:48.879269: step 3870, loss = 8.31 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 08:02:53.572795: step 3880, loss = 8.29 (287.3 examples/sec; 0.446 sec/batch)
2016-07-15 08:02:58.209371: step 3890, loss = 8.38 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 08:03:02.797499: step 3900, loss = 8.29 (283.2 examples/sec; 0.452 sec/batch)
2016-07-15 08:03:08.414816: step 3910, loss = 8.34 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 08:03:14.164314: step 3920, loss = 8.31 (224.6 examples/sec; 0.570 sec/batch)
2016-07-15 08:03:18.990315: step 3930, loss = 8.30 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 08:03:23.670137: step 3940, loss = 8.37 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 08:03:28.316049: step 3950, loss = 8.33 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 08:03:34.005632: step 3960, loss = 8.33 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 08:03:38.850177: step 3970, loss = 8.34 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 08:03:43.588238: step 3980, loss = 8.36 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 08:03:49.426002: step 3990, loss = 8.31 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 08:03:54.237983: step 4000, loss = 8.31 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 08:03:59.795899: step 4010, loss = 8.30 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 08:04:04.403881: step 4020, loss = 8.30 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 08:04:10.073887: step 4030, loss = 8.29 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 08:04:14.949594: step 4040, loss = 8.29 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 08:04:19.704805: step 4050, loss = 8.29 (241.6 examples/sec; 0.530 sec/batch)
2016-07-15 08:04:24.476605: step 4060, loss = 8.26 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 08:04:29.247667: step 4070, loss = 8.31 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 08:04:33.954164: step 4080, loss = 8.23 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 08:04:38.555024: step 4090, loss = 8.24 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 08:04:43.265721: step 4100, loss = 8.14 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 08:04:48.771214: step 4110, loss = 8.29 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 08:04:53.395201: step 4120, loss = 8.26 (282.7 examples/sec; 0.453 sec/batch)
2016-07-15 08:04:58.115142: step 4130, loss = 8.20 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 08:05:03.833214: step 4140, loss = 8.24 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 08:05:08.614808: step 4150, loss = 8.30 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 08:05:13.238043: step 4160, loss = 8.30 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 08:05:17.863734: step 4170, loss = 8.28 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 08:05:23.621309: step 4180, loss = 8.30 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 08:05:28.364850: step 4190, loss = 8.24 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 08:05:33.146894: step 4200, loss = 8.22 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 08:05:38.923082: step 4210, loss = 8.29 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 08:05:43.588015: step 4220, loss = 8.28 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 08:05:48.242005: step 4230, loss = 8.23 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 08:05:52.807659: step 4240, loss = 8.30 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 08:05:57.436129: step 4250, loss = 8.22 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 08:06:03.205383: step 4260, loss = 8.16 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 08:06:07.996132: step 4270, loss = 8.26 (282.3 examples/sec; 0.453 sec/batch)
2016-07-15 08:06:12.684363: step 4280, loss = 8.23 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 08:06:17.309686: step 4290, loss = 8.20 (286.2 examples/sec; 0.447 sec/batch)
2016-07-15 08:06:22.036362: step 4300, loss = 8.23 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 08:06:27.548519: step 4310, loss = 8.24 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 08:06:32.921114: step 4320, loss = 8.26 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 08:06:38.232271: step 4330, loss = 8.28 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 08:06:42.944204: step 4340, loss = 8.29 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 08:06:47.577210: step 4350, loss = 8.23 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 08:06:52.966887: step 4360, loss = 8.19 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 08:06:58.144452: step 4370, loss = 8.24 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 08:07:02.839081: step 4380, loss = 8.20 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 08:07:08.270881: step 4390, loss = 8.08 (191.2 examples/sec; 0.669 sec/batch)
2016-07-15 08:07:14.552724: step 4400, loss = 8.25 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 08:07:20.096376: step 4410, loss = 8.13 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 08:07:25.674250: step 4420, loss = 8.29 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 08:07:30.690738: step 4430, loss = 8.17 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 08:07:35.372082: step 4440, loss = 8.15 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 08:07:40.060871: step 4450, loss = 8.18 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 08:07:44.773532: step 4460, loss = 8.23 (283.8 examples/sec; 0.451 sec/batch)
2016-07-15 08:07:49.378274: step 4470, loss = 8.29 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 08:07:54.073879: step 4480, loss = 8.17 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 08:07:59.931346: step 4490, loss = 8.22 (234.3 examples/sec; 0.546 sec/batch)
2016-07-15 08:08:04.741184: step 4500, loss = 8.09 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 08:08:10.554441: step 4510, loss = 8.13 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 08:08:16.036933: step 4520, loss = 8.11 (186.0 examples/sec; 0.688 sec/batch)
2016-07-15 08:08:21.701050: step 4530, loss = 8.11 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 08:08:26.501931: step 4540, loss = 8.08 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 08:08:31.356968: step 4550, loss = 8.24 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 08:08:36.003221: step 4560, loss = 8.19 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 08:08:40.677606: step 4570, loss = 8.16 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 08:08:46.385433: step 4580, loss = 8.11 (218.5 examples/sec; 0.586 sec/batch)
2016-07-15 08:08:51.587009: step 4590, loss = 8.14 (198.3 examples/sec; 0.646 sec/batch)
2016-07-15 08:08:56.880394: step 4600, loss = 8.13 (282.6 examples/sec; 0.453 sec/batch)
2016-07-15 08:09:02.618848: step 4610, loss = 8.13 (219.6 examples/sec; 0.583 sec/batch)
2016-07-15 08:09:08.353109: step 4620, loss = 8.24 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 08:09:13.091527: step 4630, loss = 8.10 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 08:09:17.904262: step 4640, loss = 8.14 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 08:09:22.676360: step 4650, loss = 8.09 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 08:09:27.307462: step 4660, loss = 8.12 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 08:09:31.992926: step 4670, loss = 8.03 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 08:09:36.677688: step 4680, loss = 8.16 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 08:09:42.397685: step 4690, loss = 8.22 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 08:09:47.228893: step 4700, loss = 8.26 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 08:09:52.901015: step 4710, loss = 8.20 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 08:09:58.909128: step 4720, loss = 8.19 (282.4 examples/sec; 0.453 sec/batch)
2016-07-15 08:10:03.507407: step 4730, loss = 8.06 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 08:10:08.133416: step 4740, loss = 8.12 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 08:10:12.843472: step 4750, loss = 8.10 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 08:10:17.465649: step 4760, loss = 8.11 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 08:10:22.167251: step 4770, loss = 8.05 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 08:10:27.950698: step 4780, loss = 8.06 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 08:10:32.761541: step 4790, loss = 8.14 (250.5 examples/sec; 0.511 sec/batch)
2016-07-15 08:10:37.416495: step 4800, loss = 8.05 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 08:10:42.984354: step 4810, loss = 8.09 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 08:10:47.661208: step 4820, loss = 8.19 (280.4 examples/sec; 0.457 sec/batch)
2016-07-15 08:10:52.285501: step 4830, loss = 8.15 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 08:10:57.904558: step 4840, loss = 8.14 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 08:11:03.036412: step 4850, loss = 8.05 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 08:11:08.657729: step 4860, loss = 8.21 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 08:11:13.468112: step 4870, loss = 8.06 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 08:11:18.081213: step 4880, loss = 8.14 (270.3 examples/sec; 0.473 sec/batch)
2016-07-15 08:11:22.739091: step 4890, loss = 8.07 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 08:11:27.373337: step 4900, loss = 8.13 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 08:11:33.059265: step 4910, loss = 8.15 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 08:11:37.711490: step 4920, loss = 8.05 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 08:11:43.475354: step 4930, loss = 8.13 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 08:11:48.294734: step 4940, loss = 8.12 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 08:11:53.117430: step 4950, loss = 8.14 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 08:11:57.916647: step 4960, loss = 8.00 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 08:12:02.536948: step 4970, loss = 8.04 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 08:12:07.147489: step 4980, loss = 8.12 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 08:12:11.796756: step 4990, loss = 8.17 (282.4 examples/sec; 0.453 sec/batch)
2016-07-15 08:12:16.483936: step 5000, loss = 8.16 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 08:12:23.112063: step 5010, loss = 8.00 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 08:12:27.908609: step 5020, loss = 8.10 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 08:12:32.577417: step 5030, loss = 8.02 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 08:12:37.219739: step 5040, loss = 7.98 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 08:12:42.904358: step 5050, loss = 8.23 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 08:12:47.515211: step 5060, loss = 7.97 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 08:12:52.147454: step 5070, loss = 7.99 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 08:12:56.731044: step 5080, loss = 8.03 (284.2 examples/sec; 0.450 sec/batch)
2016-07-15 08:13:01.368566: step 5090, loss = 8.12 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 08:13:07.074866: step 5100, loss = 8.05 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 08:13:12.845436: step 5110, loss = 8.04 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 08:13:17.715729: step 5120, loss = 8.16 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 08:13:24.069479: step 5130, loss = 8.12 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 08:13:29.493997: step 5140, loss = 8.15 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 08:13:34.168232: step 5150, loss = 8.10 (282.3 examples/sec; 0.453 sec/batch)
2016-07-15 08:13:38.988773: step 5160, loss = 8.12 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 08:13:43.702050: step 5170, loss = 7.95 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 08:13:48.506190: step 5180, loss = 8.11 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 08:13:53.157285: step 5190, loss = 8.03 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 08:13:57.795097: step 5200, loss = 8.06 (281.8 examples/sec; 0.454 sec/batch)
2016-07-15 08:14:03.328324: step 5210, loss = 8.09 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 08:14:07.999665: step 5220, loss = 7.96 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 08:14:12.669923: step 5230, loss = 8.07 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 08:14:17.347101: step 5240, loss = 7.98 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 08:14:23.034221: step 5250, loss = 8.00 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 08:14:27.865264: step 5260, loss = 7.91 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 08:14:32.704688: step 5270, loss = 8.01 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 08:14:37.435461: step 5280, loss = 7.92 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 08:14:42.078971: step 5290, loss = 7.95 (283.1 examples/sec; 0.452 sec/batch)
2016-07-15 08:14:46.800118: step 5300, loss = 8.01 (244.1 examples/sec; 0.524 sec/batch)
2016-07-15 08:14:53.697880: step 5310, loss = 8.03 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 08:14:58.418780: step 5320, loss = 8.10 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 08:15:03.036783: step 5330, loss = 8.10 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 08:15:08.081794: step 5340, loss = 8.18 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 08:15:13.523715: step 5350, loss = 7.98 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 08:15:18.302289: step 5360, loss = 7.97 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 08:15:23.400155: step 5370, loss = 8.06 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 08:15:29.875988: step 5380, loss = 8.04 (207.3 examples/sec; 0.618 sec/batch)
2016-07-15 08:15:34.693894: step 5390, loss = 8.02 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 08:15:39.312251: step 5400, loss = 8.05 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 08:15:44.882640: step 5410, loss = 8.05 (281.9 examples/sec; 0.454 sec/batch)
2016-07-15 08:15:49.505087: step 5420, loss = 7.90 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 08:15:55.180424: step 5430, loss = 8.00 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 08:15:59.794853: step 5440, loss = 8.02 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 08:16:04.451947: step 5450, loss = 8.01 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 08:16:09.100027: step 5460, loss = 8.00 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 08:16:14.810665: step 5470, loss = 7.89 (222.3 examples/sec; 0.576 sec/batch)
2016-07-15 08:16:19.657750: step 5480, loss = 8.04 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 08:16:24.407211: step 5490, loss = 8.04 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 08:16:30.292493: step 5500, loss = 7.97 (187.8 examples/sec; 0.681 sec/batch)
2016-07-15 08:16:37.371773: step 5510, loss = 8.02 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 08:16:42.115091: step 5520, loss = 7.96 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 08:16:46.663137: step 5530, loss = 8.11 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 08:16:51.542643: step 5540, loss = 8.08 (209.3 examples/sec; 0.612 sec/batch)
2016-07-15 08:16:56.988998: step 5550, loss = 7.93 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 08:17:01.831570: step 5560, loss = 8.15 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 08:17:08.211992: step 5570, loss = 7.78 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 08:17:13.026169: step 5580, loss = 8.07 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 08:17:17.811237: step 5590, loss = 7.95 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 08:17:22.502925: step 5600, loss = 8.09 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 08:17:28.332198: step 5610, loss = 7.90 (273.2 examples/sec; 0.468 sec/batch)
2016-07-15 08:17:32.943084: step 5620, loss = 7.97 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 08:17:37.531835: step 5630, loss = 8.05 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 08:17:43.265487: step 5640, loss = 7.97 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 08:17:48.127673: step 5650, loss = 8.07 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 08:17:53.116010: step 5660, loss = 7.88 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 08:17:58.019656: step 5670, loss = 7.90 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 08:18:02.821235: step 5680, loss = 8.13 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 08:18:07.539914: step 5690, loss = 7.95 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 08:18:12.181673: step 5700, loss = 7.99 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 08:18:18.852623: step 5710, loss = 8.08 (210.2 examples/sec; 0.609 sec/batch)
2016-07-15 08:18:23.691087: step 5720, loss = 7.96 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 08:18:28.513836: step 5730, loss = 7.98 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 08:18:33.347413: step 5740, loss = 7.89 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 08:18:37.955811: step 5750, loss = 7.99 (275.0 examples/sec; 0.466 sec/batch)
2016-07-15 08:18:42.616680: step 5760, loss = 7.89 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 08:18:47.211357: step 5770, loss = 7.97 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 08:18:51.816550: step 5780, loss = 8.12 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 08:18:56.459515: step 5790, loss = 8.05 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 08:19:01.115653: step 5800, loss = 8.11 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 08:19:07.857192: step 5810, loss = 7.96 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 08:19:12.658917: step 5820, loss = 8.04 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 08:19:17.413418: step 5830, loss = 7.92 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 08:19:23.577762: step 5840, loss = 7.97 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 08:19:29.169696: step 5850, loss = 8.11 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 08:19:33.908066: step 5860, loss = 8.04 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 08:19:38.751277: step 5870, loss = 7.91 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 08:19:43.456484: step 5880, loss = 7.93 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 08:19:48.260807: step 5890, loss = 7.94 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 08:19:53.005961: step 5900, loss = 7.93 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 08:19:58.708784: step 5910, loss = 7.89 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 08:20:03.318215: step 5920, loss = 7.84 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 08:20:08.359995: step 5930, loss = 7.80 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 08:20:13.810926: step 5940, loss = 7.91 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 08:20:18.499198: step 5950, loss = 7.85 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 08:20:23.142611: step 5960, loss = 7.85 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 08:20:28.113724: step 5970, loss = 8.00 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 08:20:33.597268: step 5980, loss = 8.04 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 08:20:38.381500: step 5990, loss = 8.01 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 08:20:43.020751: step 6000, loss = 7.91 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 08:20:48.618104: step 6010, loss = 7.87 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 08:20:53.262684: step 6020, loss = 8.05 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 08:20:57.906284: step 6030, loss = 7.87 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 08:21:02.490351: step 6040, loss = 7.99 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 08:21:07.285124: step 6050, loss = 7.93 (217.4 examples/sec; 0.589 sec/batch)
2016-07-15 08:21:12.881338: step 6060, loss = 8.01 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 08:21:17.502295: step 6070, loss = 7.87 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 08:21:23.220513: step 6080, loss = 7.79 (244.9 examples/sec; 0.523 sec/batch)
2016-07-15 08:21:28.115921: step 6090, loss = 7.81 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 08:21:32.892165: step 6100, loss = 7.98 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 08:21:38.617669: step 6110, loss = 7.91 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 08:21:43.506432: step 6120, loss = 7.90 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 08:21:48.220433: step 6130, loss = 7.91 (284.1 examples/sec; 0.451 sec/batch)
2016-07-15 08:21:52.830968: step 6140, loss = 8.08 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 08:21:58.389941: step 6150, loss = 8.09 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 08:22:03.361387: step 6160, loss = 8.02 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 08:22:08.108822: step 6170, loss = 7.99 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 08:22:12.932650: step 6180, loss = 7.84 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 08:22:17.631090: step 6190, loss = 7.91 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 08:22:22.285268: step 6200, loss = 7.77 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 08:22:28.978457: step 6210, loss = 8.04 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 08:22:33.734137: step 6220, loss = 7.82 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 08:22:38.589585: step 6230, loss = 7.83 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 08:22:43.252087: step 6240, loss = 7.92 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 08:22:48.680056: step 6250, loss = 7.99 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 08:22:55.174033: step 6260, loss = 7.89 (213.0 examples/sec; 0.601 sec/batch)
2016-07-15 08:23:00.179843: step 6270, loss = 8.04 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 08:23:05.627524: step 6280, loss = 7.93 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 08:23:10.343451: step 6290, loss = 7.91 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 08:23:14.941709: step 6300, loss = 7.87 (284.5 examples/sec; 0.450 sec/batch)
2016-07-15 08:23:20.533505: step 6310, loss = 7.86 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 08:23:25.974272: step 6320, loss = 7.98 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 08:23:31.079057: step 6330, loss = 7.84 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 08:23:35.798181: step 6340, loss = 7.92 (247.2 examples/sec; 0.518 sec/batch)
2016-07-15 08:23:40.605240: step 6350, loss = 7.88 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 08:23:45.263277: step 6360, loss = 7.92 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 08:23:49.950150: step 6370, loss = 8.02 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 08:23:54.576030: step 6380, loss = 7.94 (281.2 examples/sec; 0.455 sec/batch)
2016-07-15 08:23:59.309461: step 6390, loss = 7.75 (232.8 examples/sec; 0.550 sec/batch)
2016-07-15 08:24:05.027846: step 6400, loss = 7.77 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 08:24:10.906031: step 6410, loss = 7.80 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 08:24:15.530835: step 6420, loss = 7.77 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 08:24:20.644097: step 6430, loss = 7.95 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 08:24:26.078017: step 6440, loss = 7.80 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 08:24:30.791937: step 6450, loss = 7.79 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 08:24:35.636596: step 6460, loss = 7.91 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 08:24:40.358963: step 6470, loss = 7.82 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 08:24:45.007554: step 6480, loss = 7.87 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 08:24:50.608499: step 6490, loss = 7.76 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 08:24:55.566335: step 6500, loss = 7.86 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 08:25:01.242104: step 6510, loss = 7.83 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 08:25:05.914894: step 6520, loss = 7.80 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 08:25:11.687688: step 6530, loss = 7.91 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 08:25:16.547727: step 6540, loss = 7.93 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 08:25:21.336893: step 6550, loss = 7.95 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 08:25:26.057759: step 6560, loss = 7.93 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 08:25:30.647217: step 6570, loss = 7.85 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 08:25:35.408483: step 6580, loss = 7.90 (233.2 examples/sec; 0.549 sec/batch)
2016-07-15 08:25:41.158319: step 6590, loss = 7.90 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 08:25:45.909593: step 6600, loss = 7.83 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 08:25:51.683330: step 6610, loss = 7.90 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 08:25:56.373759: step 6620, loss = 7.80 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 08:26:01.179738: step 6630, loss = 7.75 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 08:26:05.830119: step 6640, loss = 7.93 (283.7 examples/sec; 0.451 sec/batch)
2016-07-15 08:26:10.481092: step 6650, loss = 7.90 (281.0 examples/sec; 0.456 sec/batch)
2016-07-15 08:26:16.244783: step 6660, loss = 7.71 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 08:26:21.673635: step 6670, loss = 7.93 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 08:26:26.904861: step 6680, loss = 7.92 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 08:26:32.696205: step 6690, loss = 7.83 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 08:26:37.415238: step 6700, loss = 7.84 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 08:26:43.263394: step 6710, loss = 7.81 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 08:26:47.985374: step 6720, loss = 7.96 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 08:26:52.629573: step 6730, loss = 7.65 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 08:26:58.089663: step 6740, loss = 7.92 (197.7 examples/sec; 0.647 sec/batch)
2016-07-15 08:27:03.290897: step 6750, loss = 7.82 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 08:27:08.007415: step 6760, loss = 7.97 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 08:27:12.766646: step 6770, loss = 7.77 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 08:27:17.444031: step 6780, loss = 7.71 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 08:27:22.097963: step 6790, loss = 7.73 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 08:27:27.817938: step 6800, loss = 7.86 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 08:27:33.563282: step 6810, loss = 7.92 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 08:27:38.394680: step 6820, loss = 7.67 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 08:27:44.721490: step 6830, loss = 7.71 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 08:27:50.163157: step 6840, loss = 7.91 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 08:27:54.894368: step 6850, loss = 7.84 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 08:28:00.017471: step 6860, loss = 7.81 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 08:28:05.695786: step 6870, loss = 7.70 (282.9 examples/sec; 0.452 sec/batch)
2016-07-15 08:28:10.339537: step 6880, loss = 8.09 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 08:28:16.097328: step 6890, loss = 7.98 (240.6 examples/sec; 0.532 sec/batch)
2016-07-15 08:28:20.909965: step 6900, loss = 7.80 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 08:28:26.488810: step 6910, loss = 7.85 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 08:28:31.101282: step 6920, loss = 7.66 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 08:28:35.789111: step 6930, loss = 7.80 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 08:28:40.901017: step 6940, loss = 7.95 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 08:28:46.339884: step 6950, loss = 7.94 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 08:28:51.066009: step 6960, loss = 7.86 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 08:28:55.672913: step 6970, loss = 7.77 (287.9 examples/sec; 0.445 sec/batch)
2016-07-15 08:29:00.300794: step 6980, loss = 7.79 (281.0 examples/sec; 0.456 sec/batch)
2016-07-15 08:29:05.527043: step 6990, loss = 7.70 (208.3 examples/sec; 0.614 sec/batch)
2016-07-15 08:29:10.812317: step 7000, loss = 7.76 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 08:29:17.743779: step 7010, loss = 7.88 (250.3 examples/sec; 0.511 sec/batch)
2016-07-15 08:29:22.508479: step 7020, loss = 7.91 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 08:29:27.468921: step 7030, loss = 7.80 (241.0 examples/sec; 0.531 sec/batch)
2016-07-15 08:29:34.034472: step 7040, loss = 7.88 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 08:29:38.976347: step 7050, loss = 7.76 (284.2 examples/sec; 0.450 sec/batch)
2016-07-15 08:29:43.604889: step 7060, loss = 7.81 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 08:29:48.695757: step 7070, loss = 7.77 (204.3 examples/sec; 0.626 sec/batch)
2016-07-15 08:29:54.126426: step 7080, loss = 7.77 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 08:29:59.881662: step 7090, loss = 7.69 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 08:30:04.702559: step 7100, loss = 7.95 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 08:30:10.553871: step 7110, loss = 7.63 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 08:30:15.268605: step 7120, loss = 7.70 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 08:30:19.858409: step 7130, loss = 7.83 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 08:30:25.038673: step 7140, loss = 7.90 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 08:30:30.469101: step 7150, loss = 7.78 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 08:30:35.184476: step 7160, loss = 7.75 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 08:30:40.028202: step 7170, loss = 7.67 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 08:30:44.632445: step 7180, loss = 7.85 (285.4 examples/sec; 0.449 sec/batch)
2016-07-15 08:30:49.246025: step 7190, loss = 7.70 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 08:30:54.808731: step 7200, loss = 7.80 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 08:31:00.850747: step 7210, loss = 7.76 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 08:31:05.629727: step 7220, loss = 7.93 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 08:31:10.332085: step 7230, loss = 7.83 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 08:31:15.196224: step 7240, loss = 7.72 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 08:31:19.930277: step 7250, loss = 7.69 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 08:31:24.767079: step 7260, loss = 7.83 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 08:31:29.567702: step 7270, loss = 7.74 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 08:31:35.582488: step 7280, loss = 7.75 (197.5 examples/sec; 0.648 sec/batch)
2016-07-15 08:31:40.195920: step 7290, loss = 7.61 (281.1 examples/sec; 0.455 sec/batch)
2016-07-15 08:31:45.365551: step 7300, loss = 7.84 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 08:31:52.123630: step 7310, loss = 8.00 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 08:31:57.787925: step 7320, loss = 7.90 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 08:32:02.563467: step 7330, loss = 7.80 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 08:32:07.412119: step 7340, loss = 7.73 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 08:32:12.140320: step 7350, loss = 7.76 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 08:32:17.641215: step 7360, loss = 7.67 (182.9 examples/sec; 0.700 sec/batch)
2016-07-15 08:32:23.936434: step 7370, loss = 7.81 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 08:32:28.765955: step 7380, loss = 7.70 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 08:32:33.565876: step 7390, loss = 7.81 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 08:32:38.373200: step 7400, loss = 7.86 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 08:32:43.939223: step 7410, loss = 7.84 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 08:32:48.630566: step 7420, loss = 7.83 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 08:32:53.891493: step 7430, loss = 7.81 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 08:32:59.156087: step 7440, loss = 7.81 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 08:33:03.817888: step 7450, loss = 7.78 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 08:33:08.402917: step 7460, loss = 7.77 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 08:33:13.021442: step 7470, loss = 7.82 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 08:33:18.446886: step 7480, loss = 7.71 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 08:33:23.563874: step 7490, loss = 7.80 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 08:33:29.314698: step 7500, loss = 7.89 (251.9 examples/sec; 0.508 sec/batch)
2016-07-15 08:33:35.065954: step 7510, loss = 7.84 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 08:33:39.656096: step 7520, loss = 7.86 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 08:33:44.604584: step 7530, loss = 7.83 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 08:33:50.126695: step 7540, loss = 7.72 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 08:33:55.878892: step 7550, loss = 7.69 (246.6 examples/sec; 0.519 sec/batch)
2016-07-15 08:34:01.085901: step 7560, loss = 7.62 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 08:34:06.523157: step 7570, loss = 7.78 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 08:34:11.231306: step 7580, loss = 7.76 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 08:34:16.302831: step 7590, loss = 7.68 (187.8 examples/sec; 0.681 sec/batch)
2016-07-15 08:34:22.784326: step 7600, loss = 7.84 (210.0 examples/sec; 0.609 sec/batch)
2016-07-15 08:34:28.948136: step 7610, loss = 7.68 (268.1 examples/sec; 0.478 sec/batch)
2016-07-15 08:34:33.746462: step 7620, loss = 7.69 (247.6 examples/sec; 0.517 sec/batch)
2016-07-15 08:34:39.879504: step 7630, loss = 7.73 (195.8 examples/sec; 0.654 sec/batch)
2016-07-15 08:34:45.563462: step 7640, loss = 7.76 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 08:34:50.302569: step 7650, loss = 7.63 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 08:34:55.136716: step 7660, loss = 7.74 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 08:34:59.884577: step 7670, loss = 7.69 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 08:35:05.292731: step 7680, loss = 7.74 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 08:35:11.719463: step 7690, loss = 7.65 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 08:35:16.943920: step 7700, loss = 7.72 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 08:35:23.674710: step 7710, loss = 7.79 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 08:35:28.386096: step 7720, loss = 7.88 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 08:35:32.974282: step 7730, loss = 7.71 (283.0 examples/sec; 0.452 sec/batch)
2016-07-15 08:35:37.601613: step 7740, loss = 7.66 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 08:35:43.164421: step 7750, loss = 7.78 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 08:35:48.124741: step 7760, loss = 7.69 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 08:35:52.868651: step 7770, loss = 7.73 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 08:35:57.622692: step 7780, loss = 7.75 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 08:36:02.439215: step 7790, loss = 7.66 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 08:36:08.677857: step 7800, loss = 7.82 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 08:36:15.422845: step 7810, loss = 7.74 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 08:36:21.181884: step 7820, loss = 7.61 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 08:36:25.942695: step 7830, loss = 7.87 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 08:36:30.747782: step 7840, loss = 7.69 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 08:36:35.498051: step 7850, loss = 7.75 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 08:36:40.686455: step 7860, loss = 7.73 (190.7 examples/sec; 0.671 sec/batch)
2016-07-15 08:36:47.136186: step 7870, loss = 7.50 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 08:36:52.156718: step 7880, loss = 7.88 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 08:36:56.873344: step 7890, loss = 7.63 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 08:37:02.052943: step 7900, loss = 7.68 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 08:37:07.887909: step 7910, loss = 7.74 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 08:37:12.555638: step 7920, loss = 7.82 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 08:37:18.121927: step 7930, loss = 7.55 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 08:37:24.300519: step 7940, loss = 7.65 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 08:37:29.117964: step 7950, loss = 7.59 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 08:37:33.898915: step 7960, loss = 7.76 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 08:37:38.658503: step 7970, loss = 7.58 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 08:37:43.484294: step 7980, loss = 7.45 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 08:37:49.988506: step 7990, loss = 7.79 (199.4 examples/sec; 0.642 sec/batch)
2016-07-15 08:37:55.279502: step 8000, loss = 7.88 (256.8 examples/sec; 0.499 sec/batch)
2016-07-15 08:38:02.208732: step 8010, loss = 7.60 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 08:38:06.983978: step 8020, loss = 7.64 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 08:38:11.839312: step 8030, loss = 7.55 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 08:38:16.482065: step 8040, loss = 8.00 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 08:38:21.909412: step 8050, loss = 7.83 (188.5 examples/sec; 0.679 sec/batch)
2016-07-15 08:38:28.151885: step 8060, loss = 7.67 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 08:38:32.785815: step 8070, loss = 7.68 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 08:38:38.048894: step 8080, loss = 7.65 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 08:38:43.347504: step 8090, loss = 7.72 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 08:38:48.049038: step 8100, loss = 7.77 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 08:38:53.813595: step 8110, loss = 7.86 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 08:38:58.493068: step 8120, loss = 7.70 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 08:39:03.184627: step 8130, loss = 7.61 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 08:39:08.933341: step 8140, loss = 7.68 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 08:39:13.772245: step 8150, loss = 7.61 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 08:39:18.560177: step 8160, loss = 7.68 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 08:39:23.280345: step 8170, loss = 7.64 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 08:39:27.882656: step 8180, loss = 7.79 (286.4 examples/sec; 0.447 sec/batch)
2016-07-15 08:39:32.827983: step 8190, loss = 7.66 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 08:39:38.376645: step 8200, loss = 7.60 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 08:39:45.196920: step 8210, loss = 7.56 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 08:39:50.671366: step 8220, loss = 7.70 (207.6 examples/sec; 0.617 sec/batch)
2016-07-15 08:39:55.828843: step 8230, loss = 7.57 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 08:40:00.556002: step 8240, loss = 7.64 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 08:40:06.031135: step 8250, loss = 7.61 (188.9 examples/sec; 0.677 sec/batch)
2016-07-15 08:40:11.247238: step 8260, loss = 7.90 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 08:40:16.006587: step 8270, loss = 7.64 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 08:40:21.750789: step 8280, loss = 7.74 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 08:40:26.515951: step 8290, loss = 7.73 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 08:40:31.160046: step 8300, loss = 7.94 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 08:40:36.787942: step 8310, loss = 7.73 (282.2 examples/sec; 0.454 sec/batch)
2016-07-15 08:40:41.408294: step 8320, loss = 7.67 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 08:40:46.844464: step 8330, loss = 7.59 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 08:40:52.063773: step 8340, loss = 7.73 (236.9 examples/sec; 0.540 sec/batch)
2016-07-15 08:40:57.764622: step 8350, loss = 7.52 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 08:41:02.590777: step 8360, loss = 7.72 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 08:41:07.432665: step 8370, loss = 7.61 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 08:41:12.148629: step 8380, loss = 7.75 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 08:41:17.482261: step 8390, loss = 7.63 (190.9 examples/sec; 0.670 sec/batch)
2016-07-15 08:41:23.962576: step 8400, loss = 7.46 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 08:41:29.809969: step 8410, loss = 7.61 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 08:41:34.633115: step 8420, loss = 7.65 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 08:41:40.819241: step 8430, loss = 7.52 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 08:41:46.407682: step 8440, loss = 7.50 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 08:41:51.124915: step 8450, loss = 7.52 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 08:41:55.983565: step 8460, loss = 7.71 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 08:42:00.686734: step 8470, loss = 7.66 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 08:42:06.348745: step 8480, loss = 7.65 (184.5 examples/sec; 0.694 sec/batch)
2016-07-15 08:42:13.037271: step 8490, loss = 7.62 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 08:42:17.857459: step 8500, loss = 7.65 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 08:42:23.643082: step 8510, loss = 7.59 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 08:42:28.329714: step 8520, loss = 7.53 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 08:42:33.166513: step 8530, loss = 7.69 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 08:42:37.835193: step 8540, loss = 7.69 (285.6 examples/sec; 0.448 sec/batch)
2016-07-15 08:42:42.450428: step 8550, loss = 7.71 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 08:42:48.271747: step 8560, loss = 7.69 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 08:42:53.170838: step 8570, loss = 7.51 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 08:42:57.853572: step 8580, loss = 7.59 (281.1 examples/sec; 0.455 sec/batch)
2016-07-15 08:43:02.483934: step 8590, loss = 7.70 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 08:43:07.139990: step 8600, loss = 7.51 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 08:43:12.995003: step 8610, loss = 7.72 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 08:43:18.596381: step 8620, loss = 7.67 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 08:43:23.343203: step 8630, loss = 7.62 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 08:43:28.188936: step 8640, loss = 7.65 (250.3 examples/sec; 0.511 sec/batch)
2016-07-15 08:43:34.766818: step 8650, loss = 7.71 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 08:43:39.918090: step 8660, loss = 7.70 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 08:43:44.683634: step 8670, loss = 7.46 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 08:43:50.186137: step 8680, loss = 7.51 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 08:43:56.514521: step 8690, loss = 7.55 (246.2 examples/sec; 0.520 sec/batch)
2016-07-15 08:44:01.364988: step 8700, loss = 7.67 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 08:44:07.096394: step 8710, loss = 7.57 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 08:44:11.835496: step 8720, loss = 7.43 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 08:44:16.701340: step 8730, loss = 7.61 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 08:44:21.418793: step 8740, loss = 7.57 (248.8 examples/sec; 0.514 sec/batch)
2016-07-15 08:44:27.150847: step 8750, loss = 7.55 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 08:44:33.224313: step 8760, loss = 7.67 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 08:44:38.031897: step 8770, loss = 7.67 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 08:44:42.811056: step 8780, loss = 7.58 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 08:44:47.573333: step 8790, loss = 7.70 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 08:44:52.180903: step 8800, loss = 7.67 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 08:44:58.399494: step 8810, loss = 7.42 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 08:45:03.666615: step 8820, loss = 7.55 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 08:45:08.393857: step 8830, loss = 7.42 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 08:45:13.209943: step 8840, loss = 7.65 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 08:45:17.981062: step 8850, loss = 7.48 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 08:45:23.901079: step 8860, loss = 7.49 (192.2 examples/sec; 0.666 sec/batch)
2016-07-15 08:45:29.738713: step 8870, loss = 7.55 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 08:45:34.492986: step 8880, loss = 7.66 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 08:45:39.300764: step 8890, loss = 7.58 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 08:45:45.589586: step 8900, loss = 7.74 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 08:45:52.316388: step 8910, loss = 7.55 (256.8 examples/sec; 0.499 sec/batch)
2016-07-15 08:45:57.079531: step 8920, loss = 7.58 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 08:46:01.901907: step 8930, loss = 7.56 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 08:46:06.684846: step 8940, loss = 7.53 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 08:46:11.415668: step 8950, loss = 7.46 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 08:46:16.322477: step 8960, loss = 7.58 (248.8 examples/sec; 0.514 sec/batch)
2016-07-15 08:46:22.901053: step 8970, loss = 7.67 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 08:46:28.098761: step 8980, loss = 7.57 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 08:46:32.806597: step 8990, loss = 7.72 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 08:46:37.623104: step 9000, loss = 7.40 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 08:46:43.347704: step 9010, loss = 7.64 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 08:46:49.651597: step 9020, loss = 7.65 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 08:46:55.117847: step 9030, loss = 7.33 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 08:46:59.841650: step 9040, loss = 7.63 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 08:47:04.704166: step 9050, loss = 7.51 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 08:47:09.457205: step 9060, loss = 7.58 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 08:47:15.166929: step 9070, loss = 7.60 (184.8 examples/sec; 0.693 sec/batch)
2016-07-15 08:47:21.224891: step 9080, loss = 7.51 (273.2 examples/sec; 0.468 sec/batch)
2016-07-15 08:47:25.898189: step 9090, loss = 7.30 (276.2 examples/sec; 0.464 sec/batch)
2016-07-15 08:47:30.575166: step 9100, loss = 7.50 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 08:47:36.208561: step 9110, loss = 7.51 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 08:47:40.869041: step 9120, loss = 7.70 (247.9 examples/sec; 0.516 sec/batch)
2016-07-15 08:47:45.912704: step 9130, loss = 7.42 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 08:47:51.372699: step 9140, loss = 7.36 (248.2 examples/sec; 0.516 sec/batch)
2016-07-15 08:47:56.077057: step 9150, loss = 7.58 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 08:48:00.905716: step 9160, loss = 7.75 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 08:48:05.652323: step 9170, loss = 7.66 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 08:48:11.358312: step 9180, loss = 7.49 (183.8 examples/sec; 0.697 sec/batch)
2016-07-15 08:48:17.406394: step 9190, loss = 7.54 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 08:48:22.187848: step 9200, loss = 7.66 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 08:48:28.155714: step 9210, loss = 7.67 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 08:48:32.939066: step 9220, loss = 7.55 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 08:48:37.829812: step 9230, loss = 7.62 (248.2 examples/sec; 0.516 sec/batch)
2016-07-15 08:48:42.676932: step 9240, loss = 7.48 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 08:48:48.751600: step 9250, loss = 7.59 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 08:48:54.448877: step 9260, loss = 7.67 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 08:48:59.234270: step 9270, loss = 7.41 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 08:49:04.066747: step 9280, loss = 7.76 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 08:49:10.545458: step 9290, loss = 7.40 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 08:49:15.855397: step 9300, loss = 7.68 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 08:49:22.600872: step 9310, loss = 7.48 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 08:49:27.269263: step 9320, loss = 7.59 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 08:49:33.030139: step 9330, loss = 7.58 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 08:49:38.359722: step 9340, loss = 7.75 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 08:49:43.686560: step 9350, loss = 7.40 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 08:49:49.510386: step 9360, loss = 7.45 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 08:49:54.356482: step 9370, loss = 7.52 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 08:49:59.470746: step 9380, loss = 7.46 (243.7 examples/sec; 0.525 sec/batch)
2016-07-15 08:50:05.898511: step 9390, loss = 7.58 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 08:50:11.219764: step 9400, loss = 7.52 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 08:50:16.793527: step 9410, loss = 7.46 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 08:50:21.395802: step 9420, loss = 7.90 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 08:50:26.631191: step 9430, loss = 7.56 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 08:50:31.930173: step 9440, loss = 7.60 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 08:50:36.679094: step 9450, loss = 7.64 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 08:50:41.565975: step 9460, loss = 7.85 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 08:50:46.280433: step 9470, loss = 7.65 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 08:50:52.252367: step 9480, loss = 7.41 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 08:50:58.110729: step 9490, loss = 7.36 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 08:51:02.932317: step 9500, loss = 7.35 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 08:51:08.501794: step 9510, loss = 7.40 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 08:51:13.498910: step 9520, loss = 7.54 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 08:51:19.025727: step 9530, loss = 7.37 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 08:51:23.733734: step 9540, loss = 7.45 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 08:51:28.547803: step 9550, loss = 7.44 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 08:51:35.114541: step 9560, loss = 7.33 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 08:51:40.336077: step 9570, loss = 7.32 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 08:51:44.997968: step 9580, loss = 7.73 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 08:51:49.612821: step 9590, loss = 7.62 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 08:51:54.860549: step 9600, loss = 7.48 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 08:52:01.533500: step 9610, loss = 7.35 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 08:52:07.091388: step 9620, loss = 7.54 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 08:52:11.798707: step 9630, loss = 7.52 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 08:52:16.725326: step 9640, loss = 7.52 (237.1 examples/sec; 0.540 sec/batch)
2016-07-15 08:52:23.250657: step 9650, loss = 7.56 (207.6 examples/sec; 0.617 sec/batch)
2016-07-15 08:52:28.223113: step 9660, loss = 7.43 (283.3 examples/sec; 0.452 sec/batch)
2016-07-15 08:52:33.016893: step 9670, loss = 7.61 (215.4 examples/sec; 0.594 sec/batch)
2016-07-15 08:52:38.669803: step 9680, loss = 7.33 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 08:52:44.385033: step 9690, loss = 7.40 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 08:52:49.519959: step 9700, loss = 7.47 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 08:52:56.316800: step 9710, loss = 7.55 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 08:53:02.069697: step 9720, loss = 7.49 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 08:53:06.809266: step 9730, loss = 7.36 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 08:53:11.596485: step 9740, loss = 7.55 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 08:53:16.266922: step 9750, loss = 7.59 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 08:53:21.133884: step 9760, loss = 7.42 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 08:53:25.895908: step 9770, loss = 7.51 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 08:53:31.614649: step 9780, loss = 7.55 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 08:53:37.700894: step 9790, loss = 7.63 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 08:53:42.514888: step 9800, loss = 7.60 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 08:53:48.337567: step 9810, loss = 7.49 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 08:53:54.808287: step 9820, loss = 7.51 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 08:54:00.093070: step 9830, loss = 7.42 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 08:54:04.832337: step 9840, loss = 7.35 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 08:54:10.164343: step 9850, loss = 7.49 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 08:54:16.644579: step 9860, loss = 7.58 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 08:54:21.286826: step 9870, loss = 7.44 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 08:54:26.410466: step 9880, loss = 7.63 (209.6 examples/sec; 0.611 sec/batch)
2016-07-15 08:54:31.789664: step 9890, loss = 7.43 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 08:54:36.525508: step 9900, loss = 7.51 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 08:54:43.112324: step 9910, loss = 7.32 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 08:54:49.301807: step 9920, loss = 7.46 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 08:54:54.593579: step 9930, loss = 7.49 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 08:54:59.982752: step 9940, loss = 7.53 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 08:55:04.839858: step 9950, loss = 7.57 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 08:55:10.377604: step 9960, loss = 7.47 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 08:55:15.203543: step 9970, loss = 7.39 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 08:55:19.999601: step 9980, loss = 7.61 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 08:55:24.822952: step 9990, loss = 7.56 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 08:55:29.563707: step 10000, loss = 7.62 (252.8 examples/sec; 0.506 sec/batch)
2016-07-15 08:55:35.405842: step 10010, loss = 7.40 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 08:55:40.243969: step 10020, loss = 7.44 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 08:55:44.929810: step 10030, loss = 7.46 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 08:55:49.811427: step 10040, loss = 7.45 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 08:55:54.467673: step 10050, loss = 7.57 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 08:55:59.119065: step 10060, loss = 7.60 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 08:56:05.026737: step 10070, loss = 7.60 (198.9 examples/sec; 0.644 sec/batch)
2016-07-15 08:56:09.973820: step 10080, loss = 7.34 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 08:56:14.769062: step 10090, loss = 7.30 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 08:56:19.559025: step 10100, loss = 7.55 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 08:56:25.395777: step 10110, loss = 7.36 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 08:56:30.136923: step 10120, loss = 7.47 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 08:56:35.879222: step 10130, loss = 7.54 (189.5 examples/sec; 0.676 sec/batch)
2016-07-15 08:56:41.945256: step 10140, loss = 7.44 (242.3 examples/sec; 0.528 sec/batch)
2016-07-15 08:56:47.800204: step 10150, loss = 7.62 (208.7 examples/sec; 0.613 sec/batch)
2016-07-15 08:56:52.944847: step 10160, loss = 7.21 (245.1 examples/sec; 0.522 sec/batch)
2016-07-15 08:56:58.702046: step 10170, loss = 7.32 (248.7 examples/sec; 0.515 sec/batch)
2016-07-15 08:57:04.932308: step 10180, loss = 7.61 (212.8 examples/sec; 0.602 sec/batch)
2016-07-15 08:57:09.764970: step 10190, loss = 7.45 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 08:57:14.578676: step 10200, loss = 7.40 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 08:57:20.361840: step 10210, loss = 7.38 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 08:57:24.932879: step 10220, loss = 7.41 (282.4 examples/sec; 0.453 sec/batch)
2016-07-15 08:57:29.896532: step 10230, loss = 7.60 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 08:57:35.402150: step 10240, loss = 7.38 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 08:57:41.191105: step 10250, loss = 7.17 (243.1 examples/sec; 0.526 sec/batch)
2016-07-15 08:57:46.380134: step 10260, loss = 7.22 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 08:57:51.824163: step 10270, loss = 7.32 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 08:57:56.588154: step 10280, loss = 7.57 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 08:58:01.737434: step 10290, loss = 7.50 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 08:58:08.201483: step 10300, loss = 7.15 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 08:58:14.899192: step 10310, loss = 7.40 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 08:58:20.187624: step 10320, loss = 7.60 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 08:58:24.870724: step 10330, loss = 7.45 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 08:58:29.505147: step 10340, loss = 7.42 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 08:58:34.120891: step 10350, loss = 7.36 (283.6 examples/sec; 0.451 sec/batch)
2016-07-15 08:58:38.733401: step 10360, loss = 7.58 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 08:58:44.209873: step 10370, loss = 7.35 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 08:58:49.224394: step 10380, loss = 7.43 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 08:58:53.978293: step 10390, loss = 7.29 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 08:58:58.744059: step 10400, loss = 7.39 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 08:59:04.536617: step 10410, loss = 7.45 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 08:59:09.220608: step 10420, loss = 7.54 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 08:59:13.860835: step 10430, loss = 7.23 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 08:59:18.490985: step 10440, loss = 7.66 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 08:59:23.951559: step 10450, loss = 7.28 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 08:59:29.072621: step 10460, loss = 7.54 (242.8 examples/sec; 0.527 sec/batch)
2016-07-15 08:59:34.766831: step 10470, loss = 7.59 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 08:59:40.460152: step 10480, loss = 7.49 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 08:59:45.447953: step 10490, loss = 7.38 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 08:59:50.198502: step 10500, loss = 7.50 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 08:59:55.893301: step 10510, loss = 7.39 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 09:00:00.506256: step 10520, loss = 7.35 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 09:00:05.232505: step 10530, loss = 7.45 (242.4 examples/sec; 0.528 sec/batch)
2016-07-15 09:00:10.924743: step 10540, loss = 7.38 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 09:00:15.701203: step 10550, loss = 7.43 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 09:00:20.513522: step 10560, loss = 7.41 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 09:00:25.286989: step 10570, loss = 7.44 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 09:00:30.070042: step 10580, loss = 7.21 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 09:00:34.690488: step 10590, loss = 7.45 (282.7 examples/sec; 0.453 sec/batch)
2016-07-15 09:00:39.318895: step 10600, loss = 7.37 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 09:00:46.113257: step 10610, loss = 7.53 (252.8 examples/sec; 0.506 sec/batch)
2016-07-15 09:00:51.545033: step 10620, loss = 7.73 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 09:00:56.768746: step 10630, loss = 7.47 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 09:01:02.528175: step 10640, loss = 7.31 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 09:01:07.358499: step 10650, loss = 7.30 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 09:01:12.197680: step 10660, loss = 7.15 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 09:01:16.929515: step 10670, loss = 7.43 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 09:01:21.509638: step 10680, loss = 7.30 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 09:01:26.566753: step 10690, loss = 7.33 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 09:01:31.976531: step 10700, loss = 7.49 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 09:01:37.823648: step 10710, loss = 7.29 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 09:01:43.344523: step 10720, loss = 7.18 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 09:01:49.583054: step 10730, loss = 7.28 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 09:01:54.339984: step 10740, loss = 7.56 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 09:01:59.105274: step 10750, loss = 7.39 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 09:02:03.853653: step 10760, loss = 7.27 (282.7 examples/sec; 0.453 sec/batch)
2016-07-15 09:02:08.676208: step 10770, loss = 7.49 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 09:02:15.226010: step 10780, loss = 7.36 (199.9 examples/sec; 0.640 sec/batch)
2016-07-15 09:02:20.743811: step 10790, loss = 7.20 (238.6 examples/sec; 0.537 sec/batch)
2016-07-15 09:02:26.257091: step 10800, loss = 7.47 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 09:02:33.537776: step 10810, loss = 7.34 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 09:02:39.220762: step 10820, loss = 7.24 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 09:02:44.900572: step 10830, loss = 7.49 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 09:02:50.080666: step 10840, loss = 7.24 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 09:02:55.676469: step 10850, loss = 7.31 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 09:03:00.369200: step 10860, loss = 7.41 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 09:03:05.243740: step 10870, loss = 7.52 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 09:03:11.773581: step 10880, loss = 7.64 (209.8 examples/sec; 0.610 sec/batch)
2016-07-15 09:03:16.957532: step 10890, loss = 7.21 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 09:03:21.737175: step 10900, loss = 7.43 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 09:03:27.538277: step 10910, loss = 7.49 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 09:03:32.461251: step 10920, loss = 7.34 (245.7 examples/sec; 0.521 sec/batch)
2016-07-15 09:03:37.128653: step 10930, loss = 7.29 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 09:03:41.757373: step 10940, loss = 7.37 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 09:03:46.403548: step 10950, loss = 7.51 (285.5 examples/sec; 0.448 sec/batch)
2016-07-15 09:03:51.015237: step 10960, loss = 7.45 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 09:03:56.558457: step 10970, loss = 7.45 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 09:04:01.542227: step 10980, loss = 7.49 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 09:04:06.236290: step 10990, loss = 7.00 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 09:04:11.093539: step 11000, loss = 7.40 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 09:04:16.881729: step 11010, loss = 7.41 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 09:04:23.349977: step 11020, loss = 7.46 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 09:04:28.661318: step 11030, loss = 7.29 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 09:04:33.339112: step 11040, loss = 7.47 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 09:04:38.121892: step 11050, loss = 7.32 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 09:04:42.866635: step 11060, loss = 7.38 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 09:04:48.719824: step 11070, loss = 7.48 (185.9 examples/sec; 0.689 sec/batch)
2016-07-15 09:04:54.612611: step 11080, loss = 7.48 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 09:04:59.405829: step 11090, loss = 7.68 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 09:05:04.250569: step 11100, loss = 7.52 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 09:05:10.085944: step 11110, loss = 7.38 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 09:05:15.717434: step 11120, loss = 7.33 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 09:05:21.886297: step 11130, loss = 7.27 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 09:05:27.197444: step 11140, loss = 7.41 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 09:05:32.568435: step 11150, loss = 7.45 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 09:05:37.249572: step 11160, loss = 7.54 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 09:05:41.837642: step 11170, loss = 7.07 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 09:05:46.982454: step 11180, loss = 7.29 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 09:05:52.353989: step 11190, loss = 7.14 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 09:05:58.150603: step 11200, loss = 7.46 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 09:06:03.924080: step 11210, loss = 7.43 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 09:06:08.706495: step 11220, loss = 7.48 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 09:06:13.420235: step 11230, loss = 7.38 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 09:06:18.681281: step 11240, loss = 7.24 (190.3 examples/sec; 0.672 sec/batch)
2016-07-15 09:06:25.233924: step 11250, loss = 7.30 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 09:06:30.054268: step 11260, loss = 7.38 (281.1 examples/sec; 0.455 sec/batch)
2016-07-15 09:06:34.969133: step 11270, loss = 7.27 (236.0 examples/sec; 0.542 sec/batch)
2016-07-15 09:06:41.476081: step 11280, loss = 7.17 (170.8 examples/sec; 0.749 sec/batch)
2016-07-15 09:06:47.826988: step 11290, loss = 7.17 (251.1 examples/sec; 0.510 sec/batch)
2016-07-15 09:06:52.749558: step 11300, loss = 7.30 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 09:06:58.660584: step 11310, loss = 7.39 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 09:07:03.482523: step 11320, loss = 7.16 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 09:07:08.349295: step 11330, loss = 7.21 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 09:07:13.386161: step 11340, loss = 7.48 (251.0 examples/sec; 0.510 sec/batch)
2016-07-15 09:07:20.252902: step 11350, loss = 7.30 (198.3 examples/sec; 0.646 sec/batch)
2016-07-15 09:07:25.475110: step 11360, loss = 7.24 (241.2 examples/sec; 0.531 sec/batch)
2016-07-15 09:07:31.278150: step 11370, loss = 7.32 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 09:07:36.196997: step 11380, loss = 7.33 (251.9 examples/sec; 0.508 sec/batch)
2016-07-15 09:07:41.211726: step 11390, loss = 7.37 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 09:07:48.014788: step 11400, loss = 7.16 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 09:07:54.171749: step 11410, loss = 7.07 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 09:07:59.350989: step 11420, loss = 7.03 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 09:08:04.725793: step 11430, loss = 7.24 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 09:08:10.582688: step 11440, loss = 7.46 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 09:08:15.400565: step 11450, loss = 7.33 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 09:08:20.250692: step 11460, loss = 7.03 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 09:08:24.991532: step 11470, loss = 7.49 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 09:08:29.880915: step 11480, loss = 7.34 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 09:08:34.708211: step 11490, loss = 7.14 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 09:08:40.514622: step 11500, loss = 7.13 (184.2 examples/sec; 0.695 sec/batch)
2016-07-15 09:08:47.780519: step 11510, loss = 7.19 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 09:08:53.706784: step 11520, loss = 7.45 (219.8 examples/sec; 0.582 sec/batch)
2016-07-15 09:08:59.078195: step 11530, loss = 7.29 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 09:09:04.537818: step 11540, loss = 7.39 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 09:09:10.313231: step 11550, loss = 7.25 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 09:09:15.812512: step 11560, loss = 7.22 (196.9 examples/sec; 0.650 sec/batch)
2016-07-15 09:09:21.164272: step 11570, loss = 7.34 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 09:09:26.969447: step 11580, loss = 7.49 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 09:09:31.822867: step 11590, loss = 7.36 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 09:09:36.597770: step 11600, loss = 7.45 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 09:09:42.334047: step 11610, loss = 7.12 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 09:09:47.003098: step 11620, loss = 7.38 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 09:09:52.565973: step 11630, loss = 7.32 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 09:09:57.691363: step 11640, loss = 7.32 (239.3 examples/sec; 0.535 sec/batch)
2016-07-15 09:10:03.405166: step 11650, loss = 7.26 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 09:10:08.221527: step 11660, loss = 7.28 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 09:10:13.087186: step 11670, loss = 7.29 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 09:10:19.711104: step 11680, loss = 7.36 (194.9 examples/sec; 0.657 sec/batch)
2016-07-15 09:10:25.116220: step 11690, loss = 7.21 (254.0 examples/sec; 0.504 sec/batch)
2016-07-15 09:10:30.931873: step 11700, loss = 7.32 (251.5 examples/sec; 0.509 sec/batch)
2016-07-15 09:10:36.742316: step 11710, loss = 7.31 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 09:10:41.613303: step 11720, loss = 7.20 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 09:10:48.874156: step 11730, loss = 7.38 (159.5 examples/sec; 0.803 sec/batch)
2016-07-15 09:10:54.265175: step 11740, loss = 7.26 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 09:11:00.237463: step 11750, loss = 7.24 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 09:11:04.968879: step 11760, loss = 7.25 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 09:11:09.960514: step 11770, loss = 7.22 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 09:11:16.572986: step 11780, loss = 7.26 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 09:11:21.820489: step 11790, loss = 7.48 (217.4 examples/sec; 0.589 sec/batch)
2016-07-15 09:11:27.654831: step 11800, loss = 7.19 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 09:11:33.430102: step 11810, loss = 7.15 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 09:11:38.707211: step 11820, loss = 7.17 (180.4 examples/sec; 0.709 sec/batch)
2016-07-15 09:11:45.348001: step 11830, loss = 7.35 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 09:11:50.489619: step 11840, loss = 7.29 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 09:11:55.272914: step 11850, loss = 7.29 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 09:12:01.693116: step 11860, loss = 7.35 (171.1 examples/sec; 0.748 sec/batch)
2016-07-15 09:12:07.762452: step 11870, loss = 7.35 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 09:12:13.899150: step 11880, loss = 7.34 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 09:12:19.437437: step 11890, loss = 7.38 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 09:12:24.904014: step 11900, loss = 7.21 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 09:12:30.666970: step 11910, loss = 7.38 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 09:12:35.504615: step 11920, loss = 7.12 (282.8 examples/sec; 0.453 sec/batch)
2016-07-15 09:12:40.388992: step 11930, loss = 7.34 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 09:12:46.718546: step 11940, loss = 7.28 (200.8 examples/sec; 0.638 sec/batch)
2016-07-15 09:12:52.358613: step 11950, loss = 7.05 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 09:12:58.262924: step 11960, loss = 7.21 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 09:13:03.949243: step 11970, loss = 7.34 (188.6 examples/sec; 0.679 sec/batch)
2016-07-15 09:13:09.377237: step 11980, loss = 7.29 (245.2 examples/sec; 0.522 sec/batch)
2016-07-15 09:13:15.387785: step 11990, loss = 7.47 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 09:13:21.124234: step 12000, loss = 7.20 (194.3 examples/sec; 0.659 sec/batch)
2016-07-15 09:13:27.671965: step 12010, loss = 7.24 (201.1 examples/sec; 0.637 sec/batch)
2016-07-15 09:13:32.997952: step 12020, loss = 7.23 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 09:13:37.735003: step 12030, loss = 7.34 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 09:13:42.548264: step 12040, loss = 7.47 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 09:13:47.358243: step 12050, loss = 7.02 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 09:13:53.275304: step 12060, loss = 7.32 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 09:13:59.249607: step 12070, loss = 7.01 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 09:14:05.102500: step 12080, loss = 7.19 (199.1 examples/sec; 0.643 sec/batch)
2016-07-15 09:14:10.108030: step 12090, loss = 7.23 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 09:14:14.870856: step 12100, loss = 7.21 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 09:14:21.991592: step 12110, loss = 7.45 (196.2 examples/sec; 0.652 sec/batch)
2016-07-15 09:14:27.795868: step 12120, loss = 7.38 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 09:14:33.598641: step 12130, loss = 7.45 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 09:14:38.837830: step 12140, loss = 7.26 (192.2 examples/sec; 0.666 sec/batch)
2016-07-15 09:14:44.443062: step 12150, loss = 6.95 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 09:14:50.345334: step 12160, loss = 7.24 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 09:14:55.738431: step 12170, loss = 7.20 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 09:15:01.058132: step 12180, loss = 7.18 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 09:15:06.879642: step 12190, loss = 7.28 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 09:15:12.336447: step 12200, loss = 7.20 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 09:15:18.575443: step 12210, loss = 7.27 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 09:15:23.804674: step 12220, loss = 7.31 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 09:15:29.122747: step 12230, loss = 7.28 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 09:15:33.861950: step 12240, loss = 7.25 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 09:15:39.266311: step 12250, loss = 7.57 (180.4 examples/sec; 0.709 sec/batch)
2016-07-15 09:15:45.821417: step 12260, loss = 7.39 (198.3 examples/sec; 0.645 sec/batch)
2016-07-15 09:15:50.686016: step 12270, loss = 7.37 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 09:15:55.465783: step 12280, loss = 7.33 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 09:16:01.359069: step 12290, loss = 7.25 (187.8 examples/sec; 0.681 sec/batch)
2016-07-15 09:16:07.328380: step 12300, loss = 7.18 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 09:16:13.165192: step 12310, loss = 7.30 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 09:16:18.131997: step 12320, loss = 7.45 (223.7 examples/sec; 0.572 sec/batch)
2016-07-15 09:16:24.970891: step 12330, loss = 6.93 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 09:16:30.207283: step 12340, loss = 7.08 (215.5 examples/sec; 0.594 sec/batch)
2016-07-15 09:16:35.967841: step 12350, loss = 7.10 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 09:16:41.724511: step 12360, loss = 7.36 (199.8 examples/sec; 0.641 sec/batch)
2016-07-15 09:16:46.641800: step 12370, loss = 7.26 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 09:16:51.448056: step 12380, loss = 7.41 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 09:16:56.272737: step 12390, loss = 7.24 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 09:17:01.115882: step 12400, loss = 7.20 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 09:17:06.945220: step 12410, loss = 7.23 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 09:17:11.784893: step 12420, loss = 7.25 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 09:17:16.655627: step 12430, loss = 7.16 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 09:17:22.906511: step 12440, loss = 7.04 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 09:17:28.506235: step 12450, loss = 7.20 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 09:17:33.268951: step 12460, loss = 7.20 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 09:17:38.318416: step 12470, loss = 7.27 (201.4 examples/sec; 0.635 sec/batch)
2016-07-15 09:17:45.071997: step 12480, loss = 7.00 (199.4 examples/sec; 0.642 sec/batch)
2016-07-15 09:17:50.203835: step 12490, loss = 7.07 (232.2 examples/sec; 0.551 sec/batch)
2016-07-15 09:17:55.928031: step 12500, loss = 7.37 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 09:18:02.759784: step 12510, loss = 7.20 (252.2 examples/sec; 0.508 sec/batch)
2016-07-15 09:18:08.177694: step 12520, loss = 7.20 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 09:18:13.508101: step 12530, loss = 7.20 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 09:18:19.260553: step 12540, loss = 7.19 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 09:18:24.844359: step 12550, loss = 7.28 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 09:18:30.073448: step 12560, loss = 7.01 (241.7 examples/sec; 0.530 sec/batch)
2016-07-15 09:18:36.021484: step 12570, loss = 7.17 (239.4 examples/sec; 0.535 sec/batch)
2016-07-15 09:18:40.934022: step 12580, loss = 7.37 (248.4 examples/sec; 0.515 sec/batch)
2016-07-15 09:18:45.882978: step 12590, loss = 7.17 (239.9 examples/sec; 0.534 sec/batch)
2016-07-15 09:18:50.801972: step 12600, loss = 7.51 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 09:18:56.676981: step 12610, loss = 7.24 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 09:19:01.506560: step 12620, loss = 6.99 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 09:19:08.145500: step 12630, loss = 7.31 (198.1 examples/sec; 0.646 sec/batch)
2016-07-15 09:19:13.450649: step 12640, loss = 7.28 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 09:19:18.166330: step 12650, loss = 7.06 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 09:19:23.539500: step 12660, loss = 7.03 (183.8 examples/sec; 0.697 sec/batch)
2016-07-15 09:19:30.112486: step 12670, loss = 7.06 (196.0 examples/sec; 0.653 sec/batch)
2016-07-15 09:19:35.358988: step 12680, loss = 7.24 (210.1 examples/sec; 0.609 sec/batch)
2016-07-15 09:19:40.857472: step 12690, loss = 7.13 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 09:19:45.561933: step 12700, loss = 7.17 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 09:19:51.442720: step 12710, loss = 7.27 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 09:19:56.262629: step 12720, loss = 7.16 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 09:20:02.408320: step 12730, loss = 7.25 (187.6 examples/sec; 0.682 sec/batch)
2016-07-15 09:20:08.164749: step 12740, loss = 7.36 (254.8 examples/sec; 0.502 sec/batch)
2016-07-15 09:20:14.164595: step 12750, loss = 7.29 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 09:20:19.703576: step 12760, loss = 7.39 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 09:20:25.242384: step 12770, loss = 7.25 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 09:20:31.046507: step 12780, loss = 7.02 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 09:20:35.823009: step 12790, loss = 7.32 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 09:20:40.607875: step 12800, loss = 7.27 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 09:20:48.261805: step 12810, loss = 7.01 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 09:20:53.591952: step 12820, loss = 7.13 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 09:20:58.391985: step 12830, loss = 7.09 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 09:21:03.815650: step 12840, loss = 7.25 (184.7 examples/sec; 0.693 sec/batch)
2016-07-15 09:21:10.358915: step 12850, loss = 7.07 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 09:21:15.753650: step 12860, loss = 7.28 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 09:21:21.178400: step 12870, loss = 7.12 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 09:21:26.957700: step 12880, loss = 7.24 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 09:21:32.296796: step 12890, loss = 7.19 (204.3 examples/sec; 0.626 sec/batch)
2016-07-15 09:21:37.572108: step 12900, loss = 7.03 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 09:21:43.295870: step 12910, loss = 7.15 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 09:21:48.191011: step 12920, loss = 7.24 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 09:21:52.848854: step 12930, loss = 7.09 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 09:21:57.505072: step 12940, loss = 7.31 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 09:22:03.336688: step 12950, loss = 7.31 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 09:22:08.800750: step 12960, loss = 7.20 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 09:22:14.020638: step 12970, loss = 7.05 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 09:22:19.835621: step 12980, loss = 7.12 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 09:22:24.642653: step 12990, loss = 7.15 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 09:22:29.448935: step 13000, loss = 7.41 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 09:22:35.207250: step 13010, loss = 7.20 (235.4 examples/sec; 0.544 sec/batch)
2016-07-15 09:22:40.872898: step 13020, loss = 7.17 (184.8 examples/sec; 0.692 sec/batch)
2016-07-15 09:22:47.024566: step 13030, loss = 7.24 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 09:22:52.412607: step 13040, loss = 7.17 (197.9 examples/sec; 0.647 sec/batch)
2016-07-15 09:22:57.757057: step 13050, loss = 7.24 (254.8 examples/sec; 0.502 sec/batch)
2016-07-15 09:23:03.604850: step 13060, loss = 7.09 (250.6 examples/sec; 0.511 sec/batch)
2016-07-15 09:23:08.446863: step 13070, loss = 7.23 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 09:23:13.244507: step 13080, loss = 6.96 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 09:23:18.026810: step 13090, loss = 7.24 (282.5 examples/sec; 0.453 sec/batch)
2016-07-15 09:23:23.439503: step 13100, loss = 7.41 (186.4 examples/sec; 0.687 sec/batch)
2016-07-15 09:23:31.311412: step 13110, loss = 7.10 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 09:23:36.097409: step 13120, loss = 7.03 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 09:23:41.010716: step 13130, loss = 7.20 (254.2 examples/sec; 0.504 sec/batch)
2016-07-15 09:23:47.254399: step 13140, loss = 7.10 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 09:23:52.885610: step 13150, loss = 7.39 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 09:23:58.673208: step 13160, loss = 7.14 (252.0 examples/sec; 0.508 sec/batch)
2016-07-15 09:24:03.544072: step 13170, loss = 7.01 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 09:24:08.310614: step 13180, loss = 7.40 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 09:24:13.093296: step 13190, loss = 6.84 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 09:24:18.016373: step 13200, loss = 7.37 (240.8 examples/sec; 0.532 sec/batch)
2016-07-15 09:24:26.255133: step 13210, loss = 6.97 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 09:24:31.123138: step 13220, loss = 7.16 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 09:24:35.897856: step 13230, loss = 6.93 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 09:24:41.884813: step 13240, loss = 7.06 (179.5 examples/sec; 0.713 sec/batch)
2016-07-15 09:24:47.772176: step 13250, loss = 7.06 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 09:24:52.627850: step 13260, loss = 7.15 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 09:24:57.483534: step 13270, loss = 7.48 (253.0 examples/sec; 0.506 sec/batch)
2016-07-15 09:25:02.251703: step 13280, loss = 7.29 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 09:25:06.981689: step 13290, loss = 6.92 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 09:25:11.661428: step 13300, loss = 7.31 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 09:25:17.210229: step 13310, loss = 6.86 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 09:25:23.080769: step 13320, loss = 7.22 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 09:25:28.486121: step 13330, loss = 7.04 (199.2 examples/sec; 0.642 sec/batch)
2016-07-15 09:25:33.976000: step 13340, loss = 7.15 (236.5 examples/sec; 0.541 sec/batch)
2016-07-15 09:25:39.830858: step 13350, loss = 7.09 (252.3 examples/sec; 0.507 sec/batch)
2016-07-15 09:25:44.657611: step 13360, loss = 7.08 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 09:25:49.512978: step 13370, loss = 7.15 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 09:25:56.055774: step 13380, loss = 7.03 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 09:26:01.619401: step 13390, loss = 7.24 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 09:26:07.594116: step 13400, loss = 7.21 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 09:26:14.493498: step 13410, loss = 7.40 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 09:26:19.648220: step 13420, loss = 7.33 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 09:26:25.186871: step 13430, loss = 7.32 (250.0 examples/sec; 0.512 sec/batch)
2016-07-15 09:26:31.097952: step 13440, loss = 7.27 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 09:26:36.675643: step 13450, loss = 7.01 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 09:26:42.084215: step 13460, loss = 7.06 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 09:26:47.873017: step 13470, loss = 6.93 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 09:26:52.714391: step 13480, loss = 7.20 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 09:26:57.711671: step 13490, loss = 7.09 (247.2 examples/sec; 0.518 sec/batch)
2016-07-15 09:27:02.440930: step 13500, loss = 7.19 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 09:27:09.551203: step 13510, loss = 7.06 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 09:27:15.544872: step 13520, loss = 7.18 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 09:27:21.150787: step 13530, loss = 7.14 (177.6 examples/sec; 0.721 sec/batch)
2016-07-15 09:27:26.594297: step 13540, loss = 7.34 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 09:27:31.290884: step 13550, loss = 7.22 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 09:27:35.927440: step 13560, loss = 7.07 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 09:27:41.585892: step 13570, loss = 7.09 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 09:27:46.773162: step 13580, loss = 7.16 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 09:27:51.640267: step 13590, loss = 7.10 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 09:27:57.381970: step 13600, loss = 7.14 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 09:28:04.791712: step 13610, loss = 7.12 (236.3 examples/sec; 0.542 sec/batch)
2016-07-15 09:28:10.657575: step 13620, loss = 6.94 (211.3 examples/sec; 0.606 sec/batch)
2016-07-15 09:28:15.992981: step 13630, loss = 7.24 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 09:28:21.489242: step 13640, loss = 6.98 (238.5 examples/sec; 0.537 sec/batch)
2016-07-15 09:28:27.321030: step 13650, loss = 6.86 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 09:28:32.701459: step 13660, loss = 7.16 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 09:28:37.973453: step 13670, loss = 7.22 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 09:28:42.687792: step 13680, loss = 7.13 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 09:28:47.809011: step 13690, loss = 7.29 (246.1 examples/sec; 0.520 sec/batch)
2016-07-15 09:28:52.629161: step 13700, loss = 7.09 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 09:29:00.154862: step 13710, loss = 7.22 (198.4 examples/sec; 0.645 sec/batch)
2016-07-15 09:29:05.658839: step 13720, loss = 6.96 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 09:29:11.572341: step 13730, loss = 6.99 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 09:29:16.360115: step 13740, loss = 7.18 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 09:29:21.236181: step 13750, loss = 7.14 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 09:29:25.969120: step 13760, loss = 7.28 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 09:29:30.851407: step 13770, loss = 7.15 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 09:29:35.582281: step 13780, loss = 6.92 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 09:29:40.444418: step 13790, loss = 7.20 (239.5 examples/sec; 0.534 sec/batch)
2016-07-15 09:29:45.262859: step 13800, loss = 7.20 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 09:29:53.034416: step 13810, loss = 6.95 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 09:29:58.327432: step 13820, loss = 6.97 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 09:30:04.277906: step 13830, loss = 6.94 (244.8 examples/sec; 0.523 sec/batch)
2016-07-15 09:30:09.891781: step 13840, loss = 7.21 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 09:30:14.922271: step 13850, loss = 6.87 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 09:30:19.658675: step 13860, loss = 7.22 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 09:30:24.456751: step 13870, loss = 7.08 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 09:30:29.243709: step 13880, loss = 7.08 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 09:30:35.546799: step 13890, loss = 7.16 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 09:30:41.266873: step 13900, loss = 7.07 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 09:30:46.986184: step 13910, loss = 7.16 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 09:30:51.866813: step 13920, loss = 7.37 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 09:30:56.684107: step 13930, loss = 7.05 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 09:31:01.474639: step 13940, loss = 6.96 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 09:31:06.344318: step 13950, loss = 7.34 (243.1 examples/sec; 0.527 sec/batch)
2016-07-15 09:31:13.131271: step 13960, loss = 7.16 (208.8 examples/sec; 0.613 sec/batch)
2016-07-15 09:31:18.644692: step 13970, loss = 7.03 (246.6 examples/sec; 0.519 sec/batch)
2016-07-15 09:31:23.379026: step 13980, loss = 7.20 (252.8 examples/sec; 0.506 sec/batch)
2016-07-15 09:31:29.107616: step 13990, loss = 7.03 (180.0 examples/sec; 0.711 sec/batch)
2016-07-15 09:31:35.590206: step 14000, loss = 7.22 (251.7 examples/sec; 0.509 sec/batch)
2016-07-15 09:31:42.622266: step 14010, loss = 7.04 (192.0 examples/sec; 0.667 sec/batch)
2016-07-15 09:31:47.515371: step 14020, loss = 7.26 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 09:31:52.276540: step 14030, loss = 7.08 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 09:31:58.101556: step 14040, loss = 7.17 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 09:32:04.013935: step 14050, loss = 7.12 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 09:32:09.550799: step 14060, loss = 6.97 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 09:32:14.682543: step 14070, loss = 7.23 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 09:32:19.370840: step 14080, loss = 6.95 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 09:32:24.902673: step 14090, loss = 7.15 (189.5 examples/sec; 0.676 sec/batch)
2016-07-15 09:32:31.176547: step 14100, loss = 7.04 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 09:32:36.937818: step 14110, loss = 7.26 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 09:32:41.736683: step 14120, loss = 7.01 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 09:32:46.679297: step 14130, loss = 6.95 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 09:32:51.588029: step 14140, loss = 6.90 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 09:32:56.451914: step 14150, loss = 7.08 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 09:33:02.660993: step 14160, loss = 7.24 (180.9 examples/sec; 0.707 sec/batch)
2016-07-15 09:33:08.481393: step 14170, loss = 7.10 (249.5 examples/sec; 0.513 sec/batch)
2016-07-15 09:33:14.074992: step 14180, loss = 6.81 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 09:33:19.078585: step 14190, loss = 7.00 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 09:33:23.805534: step 14200, loss = 7.09 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 09:33:30.908712: step 14210, loss = 7.23 (195.4 examples/sec; 0.655 sec/batch)
2016-07-15 09:33:36.595726: step 14220, loss = 6.90 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 09:33:42.292061: step 14230, loss = 7.16 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 09:33:47.432668: step 14240, loss = 6.85 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 09:33:53.128217: step 14250, loss = 7.16 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 09:33:58.963368: step 14260, loss = 7.31 (224.7 examples/sec; 0.570 sec/batch)
2016-07-15 09:34:04.182191: step 14270, loss = 6.99 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 09:34:09.580775: step 14280, loss = 7.12 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 09:34:15.486858: step 14290, loss = 7.11 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 09:34:21.022355: step 14300, loss = 7.01 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 09:34:27.477749: step 14310, loss = 7.25 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 09:34:32.228977: step 14320, loss = 7.02 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 09:34:37.111695: step 14330, loss = 6.86 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 09:34:41.970024: step 14340, loss = 7.19 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 09:34:48.382702: step 14350, loss = 6.73 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 09:34:53.800810: step 14360, loss = 7.35 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 09:34:58.662868: step 14370, loss = 6.92 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 09:35:03.984501: step 14380, loss = 7.08 (184.3 examples/sec; 0.694 sec/batch)
2016-07-15 09:35:10.800054: step 14390, loss = 6.92 (177.5 examples/sec; 0.721 sec/batch)
2016-07-15 09:35:15.769742: step 14400, loss = 7.11 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 09:35:21.568343: step 14410, loss = 7.09 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 09:35:27.844793: step 14420, loss = 7.12 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 09:35:33.452994: step 14430, loss = 7.27 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 09:35:39.185877: step 14440, loss = 7.16 (240.9 examples/sec; 0.531 sec/batch)
2016-07-15 09:35:44.430377: step 14450, loss = 6.89 (202.4 examples/sec; 0.633 sec/batch)
2016-07-15 09:35:50.043176: step 14460, loss = 7.30 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 09:35:54.772582: step 14470, loss = 6.93 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 09:35:59.977962: step 14480, loss = 7.13 (182.2 examples/sec; 0.702 sec/batch)
2016-07-15 09:36:06.626315: step 14490, loss = 7.17 (198.6 examples/sec; 0.644 sec/batch)
2016-07-15 09:36:11.880869: step 14500, loss = 7.10 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 09:36:18.735759: step 14510, loss = 7.11 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 09:36:23.484388: step 14520, loss = 7.04 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 09:36:29.172337: step 14530, loss = 7.13 (188.7 examples/sec; 0.679 sec/batch)
2016-07-15 09:36:35.578076: step 14540, loss = 6.96 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 09:36:41.056965: step 14550, loss = 7.12 (197.6 examples/sec; 0.648 sec/batch)
2016-07-15 09:36:46.434888: step 14560, loss = 7.13 (262.6 examples/sec; 0.488 sec/batch)
2016-07-15 09:36:51.120357: step 14570, loss = 7.25 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 09:36:56.429218: step 14580, loss = 7.10 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 09:37:02.895684: step 14590, loss = 6.96 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 09:37:07.998675: step 14600, loss = 7.16 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 09:37:14.712666: step 14610, loss = 7.00 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 09:37:20.478429: step 14620, loss = 7.10 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 09:37:25.215964: step 14630, loss = 6.92 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 09:37:30.042566: step 14640, loss = 7.14 (246.6 examples/sec; 0.519 sec/batch)
2016-07-15 09:37:36.317077: step 14650, loss = 6.97 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 09:37:41.759968: step 14660, loss = 6.99 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 09:37:47.541675: step 14670, loss = 7.03 (257.3 examples/sec; 0.498 sec/batch)
2016-07-15 09:37:52.396634: step 14680, loss = 7.17 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 09:37:57.149901: step 14690, loss = 7.06 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 09:38:03.203941: step 14700, loss = 7.11 (193.7 examples/sec; 0.661 sec/batch)
2016-07-15 09:38:10.203528: step 14710, loss = 7.15 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 09:38:15.992422: step 14720, loss = 6.94 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 09:38:20.775831: step 14730, loss = 6.97 (283.0 examples/sec; 0.452 sec/batch)
2016-07-15 09:38:25.546018: step 14740, loss = 7.10 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 09:38:30.223082: step 14750, loss = 7.05 (282.4 examples/sec; 0.453 sec/batch)
2016-07-15 09:38:35.071754: step 14760, loss = 6.96 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 09:38:39.831421: step 14770, loss = 6.94 (246.4 examples/sec; 0.519 sec/batch)
2016-07-15 09:38:45.524542: step 14780, loss = 7.08 (184.3 examples/sec; 0.695 sec/batch)
2016-07-15 09:38:51.702100: step 14790, loss = 7.15 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 09:38:57.056288: step 14800, loss = 7.10 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 09:39:03.805800: step 14810, loss = 6.86 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 09:39:09.356830: step 14820, loss = 7.15 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 09:39:15.138780: step 14830, loss = 7.18 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 09:39:20.007092: step 14840, loss = 7.18 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 09:39:24.727103: step 14850, loss = 7.27 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 09:39:30.556329: step 14860, loss = 7.09 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 09:39:36.441017: step 14870, loss = 7.17 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 09:39:41.902430: step 14880, loss = 6.95 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 09:39:47.068849: step 14890, loss = 7.11 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 09:39:51.739881: step 14900, loss = 7.00 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 09:39:57.496741: step 14910, loss = 7.01 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 09:40:02.333430: step 14920, loss = 7.27 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 09:40:07.014197: step 14930, loss = 7.08 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 09:40:12.221342: step 14940, loss = 6.84 (187.0 examples/sec; 0.684 sec/batch)
2016-07-15 09:40:18.690099: step 14950, loss = 7.09 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 09:40:23.805557: step 14960, loss = 7.06 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 09:40:29.385372: step 14970, loss = 6.75 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 09:40:35.202620: step 14980, loss = 6.99 (208.3 examples/sec; 0.615 sec/batch)
2016-07-15 09:40:40.033383: step 14990, loss = 7.06 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 09:40:44.756962: step 15000, loss = 7.18 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 09:40:52.000299: step 15010, loss = 6.95 (200.4 examples/sec; 0.639 sec/batch)
2016-07-15 09:40:57.554695: step 15020, loss = 7.11 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 09:41:02.354969: step 15030, loss = 6.94 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 09:41:07.193785: step 15040, loss = 6.99 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 09:41:11.872971: step 15050, loss = 7.05 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 09:41:17.329144: step 15060, loss = 6.97 (191.7 examples/sec; 0.668 sec/batch)
2016-07-15 09:41:23.592696: step 15070, loss = 7.05 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 09:41:28.888656: step 15080, loss = 6.97 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 09:41:34.291691: step 15090, loss = 7.12 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 09:41:40.018944: step 15100, loss = 6.99 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 09:41:45.935780: step 15110, loss = 7.19 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 09:41:51.017950: step 15120, loss = 7.10 (245.3 examples/sec; 0.522 sec/batch)
2016-07-15 09:41:58.028066: step 15130, loss = 6.90 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 09:42:03.210636: step 15140, loss = 7.05 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 09:42:07.994793: step 15150, loss = 6.91 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 09:42:13.541677: step 15160, loss = 7.11 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 09:42:19.837538: step 15170, loss = 6.89 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 09:42:25.144407: step 15180, loss = 6.95 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 09:42:30.496733: step 15190, loss = 6.89 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 09:42:36.263410: step 15200, loss = 7.15 (258.3 examples/sec; 0.495 sec/batch)
2016-07-15 09:42:42.093366: step 15210, loss = 6.94 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 09:42:46.926188: step 15220, loss = 6.81 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 09:42:51.649570: step 15230, loss = 7.06 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 09:42:57.084045: step 15240, loss = 6.91 (192.0 examples/sec; 0.667 sec/batch)
2016-07-15 09:43:03.454009: step 15250, loss = 6.91 (221.4 examples/sec; 0.578 sec/batch)
2016-07-15 09:43:08.385447: step 15260, loss = 6.85 (250.0 examples/sec; 0.512 sec/batch)
2016-07-15 09:43:13.028854: step 15270, loss = 7.00 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 09:43:17.686778: step 15280, loss = 6.89 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 09:43:23.400647: step 15290, loss = 7.02 (208.2 examples/sec; 0.615 sec/batch)
2016-07-15 09:43:28.607270: step 15300, loss = 6.91 (199.6 examples/sec; 0.641 sec/batch)
2016-07-15 09:43:35.331039: step 15310, loss = 6.85 (260.4 examples/sec; 0.491 sec/batch)
2016-07-15 09:43:40.020683: step 15320, loss = 7.12 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 09:43:44.833390: step 15330, loss = 6.95 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 09:43:49.639900: step 15340, loss = 6.83 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 09:43:55.766356: step 15350, loss = 6.88 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 09:44:01.504081: step 15360, loss = 7.02 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 09:44:07.489436: step 15370, loss = 6.89 (221.3 examples/sec; 0.578 sec/batch)
2016-07-15 09:44:12.627585: step 15380, loss = 7.20 (209.5 examples/sec; 0.611 sec/batch)
2016-07-15 09:44:18.113360: step 15390, loss = 6.89 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 09:44:23.927704: step 15400, loss = 6.92 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 09:44:30.619772: step 15410, loss = 7.19 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 09:44:35.775664: step 15420, loss = 7.07 (213.7 examples/sec; 0.599 sec/batch)
2016-07-15 09:44:41.448043: step 15430, loss = 6.82 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 09:44:46.243517: step 15440, loss = 6.82 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 09:44:51.077760: step 15450, loss = 6.99 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 09:44:57.518645: step 15460, loss = 6.86 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 09:45:02.878220: step 15470, loss = 7.00 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 09:45:07.608439: step 15480, loss = 6.86 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 09:45:12.510255: step 15490, loss = 7.00 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 09:45:17.302330: step 15500, loss = 7.17 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 09:45:23.057287: step 15510, loss = 6.66 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 09:45:28.164707: step 15520, loss = 7.20 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 09:45:34.719877: step 15530, loss = 6.88 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 09:45:39.780517: step 15540, loss = 6.92 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 09:45:44.532684: step 15550, loss = 6.68 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 09:45:50.245535: step 15560, loss = 6.90 (182.1 examples/sec; 0.703 sec/batch)
2016-07-15 09:45:56.327816: step 15570, loss = 6.94 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 09:46:01.740660: step 15580, loss = 6.89 (209.5 examples/sec; 0.611 sec/batch)
2016-07-15 09:46:07.076196: step 15590, loss = 6.87 (240.1 examples/sec; 0.533 sec/batch)
2016-07-15 09:46:11.778730: step 15600, loss = 6.86 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 09:46:17.645327: step 15610, loss = 6.86 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 09:46:22.440380: step 15620, loss = 7.18 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 09:46:27.182720: step 15630, loss = 6.83 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 09:46:32.290224: step 15640, loss = 7.16 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 09:46:38.812726: step 15650, loss = 6.98 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 09:46:43.949409: step 15660, loss = 6.85 (209.8 examples/sec; 0.610 sec/batch)
2016-07-15 09:46:49.599981: step 15670, loss = 6.81 (252.7 examples/sec; 0.507 sec/batch)
2016-07-15 09:46:55.293376: step 15680, loss = 6.84 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 09:47:00.443862: step 15690, loss = 7.20 (196.3 examples/sec; 0.652 sec/batch)
2016-07-15 09:47:06.019883: step 15700, loss = 6.69 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 09:47:12.822664: step 15710, loss = 7.20 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 09:47:18.312609: step 15720, loss = 7.07 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 09:47:23.503406: step 15730, loss = 7.12 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 09:47:28.242216: step 15740, loss = 7.08 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 09:47:33.795420: step 15750, loss = 6.88 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 09:47:40.058064: step 15760, loss = 7.12 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 09:47:44.871646: step 15770, loss = 7.04 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 09:47:49.655357: step 15780, loss = 6.76 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 09:47:55.792990: step 15790, loss = 6.96 (192.6 examples/sec; 0.665 sec/batch)
2016-07-15 09:48:01.533824: step 15800, loss = 6.91 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 09:48:07.183886: step 15810, loss = 6.80 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 09:48:12.045743: step 15820, loss = 6.76 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 09:48:16.796162: step 15830, loss = 6.90 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 09:48:22.525497: step 15840, loss = 6.93 (186.5 examples/sec; 0.686 sec/batch)
2016-07-15 09:48:28.637569: step 15850, loss = 6.91 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 09:48:33.435570: step 15860, loss = 7.08 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 09:48:38.228631: step 15870, loss = 6.67 (254.0 examples/sec; 0.504 sec/batch)
2016-07-15 09:48:44.442087: step 15880, loss = 7.25 (193.1 examples/sec; 0.663 sec/batch)
2016-07-15 09:48:50.202344: step 15890, loss = 6.86 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 09:48:55.981136: step 15900, loss = 6.91 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 09:49:01.837379: step 15910, loss = 6.79 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 09:49:06.660941: step 15920, loss = 6.80 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 09:49:12.952063: step 15930, loss = 6.83 (203.3 examples/sec; 0.629 sec/batch)
2016-07-15 09:49:18.388809: step 15940, loss = 6.94 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 09:49:24.199078: step 15950, loss = 7.10 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 09:49:29.517011: step 15960, loss = 6.99 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 09:49:34.857933: step 15970, loss = 6.95 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 09:49:40.620782: step 15980, loss = 6.94 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 09:49:45.408836: step 15990, loss = 7.12 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 09:49:50.197097: step 16000, loss = 7.09 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 09:49:57.686920: step 16010, loss = 7.09 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 09:50:02.978406: step 16020, loss = 6.94 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 09:50:08.781284: step 16030, loss = 7.02 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 09:50:14.226441: step 16040, loss = 6.87 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 09:50:19.466931: step 16050, loss = 7.01 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 09:50:25.283311: step 16060, loss = 6.95 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 09:50:30.032076: step 16070, loss = 6.80 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 09:50:34.840979: step 16080, loss = 7.07 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 09:50:39.560596: step 16090, loss = 6.93 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 09:50:44.428525: step 16100, loss = 6.88 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 09:50:50.177222: step 16110, loss = 6.94 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 09:50:54.930839: step 16120, loss = 6.95 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 09:50:59.742343: step 16130, loss = 6.56 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 09:51:06.329843: step 16140, loss = 6.87 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 09:51:11.509646: step 16150, loss = 6.91 (268.6 examples/sec; 0.476 sec/batch)
2016-07-15 09:51:17.336782: step 16160, loss = 7.12 (253.9 examples/sec; 0.504 sec/batch)
2016-07-15 09:51:22.887042: step 16170, loss = 7.17 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 09:51:27.968819: step 16180, loss = 6.95 (244.3 examples/sec; 0.524 sec/batch)
2016-07-15 09:51:33.676753: step 16190, loss = 6.86 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 09:51:38.436030: step 16200, loss = 7.10 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 09:51:44.502336: step 16210, loss = 6.99 (188.1 examples/sec; 0.680 sec/batch)
2016-07-15 09:51:51.002529: step 16220, loss = 6.98 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 09:51:56.110769: step 16230, loss = 6.95 (219.3 examples/sec; 0.584 sec/batch)
2016-07-15 09:52:01.845513: step 16240, loss = 6.91 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 09:52:06.699850: step 16250, loss = 7.08 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 09:52:11.421730: step 16260, loss = 6.89 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 09:52:16.239570: step 16270, loss = 6.93 (210.3 examples/sec; 0.609 sec/batch)
2016-07-15 09:52:21.880221: step 16280, loss = 6.92 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 09:52:27.606398: step 16290, loss = 6.97 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 09:52:32.743542: step 16300, loss = 6.97 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 09:52:39.536663: step 16310, loss = 6.98 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 09:52:45.313017: step 16320, loss = 7.07 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 09:52:50.085561: step 16330, loss = 6.90 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 09:52:54.889133: step 16340, loss = 6.85 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 09:53:01.209653: step 16350, loss = 6.78 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 09:53:06.700013: step 16360, loss = 6.95 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 09:53:11.516157: step 16370, loss = 6.75 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 09:53:16.310242: step 16380, loss = 6.96 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 09:53:22.515399: step 16390, loss = 7.00 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 09:53:28.061216: step 16400, loss = 6.70 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 09:53:34.814283: step 16410, loss = 6.76 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 09:53:39.654345: step 16420, loss = 6.75 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 09:53:44.438878: step 16430, loss = 7.03 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 09:53:49.228892: step 16440, loss = 6.75 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 09:53:54.086582: step 16450, loss = 7.00 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 09:53:58.816743: step 16460, loss = 6.78 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 09:54:03.632287: step 16470, loss = 6.93 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 09:54:08.417033: step 16480, loss = 7.10 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 09:54:14.618829: step 16490, loss = 6.85 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 09:54:20.184151: step 16500, loss = 6.76 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 09:54:25.926831: step 16510, loss = 6.82 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 09:54:30.722548: step 16520, loss = 6.92 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 09:54:35.502840: step 16530, loss = 6.90 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 09:54:41.423470: step 16540, loss = 6.97 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 09:54:47.308265: step 16550, loss = 6.86 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 09:54:52.068482: step 16560, loss = 6.83 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 09:54:56.865339: step 16570, loss = 6.79 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 09:55:01.584134: step 16580, loss = 7.09 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 09:55:06.690951: step 16590, loss = 6.75 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 09:55:13.182143: step 16600, loss = 6.91 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 09:55:19.843623: step 16610, loss = 6.83 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 09:55:25.146990: step 16620, loss = 6.88 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 09:55:30.954734: step 16630, loss = 6.95 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 09:55:35.702067: step 16640, loss = 6.86 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 09:55:40.493205: step 16650, loss = 6.84 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 09:55:46.694845: step 16660, loss = 7.15 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 09:55:52.238153: step 16670, loss = 6.88 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 09:55:58.004097: step 16680, loss = 6.87 (212.1 examples/sec; 0.604 sec/batch)
2016-07-15 09:56:02.822614: step 16690, loss = 7.04 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 09:56:07.515357: step 16700, loss = 7.09 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 09:56:14.670004: step 16710, loss = 6.54 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 09:56:20.230579: step 16720, loss = 6.82 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 09:56:24.993064: step 16730, loss = 6.79 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 09:56:29.821557: step 16740, loss = 6.77 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 09:56:34.489441: step 16750, loss = 6.87 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 09:56:39.298146: step 16760, loss = 6.77 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 09:56:44.132663: step 16770, loss = 7.00 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 09:56:50.242679: step 16780, loss = 6.89 (197.1 examples/sec; 0.649 sec/batch)
2016-07-15 09:56:55.968926: step 16790, loss = 6.60 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 09:57:00.690925: step 16800, loss = 6.89 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 09:57:06.476427: step 16810, loss = 6.96 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 09:57:11.216878: step 16820, loss = 6.79 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 09:57:17.216291: step 16830, loss = 6.87 (186.0 examples/sec; 0.688 sec/batch)
2016-07-15 09:57:23.641007: step 16840, loss = 6.81 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 09:57:29.237777: step 16850, loss = 7.19 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 09:57:34.351416: step 16860, loss = 6.61 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 09:57:39.088557: step 16870, loss = 7.04 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 09:57:44.802327: step 16880, loss = 6.96 (190.6 examples/sec; 0.671 sec/batch)
2016-07-15 09:57:50.903759: step 16890, loss = 7.11 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 09:57:56.300463: step 16900, loss = 6.83 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 09:58:03.004236: step 16910, loss = 6.93 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 09:58:08.565965: step 16920, loss = 6.87 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 09:58:14.364309: step 16930, loss = 6.93 (250.0 examples/sec; 0.512 sec/batch)
2016-07-15 09:58:19.587797: step 16940, loss = 6.61 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 09:58:25.029273: step 16950, loss = 6.79 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 09:58:29.715878: step 16960, loss = 6.78 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 09:58:34.840243: step 16970, loss = 7.11 (191.5 examples/sec; 0.668 sec/batch)
2016-07-15 09:58:41.345233: step 16980, loss = 7.05 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 09:58:46.428761: step 16990, loss = 6.88 (218.6 examples/sec; 0.585 sec/batch)
2016-07-15 09:58:52.180155: step 17000, loss = 6.57 (240.6 examples/sec; 0.532 sec/batch)
2016-07-15 09:58:58.906264: step 17010, loss = 6.79 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 09:59:04.328901: step 17020, loss = 6.96 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 09:59:09.651549: step 17030, loss = 6.76 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 09:59:15.395944: step 17040, loss = 7.15 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 09:59:20.819567: step 17050, loss = 6.89 (209.2 examples/sec; 0.612 sec/batch)
2016-07-15 09:59:26.026423: step 17060, loss = 7.01 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 09:59:30.720632: step 17070, loss = 7.02 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 09:59:36.216196: step 17080, loss = 6.79 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 09:59:42.493155: step 17090, loss = 6.77 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 09:59:47.340037: step 17100, loss = 6.87 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 09:59:53.102251: step 17110, loss = 6.99 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 09:59:57.801512: step 17120, loss = 6.85 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 10:00:02.876672: step 17130, loss = 6.82 (192.0 examples/sec; 0.667 sec/batch)
2016-07-15 10:00:09.391420: step 17140, loss = 6.80 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 10:00:14.415860: step 17150, loss = 6.89 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 10:00:19.105457: step 17160, loss = 7.13 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 10:00:23.910511: step 17170, loss = 6.77 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 10:00:28.722979: step 17180, loss = 7.01 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:00:34.874564: step 17190, loss = 6.99 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 10:00:40.468017: step 17200, loss = 6.95 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 10:00:46.216130: step 17210, loss = 6.94 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 10:00:51.083746: step 17220, loss = 6.87 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 10:00:55.837208: step 17230, loss = 6.77 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 10:01:01.795069: step 17240, loss = 7.04 (190.3 examples/sec; 0.673 sec/batch)
2016-07-15 10:01:07.638966: step 17250, loss = 6.86 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 10:01:12.408736: step 17260, loss = 6.81 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 10:01:17.248365: step 17270, loss = 6.80 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 10:01:23.531731: step 17280, loss = 6.80 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 10:01:29.059975: step 17290, loss = 6.49 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 10:01:34.826040: step 17300, loss = 6.61 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 10:01:41.384072: step 17310, loss = 6.75 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 10:01:46.517202: step 17320, loss = 6.97 (232.1 examples/sec; 0.551 sec/batch)
2016-07-15 10:01:52.237695: step 17330, loss = 7.13 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 10:01:56.996954: step 17340, loss = 6.87 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 10:02:01.863370: step 17350, loss = 6.81 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 10:02:06.592781: step 17360, loss = 7.04 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 10:02:12.084964: step 17370, loss = 7.06 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 10:02:18.480177: step 17380, loss = 6.91 (225.4 examples/sec; 0.568 sec/batch)
2016-07-15 10:02:23.738968: step 17390, loss = 6.89 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 10:02:29.214824: step 17400, loss = 6.58 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 10:02:34.951563: step 17410, loss = 6.93 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 10:02:40.432802: step 17420, loss = 6.78 (186.4 examples/sec; 0.687 sec/batch)
2016-07-15 10:02:46.707885: step 17430, loss = 7.04 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 10:02:51.529615: step 17440, loss = 6.92 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 10:02:56.296512: step 17450, loss = 6.76 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 10:03:01.066316: step 17460, loss = 6.98 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 10:03:05.929359: step 17470, loss = 6.95 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 10:03:12.368770: step 17480, loss = 6.83 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 10:03:17.675724: step 17490, loss = 6.76 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:03:22.378696: step 17500, loss = 7.01 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 10:03:29.046613: step 17510, loss = 6.80 (192.9 examples/sec; 0.663 sec/batch)
2016-07-15 10:03:35.149988: step 17520, loss = 6.95 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 10:03:40.538132: step 17530, loss = 6.74 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 10:03:45.826541: step 17540, loss = 6.94 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 10:03:50.581178: step 17550, loss = 6.96 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 10:03:55.988585: step 17560, loss = 6.60 (184.1 examples/sec; 0.695 sec/batch)
2016-07-15 10:04:02.441268: step 17570, loss = 6.99 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 10:04:07.312240: step 17580, loss = 6.69 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 10:04:12.036229: step 17590, loss = 6.87 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 10:04:16.813629: step 17600, loss = 6.60 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 10:04:22.665379: step 17610, loss = 6.87 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 10:04:27.419890: step 17620, loss = 6.68 (250.3 examples/sec; 0.511 sec/batch)
2016-07-15 10:04:32.199500: step 17630, loss = 6.74 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 10:04:36.990587: step 17640, loss = 6.97 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 10:04:43.204560: step 17650, loss = 6.89 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 10:04:48.770059: step 17660, loss = 6.68 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 10:04:53.560308: step 17670, loss = 6.64 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 10:04:58.392087: step 17680, loss = 6.71 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 10:05:03.142725: step 17690, loss = 6.73 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 10:05:08.684150: step 17700, loss = 6.78 (191.7 examples/sec; 0.668 sec/batch)
2016-07-15 10:05:16.180989: step 17710, loss = 6.84 (249.3 examples/sec; 0.513 sec/batch)
2016-07-15 10:05:21.884467: step 17720, loss = 6.76 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 10:05:27.078012: step 17730, loss = 6.70 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 10:05:32.702550: step 17740, loss = 7.12 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 10:05:38.518971: step 17750, loss = 6.66 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 10:05:43.725197: step 17760, loss = 6.77 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 10:05:49.229510: step 17770, loss = 6.92 (253.1 examples/sec; 0.506 sec/batch)
2016-07-15 10:05:55.016993: step 17780, loss = 6.77 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 10:05:59.866431: step 17790, loss = 6.84 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 10:06:04.614787: step 17800, loss = 6.79 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 10:06:10.388630: step 17810, loss = 6.72 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 10:06:15.578304: step 17820, loss = 7.09 (187.5 examples/sec; 0.683 sec/batch)
2016-07-15 10:06:22.042547: step 17830, loss = 6.63 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 10:06:27.156557: step 17840, loss = 6.74 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 10:06:32.737737: step 17850, loss = 6.80 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 10:06:38.525027: step 17860, loss = 7.00 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 10:06:43.375310: step 17870, loss = 6.78 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 10:06:48.200097: step 17880, loss = 6.81 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 10:06:54.186224: step 17890, loss = 6.86 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 10:07:00.088113: step 17900, loss = 6.68 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 10:07:06.910523: step 17910, loss = 6.82 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 10:07:11.739991: step 17920, loss = 6.74 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 10:07:16.543297: step 17930, loss = 6.88 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 10:07:21.350205: step 17940, loss = 6.89 (270.9 examples/sec; 0.473 sec/batch)
2016-07-15 10:07:26.198161: step 17950, loss = 6.59 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 10:07:32.696286: step 17960, loss = 6.78 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 10:07:38.133871: step 17970, loss = 6.85 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 10:07:43.882828: step 17980, loss = 7.13 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 10:07:49.453153: step 17990, loss = 6.88 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 10:07:54.678695: step 18000, loss = 6.81 (244.8 examples/sec; 0.523 sec/batch)
2016-07-15 10:08:01.654997: step 18010, loss = 6.93 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 10:08:07.377462: step 18020, loss = 6.59 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 10:08:12.818569: step 18030, loss = 6.85 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 10:08:18.109450: step 18040, loss = 6.78 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 10:08:22.821106: step 18050, loss = 7.06 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 10:08:28.154791: step 18060, loss = 6.81 (186.4 examples/sec; 0.687 sec/batch)
2016-07-15 10:08:34.631724: step 18070, loss = 6.77 (197.6 examples/sec; 0.648 sec/batch)
2016-07-15 10:08:39.508290: step 18080, loss = 6.75 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 10:08:44.289849: step 18090, loss = 6.72 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 10:08:49.077114: step 18100, loss = 6.82 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 10:08:54.702114: step 18110, loss = 6.77 (281.6 examples/sec; 0.454 sec/batch)
2016-07-15 10:08:59.700057: step 18120, loss = 6.74 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 10:09:05.260442: step 18130, loss = 7.07 (248.3 examples/sec; 0.515 sec/batch)
2016-07-15 10:09:11.008670: step 18140, loss = 6.87 (246.3 examples/sec; 0.520 sec/batch)
2016-07-15 10:09:15.858627: step 18150, loss = 6.82 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:09:20.626336: step 18160, loss = 6.48 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 10:09:25.454930: step 18170, loss = 6.79 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 10:09:30.291502: step 18180, loss = 6.74 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 10:09:36.784249: step 18190, loss = 6.81 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 10:09:42.123799: step 18200, loss = 6.85 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 10:09:49.068838: step 18210, loss = 6.85 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 10:09:53.836182: step 18220, loss = 6.80 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:09:58.703664: step 18230, loss = 6.94 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 10:10:05.288975: step 18240, loss = 6.67 (201.4 examples/sec; 0.635 sec/batch)
2016-07-15 10:10:10.461018: step 18250, loss = 6.80 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 10:10:16.263320: step 18260, loss = 6.70 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 10:10:21.848670: step 18270, loss = 6.78 (207.3 examples/sec; 0.618 sec/batch)
2016-07-15 10:10:26.929385: step 18280, loss = 6.79 (239.7 examples/sec; 0.534 sec/batch)
2016-07-15 10:10:32.633144: step 18290, loss = 6.92 (264.7 examples/sec; 0.483 sec/batch)
2016-07-15 10:10:37.344097: step 18300, loss = 6.73 (271.5 examples/sec; 0.472 sec/batch)
2016-07-15 10:10:43.854647: step 18310, loss = 6.82 (183.3 examples/sec; 0.698 sec/batch)
2016-07-15 10:10:50.334381: step 18320, loss = 7.05 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 10:10:55.325044: step 18330, loss = 6.75 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 10:11:00.047823: step 18340, loss = 6.81 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 10:11:05.797209: step 18350, loss = 6.98 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 10:11:11.824243: step 18360, loss = 6.83 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 10:11:17.292959: step 18370, loss = 6.67 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 10:11:22.489205: step 18380, loss = 6.72 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 10:11:28.268220: step 18390, loss = 6.71 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 10:11:33.062798: step 18400, loss = 6.68 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 10:11:38.908565: step 18410, loss = 6.81 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 10:11:43.636883: step 18420, loss = 6.68 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 10:11:49.142473: step 18430, loss = 6.62 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 10:11:55.370288: step 18440, loss = 6.79 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 10:12:00.169670: step 18450, loss = 6.51 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 10:12:04.918901: step 18460, loss = 6.68 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 10:12:10.928041: step 18470, loss = 6.70 (193.9 examples/sec; 0.660 sec/batch)
2016-07-15 10:12:16.642768: step 18480, loss = 6.61 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 10:12:22.285803: step 18490, loss = 6.95 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 10:12:27.297600: step 18500, loss = 6.79 (273.2 examples/sec; 0.468 sec/batch)
2016-07-15 10:12:33.117542: step 18510, loss = 6.70 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 10:12:39.174201: step 18520, loss = 6.95 (197.2 examples/sec; 0.649 sec/batch)
2016-07-15 10:12:44.878152: step 18530, loss = 7.02 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 10:12:49.605888: step 18540, loss = 6.73 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 10:12:54.486394: step 18550, loss = 6.79 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 10:13:00.945295: step 18560, loss = 6.57 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 10:13:06.243813: step 18570, loss = 6.71 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 10:13:10.923807: step 18580, loss = 6.49 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 10:13:15.741703: step 18590, loss = 6.78 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 10:13:20.510921: step 18600, loss = 6.68 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 10:13:27.692403: step 18610, loss = 6.60 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 10:13:33.248839: step 18620, loss = 6.89 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 10:13:37.984125: step 18630, loss = 6.87 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 10:13:42.815467: step 18640, loss = 6.79 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 10:13:47.497162: step 18650, loss = 6.86 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 10:13:52.282068: step 18660, loss = 6.96 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 10:13:57.030019: step 18670, loss = 6.83 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 10:14:03.048783: step 18680, loss = 6.91 (192.6 examples/sec; 0.664 sec/batch)
2016-07-15 10:14:08.784971: step 18690, loss = 6.67 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 10:14:13.542842: step 18700, loss = 6.52 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 10:14:19.352075: step 18710, loss = 6.99 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 10:14:24.103202: step 18720, loss = 6.66 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 10:14:29.711903: step 18730, loss = 6.92 (193.6 examples/sec; 0.661 sec/batch)
2016-07-15 10:14:35.800529: step 18740, loss = 6.49 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 10:14:41.183210: step 18750, loss = 6.75 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 10:14:46.515879: step 18760, loss = 6.46 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 10:14:52.223697: step 18770, loss = 6.77 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 10:14:57.740355: step 18780, loss = 6.80 (206.6 examples/sec; 0.619 sec/batch)
2016-07-15 10:15:02.881891: step 18790, loss = 6.66 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 10:15:07.567439: step 18800, loss = 6.83 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 10:15:14.421011: step 18810, loss = 6.80 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 10:15:20.327155: step 18820, loss = 6.62 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 10:15:25.132762: step 18830, loss = 6.77 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 10:15:29.970520: step 18840, loss = 6.76 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 10:15:34.701833: step 18850, loss = 6.53 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 10:15:39.771281: step 18860, loss = 6.75 (191.0 examples/sec; 0.670 sec/batch)
2016-07-15 10:15:46.438059: step 18870, loss = 6.80 (199.7 examples/sec; 0.641 sec/batch)
2016-07-15 10:15:51.619752: step 18880, loss = 6.74 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 10:15:56.727656: step 18890, loss = 6.56 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 10:16:01.354892: step 18900, loss = 6.74 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 10:16:08.023494: step 18910, loss = 6.59 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 10:16:12.848657: step 18920, loss = 6.75 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 10:16:17.670020: step 18930, loss = 6.73 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 10:16:23.890800: step 18940, loss = 6.62 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 10:16:29.484489: step 18950, loss = 6.80 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 10:16:34.227088: step 18960, loss = 6.80 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 10:16:39.134352: step 18970, loss = 6.61 (251.1 examples/sec; 0.510 sec/batch)
2016-07-15 10:16:45.741662: step 18980, loss = 6.70 (208.0 examples/sec; 0.616 sec/batch)
2016-07-15 10:16:50.894908: step 18990, loss = 6.78 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 10:16:55.679567: step 19000, loss = 7.13 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 10:17:01.555763: step 19010, loss = 6.68 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 10:17:06.359235: step 19020, loss = 6.86 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 10:17:12.737494: step 19030, loss = 6.78 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 10:17:18.130651: step 19040, loss = 6.68 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 10:17:22.916391: step 19050, loss = 6.77 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 10:17:28.062311: step 19060, loss = 6.53 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 10:17:34.571118: step 19070, loss = 6.73 (198.3 examples/sec; 0.645 sec/batch)
2016-07-15 10:17:39.544509: step 19080, loss = 6.45 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 10:17:44.306138: step 19090, loss = 6.85 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 10:17:50.010212: step 19100, loss = 6.75 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 10:17:56.235853: step 19110, loss = 6.54 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 10:18:00.875950: step 19120, loss = 6.79 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 10:18:05.550016: step 19130, loss = 6.81 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 10:18:11.280675: step 19140, loss = 7.15 (215.7 examples/sec; 0.593 sec/batch)
2016-07-15 10:18:16.540670: step 19150, loss = 6.79 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 10:18:22.061226: step 19160, loss = 6.49 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 10:18:26.801425: step 19170, loss = 6.92 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 10:18:31.442387: step 19180, loss = 6.61 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 10:18:36.471832: step 19190, loss = 6.75 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 10:18:41.956821: step 19200, loss = 6.51 (240.5 examples/sec; 0.532 sec/batch)
2016-07-15 10:18:47.752779: step 19210, loss = 6.79 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 10:18:52.393430: step 19220, loss = 6.86 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 10:18:57.688616: step 19230, loss = 6.65 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 10:19:02.875433: step 19240, loss = 6.77 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 10:19:08.622512: step 19250, loss = 6.69 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 10:19:13.390672: step 19260, loss = 6.69 (283.3 examples/sec; 0.452 sec/batch)
2016-07-15 10:19:18.183525: step 19270, loss = 6.67 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 10:19:22.894151: step 19280, loss = 6.58 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 10:19:28.089271: step 19290, loss = 6.90 (189.2 examples/sec; 0.676 sec/batch)
2016-07-15 10:19:34.571945: step 19300, loss = 6.91 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 10:19:40.586118: step 19310, loss = 6.78 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 10:19:45.365051: step 19320, loss = 6.54 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 10:19:50.173526: step 19330, loss = 6.68 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 10:19:54.813499: step 19340, loss = 6.79 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 10:19:59.678171: step 19350, loss = 6.73 (208.3 examples/sec; 0.615 sec/batch)
2016-07-15 10:20:05.318526: step 19360, loss = 6.85 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 10:20:10.061173: step 19370, loss = 6.71 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 10:20:14.892804: step 19380, loss = 6.62 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 10:20:19.578663: step 19390, loss = 6.66 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 10:20:24.167666: step 19400, loss = 6.81 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 10:20:30.788719: step 19410, loss = 6.86 (200.3 examples/sec; 0.639 sec/batch)
2016-07-15 10:20:35.821740: step 19420, loss = 6.66 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 10:20:40.550227: step 19430, loss = 6.71 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 10:20:46.317345: step 19440, loss = 6.73 (191.4 examples/sec; 0.669 sec/batch)
2016-07-15 10:20:52.384310: step 19450, loss = 6.82 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 10:20:57.832804: step 19460, loss = 6.70 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 10:21:03.065890: step 19470, loss = 6.95 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 10:21:07.824018: step 19480, loss = 6.78 (245.9 examples/sec; 0.520 sec/batch)
2016-07-15 10:21:12.675193: step 19490, loss = 6.71 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 10:21:17.397849: step 19500, loss = 6.61 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 10:21:24.698671: step 19510, loss = 6.92 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 10:21:30.155620: step 19520, loss = 6.70 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 10:21:34.920873: step 19530, loss = 6.76 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 10:21:39.799205: step 19540, loss = 6.70 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 10:21:44.560526: step 19550, loss = 6.53 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 10:21:50.315772: step 19560, loss = 6.63 (191.7 examples/sec; 0.668 sec/batch)
2016-07-15 10:21:56.372798: step 19570, loss = 6.49 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 10:22:01.189418: step 19580, loss = 6.66 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 10:22:06.045456: step 19590, loss = 6.56 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 10:22:10.778215: step 19600, loss = 6.54 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 10:22:16.613714: step 19610, loss = 6.41 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 10:22:21.373492: step 19620, loss = 6.63 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 10:22:26.177201: step 19630, loss = 6.60 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 10:22:31.005496: step 19640, loss = 6.57 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 10:22:37.514558: step 19650, loss = 6.86 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 10:22:42.869023: step 19660, loss = 6.70 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 10:22:47.605056: step 19670, loss = 6.70 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 10:22:52.997969: step 19680, loss = 6.81 (185.4 examples/sec; 0.690 sec/batch)
2016-07-15 10:22:59.448739: step 19690, loss = 6.83 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 10:23:04.296434: step 19700, loss = 6.72 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 10:23:10.058353: step 19710, loss = 6.77 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 10:23:14.800311: step 19720, loss = 6.46 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 10:23:19.490687: step 19730, loss = 6.66 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 10:23:24.460158: step 19740, loss = 6.59 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 10:23:29.980302: step 19750, loss = 6.82 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 10:23:34.713318: step 19760, loss = 6.63 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 10:23:39.622549: step 19770, loss = 6.70 (234.6 examples/sec; 0.546 sec/batch)
2016-07-15 10:23:46.156802: step 19780, loss = 6.52 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 10:23:51.325300: step 19790, loss = 6.62 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 10:23:56.100453: step 19800, loss = 6.90 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 10:24:01.923886: step 19810, loss = 6.75 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 10:24:06.520220: step 19820, loss = 6.83 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 10:24:11.143066: step 19830, loss = 6.77 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 10:24:16.924198: step 19840, loss = 6.84 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 10:24:21.736922: step 19850, loss = 6.61 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 10:24:26.526961: step 19860, loss = 6.40 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 10:24:32.871555: step 19870, loss = 6.68 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 10:24:38.339726: step 19880, loss = 6.49 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 10:24:43.036589: step 19890, loss = 6.75 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 10:24:47.917280: step 19900, loss = 6.81 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 10:24:53.734760: step 19910, loss = 6.62 (242.8 examples/sec; 0.527 sec/batch)
2016-07-15 10:24:59.819699: step 19920, loss = 6.40 (195.1 examples/sec; 0.656 sec/batch)
2016-07-15 10:25:05.561285: step 19930, loss = 6.76 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 10:25:10.328236: step 19940, loss = 6.63 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 10:25:15.257605: step 19950, loss = 6.64 (245.9 examples/sec; 0.521 sec/batch)
2016-07-15 10:25:21.765479: step 19960, loss = 6.76 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 10:25:26.998420: step 19970, loss = 6.47 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 10:25:31.693614: step 19980, loss = 6.62 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 10:25:37.045735: step 19990, loss = 6.79 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 10:25:43.514302: step 20000, loss = 6.69 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 10:25:50.099725: step 20010, loss = 6.50 (203.3 examples/sec; 0.629 sec/batch)
2016-07-15 10:25:55.235341: step 20020, loss = 6.84 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 10:26:01.012855: step 20030, loss = 6.66 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 10:26:05.796503: step 20040, loss = 6.63 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 10:26:10.661218: step 20050, loss = 6.51 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 10:26:16.991458: step 20060, loss = 6.78 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 10:26:22.413581: step 20070, loss = 6.60 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 10:26:27.136351: step 20080, loss = 6.55 (283.6 examples/sec; 0.451 sec/batch)
2016-07-15 10:26:31.980127: step 20090, loss = 6.73 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 10:26:36.697151: step 20100, loss = 6.75 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 10:26:43.745865: step 20110, loss = 6.69 (191.1 examples/sec; 0.670 sec/batch)
2016-07-15 10:26:49.502892: step 20120, loss = 6.72 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 10:26:54.253722: step 20130, loss = 6.53 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 10:26:59.104431: step 20140, loss = 6.68 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 10:27:03.784796: step 20150, loss = 6.72 (268.6 examples/sec; 0.476 sec/batch)
2016-07-15 10:27:09.064714: step 20160, loss = 6.90 (188.4 examples/sec; 0.679 sec/batch)
2016-07-15 10:27:15.537531: step 20170, loss = 6.92 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 10:27:20.395642: step 20180, loss = 6.72 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 10:27:25.150504: step 20190, loss = 6.47 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 10:27:30.133277: step 20200, loss = 6.58 (199.0 examples/sec; 0.643 sec/batch)
2016-07-15 10:27:36.442428: step 20210, loss = 6.78 (184.1 examples/sec; 0.695 sec/batch)
2016-07-15 10:27:42.909367: step 20220, loss = 6.60 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 10:27:47.949681: step 20230, loss = 6.52 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 10:27:52.656180: step 20240, loss = 6.32 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 10:27:57.465612: step 20250, loss = 6.77 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 10:28:02.271445: step 20260, loss = 6.71 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 10:28:08.473968: step 20270, loss = 6.72 (198.7 examples/sec; 0.644 sec/batch)
2016-07-15 10:28:14.028506: step 20280, loss = 6.81 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 10:28:18.841649: step 20290, loss = 6.82 (266.9 examples/sec; 0.479 sec/batch)
2016-07-15 10:28:23.672505: step 20300, loss = 6.78 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 10:28:29.353007: step 20310, loss = 6.59 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 10:28:34.190412: step 20320, loss = 6.44 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 10:28:38.775949: step 20330, loss = 6.86 (283.6 examples/sec; 0.451 sec/batch)
2016-07-15 10:28:43.471646: step 20340, loss = 6.58 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 10:28:49.241357: step 20350, loss = 6.64 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 10:28:54.037918: step 20360, loss = 6.49 (273.2 examples/sec; 0.468 sec/batch)
2016-07-15 10:28:58.828122: step 20370, loss = 6.71 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 10:29:05.176734: step 20380, loss = 6.45 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 10:29:10.605362: step 20390, loss = 6.55 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 10:29:15.315828: step 20400, loss = 6.48 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 10:29:21.836700: step 20410, loss = 6.58 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 10:29:28.115567: step 20420, loss = 6.65 (252.2 examples/sec; 0.508 sec/batch)
2016-07-15 10:29:32.914533: step 20430, loss = 6.60 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 10:29:37.583926: step 20440, loss = 6.63 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 10:29:42.203821: step 20450, loss = 6.90 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 10:29:46.847785: step 20460, loss = 6.82 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 10:29:51.512748: step 20470, loss = 6.89 (281.2 examples/sec; 0.455 sec/batch)
2016-07-15 10:29:57.276713: step 20480, loss = 6.50 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 10:30:02.031165: step 20490, loss = 6.42 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 10:30:06.883263: step 20500, loss = 6.64 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 10:30:12.541721: step 20510, loss = 6.63 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 10:30:18.015461: step 20520, loss = 6.73 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 10:30:24.250388: step 20530, loss = 6.56 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 10:30:29.078282: step 20540, loss = 6.50 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 10:30:33.813338: step 20550, loss = 6.70 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 10:30:38.567468: step 20560, loss = 6.64 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 10:30:43.468243: step 20570, loss = 6.41 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 10:30:49.928171: step 20580, loss = 6.79 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 10:30:55.212878: step 20590, loss = 6.51 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 10:30:59.926945: step 20600, loss = 6.52 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 10:31:06.674641: step 20610, loss = 6.78 (183.2 examples/sec; 0.699 sec/batch)
2016-07-15 10:31:12.748902: step 20620, loss = 6.62 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 10:31:17.545769: step 20630, loss = 6.80 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 10:31:22.324491: step 20640, loss = 6.73 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 10:31:27.082091: step 20650, loss = 6.67 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 10:31:31.993969: step 20660, loss = 6.45 (241.2 examples/sec; 0.531 sec/batch)
2016-07-15 10:31:38.527587: step 20670, loss = 6.57 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 10:31:43.656619: step 20680, loss = 6.83 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 10:31:48.339860: step 20690, loss = 6.70 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 10:31:53.149094: step 20700, loss = 6.42 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 10:31:58.805057: step 20710, loss = 6.47 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 10:32:03.434930: step 20720, loss = 6.48 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 10:32:09.149884: step 20730, loss = 6.48 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 10:32:14.693480: step 20740, loss = 6.51 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 10:32:19.826608: step 20750, loss = 6.58 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 10:32:24.534832: step 20760, loss = 6.40 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 10:32:30.038085: step 20770, loss = 6.31 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 10:32:35.372884: step 20780, loss = 6.49 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 10:32:40.187565: step 20790, loss = 6.43 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 10:32:45.982878: step 20800, loss = 6.31 (208.6 examples/sec; 0.613 sec/batch)
2016-07-15 10:32:52.475456: step 20810, loss = 6.89 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 10:32:57.228467: step 20820, loss = 6.50 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 10:33:03.065510: step 20830, loss = 6.52 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 10:33:09.062132: step 20840, loss = 6.82 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:33:14.512113: step 20850, loss = 6.83 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 10:33:19.693492: step 20860, loss = 6.65 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 10:33:25.454935: step 20870, loss = 6.65 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:33:30.238188: step 20880, loss = 6.49 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 10:33:35.008379: step 20890, loss = 6.82 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 10:33:41.275052: step 20900, loss = 6.67 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 10:33:47.976710: step 20910, loss = 6.60 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 10:33:53.742612: step 20920, loss = 6.61 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 10:33:59.363248: step 20930, loss = 6.57 (204.3 examples/sec; 0.626 sec/batch)
2016-07-15 10:34:04.340713: step 20940, loss = 6.48 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 10:34:09.102475: step 20950, loss = 6.53 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 10:34:14.800676: step 20960, loss = 6.84 (187.5 examples/sec; 0.683 sec/batch)
2016-07-15 10:34:19.797291: step 20970, loss = 6.41 (282.2 examples/sec; 0.454 sec/batch)
2016-07-15 10:34:24.691354: step 20980, loss = 6.66 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 10:34:30.294512: step 20990, loss = 6.67 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:34:36.068340: step 21000, loss = 6.61 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 10:34:42.590580: step 21010, loss = 6.54 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 10:34:47.856618: step 21020, loss = 6.42 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 10:34:52.591486: step 21030, loss = 6.64 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 10:34:57.440232: step 21040, loss = 6.44 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 10:35:02.209970: step 21050, loss = 6.76 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 10:35:08.346432: step 21060, loss = 6.62 (197.3 examples/sec; 0.649 sec/batch)
2016-07-15 10:35:14.061561: step 21070, loss = 6.68 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 10:35:18.815960: step 21080, loss = 6.56 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 10:35:23.656384: step 21090, loss = 6.44 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 10:35:30.143406: step 21100, loss = 6.47 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 10:35:36.675195: step 21110, loss = 6.56 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 10:35:41.355882: step 21120, loss = 6.68 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 10:35:46.024432: step 21130, loss = 6.41 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 10:35:51.622025: step 21140, loss = 6.43 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 10:35:56.734249: step 21150, loss = 6.64 (200.8 examples/sec; 0.638 sec/batch)
2016-07-15 10:36:02.411532: step 21160, loss = 6.41 (242.7 examples/sec; 0.527 sec/batch)
2016-07-15 10:36:07.137316: step 21170, loss = 6.73 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 10:36:12.034981: step 21180, loss = 6.30 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 10:36:16.735692: step 21190, loss = 6.85 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 10:36:21.637443: step 21200, loss = 6.66 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 10:36:27.249031: step 21210, loss = 6.47 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 10:36:31.937169: step 21220, loss = 6.56 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 10:36:37.739414: step 21230, loss = 6.46 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 10:36:42.560318: step 21240, loss = 6.43 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 10:36:47.353057: step 21250, loss = 6.81 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 10:36:53.739070: step 21260, loss = 6.67 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 10:36:59.175778: step 21270, loss = 6.69 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 10:37:04.920869: step 21280, loss = 6.47 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 10:37:10.288563: step 21290, loss = 6.48 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 10:37:15.581187: step 21300, loss = 6.55 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 10:37:22.431979: step 21310, loss = 6.65 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 10:37:27.205043: step 21320, loss = 6.87 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 10:37:32.048149: step 21330, loss = 6.75 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 10:37:36.795599: step 21340, loss = 6.60 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 10:37:42.262163: step 21350, loss = 6.48 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 10:37:48.518697: step 21360, loss = 6.48 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 10:37:53.398557: step 21370, loss = 6.36 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 10:37:58.248552: step 21380, loss = 6.50 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 10:38:03.017637: step 21390, loss = 6.51 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 10:38:07.818458: step 21400, loss = 6.45 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 10:38:15.806526: step 21410, loss = 6.63 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 10:38:20.797389: step 21420, loss = 6.45 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 10:38:25.504281: step 21430, loss = 6.62 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 10:38:30.151610: step 21440, loss = 6.68 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 10:38:35.776724: step 21450, loss = 6.68 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 10:38:40.923097: step 21460, loss = 6.67 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 10:38:46.546760: step 21470, loss = 6.50 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 10:38:51.267962: step 21480, loss = 6.35 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 10:38:55.915210: step 21490, loss = 6.49 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 10:39:00.597473: step 21500, loss = 6.43 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 10:39:06.155915: step 21510, loss = 6.65 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 10:39:11.725890: step 21520, loss = 6.43 (204.3 examples/sec; 0.626 sec/batch)
2016-07-15 10:39:16.691983: step 21530, loss = 6.66 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 10:39:21.382360: step 21540, loss = 6.51 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 10:39:27.093362: step 21550, loss = 6.56 (183.7 examples/sec; 0.697 sec/batch)
2016-07-15 10:39:33.191214: step 21560, loss = 6.28 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 10:39:38.061791: step 21570, loss = 6.76 (284.0 examples/sec; 0.451 sec/batch)
2016-07-15 10:39:42.804136: step 21580, loss = 6.62 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 10:39:47.549359: step 21590, loss = 6.40 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 10:39:52.436158: step 21600, loss = 6.52 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 10:39:58.042238: step 21610, loss = 6.88 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 10:40:02.695730: step 21620, loss = 6.82 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 10:40:08.431145: step 21630, loss = 6.70 (221.0 examples/sec; 0.579 sec/batch)
2016-07-15 10:40:13.605487: step 21640, loss = 6.52 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 10:40:19.082917: step 21650, loss = 6.44 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 10:40:24.894776: step 21660, loss = 6.55 (245.1 examples/sec; 0.522 sec/batch)
2016-07-15 10:40:30.254690: step 21670, loss = 6.41 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 10:40:35.602878: step 21680, loss = 6.52 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 10:40:40.338404: step 21690, loss = 6.50 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 10:40:45.184771: step 21700, loss = 6.84 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 10:40:50.901244: step 21710, loss = 6.64 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 10:40:57.052710: step 21720, loss = 6.55 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 10:41:02.624442: step 21730, loss = 6.41 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 10:41:08.415911: step 21740, loss = 6.67 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 10:41:13.328576: step 21750, loss = 6.43 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 10:41:18.039046: step 21760, loss = 6.40 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 10:41:23.876873: step 21770, loss = 6.50 (191.5 examples/sec; 0.668 sec/batch)
2016-07-15 10:41:29.748089: step 21780, loss = 6.71 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 10:41:34.551964: step 21790, loss = 6.67 (283.3 examples/sec; 0.452 sec/batch)
2016-07-15 10:41:39.374445: step 21800, loss = 6.67 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 10:41:45.138482: step 21810, loss = 6.49 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 10:41:50.686661: step 21820, loss = 6.54 (188.1 examples/sec; 0.681 sec/batch)
2016-07-15 10:41:56.952288: step 21830, loss = 6.53 (252.8 examples/sec; 0.506 sec/batch)
2016-07-15 10:42:01.762636: step 21840, loss = 6.69 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 10:42:06.507568: step 21850, loss = 6.61 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 10:42:11.263327: step 21860, loss = 6.29 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 10:42:16.139402: step 21870, loss = 6.48 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 10:42:22.674912: step 21880, loss = 6.69 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 10:42:27.967903: step 21890, loss = 6.63 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 10:42:33.745393: step 21900, loss = 6.37 (246.3 examples/sec; 0.520 sec/batch)
2016-07-15 10:42:39.528617: step 21910, loss = 6.47 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 10:42:44.401524: step 21920, loss = 6.39 (254.2 examples/sec; 0.504 sec/batch)
2016-07-15 10:42:49.129064: step 21930, loss = 6.38 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 10:42:54.703217: step 21940, loss = 6.55 (184.5 examples/sec; 0.694 sec/batch)
2016-07-15 10:43:00.976523: step 21950, loss = 6.27 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 10:43:05.781137: step 21960, loss = 6.34 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 10:43:10.592777: step 21970, loss = 6.56 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:43:16.636650: step 21980, loss = 6.70 (192.0 examples/sec; 0.667 sec/batch)
2016-07-15 10:43:22.397677: step 21990, loss = 6.71 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 10:43:27.177994: step 22000, loss = 6.61 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 10:43:33.221554: step 22010, loss = 6.66 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 10:43:39.719011: step 22020, loss = 6.63 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 10:43:44.788464: step 22030, loss = 6.57 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 10:43:49.497835: step 22040, loss = 6.39 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 10:43:55.568475: step 22050, loss = 6.71 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 10:44:01.582484: step 22060, loss = 6.59 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 10:44:06.404073: step 22070, loss = 6.55 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 10:44:11.165445: step 22080, loss = 6.63 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 10:44:15.903272: step 22090, loss = 6.52 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 10:44:20.522061: step 22100, loss = 6.53 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 10:44:26.717649: step 22110, loss = 6.44 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 10:44:32.035021: step 22120, loss = 6.70 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 10:44:36.716504: step 22130, loss = 6.33 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 10:44:42.044721: step 22140, loss = 6.57 (186.7 examples/sec; 0.685 sec/batch)
2016-07-15 10:44:48.506154: step 22150, loss = 6.63 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 10:44:53.334613: step 22160, loss = 6.50 (280.4 examples/sec; 0.457 sec/batch)
2016-07-15 10:44:58.068929: step 22170, loss = 6.45 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 10:45:02.845269: step 22180, loss = 6.18 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 10:45:07.640387: step 22190, loss = 6.65 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 10:45:13.640223: step 22200, loss = 6.43 (284.2 examples/sec; 0.450 sec/batch)
2016-07-15 10:45:19.216193: step 22210, loss = 6.58 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 10:45:24.920577: step 22220, loss = 6.54 (243.1 examples/sec; 0.527 sec/batch)
2016-07-15 10:45:30.143007: step 22230, loss = 6.67 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 10:45:35.583919: step 22240, loss = 6.42 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 10:45:40.271506: step 22250, loss = 6.43 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 10:45:45.409621: step 22260, loss = 6.37 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 10:45:51.936368: step 22270, loss = 6.61 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 10:45:56.962682: step 22280, loss = 6.46 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 10:46:01.626873: step 22290, loss = 6.11 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 10:46:06.437065: step 22300, loss = 6.80 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 10:46:12.193496: step 22310, loss = 6.38 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 10:46:18.691527: step 22320, loss = 6.31 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 10:46:24.021499: step 22330, loss = 6.53 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 10:46:28.759039: step 22340, loss = 6.24 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 10:46:33.613178: step 22350, loss = 6.74 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 10:46:38.377914: step 22360, loss = 6.56 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:46:44.309182: step 22370, loss = 6.47 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 10:46:50.189881: step 22380, loss = 6.57 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 10:46:54.985896: step 22390, loss = 6.36 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 10:46:59.775204: step 22400, loss = 6.51 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 10:47:05.492664: step 22410, loss = 6.64 (282.9 examples/sec; 0.452 sec/batch)
2016-07-15 10:47:10.090156: step 22420, loss = 6.61 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 10:47:15.456456: step 22430, loss = 6.47 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 10:47:20.600277: step 22440, loss = 6.28 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 10:47:25.350980: step 22450, loss = 6.39 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 10:47:30.820052: step 22460, loss = 6.41 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 10:47:37.085442: step 22470, loss = 6.49 (243.2 examples/sec; 0.526 sec/batch)
2016-07-15 10:47:41.915053: step 22480, loss = 6.60 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 10:47:46.643351: step 22490, loss = 6.67 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 10:47:51.413007: step 22500, loss = 6.57 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 10:47:57.479940: step 22510, loss = 6.42 (190.6 examples/sec; 0.672 sec/batch)
2016-07-15 10:48:04.027595: step 22520, loss = 6.25 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 10:48:09.036524: step 22530, loss = 6.34 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 10:48:13.759564: step 22540, loss = 6.47 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 10:48:18.596794: step 22550, loss = 6.44 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 10:48:23.398354: step 22560, loss = 6.41 (251.4 examples/sec; 0.509 sec/batch)
2016-07-15 10:48:29.668097: step 22570, loss = 6.73 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 10:48:35.229245: step 22580, loss = 6.57 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 10:48:39.983965: step 22590, loss = 6.64 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 10:48:44.888785: step 22600, loss = 6.50 (247.0 examples/sec; 0.518 sec/batch)
2016-07-15 10:48:53.071495: step 22610, loss = 6.63 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 10:48:58.055753: step 22620, loss = 6.55 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 10:49:03.097796: step 22630, loss = 6.27 (196.7 examples/sec; 0.651 sec/batch)
2016-07-15 10:49:08.024789: step 22640, loss = 6.31 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 10:49:12.834912: step 22650, loss = 6.44 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 10:49:19.395434: step 22660, loss = 6.41 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 10:49:24.586401: step 22670, loss = 6.45 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 10:49:29.222888: step 22680, loss = 6.55 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 10:49:33.860382: step 22690, loss = 6.52 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 10:49:39.194583: step 22700, loss = 6.40 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 10:49:45.633420: step 22710, loss = 6.52 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 10:49:50.390128: step 22720, loss = 6.21 (249.7 examples/sec; 0.513 sec/batch)
2016-07-15 10:49:56.271624: step 22730, loss = 6.48 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 10:50:02.177981: step 22740, loss = 6.64 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 10:50:06.961894: step 22750, loss = 6.42 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 10:50:12.090181: step 22760, loss = 6.41 (248.1 examples/sec; 0.516 sec/batch)
2016-07-15 10:50:18.484984: step 22770, loss = 6.46 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 10:50:23.906363: step 22780, loss = 6.52 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 10:50:28.633395: step 22790, loss = 6.53 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 10:50:33.758709: step 22800, loss = 6.54 (190.3 examples/sec; 0.673 sec/batch)
2016-07-15 10:50:41.672012: step 22810, loss = 6.43 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 10:50:46.986000: step 22820, loss = 6.58 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 10:50:52.253003: step 22830, loss = 6.41 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 10:50:56.980202: step 22840, loss = 6.46 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 10:51:02.289991: step 22850, loss = 6.53 (191.9 examples/sec; 0.667 sec/batch)
2016-07-15 10:51:08.771148: step 22860, loss = 6.65 (200.4 examples/sec; 0.639 sec/batch)
2016-07-15 10:51:13.657371: step 22870, loss = 6.46 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 10:51:18.363824: step 22880, loss = 6.17 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 10:51:23.131282: step 22890, loss = 6.49 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 10:51:27.970905: step 22900, loss = 6.62 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 10:51:33.674542: step 22910, loss = 6.38 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 10:51:38.445143: step 22920, loss = 6.40 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 10:51:43.149875: step 22930, loss = 6.41 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 10:51:49.242930: step 22940, loss = 6.62 (198.1 examples/sec; 0.646 sec/batch)
2016-07-15 10:51:55.008558: step 22950, loss = 6.63 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 10:52:00.717805: step 22960, loss = 6.65 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 10:52:05.880399: step 22970, loss = 6.45 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 10:52:11.429013: step 22980, loss = 6.27 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 10:52:16.158413: step 22990, loss = 6.60 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 10:52:20.985394: step 23000, loss = 6.42 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 10:52:26.770844: step 23010, loss = 6.42 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 10:52:32.748358: step 23020, loss = 6.44 (191.8 examples/sec; 0.667 sec/batch)
2016-07-15 10:52:38.612614: step 23030, loss = 6.39 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 10:52:44.227833: step 23040, loss = 6.61 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 10:52:49.237876: step 23050, loss = 6.61 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 10:52:53.996631: step 23060, loss = 6.45 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 10:52:58.789106: step 23070, loss = 6.57 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 10:53:03.625042: step 23080, loss = 6.53 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 10:53:09.593562: step 23090, loss = 6.40 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 10:53:14.171092: step 23100, loss = 6.56 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 10:53:20.770280: step 23110, loss = 6.32 (210.6 examples/sec; 0.608 sec/batch)
2016-07-15 10:53:25.968716: step 23120, loss = 6.29 (199.5 examples/sec; 0.642 sec/batch)
2016-07-15 10:53:31.535104: step 23130, loss = 6.45 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 10:53:36.285283: step 23140, loss = 6.52 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 10:53:41.124900: step 23150, loss = 6.47 (250.7 examples/sec; 0.511 sec/batch)
2016-07-15 10:53:47.675671: step 23160, loss = 6.45 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 10:53:52.822290: step 23170, loss = 6.43 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 10:53:57.531220: step 23180, loss = 6.37 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 10:54:02.380788: step 23190, loss = 6.61 (282.6 examples/sec; 0.453 sec/batch)
2016-07-15 10:54:07.024764: step 23200, loss = 6.59 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 10:54:12.559043: step 23210, loss = 6.64 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 10:54:18.319655: step 23220, loss = 6.51 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 10:54:23.799592: step 23230, loss = 6.47 (208.9 examples/sec; 0.613 sec/batch)
2016-07-15 10:54:28.929924: step 23240, loss = 6.39 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 10:54:33.609883: step 23250, loss = 6.41 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 10:54:38.401876: step 23260, loss = 6.41 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 10:54:43.181004: step 23270, loss = 6.55 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 10:54:49.218046: step 23280, loss = 6.67 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 10:54:54.905158: step 23290, loss = 6.57 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 10:55:00.550494: step 23300, loss = 6.68 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 10:55:07.167743: step 23310, loss = 6.65 (202.4 examples/sec; 0.633 sec/batch)
2016-07-15 10:55:12.554059: step 23320, loss = 6.55 (248.6 examples/sec; 0.515 sec/batch)
2016-07-15 10:55:17.409636: step 23330, loss = 6.57 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 10:55:22.938521: step 23340, loss = 6.18 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 10:55:27.739034: step 23350, loss = 6.57 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 10:55:33.943925: step 23360, loss = 6.39 (199.3 examples/sec; 0.642 sec/batch)
2016-07-15 10:55:39.501643: step 23370, loss = 6.36 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 10:55:44.253616: step 23380, loss = 6.57 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 10:55:49.120345: step 23390, loss = 6.43 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 10:55:53.796673: step 23400, loss = 6.28 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 10:55:59.608586: step 23410, loss = 6.46 (272.6 examples/sec; 0.469 sec/batch)
2016-07-15 10:56:04.445795: step 23420, loss = 6.34 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 10:56:10.848243: step 23430, loss = 6.64 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 10:56:16.227558: step 23440, loss = 6.45 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 10:56:20.942813: step 23450, loss = 6.04 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 10:56:25.763964: step 23460, loss = 6.67 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 10:56:30.520297: step 23470, loss = 6.39 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 10:56:35.395646: step 23480, loss = 6.41 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 10:56:40.202475: step 23490, loss = 6.47 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 10:56:44.962554: step 23500, loss = 6.24 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 10:56:51.452556: step 23510, loss = 6.64 (188.5 examples/sec; 0.679 sec/batch)
2016-07-15 10:56:57.737799: step 23520, loss = 6.57 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 10:57:02.528193: step 23530, loss = 6.42 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 10:57:07.310158: step 23540, loss = 6.60 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 10:57:13.320129: step 23550, loss = 6.21 (192.3 examples/sec; 0.666 sec/batch)
2016-07-15 10:57:19.060826: step 23560, loss = 6.46 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 10:57:24.712790: step 23570, loss = 6.64 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 10:57:29.711706: step 23580, loss = 6.45 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 10:57:34.409562: step 23590, loss = 6.54 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 10:57:40.087123: step 23600, loss = 6.25 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 10:57:47.482561: step 23610, loss = 6.29 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 10:57:53.227999: step 23620, loss = 6.46 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 10:57:58.105798: step 23630, loss = 6.42 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 10:58:02.909018: step 23640, loss = 6.55 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 10:58:08.789548: step 23650, loss = 6.54 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 10:58:14.706546: step 23660, loss = 6.45 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 10:58:19.568266: step 23670, loss = 6.27 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 10:58:24.390364: step 23680, loss = 6.46 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 10:58:30.755719: step 23690, loss = 6.47 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 10:58:36.196379: step 23700, loss = 6.29 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 10:58:41.973261: step 23710, loss = 6.50 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 10:58:47.443435: step 23720, loss = 6.36 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 10:58:53.712591: step 23730, loss = 6.79 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 10:58:58.547691: step 23740, loss = 6.44 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 10:59:03.369052: step 23750, loss = 6.68 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 10:59:08.142030: step 23760, loss = 6.56 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 10:59:12.958372: step 23770, loss = 6.73 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 10:59:19.409450: step 23780, loss = 6.57 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 10:59:24.764838: step 23790, loss = 6.33 (251.8 examples/sec; 0.508 sec/batch)
2016-07-15 10:59:29.517996: step 23800, loss = 6.23 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 10:59:36.134603: step 23810, loss = 6.46 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 10:59:42.245519: step 23820, loss = 6.35 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 10:59:47.130191: step 23830, loss = 6.56 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 10:59:51.916312: step 23840, loss = 6.43 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 10:59:56.633851: step 23850, loss = 6.44 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 11:00:01.506936: step 23860, loss = 6.26 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 11:00:06.215513: step 23870, loss = 6.16 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 11:00:11.840906: step 23880, loss = 6.51 (188.4 examples/sec; 0.679 sec/batch)
2016-07-15 11:00:18.026892: step 23890, loss = 6.66 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 11:00:22.944948: step 23900, loss = 6.63 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 11:00:28.541169: step 23910, loss = 6.28 (283.9 examples/sec; 0.451 sec/batch)
2016-07-15 11:00:33.183809: step 23920, loss = 6.42 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 11:00:38.990581: step 23930, loss = 6.55 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 11:00:44.591340: step 23940, loss = 6.51 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 11:00:49.794069: step 23950, loss = 6.51 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 11:00:55.523122: step 23960, loss = 6.49 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 11:01:01.292363: step 23970, loss = 6.35 (198.6 examples/sec; 0.644 sec/batch)
2016-07-15 11:01:06.422751: step 23980, loss = 6.53 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 11:01:11.998409: step 23990, loss = 6.19 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 11:01:16.745907: step 24000, loss = 6.29 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 11:01:22.493612: step 24010, loss = 6.57 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 11:01:28.059852: step 24020, loss = 6.61 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 11:01:33.250360: step 24030, loss = 6.40 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 11:01:39.035756: step 24040, loss = 6.26 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 11:01:44.644390: step 24050, loss = 6.51 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 11:01:49.728184: step 24060, loss = 6.51 (241.7 examples/sec; 0.529 sec/batch)
2016-07-15 11:01:55.425329: step 24070, loss = 6.27 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 11:02:00.135247: step 24080, loss = 6.37 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 11:02:04.943161: step 24090, loss = 6.44 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 11:02:09.642625: step 24100, loss = 6.63 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 11:02:16.425412: step 24110, loss = 6.43 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 11:02:22.465936: step 24120, loss = 6.55 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 11:02:27.854460: step 24130, loss = 6.35 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 11:02:33.089010: step 24140, loss = 6.19 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 11:02:38.827163: step 24150, loss = 6.44 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 11:02:44.325481: step 24160, loss = 6.31 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 11:02:49.510527: step 24170, loss = 6.70 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 11:02:54.151511: step 24180, loss = 6.32 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 11:02:59.622269: step 24190, loss = 6.61 (187.3 examples/sec; 0.684 sec/batch)
2016-07-15 11:03:05.898312: step 24200, loss = 6.51 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 11:03:11.732061: step 24210, loss = 6.36 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 11:03:16.513317: step 24220, loss = 6.39 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 11:03:21.195019: step 24230, loss = 6.45 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 11:03:26.278625: step 24240, loss = 6.45 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 11:03:32.795733: step 24250, loss = 6.23 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 11:03:37.907627: step 24260, loss = 6.55 (223.0 examples/sec; 0.574 sec/batch)
2016-07-15 11:03:43.630398: step 24270, loss = 6.56 (253.7 examples/sec; 0.504 sec/batch)
2016-07-15 11:03:48.400750: step 24280, loss = 6.28 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 11:03:53.203832: step 24290, loss = 6.48 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 11:03:57.967670: step 24300, loss = 6.54 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 11:04:03.519285: step 24310, loss = 6.38 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 11:04:09.068045: step 24320, loss = 6.37 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 11:04:14.255893: step 24330, loss = 6.41 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 11:04:19.862907: step 24340, loss = 6.21 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 11:04:25.554597: step 24350, loss = 6.44 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 11:04:30.428808: step 24360, loss = 6.51 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 11:04:35.192227: step 24370, loss = 6.58 (253.9 examples/sec; 0.504 sec/batch)
2016-07-15 11:04:41.071880: step 24380, loss = 6.64 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 11:04:46.949879: step 24390, loss = 6.20 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 11:04:52.409269: step 24400, loss = 6.22 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 11:04:58.537798: step 24410, loss = 6.24 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 11:05:03.171944: step 24420, loss = 6.38 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 11:05:07.804343: step 24430, loss = 6.30 (284.2 examples/sec; 0.450 sec/batch)
2016-07-15 11:05:12.444248: step 24440, loss = 6.60 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 11:05:18.157989: step 24450, loss = 6.52 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 11:05:22.976507: step 24460, loss = 6.43 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 11:05:27.758838: step 24470, loss = 6.08 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 11:05:33.829791: step 24480, loss = 6.21 (191.1 examples/sec; 0.670 sec/batch)
2016-07-15 11:05:39.502388: step 24490, loss = 6.43 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 11:05:45.194530: step 24500, loss = 6.25 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 11:05:51.812053: step 24510, loss = 6.45 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 11:05:57.143790: step 24520, loss = 6.35 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 11:06:01.880001: step 24530, loss = 6.23 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 11:06:07.183936: step 24540, loss = 6.37 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 11:06:13.647017: step 24550, loss = 6.42 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 11:06:18.767716: step 24560, loss = 6.19 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 11:06:24.271106: step 24570, loss = 6.30 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 11:06:30.164748: step 24580, loss = 6.36 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 11:06:35.504851: step 24590, loss = 6.47 (157.2 examples/sec; 0.814 sec/batch)
2016-07-15 11:06:40.277357: step 24600, loss = 6.45 (261.0 examples/sec; 0.491 sec/batch)
2016-07-15 11:06:47.753301: step 24610, loss = 5.95 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 11:06:53.100353: step 24620, loss = 6.48 (251.1 examples/sec; 0.510 sec/batch)
2016-07-15 11:06:57.801326: step 24630, loss = 6.43 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 11:07:02.637725: step 24640, loss = 6.37 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 11:07:07.470733: step 24650, loss = 6.44 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 11:07:12.187306: step 24660, loss = 6.14 (283.5 examples/sec; 0.451 sec/batch)
2016-07-15 11:07:17.053271: step 24670, loss = 6.39 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 11:07:23.428378: step 24680, loss = 6.21 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 11:07:28.829007: step 24690, loss = 6.44 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 11:07:33.538933: step 24700, loss = 6.22 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 11:07:40.544313: step 24710, loss = 6.19 (135.9 examples/sec; 0.942 sec/batch)
2016-07-15 11:07:46.865211: step 24720, loss = 6.34 (253.1 examples/sec; 0.506 sec/batch)
2016-07-15 11:07:51.700226: step 24730, loss = 6.62 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 11:07:56.513619: step 24740, loss = 6.46 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 11:08:02.754722: step 24750, loss = 6.48 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 11:08:08.312824: step 24760, loss = 6.16 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 11:08:13.018627: step 24770, loss = 6.33 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 11:08:17.848235: step 24780, loss = 6.48 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 11:08:22.567974: step 24790, loss = 6.36 (282.8 examples/sec; 0.453 sec/batch)
2016-07-15 11:08:27.165953: step 24800, loss = 6.43 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 11:08:33.751226: step 24810, loss = 6.62 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 11:08:38.903869: step 24820, loss = 6.42 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 11:08:44.424300: step 24830, loss = 6.29 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 11:08:50.209409: step 24840, loss = 6.35 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 11:08:55.444126: step 24850, loss = 6.21 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 11:09:00.868598: step 24860, loss = 6.32 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 11:09:05.603803: step 24870, loss = 6.32 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 11:09:10.669583: step 24880, loss = 6.39 (189.5 examples/sec; 0.676 sec/batch)
2016-07-15 11:09:17.172194: step 24890, loss = 6.15 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 11:09:22.195920: step 24900, loss = 6.34 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 11:09:27.909132: step 24910, loss = 6.71 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 11:09:32.679073: step 24920, loss = 6.65 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 11:09:37.503343: step 24930, loss = 6.72 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 11:09:44.029047: step 24940, loss = 6.52 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 11:09:49.323888: step 24950, loss = 6.24 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 11:09:54.050532: step 24960, loss = 6.04 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 11:09:59.273435: step 24970, loss = 6.06 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 11:10:05.717018: step 24980, loss = 6.23 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 11:10:10.897867: step 24990, loss = 6.12 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 11:10:16.478752: step 25000, loss = 6.24 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 11:10:23.275081: step 25010, loss = 6.20 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 11:10:28.811717: step 25020, loss = 6.29 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 11:10:33.989768: step 25030, loss = 6.15 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 11:10:39.731299: step 25040, loss = 6.35 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 11:10:45.342559: step 25050, loss = 6.55 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 11:10:50.342744: step 25060, loss = 6.56 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 11:10:55.082971: step 25070, loss = 6.22 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 11:11:00.714896: step 25080, loss = 6.48 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 11:11:06.833212: step 25090, loss = 6.14 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 11:11:12.198139: step 25100, loss = 6.14 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 11:11:18.896969: step 25110, loss = 6.37 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 11:11:24.467823: step 25120, loss = 6.43 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 11:11:29.192515: step 25130, loss = 6.28 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 11:11:34.048251: step 25140, loss = 6.39 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 11:11:40.655418: step 25150, loss = 6.12 (198.2 examples/sec; 0.646 sec/batch)
2016-07-15 11:11:45.804789: step 25160, loss = 6.07 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 11:11:51.575541: step 25170, loss = 6.41 (257.8 examples/sec; 0.497 sec/batch)
2016-07-15 11:11:56.337980: step 25180, loss = 6.48 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 11:12:01.176869: step 25190, loss = 6.47 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 11:12:07.527228: step 25200, loss = 6.33 (198.1 examples/sec; 0.646 sec/batch)
2016-07-15 11:12:14.273999: step 25210, loss = 6.40 (230.4 examples/sec; 0.556 sec/batch)
2016-07-15 11:12:20.001463: step 25220, loss = 6.38 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 11:12:24.744561: step 25230, loss = 6.14 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 11:12:29.574780: step 25240, loss = 6.30 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 11:12:36.025684: step 25250, loss = 6.28 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 11:12:41.320376: step 25260, loss = 6.60 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 11:12:47.268373: step 25270, loss = 6.59 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 11:12:53.344559: step 25280, loss = 6.42 (175.6 examples/sec; 0.729 sec/batch)
2016-07-15 11:12:58.471567: step 25290, loss = 6.56 (216.3 examples/sec; 0.592 sec/batch)
2016-07-15 11:13:04.159412: step 25300, loss = 6.21 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 11:13:10.908471: step 25310, loss = 6.39 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 11:13:16.282968: step 25320, loss = 6.41 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 11:13:21.594072: step 25330, loss = 6.61 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 11:13:26.321114: step 25340, loss = 6.36 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 11:13:31.524525: step 25350, loss = 6.52 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 11:13:37.989264: step 25360, loss = 6.44 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 11:13:42.849483: step 25370, loss = 6.48 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 11:13:47.619589: step 25380, loss = 6.54 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 11:13:52.445762: step 25390, loss = 6.53 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 11:13:57.253074: step 25400, loss = 6.65 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 11:14:05.030679: step 25410, loss = 6.30 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 11:14:10.168039: step 25420, loss = 6.49 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 11:14:15.905126: step 25430, loss = 6.50 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 11:14:21.476075: step 25440, loss = 6.27 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 11:14:26.517445: step 25450, loss = 6.43 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 11:14:31.225850: step 25460, loss = 6.46 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 11:14:36.933584: step 25470, loss = 6.58 (187.8 examples/sec; 0.682 sec/batch)
2016-07-15 11:14:43.038040: step 25480, loss = 6.23 (244.1 examples/sec; 0.524 sec/batch)
2016-07-15 11:14:48.400327: step 25490, loss = 6.16 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 11:14:53.681964: step 25500, loss = 6.23 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 11:15:00.561871: step 25510, loss = 6.24 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 11:15:05.318833: step 25520, loss = 6.48 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 11:15:10.195291: step 25530, loss = 6.59 (247.0 examples/sec; 0.518 sec/batch)
2016-07-15 11:15:14.899046: step 25540, loss = 6.47 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 11:15:20.423461: step 25550, loss = 6.22 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 11:15:26.713460: step 25560, loss = 6.31 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 11:15:31.561453: step 25570, loss = 6.45 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 11:15:36.324317: step 25580, loss = 6.06 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 11:15:42.353527: step 25590, loss = 5.95 (190.9 examples/sec; 0.671 sec/batch)
2016-07-15 11:15:48.080013: step 25600, loss = 6.15 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 11:15:53.862150: step 25610, loss = 6.36 (281.6 examples/sec; 0.455 sec/batch)
2016-07-15 11:15:58.985885: step 25620, loss = 6.35 (190.3 examples/sec; 0.672 sec/batch)
2016-07-15 11:16:05.540844: step 25630, loss = 6.05 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 11:16:10.704998: step 25640, loss = 6.20 (213.3 examples/sec; 0.600 sec/batch)
2016-07-15 11:16:16.342173: step 25650, loss = 6.41 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 11:16:22.048954: step 25660, loss = 6.21 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 11:16:27.233097: step 25670, loss = 6.30 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 11:16:32.814361: step 25680, loss = 6.33 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 11:16:37.538368: step 25690, loss = 6.38 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 11:16:42.418462: step 25700, loss = 6.43 (242.3 examples/sec; 0.528 sec/batch)
2016-07-15 11:16:50.509645: step 25710, loss = 6.34 (219.3 examples/sec; 0.584 sec/batch)
2016-07-15 11:16:55.715859: step 25720, loss = 6.21 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 11:17:01.181750: step 25730, loss = 6.13 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 11:17:05.888914: step 25740, loss = 6.15 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 11:17:10.740750: step 25750, loss = 6.46 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 11:17:15.503679: step 25760, loss = 6.36 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 11:17:20.308750: step 25770, loss = 5.94 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 11:17:24.968470: step 25780, loss = 6.30 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 11:17:29.622478: step 25790, loss = 6.35 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 11:17:35.399147: step 25800, loss = 6.14 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 11:17:42.220241: step 25810, loss = 6.43 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 11:17:47.094788: step 25820, loss = 6.10 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 11:17:51.818774: step 25830, loss = 6.36 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 11:17:56.612692: step 25840, loss = 6.21 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 11:18:01.395332: step 25850, loss = 6.41 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 11:18:07.768881: step 25860, loss = 6.21 (201.1 examples/sec; 0.636 sec/batch)
2016-07-15 11:18:13.228403: step 25870, loss = 6.26 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 11:18:17.950910: step 25880, loss = 6.41 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 11:18:22.793258: step 25890, loss = 6.38 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 11:18:27.498715: step 25900, loss = 6.36 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 11:18:34.666333: step 25910, loss = 6.40 (195.0 examples/sec; 0.656 sec/batch)
2016-07-15 11:18:40.423866: step 25920, loss = 5.97 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 11:18:45.162387: step 25930, loss = 6.35 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 11:18:50.064694: step 25940, loss = 6.41 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 11:18:54.809947: step 25950, loss = 6.32 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 11:18:59.651118: step 25960, loss = 6.23 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 11:19:04.390667: step 25970, loss = 6.20 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 11:19:09.142406: step 25980, loss = 6.32 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 11:19:13.947415: step 25990, loss = 6.30 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 11:19:20.344193: step 26000, loss = 6.23 (210.1 examples/sec; 0.609 sec/batch)
2016-07-15 11:19:26.844629: step 26010, loss = 6.10 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 11:19:31.574877: step 26020, loss = 6.32 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 11:19:37.221250: step 26030, loss = 6.17 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 11:19:43.346346: step 26040, loss = 6.40 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 11:19:48.763885: step 26050, loss = 6.21 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 11:19:54.010882: step 26060, loss = 6.32 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 11:19:59.720631: step 26070, loss = 6.24 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 11:20:05.214615: step 26080, loss = 6.14 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 11:20:10.373664: step 26090, loss = 6.21 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 11:20:15.060589: step 26100, loss = 6.66 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 11:20:22.016729: step 26110, loss = 6.24 (185.6 examples/sec; 0.690 sec/batch)
2016-07-15 11:20:27.906811: step 26120, loss = 6.02 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 11:20:32.706444: step 26130, loss = 5.91 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 11:20:37.553703: step 26140, loss = 6.28 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 11:20:42.323253: step 26150, loss = 6.21 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 11:20:47.427014: step 26160, loss = 6.30 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 11:20:53.934583: step 26170, loss = 6.20 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 11:20:59.016922: step 26180, loss = 6.13 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 11:21:03.771247: step 26190, loss = 6.31 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 11:21:09.496818: step 26200, loss = 6.32 (192.9 examples/sec; 0.664 sec/batch)
2016-07-15 11:21:16.832259: step 26210, loss = 6.37 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 11:21:22.607738: step 26220, loss = 6.25 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 11:21:27.448583: step 26230, loss = 6.22 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 11:21:32.238474: step 26240, loss = 6.37 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 11:21:38.216945: step 26250, loss = 6.02 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 11:21:44.063781: step 26260, loss = 6.10 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 11:21:48.836497: step 26270, loss = 6.30 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 11:21:53.692302: step 26280, loss = 6.36 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 11:22:00.031543: step 26290, loss = 6.47 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 11:22:05.494953: step 26300, loss = 6.44 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 11:22:11.200400: step 26310, loss = 6.03 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 11:22:16.663807: step 26320, loss = 6.08 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 11:22:22.929335: step 26330, loss = 6.30 (251.7 examples/sec; 0.509 sec/batch)
2016-07-15 11:22:28.163114: step 26340, loss = 6.53 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 11:22:33.639900: step 26350, loss = 6.42 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 11:22:38.377670: step 26360, loss = 6.12 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 11:22:43.221189: step 26370, loss = 6.12 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 11:22:47.942498: step 26380, loss = 5.79 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 11:22:52.770547: step 26390, loss = 6.10 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 11:22:57.585396: step 26400, loss = 6.28 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 11:23:03.329367: step 26410, loss = 6.17 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 11:23:08.811238: step 26420, loss = 6.45 (191.2 examples/sec; 0.670 sec/batch)
2016-07-15 11:23:15.090105: step 26430, loss = 6.47 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 11:23:19.928887: step 26440, loss = 6.38 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 11:23:24.694063: step 26450, loss = 6.46 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 11:23:29.485700: step 26460, loss = 6.03 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 11:23:34.329755: step 26470, loss = 6.48 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 11:23:39.034088: step 26480, loss = 6.22 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 11:23:44.474239: step 26490, loss = 6.24 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 11:23:50.860846: step 26500, loss = 6.32 (221.4 examples/sec; 0.578 sec/batch)
2016-07-15 11:23:57.439229: step 26510, loss = 6.44 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 11:24:02.612404: step 26520, loss = 6.37 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 11:24:07.328053: step 26530, loss = 6.38 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 11:24:13.233812: step 26540, loss = 6.46 (170.4 examples/sec; 0.751 sec/batch)
2016-07-15 11:24:19.518136: step 26550, loss = 6.52 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 11:24:24.860954: step 26560, loss = 6.65 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 11:24:30.162562: step 26570, loss = 6.65 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 11:24:34.922174: step 26580, loss = 6.36 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 11:24:40.185639: step 26590, loss = 6.29 (191.8 examples/sec; 0.667 sec/batch)
2016-07-15 11:24:46.627915: step 26600, loss = 6.27 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 11:24:52.424099: step 26610, loss = 6.26 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 11:24:57.250254: step 26620, loss = 6.39 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 11:25:02.019995: step 26630, loss = 6.47 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 11:25:06.947071: step 26640, loss = 6.09 (232.1 examples/sec; 0.551 sec/batch)
2016-07-15 11:25:13.531413: step 26650, loss = 6.04 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 11:25:18.734099: step 26660, loss = 6.09 (250.2 examples/sec; 0.512 sec/batch)
2016-07-15 11:25:23.483960: step 26670, loss = 6.45 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 11:25:28.111584: step 26680, loss = 6.44 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 11:25:33.481489: step 26690, loss = 6.64 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 11:25:38.728451: step 26700, loss = 6.32 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 11:25:44.446600: step 26710, loss = 6.19 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 11:25:50.352335: step 26720, loss = 6.32 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 11:25:56.184225: step 26730, loss = 6.24 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 11:26:00.984162: step 26740, loss = 6.05 (282.6 examples/sec; 0.453 sec/batch)
2016-07-15 11:26:05.741830: step 26750, loss = 5.99 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 11:26:10.470872: step 26760, loss = 6.17 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 11:26:15.525968: step 26770, loss = 6.28 (186.0 examples/sec; 0.688 sec/batch)
2016-07-15 11:26:22.044694: step 26780, loss = 6.42 (211.3 examples/sec; 0.606 sec/batch)
2016-07-15 11:26:27.170569: step 26790, loss = 6.10 (217.0 examples/sec; 0.590 sec/batch)
2016-07-15 11:26:32.852062: step 26800, loss = 6.15 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 11:26:38.664362: step 26810, loss = 6.33 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 11:26:43.841081: step 26820, loss = 6.15 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 11:26:50.318914: step 26830, loss = 6.23 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 11:26:55.255941: step 26840, loss = 6.32 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 11:27:00.026721: step 26850, loss = 6.25 (244.8 examples/sec; 0.523 sec/batch)
2016-07-15 11:27:04.807011: step 26860, loss = 6.33 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 11:27:09.652632: step 26870, loss = 6.11 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 11:27:15.914524: step 26880, loss = 6.30 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 11:27:21.503877: step 26890, loss = 6.35 (244.6 examples/sec; 0.523 sec/batch)
2016-07-15 11:27:26.256976: step 26900, loss = 6.14 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 11:27:32.461801: step 26910, loss = 6.47 (191.5 examples/sec; 0.669 sec/batch)
2016-07-15 11:27:38.897214: step 26920, loss = 6.28 (203.3 examples/sec; 0.629 sec/batch)
2016-07-15 11:27:43.783158: step 26930, loss = 5.91 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 11:27:48.571523: step 26940, loss = 6.35 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 11:27:53.345770: step 26950, loss = 6.29 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 11:27:58.160439: step 26960, loss = 6.13 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 11:28:04.556065: step 26970, loss = 6.13 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 11:28:09.986105: step 26980, loss = 6.11 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 11:28:15.755118: step 26990, loss = 6.23 (245.0 examples/sec; 0.522 sec/batch)
2016-07-15 11:28:20.574650: step 27000, loss = 6.25 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 11:28:26.346951: step 27010, loss = 6.20 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 11:28:31.055239: step 27020, loss = 6.21 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 11:28:35.684553: step 27030, loss = 6.10 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 11:28:40.830850: step 27040, loss = 6.22 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 11:28:46.173857: step 27050, loss = 6.37 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 11:28:51.961089: step 27060, loss = 6.20 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 11:28:56.782453: step 27070, loss = 6.19 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 11:29:01.595531: step 27080, loss = 6.39 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 11:29:06.328949: step 27090, loss = 6.08 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 11:29:11.262082: step 27100, loss = 6.44 (224.8 examples/sec; 0.569 sec/batch)
2016-07-15 11:29:19.636515: step 27110, loss = 6.31 (241.1 examples/sec; 0.531 sec/batch)
2016-07-15 11:29:25.495612: step 27120, loss = 6.28 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 11:29:30.737025: step 27130, loss = 6.12 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 11:29:36.532343: step 27140, loss = 6.21 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 11:29:41.306429: step 27150, loss = 6.22 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 11:29:46.121226: step 27160, loss = 6.25 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 11:29:52.468746: step 27170, loss = 6.28 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 11:29:57.868701: step 27180, loss = 6.18 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 11:30:02.664895: step 27190, loss = 6.26 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 11:30:07.731283: step 27200, loss = 6.12 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 11:30:15.709471: step 27210, loss = 6.25 (251.5 examples/sec; 0.509 sec/batch)
2016-07-15 11:30:21.069469: step 27220, loss = 6.18 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 11:30:26.333779: step 27230, loss = 6.21 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 11:30:31.057305: step 27240, loss = 6.00 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 11:30:35.885071: step 27250, loss = 6.04 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 11:30:40.580324: step 27260, loss = 6.55 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 11:30:45.224898: step 27270, loss = 6.22 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 11:30:50.847865: step 27280, loss = 6.16 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 11:30:55.923038: step 27290, loss = 6.06 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 11:31:01.445331: step 27300, loss = 6.01 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 11:31:07.172575: step 27310, loss = 6.27 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 11:31:11.997369: step 27320, loss = 6.20 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 11:31:16.707903: step 27330, loss = 6.26 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 11:31:22.526645: step 27340, loss = 6.19 (191.6 examples/sec; 0.668 sec/batch)
2016-07-15 11:31:28.485652: step 27350, loss = 6.20 (250.5 examples/sec; 0.511 sec/batch)
2016-07-15 11:31:33.310336: step 27360, loss = 6.34 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 11:31:38.124511: step 27370, loss = 6.28 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 11:31:42.863055: step 27380, loss = 6.33 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 11:31:48.006217: step 27390, loss = 6.06 (184.8 examples/sec; 0.693 sec/batch)
2016-07-15 11:31:54.528584: step 27400, loss = 6.10 (198.9 examples/sec; 0.644 sec/batch)
2016-07-15 11:32:01.083197: step 27410, loss = 6.03 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 11:32:06.452482: step 27420, loss = 6.12 (242.1 examples/sec; 0.529 sec/batch)
2016-07-15 11:32:11.194532: step 27430, loss = 6.17 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 11:32:16.076059: step 27440, loss = 6.36 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 11:32:20.772415: step 27450, loss = 6.23 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 11:32:25.378580: step 27460, loss = 6.39 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 11:32:31.062412: step 27470, loss = 6.20 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 11:32:36.162138: step 27480, loss = 6.04 (207.6 examples/sec; 0.617 sec/batch)
2016-07-15 11:32:41.697923: step 27490, loss = 6.53 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 11:32:46.430309: step 27500, loss = 5.87 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 11:32:52.037313: step 27510, loss = 6.25 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 11:32:56.636183: step 27520, loss = 6.18 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 11:33:02.092257: step 27530, loss = 6.15 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 11:33:07.213628: step 27540, loss = 5.89 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 11:33:12.961365: step 27550, loss = 6.06 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 11:33:18.586755: step 27560, loss = 6.23 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 11:33:23.609968: step 27570, loss = 6.14 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 11:33:28.331442: step 27580, loss = 6.28 (247.6 examples/sec; 0.517 sec/batch)
2016-07-15 11:33:33.980049: step 27590, loss = 6.58 (184.7 examples/sec; 0.693 sec/batch)
2016-07-15 11:33:40.072728: step 27600, loss = 6.07 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 11:33:45.859497: step 27610, loss = 6.25 (275.0 examples/sec; 0.466 sec/batch)
2016-07-15 11:33:50.729138: step 27620, loss = 6.11 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 11:33:55.466330: step 27630, loss = 6.34 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 11:34:00.265898: step 27640, loss = 6.12 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 11:34:05.030427: step 27650, loss = 6.17 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 11:34:09.801377: step 27660, loss = 6.25 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 11:34:14.655667: step 27670, loss = 6.09 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 11:34:19.357491: step 27680, loss = 6.32 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 11:34:24.674712: step 27690, loss = 6.04 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 11:34:31.116937: step 27700, loss = 6.37 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 11:34:37.040819: step 27710, loss = 6.35 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 11:34:41.866917: step 27720, loss = 6.55 (248.8 examples/sec; 0.514 sec/batch)
2016-07-15 11:34:48.044132: step 27730, loss = 6.46 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 11:34:53.639397: step 27740, loss = 6.36 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 11:34:58.346724: step 27750, loss = 6.29 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 11:35:03.212167: step 27760, loss = 6.39 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 11:35:07.885568: step 27770, loss = 6.07 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 11:35:13.392896: step 27780, loss = 5.99 (190.6 examples/sec; 0.671 sec/batch)
2016-07-15 11:35:19.680819: step 27790, loss = 6.23 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 11:35:24.723128: step 27800, loss = 6.25 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 11:35:30.383505: step 27810, loss = 5.89 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 11:35:35.046947: step 27820, loss = 5.85 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 11:35:40.797097: step 27830, loss = 6.32 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 11:35:45.580441: step 27840, loss = 6.39 (282.5 examples/sec; 0.453 sec/batch)
2016-07-15 11:35:50.433901: step 27850, loss = 6.51 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 11:35:56.711440: step 27860, loss = 6.28 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 11:36:02.130204: step 27870, loss = 6.09 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 11:36:06.890892: step 27880, loss = 6.19 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 11:36:11.794955: step 27890, loss = 6.15 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 11:36:16.558653: step 27900, loss = 6.14 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 11:36:22.362777: step 27910, loss = 5.98 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 11:36:27.010964: step 27920, loss = 6.20 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 11:36:31.709194: step 27930, loss = 6.20 (282.2 examples/sec; 0.454 sec/batch)
2016-07-15 11:36:36.388546: step 27940, loss = 6.25 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 11:36:41.025534: step 27950, loss = 5.93 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 11:36:45.659472: step 27960, loss = 6.09 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 11:36:50.295222: step 27970, loss = 6.29 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 11:36:56.007152: step 27980, loss = 6.29 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 11:37:00.819370: step 27990, loss = 6.39 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 11:37:05.629729: step 28000, loss = 6.19 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 11:37:11.407626: step 28010, loss = 6.11 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 11:37:16.034045: step 28020, loss = 6.31 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 11:37:20.671288: step 28030, loss = 5.98 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 11:37:25.295461: step 28040, loss = 6.15 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 11:37:30.842939: step 28050, loss = 6.19 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 11:37:35.801312: step 28060, loss = 6.16 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 11:37:40.542509: step 28070, loss = 6.61 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 11:37:46.208277: step 28080, loss = 6.24 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 11:37:52.303671: step 28090, loss = 6.00 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 11:37:57.072650: step 28100, loss = 6.12 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 11:38:02.644545: step 28110, loss = 6.12 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 11:38:07.382997: step 28120, loss = 6.05 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 11:38:12.046194: step 28130, loss = 6.18 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 11:38:17.157074: step 28140, loss = 5.95 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 11:38:22.554494: step 28150, loss = 6.04 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 11:38:28.301615: step 28160, loss = 6.08 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 11:38:33.100265: step 28170, loss = 6.40 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 11:38:37.900626: step 28180, loss = 6.14 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 11:38:42.628987: step 28190, loss = 6.06 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 11:38:47.586744: step 28200, loss = 6.42 (245.7 examples/sec; 0.521 sec/batch)
2016-07-15 11:38:54.627729: step 28210, loss = 6.27 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 11:38:59.322689: step 28220, loss = 6.29 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 11:39:05.078455: step 28230, loss = 6.38 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 11:39:10.649884: step 28240, loss = 6.22 (200.3 examples/sec; 0.639 sec/batch)
2016-07-15 11:39:15.781103: step 28250, loss = 5.90 (229.5 examples/sec; 0.558 sec/batch)
2016-07-15 11:39:21.535784: step 28260, loss = 6.26 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 11:39:26.307492: step 28270, loss = 6.32 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 11:39:31.160039: step 28280, loss = 6.09 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 11:39:35.860203: step 28290, loss = 6.32 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 11:39:40.485847: step 28300, loss = 6.30 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 11:39:46.073810: step 28310, loss = 6.16 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 11:39:50.739657: step 28320, loss = 6.09 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 11:39:56.473686: step 28330, loss = 6.39 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 11:40:01.246168: step 28340, loss = 6.11 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 11:40:06.035244: step 28350, loss = 6.19 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 11:40:10.777363: step 28360, loss = 6.14 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 11:40:15.631847: step 28370, loss = 6.20 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 11:40:20.381334: step 28380, loss = 6.29 (246.6 examples/sec; 0.519 sec/batch)
2016-07-15 11:40:26.044887: step 28390, loss = 6.14 (179.6 examples/sec; 0.713 sec/batch)
2016-07-15 11:40:32.432840: step 28400, loss = 6.34 (250.5 examples/sec; 0.511 sec/batch)
2016-07-15 11:40:39.843334: step 28410, loss = 6.05 (210.2 examples/sec; 0.609 sec/batch)
2016-07-15 11:40:45.116147: step 28420, loss = 6.36 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 11:40:50.596636: step 28430, loss = 6.30 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 11:40:55.372409: step 28440, loss = 6.12 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 11:41:00.049888: step 28450, loss = 6.07 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 11:41:05.176664: step 28460, loss = 5.92 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 11:41:10.576956: step 28470, loss = 6.36 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 11:41:15.326008: step 28480, loss = 6.30 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 11:41:20.194240: step 28490, loss = 5.96 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 11:41:24.877936: step 28500, loss = 6.04 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 11:41:30.473049: step 28510, loss = 6.07 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 11:41:36.202448: step 28520, loss = 6.37 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 11:41:41.551867: step 28530, loss = 6.02 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 11:41:46.899801: step 28540, loss = 6.24 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 11:41:52.668875: step 28550, loss = 6.03 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 11:41:57.476347: step 28560, loss = 6.09 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 11:42:02.285859: step 28570, loss = 6.35 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 11:42:07.029083: step 28580, loss = 6.14 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 11:42:12.002932: step 28590, loss = 6.31 (216.3 examples/sec; 0.592 sec/batch)
2016-07-15 11:42:18.564336: step 28600, loss = 6.06 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 11:42:24.590866: step 28610, loss = 6.05 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 11:42:29.823170: step 28620, loss = 6.37 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 11:42:35.136957: step 28630, loss = 5.99 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 11:42:39.815239: step 28640, loss = 5.96 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 11:42:44.733233: step 28650, loss = 6.37 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 11:42:49.346606: step 28660, loss = 5.82 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 11:42:53.960848: step 28670, loss = 6.38 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 11:42:59.633898: step 28680, loss = 6.12 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 11:43:04.487350: step 28690, loss = 6.11 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 11:43:09.256230: step 28700, loss = 6.11 (262.6 examples/sec; 0.488 sec/batch)
2016-07-15 11:43:16.496372: step 28710, loss = 5.90 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 11:43:22.094189: step 28720, loss = 6.05 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 11:43:26.843330: step 28730, loss = 6.25 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 11:43:31.745582: step 28740, loss = 6.38 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 11:43:38.344476: step 28750, loss = 6.38 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 11:43:43.498934: step 28760, loss = 5.94 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 11:43:49.289459: step 28770, loss = 6.41 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 11:43:54.136518: step 28780, loss = 5.98 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 11:43:58.788268: step 28790, loss = 6.18 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 11:44:03.423790: step 28800, loss = 6.27 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 11:44:10.328881: step 28810, loss = 6.24 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 11:44:16.116528: step 28820, loss = 6.25 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 11:44:20.946241: step 28830, loss = 6.08 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 11:44:25.763603: step 28840, loss = 6.22 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 11:44:30.519163: step 28850, loss = 6.03 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 11:44:35.359219: step 28860, loss = 6.27 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 11:44:41.825921: step 28870, loss = 6.15 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 11:44:46.946335: step 28880, loss = 6.36 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 11:44:51.596990: step 28890, loss = 6.26 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 11:44:57.361063: step 28900, loss = 6.01 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 11:45:03.436389: step 28910, loss = 6.24 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 11:45:08.239564: step 28920, loss = 6.18 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 11:45:12.925652: step 28930, loss = 6.06 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 11:45:18.566820: step 28940, loss = 5.98 (185.1 examples/sec; 0.692 sec/batch)
2016-07-15 11:45:24.639309: step 28950, loss = 6.28 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 11:45:30.041317: step 28960, loss = 6.12 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 11:45:35.286560: step 28970, loss = 6.05 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 11:45:40.069535: step 28980, loss = 6.10 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 11:45:44.894039: step 28990, loss = 6.16 (281.1 examples/sec; 0.455 sec/batch)
2016-07-15 11:45:49.552899: step 29000, loss = 6.04 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 11:45:55.118968: step 29010, loss = 6.51 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 11:46:00.874011: step 29020, loss = 6.28 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 11:46:05.648534: step 29030, loss = 6.03 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 11:46:10.265384: step 29040, loss = 6.13 (281.0 examples/sec; 0.455 sec/batch)
2016-07-15 11:46:14.983955: step 29050, loss = 6.06 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 11:46:20.715216: step 29060, loss = 6.02 (253.2 examples/sec; 0.505 sec/batch)
2016-07-15 11:46:26.106667: step 29070, loss = 6.07 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 11:46:31.433302: step 29080, loss = 5.96 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 11:46:37.155726: step 29090, loss = 6.27 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 11:46:42.662882: step 29100, loss = 6.05 (198.0 examples/sec; 0.646 sec/batch)
2016-07-15 11:46:48.723461: step 29110, loss = 5.81 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 11:46:53.943151: step 29120, loss = 5.98 (204.3 examples/sec; 0.626 sec/batch)
2016-07-15 11:46:59.076693: step 29130, loss = 6.16 (282.6 examples/sec; 0.453 sec/batch)
2016-07-15 11:47:03.792164: step 29140, loss = 6.30 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 11:47:08.400011: step 29150, loss = 6.28 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 11:47:13.539667: step 29160, loss = 6.21 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 11:47:18.950904: step 29170, loss = 6.06 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 11:47:23.684491: step 29180, loss = 6.41 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 11:47:28.848478: step 29190, loss = 6.18 (185.9 examples/sec; 0.688 sec/batch)
2016-07-15 11:47:35.350473: step 29200, loss = 6.19 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 11:47:41.363236: step 29210, loss = 6.10 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 11:47:46.164545: step 29220, loss = 6.09 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 11:47:52.223478: step 29230, loss = 6.41 (196.4 examples/sec; 0.652 sec/batch)
2016-07-15 11:47:57.935304: step 29240, loss = 6.08 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 11:48:02.697823: step 29250, loss = 5.85 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 11:48:07.556628: step 29260, loss = 6.10 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 11:48:14.035689: step 29270, loss = 6.20 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 11:48:19.373189: step 29280, loss = 5.82 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 11:48:24.092793: step 29290, loss = 5.97 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 11:48:28.921069: step 29300, loss = 6.09 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 11:48:34.682481: step 29310, loss = 6.15 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 11:48:40.868546: step 29320, loss = 5.87 (208.3 examples/sec; 0.615 sec/batch)
2016-07-15 11:48:46.505915: step 29330, loss = 6.40 (243.8 examples/sec; 0.525 sec/batch)
2016-07-15 11:48:51.221220: step 29340, loss = 6.12 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 11:48:56.180685: step 29350, loss = 5.99 (233.4 examples/sec; 0.548 sec/batch)
2016-07-15 11:49:02.720642: step 29360, loss = 6.33 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 11:49:07.655806: step 29370, loss = 6.01 (283.6 examples/sec; 0.451 sec/batch)
2016-07-15 11:49:12.489942: step 29380, loss = 6.01 (211.4 examples/sec; 0.605 sec/batch)
2016-07-15 11:49:18.138265: step 29390, loss = 6.00 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 11:49:23.902116: step 29400, loss = 6.11 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 11:49:29.742757: step 29410, loss = 5.83 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 11:49:34.547913: step 29420, loss = 6.21 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 11:49:39.279113: step 29430, loss = 6.29 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 11:49:44.180495: step 29440, loss = 5.99 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 11:49:48.903156: step 29450, loss = 6.15 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 11:49:53.544412: step 29460, loss = 6.14 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 11:49:58.977380: step 29470, loss = 6.20 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 11:50:04.244759: step 29480, loss = 6.09 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 11:50:09.120363: step 29490, loss = 6.17 (247.9 examples/sec; 0.516 sec/batch)
2016-07-15 11:50:14.381719: step 29500, loss = 5.99 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 11:50:20.312440: step 29510, loss = 6.14 (234.3 examples/sec; 0.546 sec/batch)
2016-07-15 11:50:26.896695: step 29520, loss = 5.97 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 11:50:32.086171: step 29530, loss = 5.96 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 11:50:36.802929: step 29540, loss = 6.21 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 11:50:42.313306: step 29550, loss = 6.03 (185.5 examples/sec; 0.690 sec/batch)
2016-07-15 11:50:48.943014: step 29560, loss = 6.12 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 11:50:54.335136: step 29570, loss = 5.99 (199.1 examples/sec; 0.643 sec/batch)
2016-07-15 11:50:59.688809: step 29580, loss = 6.26 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 11:51:04.383516: step 29590, loss = 5.72 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 11:51:09.197243: step 29600, loss = 6.37 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 11:51:14.843313: step 29610, loss = 5.99 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 11:51:19.467151: step 29620, loss = 6.00 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 11:51:25.605326: step 29630, loss = 6.18 (253.5 examples/sec; 0.505 sec/batch)
2016-07-15 11:51:31.476558: step 29640, loss = 6.16 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 11:51:36.495565: step 29650, loss = 5.83 (270.3 examples/sec; 0.473 sec/batch)
2016-07-15 11:51:41.235178: step 29660, loss = 6.01 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 11:51:47.207782: step 29670, loss = 6.03 (184.4 examples/sec; 0.694 sec/batch)
2016-07-15 11:51:53.286336: step 29680, loss = 6.04 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 11:51:58.858173: step 29690, loss = 6.34 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 11:52:04.217494: step 29700, loss = 6.08 (245.0 examples/sec; 0.522 sec/batch)
2016-07-15 11:52:09.968740: step 29710, loss = 5.94 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 11:52:15.917514: step 29720, loss = 6.12 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 11:52:21.691451: step 29730, loss = 6.22 (282.0 examples/sec; 0.454 sec/batch)
2016-07-15 11:52:26.364536: step 29740, loss = 6.07 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 11:52:32.050344: step 29750, loss = 6.09 (196.0 examples/sec; 0.653 sec/batch)
2016-07-15 11:52:37.066542: step 29760, loss = 6.04 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 11:52:42.016962: step 29770, loss = 5.99 (251.8 examples/sec; 0.508 sec/batch)
2016-07-15 11:52:46.957024: step 29780, loss = 6.12 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 11:52:51.935480: step 29790, loss = 6.04 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 11:52:58.711713: step 29800, loss = 6.05 (194.3 examples/sec; 0.659 sec/batch)
2016-07-15 11:53:05.111797: step 29810, loss = 6.21 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 11:53:09.886568: step 29820, loss = 5.84 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 11:53:15.535781: step 29830, loss = 6.23 (198.8 examples/sec; 0.644 sec/batch)
2016-07-15 11:53:20.423776: step 29840, loss = 6.00 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 11:53:25.403698: step 29850, loss = 5.89 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 11:53:30.982502: step 29860, loss = 5.91 (246.3 examples/sec; 0.520 sec/batch)
2016-07-15 11:53:36.995523: step 29870, loss = 6.13 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 11:53:42.409306: step 29880, loss = 6.22 (208.3 examples/sec; 0.614 sec/batch)
2016-07-15 11:53:47.715953: step 29890, loss = 6.08 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 11:53:52.429205: step 29900, loss = 6.01 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 11:53:58.281889: step 29910, loss = 6.26 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 11:54:03.056499: step 29920, loss = 5.88 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 11:54:09.305234: step 29930, loss = 6.52 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 11:54:14.869623: step 29940, loss = 5.92 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 11:54:20.661451: step 29950, loss = 6.23 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 11:54:25.498930: step 29960, loss = 6.24 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 11:54:30.419070: step 29970, loss = 6.19 (250.4 examples/sec; 0.511 sec/batch)
2016-07-15 11:54:35.315911: step 29980, loss = 6.13 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 11:54:40.098776: step 29990, loss = 5.93 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 11:54:45.217049: step 30000, loss = 6.15 (193.2 examples/sec; 0.663 sec/batch)
2016-07-15 11:54:52.224394: step 30010, loss = 6.06 (253.1 examples/sec; 0.506 sec/batch)
2016-07-15 11:54:57.098676: step 30020, loss = 6.30 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 11:55:02.103365: step 30030, loss = 6.18 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 11:55:07.087927: step 30040, loss = 6.29 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 11:55:13.712044: step 30050, loss = 6.26 (194.3 examples/sec; 0.659 sec/batch)
2016-07-15 11:55:19.251830: step 30060, loss = 5.91 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 11:55:23.998118: step 30070, loss = 6.20 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 11:55:29.070196: step 30080, loss = 6.10 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 11:55:35.776607: step 30090, loss = 5.96 (199.6 examples/sec; 0.641 sec/batch)
2016-07-15 11:55:40.802124: step 30100, loss = 5.97 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 11:55:46.549235: step 30110, loss = 6.18 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 11:55:51.383820: step 30120, loss = 6.05 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 11:55:56.026553: step 30130, loss = 5.98 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 11:56:00.987525: step 30140, loss = 6.05 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 11:56:06.673505: step 30150, loss = 6.21 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 11:56:11.453025: step 30160, loss = 6.13 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 11:56:16.105193: step 30170, loss = 6.22 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 11:56:21.064762: step 30180, loss = 5.87 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 11:56:26.839530: step 30190, loss = 6.36 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 11:56:31.612151: step 30200, loss = 6.15 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 11:56:37.921750: step 30210, loss = 6.10 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 11:56:44.397378: step 30220, loss = 6.08 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 11:56:49.263955: step 30230, loss = 5.92 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 11:56:54.039008: step 30240, loss = 6.06 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 11:56:59.888257: step 30250, loss = 6.13 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 11:57:05.786898: step 30260, loss = 6.08 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 11:57:10.547037: step 30270, loss = 6.18 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 11:57:15.196106: step 30280, loss = 6.12 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 11:57:19.859531: step 30290, loss = 5.96 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 11:57:24.491689: step 30300, loss = 6.06 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 11:57:30.760581: step 30310, loss = 6.15 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 11:57:36.060809: step 30320, loss = 6.21 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 11:57:41.884731: step 30330, loss = 6.01 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 11:57:46.663007: step 30340, loss = 6.35 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 11:57:51.525087: step 30350, loss = 5.93 (251.8 examples/sec; 0.508 sec/batch)
2016-07-15 11:57:56.303017: step 30360, loss = 6.13 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 11:58:01.510257: step 30370, loss = 6.08 (182.8 examples/sec; 0.700 sec/batch)
2016-07-15 11:58:07.963189: step 30380, loss = 6.01 (208.6 examples/sec; 0.613 sec/batch)
2016-07-15 11:58:12.947144: step 30390, loss = 6.09 (282.3 examples/sec; 0.453 sec/batch)
2016-07-15 11:58:17.731936: step 30400, loss = 5.96 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 11:58:25.064559: step 30410, loss = 5.81 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 11:58:30.757462: step 30420, loss = 6.06 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 11:58:35.697766: step 30430, loss = 6.07 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 11:58:40.541102: step 30440, loss = 5.83 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 11:58:45.265611: step 30450, loss = 6.01 (250.0 examples/sec; 0.512 sec/batch)
2016-07-15 11:58:50.063749: step 30460, loss = 5.98 (281.8 examples/sec; 0.454 sec/batch)
2016-07-15 11:58:54.857771: step 30470, loss = 6.19 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 11:59:01.006419: step 30480, loss = 6.35 (198.1 examples/sec; 0.646 sec/batch)
2016-07-15 11:59:06.691090: step 30490, loss = 6.10 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 11:59:11.494613: step 30500, loss = 6.00 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 11:59:17.410016: step 30510, loss = 5.96 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 11:59:22.230008: step 30520, loss = 6.28 (241.7 examples/sec; 0.530 sec/batch)
2016-07-15 11:59:27.071004: step 30530, loss = 6.01 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 11:59:32.327211: step 30540, loss = 6.06 (215.1 examples/sec; 0.595 sec/batch)
2016-07-15 11:59:39.219845: step 30550, loss = 5.99 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 11:59:44.556235: step 30560, loss = 6.19 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 11:59:49.363717: step 30570, loss = 6.01 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 11:59:54.968351: step 30580, loss = 6.19 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 12:00:01.231899: step 30590, loss = 6.07 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 12:00:06.571443: step 30600, loss = 6.01 (195.9 examples/sec; 0.653 sec/batch)
2016-07-15 12:00:13.295519: step 30610, loss = 5.89 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 12:00:18.888729: step 30620, loss = 6.00 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 12:00:24.725112: step 30630, loss = 6.09 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 12:00:29.949153: step 30640, loss = 6.02 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 12:00:35.456183: step 30650, loss = 6.07 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 12:00:40.243466: step 30660, loss = 5.99 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 12:00:45.445203: step 30670, loss = 6.21 (181.3 examples/sec; 0.706 sec/batch)
2016-07-15 12:00:52.051680: step 30680, loss = 5.95 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 12:00:57.126575: step 30690, loss = 6.00 (266.9 examples/sec; 0.479 sec/batch)
2016-07-15 12:01:01.934383: step 30700, loss = 6.07 (237.7 examples/sec; 0.539 sec/batch)
2016-07-15 12:01:07.747645: step 30710, loss = 6.08 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 12:01:12.639743: step 30720, loss = 5.96 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 12:01:17.328721: step 30730, loss = 5.86 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 12:01:22.950363: step 30740, loss = 6.14 (175.0 examples/sec; 0.731 sec/batch)
2016-07-15 12:01:29.307265: step 30750, loss = 5.80 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 12:01:34.721468: step 30760, loss = 6.23 (196.3 examples/sec; 0.652 sec/batch)
2016-07-15 12:01:40.152510: step 30770, loss = 5.91 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 12:01:44.878785: step 30780, loss = 5.97 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 12:01:50.233225: step 30790, loss = 6.10 (184.3 examples/sec; 0.694 sec/batch)
2016-07-15 12:01:56.783641: step 30800, loss = 5.83 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 12:02:03.317532: step 30810, loss = 5.89 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 12:02:08.592991: step 30820, loss = 5.92 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 12:02:13.274733: step 30830, loss = 5.97 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 12:02:18.822726: step 30840, loss = 5.90 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 12:02:25.275054: step 30850, loss = 6.20 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 12:02:30.207006: step 30860, loss = 5.92 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 12:02:34.896273: step 30870, loss = 6.05 (284.1 examples/sec; 0.451 sec/batch)
2016-07-15 12:02:39.552966: step 30880, loss = 5.92 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 12:02:45.386526: step 30890, loss = 6.20 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 12:02:50.920716: step 30900, loss = 5.90 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 12:02:57.723543: step 30910, loss = 5.87 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 12:03:03.729463: step 30920, loss = 6.01 (236.0 examples/sec; 0.542 sec/batch)
2016-07-15 12:03:08.457719: step 30930, loss = 5.92 (280.4 examples/sec; 0.457 sec/batch)
2016-07-15 12:03:13.771904: step 30940, loss = 6.18 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 12:03:18.534582: step 30950, loss = 5.90 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 12:03:24.549173: step 30960, loss = 6.00 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 12:03:30.530959: step 30970, loss = 6.23 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 12:03:36.342598: step 30980, loss = 5.85 (182.3 examples/sec; 0.702 sec/batch)
2016-07-15 12:03:41.563319: step 30990, loss = 6.04 (192.8 examples/sec; 0.664 sec/batch)
2016-07-15 12:03:47.201517: step 31000, loss = 6.23 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 12:03:52.996043: step 31010, loss = 5.81 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 12:03:57.884913: step 31020, loss = 5.76 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 12:04:02.680402: step 31030, loss = 5.95 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 12:04:07.419787: step 31040, loss = 6.08 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 12:04:12.760617: step 31050, loss = 5.96 (237.4 examples/sec; 0.539 sec/batch)
2016-07-15 12:04:19.471060: step 31060, loss = 6.08 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 12:04:24.667563: step 31070, loss = 5.84 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 12:04:29.390520: step 31080, loss = 6.11 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 12:04:34.242384: step 31090, loss = 5.83 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 12:04:39.035469: step 31100, loss = 5.83 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 12:04:46.465903: step 31110, loss = 6.14 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 12:04:51.924928: step 31120, loss = 6.03 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 12:04:57.668521: step 31130, loss = 6.01 (263.6 examples/sec; 0.485 sec/batch)
2016-07-15 12:05:03.017097: step 31140, loss = 5.93 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 12:05:08.401132: step 31150, loss = 5.88 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 12:05:14.244539: step 31160, loss = 6.26 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 12:05:19.112971: step 31170, loss = 5.79 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 12:05:23.937725: step 31180, loss = 6.26 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 12:05:30.508727: step 31190, loss = 6.03 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 12:05:36.055075: step 31200, loss = 6.27 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 12:05:41.831424: step 31210, loss = 6.22 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 12:05:46.620080: step 31220, loss = 5.94 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 12:05:51.402107: step 31230, loss = 5.99 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 12:05:56.173696: step 31240, loss = 6.09 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 12:06:01.051740: step 31250, loss = 5.93 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 12:06:05.770234: step 31260, loss = 5.90 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 12:06:11.445612: step 31270, loss = 5.99 (186.2 examples/sec; 0.688 sec/batch)
2016-07-15 12:06:17.660799: step 31280, loss = 6.35 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 12:06:22.471197: step 31290, loss = 6.23 (275.6 examples/sec; 0.465 sec/batch)
2016-07-15 12:06:27.279310: step 31300, loss = 6.27 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 12:06:33.154042: step 31310, loss = 6.02 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 12:06:37.972816: step 31320, loss = 6.09 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 12:06:42.764469: step 31330, loss = 6.08 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 12:06:48.830233: step 31340, loss = 5.72 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 12:06:54.713044: step 31350, loss = 6.26 (244.7 examples/sec; 0.523 sec/batch)
2016-07-15 12:07:00.399267: step 31360, loss = 6.08 (201.5 examples/sec; 0.635 sec/batch)
2016-07-15 12:07:05.540230: step 31370, loss = 5.93 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 12:07:10.271949: step 31380, loss = 6.07 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 12:07:16.117342: step 31390, loss = 5.95 (175.6 examples/sec; 0.729 sec/batch)
2016-07-15 12:07:22.171118: step 31400, loss = 5.78 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 12:07:29.047435: step 31410, loss = 6.05 (200.4 examples/sec; 0.639 sec/batch)
2016-07-15 12:07:34.261257: step 31420, loss = 6.13 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 12:07:40.076296: step 31430, loss = 6.14 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 12:07:45.853719: step 31440, loss = 5.86 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 12:07:50.725578: step 31450, loss = 5.95 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 12:07:55.688855: step 31460, loss = 6.05 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 12:08:00.386034: step 31470, loss = 6.19 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 12:08:05.307122: step 31480, loss = 5.99 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 12:08:10.125096: step 31490, loss = 6.05 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 12:08:14.943162: step 31500, loss = 6.18 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 12:08:20.865785: step 31510, loss = 5.93 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 12:08:25.635960: step 31520, loss = 6.05 (249.1 examples/sec; 0.514 sec/batch)
2016-07-15 12:08:31.163146: step 31530, loss = 5.96 (183.4 examples/sec; 0.698 sec/batch)
2016-07-15 12:08:37.610183: step 31540, loss = 5.99 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 12:08:42.480419: step 31550, loss = 6.22 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 12:08:47.284053: step 31560, loss = 5.95 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 12:08:53.408168: step 31570, loss = 5.81 (196.3 examples/sec; 0.652 sec/batch)
2016-07-15 12:08:59.116693: step 31580, loss = 6.01 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 12:09:03.895331: step 31590, loss = 5.88 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 12:09:08.776474: step 31600, loss = 5.72 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 12:09:16.908743: step 31610, loss = 6.03 (197.0 examples/sec; 0.650 sec/batch)
2016-07-15 12:09:21.817535: step 31620, loss = 5.85 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 12:09:26.449043: step 31630, loss = 6.12 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 12:09:31.086051: step 31640, loss = 6.28 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 12:09:36.738882: step 31650, loss = 6.12 (196.7 examples/sec; 0.651 sec/batch)
2016-07-15 12:09:41.723130: step 31660, loss = 5.93 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 12:09:46.348984: step 31670, loss = 6.04 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 12:09:51.048224: step 31680, loss = 6.25 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 12:09:55.744965: step 31690, loss = 5.99 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 12:10:00.370990: step 31700, loss = 6.08 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 12:10:06.380701: step 31710, loss = 5.92 (204.6 examples/sec; 0.625 sec/batch)
2016-07-15 12:10:12.093694: step 31720, loss = 6.14 (242.7 examples/sec; 0.527 sec/batch)
2016-07-15 12:10:17.025403: step 31730, loss = 6.05 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 12:10:22.074195: step 31740, loss = 6.00 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 12:10:26.991122: step 31750, loss = 6.09 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 12:10:32.985727: step 31760, loss = 6.13 (187.8 examples/sec; 0.682 sec/batch)
2016-07-15 12:10:38.896765: step 31770, loss = 5.96 (255.7 examples/sec; 0.500 sec/batch)
2016-07-15 12:10:44.719451: step 31780, loss = 6.14 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 12:10:49.967104: step 31790, loss = 6.13 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 12:10:55.603846: step 31800, loss = 5.85 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 12:11:01.324364: step 31810, loss = 5.96 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 12:11:06.621731: step 31820, loss = 5.95 (182.3 examples/sec; 0.702 sec/batch)
2016-07-15 12:11:13.132598: step 31830, loss = 6.05 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 12:11:17.988094: step 31840, loss = 5.74 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 12:11:22.762548: step 31850, loss = 5.86 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 12:11:27.602839: step 31860, loss = 6.00 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 12:11:32.401994: step 31870, loss = 5.82 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 12:11:38.797274: step 31880, loss = 6.07 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 12:11:44.327287: step 31890, loss = 6.02 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 12:11:49.045408: step 31900, loss = 6.15 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 12:11:55.652259: step 31910, loss = 5.74 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 12:12:01.867101: step 31920, loss = 6.17 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 12:12:07.246693: step 31930, loss = 5.78 (198.9 examples/sec; 0.644 sec/batch)
2016-07-15 12:12:12.549618: step 31940, loss = 5.87 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 12:12:17.414708: step 31950, loss = 6.12 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 12:12:22.984792: step 31960, loss = 5.85 (189.8 examples/sec; 0.675 sec/batch)
2016-07-15 12:12:29.393306: step 31970, loss = 5.90 (218.5 examples/sec; 0.586 sec/batch)
2016-07-15 12:12:34.591632: step 31980, loss = 5.71 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 12:12:40.086634: step 31990, loss = 5.79 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 12:12:44.818796: step 32000, loss = 5.83 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 12:12:50.643677: step 32010, loss = 6.01 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 12:12:55.422486: step 32020, loss = 5.88 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 12:13:01.439036: step 32030, loss = 6.03 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 12:13:06.078490: step 32040, loss = 5.71 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 12:13:11.204550: step 32050, loss = 6.07 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 12:13:16.584402: step 32060, loss = 5.87 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 12:13:22.391974: step 32070, loss = 5.98 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 12:13:27.861184: step 32080, loss = 6.12 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 12:13:33.064491: step 32090, loss = 6.11 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 12:13:38.878601: step 32100, loss = 6.01 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 12:13:45.843989: step 32110, loss = 5.90 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 12:13:50.702194: step 32120, loss = 5.88 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 12:13:55.524363: step 32130, loss = 6.12 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 12:14:00.293995: step 32140, loss = 5.94 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 12:14:04.905739: step 32150, loss = 5.87 (284.0 examples/sec; 0.451 sec/batch)
2016-07-15 12:14:09.684242: step 32160, loss = 6.14 (225.5 examples/sec; 0.568 sec/batch)
2016-07-15 12:14:15.392261: step 32170, loss = 6.23 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 12:14:20.179070: step 32180, loss = 5.87 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 12:14:25.036124: step 32190, loss = 5.88 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 12:14:31.545075: step 32200, loss = 6.11 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 12:14:38.271629: step 32210, loss = 5.98 (201.5 examples/sec; 0.635 sec/batch)
2016-07-15 12:14:43.869942: step 32220, loss = 6.04 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 12:14:48.693390: step 32230, loss = 5.81 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 12:14:53.756231: step 32240, loss = 6.11 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 12:15:00.399526: step 32250, loss = 5.91 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 12:15:05.579715: step 32260, loss = 5.63 (239.8 examples/sec; 0.534 sec/batch)
2016-07-15 12:15:11.453846: step 32270, loss = 5.96 (245.1 examples/sec; 0.522 sec/batch)
2016-07-15 12:15:17.358519: step 32280, loss = 5.94 (196.3 examples/sec; 0.652 sec/batch)
2016-07-15 12:15:22.213667: step 32290, loss = 6.14 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 12:15:26.917786: step 32300, loss = 6.02 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 12:15:32.839254: step 32310, loss = 5.84 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 12:15:37.494318: step 32320, loss = 6.21 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 12:15:42.478385: step 32330, loss = 5.76 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 12:15:48.109544: step 32340, loss = 6.10 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 12:15:52.853936: step 32350, loss = 5.95 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 12:15:57.736479: step 32360, loss = 6.31 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 12:16:02.518646: step 32370, loss = 5.80 (217.6 examples/sec; 0.588 sec/batch)
2016-07-15 12:16:08.242122: step 32380, loss = 5.95 (178.0 examples/sec; 0.719 sec/batch)
2016-07-15 12:16:14.377831: step 32390, loss = 5.78 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 12:16:19.264097: step 32400, loss = 5.93 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 12:16:25.187909: step 32410, loss = 5.77 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 12:16:31.827986: step 32420, loss = 5.86 (204.6 examples/sec; 0.625 sec/batch)
2016-07-15 12:16:37.205193: step 32430, loss = 5.99 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 12:16:43.350825: step 32440, loss = 6.29 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 12:16:48.358539: step 32450, loss = 6.03 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 12:16:53.424182: step 32460, loss = 6.08 (241.1 examples/sec; 0.531 sec/batch)
2016-07-15 12:16:58.367068: step 32470, loss = 5.64 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 12:17:03.359113: step 32480, loss = 5.99 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 12:17:09.137860: step 32490, loss = 5.93 (200.5 examples/sec; 0.638 sec/batch)
2016-07-15 12:17:14.260690: step 32500, loss = 6.11 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 12:17:21.215465: step 32510, loss = 6.22 (245.5 examples/sec; 0.521 sec/batch)
2016-07-15 12:17:27.067480: step 32520, loss = 6.04 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 12:17:31.902972: step 32530, loss = 5.87 (250.5 examples/sec; 0.511 sec/batch)
2016-07-15 12:17:36.904426: step 32540, loss = 6.26 (235.4 examples/sec; 0.544 sec/batch)
2016-07-15 12:17:41.768714: step 32550, loss = 5.93 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 12:17:47.151972: step 32560, loss = 5.74 (188.9 examples/sec; 0.677 sec/batch)
2016-07-15 12:17:53.816813: step 32570, loss = 6.07 (228.0 examples/sec; 0.561 sec/batch)
2016-07-15 12:17:59.138943: step 32580, loss = 5.81 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 12:18:04.623424: step 32590, loss = 5.96 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 12:18:10.644864: step 32600, loss = 6.10 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 12:18:17.680778: step 32610, loss = 5.88 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 12:18:22.603550: step 32620, loss = 5.92 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 12:18:27.421230: step 32630, loss = 6.24 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 12:18:32.250262: step 32640, loss = 5.90 (281.2 examples/sec; 0.455 sec/batch)
2016-07-15 12:18:37.167971: step 32650, loss = 6.02 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 12:18:41.932832: step 32660, loss = 6.14 (242.7 examples/sec; 0.527 sec/batch)
2016-07-15 12:18:46.802678: step 32670, loss = 5.99 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 12:18:51.566830: step 32680, loss = 5.78 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 12:18:56.403300: step 32690, loss = 5.91 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 12:19:01.319526: step 32700, loss = 5.92 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 12:19:09.656207: step 32710, loss = 6.20 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 12:19:14.568946: step 32720, loss = 6.07 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 12:19:19.357299: step 32730, loss = 5.76 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 12:19:25.328509: step 32740, loss = 5.91 (185.4 examples/sec; 0.690 sec/batch)
2016-07-15 12:19:31.242045: step 32750, loss = 5.99 (243.3 examples/sec; 0.526 sec/batch)
2016-07-15 12:19:36.034760: step 32760, loss = 5.55 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 12:19:40.700632: step 32770, loss = 5.89 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 12:19:45.389789: step 32780, loss = 5.82 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 12:19:51.271908: step 32790, loss = 5.79 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 12:19:56.990907: step 32800, loss = 5.98 (197.0 examples/sec; 0.650 sec/batch)
2016-07-15 12:20:05.032743: step 32810, loss = 6.10 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 12:20:09.917631: step 32820, loss = 5.92 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 12:20:14.674909: step 32830, loss = 6.01 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 12:20:19.490267: step 32840, loss = 6.29 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 12:20:24.357136: step 32850, loss = 5.97 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 12:20:30.640505: step 32860, loss = 5.93 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 12:20:36.297923: step 32870, loss = 5.56 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 12:20:42.088881: step 32880, loss = 6.03 (252.3 examples/sec; 0.507 sec/batch)
2016-07-15 12:20:46.915722: step 32890, loss = 5.76 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 12:20:51.729352: step 32900, loss = 5.83 (253.7 examples/sec; 0.505 sec/batch)
2016-07-15 12:20:58.259672: step 32910, loss = 6.10 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 12:21:03.081663: step 32920, loss = 6.02 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 12:21:07.870646: step 32930, loss = 6.04 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 12:21:14.004039: step 32940, loss = 6.16 (193.4 examples/sec; 0.662 sec/batch)
2016-07-15 12:21:19.825901: step 32950, loss = 5.86 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 12:21:25.606568: step 32960, loss = 5.91 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 12:21:30.473134: step 32970, loss = 6.13 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 12:21:35.277443: step 32980, loss = 6.05 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 12:21:40.085457: step 32990, loss = 6.01 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 12:21:44.929771: step 33000, loss = 6.01 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 12:21:51.451671: step 33010, loss = 5.93 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 12:21:56.277642: step 33020, loss = 5.87 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 12:22:00.906549: step 33030, loss = 5.86 (279.8 examples/sec; 0.458 sec/batch)
2016-07-15 12:22:05.691183: step 33040, loss = 5.95 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 12:22:11.587036: step 33050, loss = 5.84 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 12:22:17.268489: step 33060, loss = 5.95 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 12:22:22.470332: step 33070, loss = 5.63 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 12:22:28.097659: step 33080, loss = 5.87 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 12:22:32.883302: step 33090, loss = 5.94 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 12:22:37.722664: step 33100, loss = 5.89 (260.4 examples/sec; 0.491 sec/batch)
2016-07-15 12:22:43.491464: step 33110, loss = 5.98 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 12:22:49.399885: step 33120, loss = 5.88 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 12:22:55.397774: step 33130, loss = 5.75 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 12:23:01.032829: step 33140, loss = 5.74 (204.3 examples/sec; 0.626 sec/batch)
2016-07-15 12:23:06.129911: step 33150, loss = 5.79 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 12:23:10.875031: step 33160, loss = 5.84 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 12:23:15.752321: step 33170, loss = 5.86 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 12:23:20.564471: step 33180, loss = 5.82 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 12:23:26.729592: step 33190, loss = 5.74 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 12:23:31.413767: step 33200, loss = 5.65 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 12:23:38.338595: step 33210, loss = 6.06 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 12:23:43.148067: step 33220, loss = 5.79 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 12:23:47.916260: step 33230, loss = 5.90 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 12:23:52.659913: step 33240, loss = 6.05 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 12:23:57.538939: step 33250, loss = 5.95 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 12:24:04.154155: step 33260, loss = 6.10 (194.3 examples/sec; 0.659 sec/batch)
2016-07-15 12:24:09.403852: step 33270, loss = 5.78 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 12:24:14.099139: step 33280, loss = 5.77 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 12:24:19.513036: step 33290, loss = 5.92 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 12:24:26.036882: step 33300, loss = 5.96 (216.5 examples/sec; 0.591 sec/batch)
2016-07-15 12:24:32.660934: step 33310, loss = 5.88 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 12:24:37.525029: step 33320, loss = 5.99 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 12:24:44.042276: step 33330, loss = 6.04 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 12:24:49.366105: step 33340, loss = 5.78 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 12:24:54.250086: step 33350, loss = 5.83 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 12:24:59.242543: step 33360, loss = 6.12 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 12:25:04.543703: step 33370, loss = 5.73 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 12:25:10.900654: step 33380, loss = 5.96 (199.6 examples/sec; 0.641 sec/batch)
2016-07-15 12:25:16.540215: step 33390, loss = 5.59 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 12:25:22.371675: step 33400, loss = 6.07 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 12:25:29.711395: step 33410, loss = 5.97 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 12:25:34.612936: step 33420, loss = 5.66 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 12:25:39.335844: step 33430, loss = 6.01 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 12:25:43.994555: step 33440, loss = 5.90 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 12:25:48.738894: step 33450, loss = 6.14 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 12:25:53.386801: step 33460, loss = 5.97 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 12:25:58.036060: step 33470, loss = 5.91 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 12:26:03.047231: step 33480, loss = 5.82 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 12:26:08.638086: step 33490, loss = 5.72 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 12:26:13.493304: step 33500, loss = 5.87 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 12:26:19.327227: step 33510, loss = 5.87 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 12:26:24.282076: step 33520, loss = 6.26 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 12:26:29.085667: step 33530, loss = 5.84 (252.2 examples/sec; 0.507 sec/batch)
2016-07-15 12:26:34.829195: step 33540, loss = 5.91 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 12:26:39.744816: step 33550, loss = 5.82 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 12:26:44.550920: step 33560, loss = 5.81 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 12:26:49.339435: step 33570, loss = 5.92 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 12:26:54.338923: step 33580, loss = 5.76 (221.3 examples/sec; 0.578 sec/batch)
2016-07-15 12:27:00.963225: step 33590, loss = 5.89 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 12:27:06.068185: step 33600, loss = 5.95 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 12:27:13.119111: step 33610, loss = 5.98 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 12:27:18.855982: step 33620, loss = 5.95 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 12:27:24.299920: step 33630, loss = 5.59 (200.4 examples/sec; 0.639 sec/batch)
2016-07-15 12:27:29.622964: step 33640, loss = 5.92 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 12:27:35.469289: step 33650, loss = 5.81 (248.8 examples/sec; 0.515 sec/batch)
2016-07-15 12:27:40.234865: step 33660, loss = 6.00 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 12:27:45.142338: step 33670, loss = 6.01 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 12:27:51.543349: step 33680, loss = 5.97 (197.2 examples/sec; 0.649 sec/batch)
2016-07-15 12:27:57.059337: step 33690, loss = 5.81 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 12:28:01.816172: step 33700, loss = 5.72 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 12:28:07.654985: step 33710, loss = 5.80 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 12:28:12.444326: step 33720, loss = 5.70 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 12:28:17.204760: step 33730, loss = 5.91 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 12:28:22.068137: step 33740, loss = 5.80 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 12:28:26.819853: step 33750, loss = 5.88 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 12:28:31.675031: step 33760, loss = 6.13 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 12:28:36.393591: step 33770, loss = 5.85 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 12:28:41.077460: step 33780, loss = 5.97 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 12:28:45.750273: step 33790, loss = 5.96 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 12:28:50.655252: step 33800, loss = 5.92 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 12:28:57.629142: step 33810, loss = 5.89 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 12:29:02.342819: step 33820, loss = 6.13 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 12:29:07.774572: step 33830, loss = 6.04 (180.3 examples/sec; 0.710 sec/batch)
2016-07-15 12:29:14.319918: step 33840, loss = 5.77 (214.2 examples/sec; 0.598 sec/batch)
2016-07-15 12:29:19.191015: step 33850, loss = 5.86 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 12:29:23.896720: step 33860, loss = 5.82 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 12:29:28.534419: step 33870, loss = 5.79 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 12:29:34.320232: step 33880, loss = 5.68 (215.6 examples/sec; 0.594 sec/batch)
2016-07-15 12:29:39.583127: step 33890, loss = 5.77 (199.9 examples/sec; 0.640 sec/batch)
2016-07-15 12:29:45.088792: step 33900, loss = 5.80 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 12:29:50.975056: step 33910, loss = 5.86 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 12:29:56.616474: step 33920, loss = 5.62 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 12:30:02.939203: step 33930, loss = 5.83 (249.0 examples/sec; 0.514 sec/batch)
2016-07-15 12:30:08.366510: step 33940, loss = 5.73 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 12:30:13.755663: step 33950, loss = 5.66 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 12:30:19.623961: step 33960, loss = 6.11 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 12:30:24.415117: step 33970, loss = 5.84 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 12:30:29.228978: step 33980, loss = 5.84 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 12:30:34.006682: step 33990, loss = 5.83 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 12:30:39.147106: step 34000, loss = 6.01 (191.1 examples/sec; 0.670 sec/batch)
2016-07-15 12:30:47.081385: step 34010, loss = 5.83 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 12:30:52.496191: step 34020, loss = 5.93 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 12:30:57.825532: step 34030, loss = 5.53 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 12:31:03.613729: step 34040, loss = 5.92 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 12:31:08.428615: step 34050, loss = 5.87 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 12:31:13.214181: step 34060, loss = 5.84 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 12:31:17.917639: step 34070, loss = 5.79 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 12:31:22.753505: step 34080, loss = 5.84 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 12:31:27.466968: step 34090, loss = 5.80 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 12:31:33.129452: step 34100, loss = 5.82 (185.7 examples/sec; 0.689 sec/batch)
2016-07-15 12:31:39.334920: step 34110, loss = 5.82 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 12:31:43.988152: step 34120, loss = 6.04 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 12:31:48.627732: step 34130, loss = 6.15 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 12:31:54.367148: step 34140, loss = 6.02 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 12:31:59.280412: step 34150, loss = 5.99 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 12:32:04.018821: step 34160, loss = 6.05 (248.3 examples/sec; 0.516 sec/batch)
2016-07-15 12:32:09.937954: step 34170, loss = 5.65 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 12:32:15.809927: step 34180, loss = 5.70 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 12:32:20.580234: step 34190, loss = 5.75 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 12:32:25.442556: step 34200, loss = 5.89 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 12:32:31.186857: step 34210, loss = 5.72 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 12:32:35.828831: step 34220, loss = 5.84 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 12:32:40.442143: step 34230, loss = 5.54 (283.2 examples/sec; 0.452 sec/batch)
2016-07-15 12:32:45.004779: step 34240, loss = 5.93 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 12:32:49.655917: step 34250, loss = 5.77 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 12:32:55.450517: step 34260, loss = 5.75 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 12:33:01.203339: step 34270, loss = 5.96 (196.3 examples/sec; 0.652 sec/batch)
2016-07-15 12:33:07.032995: step 34280, loss = 5.47 (131.4 examples/sec; 0.974 sec/batch)
2016-07-15 12:33:11.868637: step 34290, loss = 5.91 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 12:33:16.686033: step 34300, loss = 5.87 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 12:33:22.683230: step 34310, loss = 5.83 (227.8 examples/sec; 0.562 sec/batch)
2016-07-15 12:33:29.193375: step 34320, loss = 5.69 (208.3 examples/sec; 0.615 sec/batch)
2016-07-15 12:33:34.389242: step 34330, loss = 5.80 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 12:33:39.066323: step 34340, loss = 5.77 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 12:33:44.559310: step 34350, loss = 5.66 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 12:33:49.819276: step 34360, loss = 5.64 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 12:33:54.570252: step 34370, loss = 5.80 (247.2 examples/sec; 0.518 sec/batch)
2016-07-15 12:34:00.358157: step 34380, loss = 5.74 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 12:34:05.125778: step 34390, loss = 5.76 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 12:34:10.019976: step 34400, loss = 6.14 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 12:34:15.772598: step 34410, loss = 5.82 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 12:34:20.581959: step 34420, loss = 6.00 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 12:34:25.419949: step 34430, loss = 5.88 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 12:34:30.091462: step 34440, loss = 5.66 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 12:34:35.200258: step 34450, loss = 5.81 (181.7 examples/sec; 0.704 sec/batch)
2016-07-15 12:34:41.700948: step 34460, loss = 5.88 (208.2 examples/sec; 0.615 sec/batch)
2016-07-15 12:34:46.756420: step 34470, loss = 6.18 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 12:34:51.515035: step 34480, loss = 5.98 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 12:34:56.309546: step 34490, loss = 5.89 (283.1 examples/sec; 0.452 sec/batch)
2016-07-15 12:35:01.097891: step 34500, loss = 5.62 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 12:35:08.691642: step 34510, loss = 5.82 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 12:35:13.979811: step 34520, loss = 6.08 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 12:35:18.711055: step 34530, loss = 5.70 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 12:35:23.591710: step 34540, loss = 5.88 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 12:35:28.246877: step 34550, loss = 5.56 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 12:35:32.931978: step 34560, loss = 5.97 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 12:35:38.662387: step 34570, loss = 5.94 (222.3 examples/sec; 0.576 sec/batch)
2016-07-15 12:35:43.551729: step 34580, loss = 6.00 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 12:35:48.185777: step 34590, loss = 6.07 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 12:35:52.863679: step 34600, loss = 6.16 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 12:35:59.629486: step 34610, loss = 6.21 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 12:36:05.123763: step 34620, loss = 5.79 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 12:36:10.305038: step 34630, loss = 5.64 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 12:36:16.076750: step 34640, loss = 5.90 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 12:36:20.834807: step 34650, loss = 5.76 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 12:36:25.741409: step 34660, loss = 5.85 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 12:36:30.524604: step 34670, loss = 5.48 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 12:36:35.121529: step 34680, loss = 5.94 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 12:36:39.788319: step 34690, loss = 5.61 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 12:36:44.450918: step 34700, loss = 5.82 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 12:36:51.172876: step 34710, loss = 5.87 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 12:36:56.492360: step 34720, loss = 5.66 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 12:37:01.846343: step 34730, loss = 5.93 (251.0 examples/sec; 0.510 sec/batch)
2016-07-15 12:37:07.699263: step 34740, loss = 5.81 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 12:37:12.550233: step 34750, loss = 5.88 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 12:37:17.313969: step 34760, loss = 5.90 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 12:37:23.610171: step 34770, loss = 5.90 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 12:37:29.154496: step 34780, loss = 6.09 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 12:37:33.879793: step 34790, loss = 5.76 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 12:37:38.757326: step 34800, loss = 5.73 (239.7 examples/sec; 0.534 sec/batch)
2016-07-15 12:37:46.943779: step 34810, loss = 5.79 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 12:37:51.631011: step 34820, loss = 5.90 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 12:37:56.913646: step 34830, loss = 5.95 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 12:38:02.174329: step 34840, loss = 5.85 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 12:38:06.884651: step 34850, loss = 5.70 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 12:38:11.689152: step 34860, loss = 5.77 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 12:38:16.448048: step 34870, loss = 5.88 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 12:38:22.385151: step 34880, loss = 5.73 (184.1 examples/sec; 0.695 sec/batch)
2016-07-15 12:38:28.227060: step 34890, loss = 5.99 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 12:38:33.041147: step 34900, loss = 5.77 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 12:38:38.943950: step 34910, loss = 5.90 (227.0 examples/sec; 0.564 sec/batch)
2016-07-15 12:38:45.525273: step 34920, loss = 6.01 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 12:38:50.664929: step 34930, loss = 5.87 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 12:38:55.431536: step 34940, loss = 5.99 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 12:39:00.216622: step 34950, loss = 5.91 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 12:39:05.013301: step 34960, loss = 5.84 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 12:39:11.159068: step 34970, loss = 5.72 (199.5 examples/sec; 0.642 sec/batch)
2016-07-15 12:39:16.856241: step 34980, loss = 5.68 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 12:39:21.680677: step 34990, loss = 5.94 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 12:39:26.560782: step 35000, loss = 6.03 (253.4 examples/sec; 0.505 sec/batch)
2016-07-15 12:39:34.722965: step 35010, loss = 5.86 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 12:39:39.593872: step 35020, loss = 5.70 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 12:39:44.314395: step 35030, loss = 5.75 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 12:39:50.184491: step 35040, loss = 5.44 (185.5 examples/sec; 0.690 sec/batch)
2016-07-15 12:39:56.128471: step 35050, loss = 5.70 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 12:40:00.930638: step 35060, loss = 5.74 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 12:40:05.746239: step 35070, loss = 5.81 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 12:40:10.483958: step 35080, loss = 5.52 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 12:40:15.637868: step 35090, loss = 5.74 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 12:40:22.175813: step 35100, loss = 5.74 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 12:40:28.680366: step 35110, loss = 5.69 (209.3 examples/sec; 0.612 sec/batch)
2016-07-15 12:40:34.018605: step 35120, loss = 6.08 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 12:40:39.840734: step 35130, loss = 5.84 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 12:40:44.706063: step 35140, loss = 5.83 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 12:40:49.563575: step 35150, loss = 5.90 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 12:40:55.822531: step 35160, loss = 5.96 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 12:41:01.377401: step 35170, loss = 5.73 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 12:41:07.190506: step 35180, loss = 5.88 (253.2 examples/sec; 0.505 sec/batch)
2016-07-15 12:41:12.030105: step 35190, loss = 5.67 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 12:41:16.790125: step 35200, loss = 6.01 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 12:41:22.563589: step 35210, loss = 5.81 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 12:41:27.764369: step 35220, loss = 5.91 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 12:41:34.250484: step 35230, loss = 5.81 (209.0 examples/sec; 0.612 sec/batch)
2016-07-15 12:41:39.252122: step 35240, loss = 5.75 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 12:41:43.993916: step 35250, loss = 5.73 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 12:41:49.645878: step 35260, loss = 5.70 (188.5 examples/sec; 0.679 sec/batch)
2016-07-15 12:41:55.679450: step 35270, loss = 6.11 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 12:42:00.355085: step 35280, loss = 5.61 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 12:42:05.851094: step 35290, loss = 5.89 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 12:42:11.006853: step 35300, loss = 5.79 (247.9 examples/sec; 0.516 sec/batch)
2016-07-15 12:42:16.609656: step 35310, loss = 6.00 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 12:42:21.241750: step 35320, loss = 6.02 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 12:42:26.973101: step 35330, loss = 5.51 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 12:42:31.796814: step 35340, loss = 5.71 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 12:42:36.561977: step 35350, loss = 5.65 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 12:42:41.364612: step 35360, loss = 5.83 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 12:42:46.021912: step 35370, loss = 5.90 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 12:42:50.726127: step 35380, loss = 5.84 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 12:42:56.545821: step 35390, loss = 5.74 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 12:43:02.150929: step 35400, loss = 5.57 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 12:43:08.837277: step 35410, loss = 5.69 (206.6 examples/sec; 0.619 sec/batch)
2016-07-15 12:43:14.215897: step 35420, loss = 5.73 (257.8 examples/sec; 0.497 sec/batch)
2016-07-15 12:43:18.909598: step 35430, loss = 5.91 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 12:43:23.556690: step 35440, loss = 5.46 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 12:43:28.223957: step 35450, loss = 5.79 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 12:43:32.933414: step 35460, loss = 5.89 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 12:43:37.568918: step 35470, loss = 5.80 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 12:43:42.267262: step 35480, loss = 6.01 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 12:43:48.046650: step 35490, loss = 5.50 (248.3 examples/sec; 0.516 sec/batch)
2016-07-15 12:43:53.481838: step 35500, loss = 5.87 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 12:43:59.698912: step 35510, loss = 5.66 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 12:44:04.898089: step 35520, loss = 5.75 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 12:44:10.196849: step 35530, loss = 5.82 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 12:44:16.042685: step 35540, loss = 5.55 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 12:44:20.884402: step 35550, loss = 5.72 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 12:44:25.702899: step 35560, loss = 6.07 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 12:44:31.929046: step 35570, loss = 5.92 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 12:44:37.503433: step 35580, loss = 5.61 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 12:44:42.263235: step 35590, loss = 5.61 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 12:44:47.179429: step 35600, loss = 5.80 (234.6 examples/sec; 0.546 sec/batch)
2016-07-15 12:44:55.306420: step 35610, loss = 5.73 (248.1 examples/sec; 0.516 sec/batch)
2016-07-15 12:45:00.532793: step 35620, loss = 5.77 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 12:45:05.961062: step 35630, loss = 6.01 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 12:45:10.701246: step 35640, loss = 5.98 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 12:45:15.830710: step 35650, loss = 5.93 (183.0 examples/sec; 0.700 sec/batch)
2016-07-15 12:45:22.327392: step 35660, loss = 5.55 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 12:45:27.329574: step 35670, loss = 5.59 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 12:45:32.034114: step 35680, loss = 5.62 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 12:45:36.833873: step 35690, loss = 5.86 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 12:45:41.708416: step 35700, loss = 5.73 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 12:45:47.459042: step 35710, loss = 5.48 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 12:45:52.109347: step 35720, loss = 5.86 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 12:45:57.430843: step 35730, loss = 5.84 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 12:46:02.675346: step 35740, loss = 5.88 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 12:46:08.443523: step 35750, loss = 5.86 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 12:46:13.228297: step 35760, loss = 6.06 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 12:46:18.048750: step 35770, loss = 5.81 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 12:46:22.832061: step 35780, loss = 5.72 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 12:46:27.680511: step 35790, loss = 5.59 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 12:46:32.377353: step 35800, loss = 5.42 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 12:46:38.005964: step 35810, loss = 5.66 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 12:46:42.698693: step 35820, loss = 5.75 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 12:46:47.423330: step 35830, loss = 5.56 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 12:46:52.042782: step 35840, loss = 5.93 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 12:46:57.216282: step 35850, loss = 5.61 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 12:47:02.494438: step 35860, loss = 5.78 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 12:47:07.179765: step 35870, loss = 5.62 (282.4 examples/sec; 0.453 sec/batch)
2016-07-15 12:47:12.012720: step 35880, loss = 6.01 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 12:47:16.754744: step 35890, loss = 5.80 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 12:47:22.635045: step 35900, loss = 5.63 (190.7 examples/sec; 0.671 sec/batch)
2016-07-15 12:47:29.723996: step 35910, loss = 5.85 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 12:47:35.456968: step 35920, loss = 5.89 (243.7 examples/sec; 0.525 sec/batch)
2016-07-15 12:47:40.710679: step 35930, loss = 5.84 (199.6 examples/sec; 0.641 sec/batch)
2016-07-15 12:47:46.157495: step 35940, loss = 6.01 (256.8 examples/sec; 0.499 sec/batch)
2016-07-15 12:47:50.834951: step 35950, loss = 5.69 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 12:47:55.736490: step 35960, loss = 5.65 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 12:48:00.477435: step 35970, loss = 5.73 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 12:48:06.239269: step 35980, loss = 5.66 (191.1 examples/sec; 0.670 sec/batch)
2016-07-15 12:48:12.295117: step 35990, loss = 5.72 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 12:48:17.708227: step 36000, loss = 5.85 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 12:48:23.908285: step 36010, loss = 5.62 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 12:48:28.556401: step 36020, loss = 5.82 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 12:48:33.207870: step 36030, loss = 5.60 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 12:48:38.857528: step 36040, loss = 5.47 (201.7 examples/sec; 0.634 sec/batch)
2016-07-15 12:48:43.801416: step 36050, loss = 5.82 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 12:48:48.564449: step 36060, loss = 5.62 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 12:48:53.406008: step 36070, loss = 5.57 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 12:48:58.168098: step 36080, loss = 5.69 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 12:49:02.945420: step 36090, loss = 5.98 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 12:49:08.084267: step 36100, loss = 5.82 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 12:49:16.041405: step 36110, loss = 5.84 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 12:49:21.428705: step 36120, loss = 5.82 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 12:49:26.766158: step 36130, loss = 5.78 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 12:49:31.504947: step 36140, loss = 5.75 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 12:49:36.881456: step 36150, loss = 5.89 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 12:49:43.338800: step 36160, loss = 5.77 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 12:49:48.568438: step 36170, loss = 5.69 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 12:49:54.022136: step 36180, loss = 5.81 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 12:49:59.772450: step 36190, loss = 5.99 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 12:50:04.626584: step 36200, loss = 6.02 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 12:50:10.373380: step 36210, loss = 5.54 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 12:50:16.707437: step 36220, loss = 5.74 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 12:50:22.139650: step 36230, loss = 5.60 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 12:50:26.846883: step 36240, loss = 5.40 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 12:50:31.728412: step 36250, loss = 5.67 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 12:50:36.438267: step 36260, loss = 5.57 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 12:50:42.065482: step 36270, loss = 5.59 (192.5 examples/sec; 0.665 sec/batch)
2016-07-15 12:50:48.235011: step 36280, loss = 5.80 (245.5 examples/sec; 0.521 sec/batch)
2016-07-15 12:50:53.088564: step 36290, loss = 5.96 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 12:50:57.858691: step 36300, loss = 5.97 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 12:51:05.300565: step 36310, loss = 5.93 (209.2 examples/sec; 0.612 sec/batch)
2016-07-15 12:51:10.676925: step 36320, loss = 5.83 (251.8 examples/sec; 0.508 sec/batch)
2016-07-15 12:51:15.374653: step 36330, loss = 5.73 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 12:51:20.215251: step 36340, loss = 5.81 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 12:51:24.999391: step 36350, loss = 5.49 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 12:51:30.865260: step 36360, loss = 5.89 (185.9 examples/sec; 0.688 sec/batch)
2016-07-15 12:51:36.789537: step 36370, loss = 5.86 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 12:51:41.601095: step 36380, loss = 5.76 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 12:51:46.380840: step 36390, loss = 5.80 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 12:51:51.166548: step 36400, loss = 5.62 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 12:51:57.693880: step 36410, loss = 5.56 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 12:52:03.945935: step 36420, loss = 5.66 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 12:52:08.751181: step 36430, loss = 5.98 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 12:52:13.523220: step 36440, loss = 5.58 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 12:52:19.568409: step 36450, loss = 5.86 (194.3 examples/sec; 0.659 sec/batch)
2016-07-15 12:52:25.285995: step 36460, loss = 5.89 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 12:52:30.065890: step 36470, loss = 5.69 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 12:52:34.906428: step 36480, loss = 5.77 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 12:52:39.598888: step 36490, loss = 5.43 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 12:52:44.468403: step 36500, loss = 5.76 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 12:52:50.173983: step 36510, loss = 5.73 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 12:52:56.385727: step 36520, loss = 5.83 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 12:53:02.019508: step 36530, loss = 5.71 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 12:53:07.849457: step 36540, loss = 5.75 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 12:53:12.723200: step 36550, loss = 5.73 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 12:53:17.515582: step 36560, loss = 5.66 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 12:53:22.298118: step 36570, loss = 5.75 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 12:53:27.165080: step 36580, loss = 6.07 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 12:53:33.677750: step 36590, loss = 5.80 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 12:53:39.016866: step 36600, loss = 5.38 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 12:53:45.937812: step 36610, loss = 5.65 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 12:53:50.690996: step 36620, loss = 5.60 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 12:53:55.579369: step 36630, loss = 5.63 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 12:54:00.315627: step 36640, loss = 5.88 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 12:54:05.935011: step 36650, loss = 5.44 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 12:54:12.147734: step 36660, loss = 5.84 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 12:54:17.026172: step 36670, loss = 5.77 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 12:54:21.841163: step 36680, loss = 5.58 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 12:54:26.572822: step 36690, loss = 5.83 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 12:54:31.460162: step 36700, loss = 5.68 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 12:54:37.158385: step 36710, loss = 5.47 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 12:54:41.950964: step 36720, loss = 5.94 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 12:54:46.812690: step 36730, loss = 5.87 (245.2 examples/sec; 0.522 sec/batch)
2016-07-15 12:54:53.250928: step 36740, loss = 5.66 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 12:54:58.674709: step 36750, loss = 5.65 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 12:55:04.466643: step 36760, loss = 5.69 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 12:55:09.271438: step 36770, loss = 5.71 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 12:55:14.040121: step 36780, loss = 5.84 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 12:55:18.740889: step 36790, loss = 5.64 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 12:55:23.595369: step 36800, loss = 5.79 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 12:55:29.277864: step 36810, loss = 5.84 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 12:55:34.118637: step 36820, loss = 5.55 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 12:55:38.738668: step 36830, loss = 5.53 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 12:55:43.468294: step 36840, loss = 5.58 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 12:55:49.251422: step 36850, loss = 5.80 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 12:55:54.891089: step 36860, loss = 5.67 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 12:55:59.982357: step 36870, loss = 5.79 (218.3 examples/sec; 0.586 sec/batch)
2016-07-15 12:56:05.727295: step 36880, loss = 6.05 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 12:56:11.464068: step 36890, loss = 5.73 (201.1 examples/sec; 0.637 sec/batch)
2016-07-15 12:56:16.579908: step 36900, loss = 5.91 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 12:56:23.371509: step 36910, loss = 5.74 (252.3 examples/sec; 0.507 sec/batch)
2016-07-15 12:56:29.197420: step 36920, loss = 5.76 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 12:56:34.018171: step 36930, loss = 5.78 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 12:56:38.850207: step 36940, loss = 5.58 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 12:56:45.176186: step 36950, loss = 5.52 (209.3 examples/sec; 0.612 sec/batch)
2016-07-15 12:56:50.651927: step 36960, loss = 5.46 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 12:56:55.377193: step 36970, loss = 5.54 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 12:57:00.596304: step 36980, loss = 5.62 (186.2 examples/sec; 0.687 sec/batch)
2016-07-15 12:57:07.108086: step 36990, loss = 5.81 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 12:57:12.329550: step 37000, loss = 5.77 (209.1 examples/sec; 0.612 sec/batch)
2016-07-15 12:57:19.200554: step 37010, loss = 5.60 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 12:57:24.958410: step 37020, loss = 5.75 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 12:57:30.445091: step 37030, loss = 5.48 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 12:57:35.631789: step 37040, loss = 5.98 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 12:57:40.377198: step 37050, loss = 5.68 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 12:57:45.247117: step 37060, loss = 5.56 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 12:57:49.956656: step 37070, loss = 5.91 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 12:57:54.609710: step 37080, loss = 5.59 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 12:57:59.326650: step 37090, loss = 5.82 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 12:58:04.072231: step 37100, loss = 5.77 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 12:58:09.626435: step 37110, loss = 5.62 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 12:58:15.182600: step 37120, loss = 5.62 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 12:58:20.400154: step 37130, loss = 5.85 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 12:58:26.038799: step 37140, loss = 5.69 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 12:58:31.798278: step 37150, loss = 5.74 (209.1 examples/sec; 0.612 sec/batch)
2016-07-15 12:58:36.648374: step 37160, loss = 5.57 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 12:58:41.436448: step 37170, loss = 5.97 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 12:58:47.370699: step 37180, loss = 5.61 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 12:58:53.222092: step 37190, loss = 5.48 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 12:58:58.076445: step 37200, loss = 5.77 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 12:59:04.016376: step 37210, loss = 5.75 (233.0 examples/sec; 0.549 sec/batch)
2016-07-15 12:59:10.609702: step 37220, loss = 5.60 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 12:59:15.756453: step 37230, loss = 5.66 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 12:59:20.414301: step 37240, loss = 5.62 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 12:59:25.876648: step 37250, loss = 5.92 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 12:59:32.190331: step 37260, loss = 5.80 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 12:59:37.102291: step 37270, loss = 5.86 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 12:59:41.882240: step 37280, loss = 5.63 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 12:59:47.914259: step 37290, loss = 5.66 (192.8 examples/sec; 0.664 sec/batch)
2016-07-15 12:59:53.689414: step 37300, loss = 5.90 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 12:59:59.489494: step 37310, loss = 5.86 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 13:00:04.434261: step 37320, loss = 5.80 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 13:00:09.185982: step 37330, loss = 5.55 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 13:00:13.971459: step 37340, loss = 5.74 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 13:00:18.830436: step 37350, loss = 5.88 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 13:00:23.524260: step 37360, loss = 5.83 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 13:00:28.675647: step 37370, loss = 5.57 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 13:00:35.207820: step 37380, loss = 5.58 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 13:00:40.951313: step 37390, loss = 5.96 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 13:00:46.436222: step 37400, loss = 5.82 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 13:00:52.172729: step 37410, loss = 5.76 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 13:00:56.841118: step 37420, loss = 5.55 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 13:01:02.141733: step 37430, loss = 5.43 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 13:01:07.381757: step 37440, loss = 5.68 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 13:01:12.054173: step 37450, loss = 5.80 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 13:01:17.385268: step 37460, loss = 5.85 (188.4 examples/sec; 0.679 sec/batch)
2016-07-15 13:01:23.831752: step 37470, loss = 5.76 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 13:01:28.685970: step 37480, loss = 5.69 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 13:01:33.428516: step 37490, loss = 5.54 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 13:01:38.193077: step 37500, loss = 5.87 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 13:01:43.798523: step 37510, loss = 5.45 (280.4 examples/sec; 0.457 sec/batch)
2016-07-15 13:01:48.690007: step 37520, loss = 5.95 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 13:01:54.228052: step 37530, loss = 5.73 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 13:02:00.046825: step 37540, loss = 5.65 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 13:02:04.887035: step 37550, loss = 5.78 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 13:02:09.715104: step 37560, loss = 5.51 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 13:02:15.656849: step 37570, loss = 5.73 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 13:02:21.477736: step 37580, loss = 5.54 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 13:02:26.311179: step 37590, loss = 5.85 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 13:02:31.129386: step 37600, loss = 5.87 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 13:02:36.796503: step 37610, loss = 5.81 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 13:02:41.496411: step 37620, loss = 5.80 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 13:02:46.929858: step 37630, loss = 5.74 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 13:02:52.124175: step 37640, loss = 5.72 (244.2 examples/sec; 0.524 sec/batch)
2016-07-15 13:02:57.844233: step 37650, loss = 5.42 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 13:03:03.490746: step 37660, loss = 5.78 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 13:03:08.484150: step 37670, loss = 5.64 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 13:03:13.239406: step 37680, loss = 5.59 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 13:03:18.994792: step 37690, loss = 5.68 (183.2 examples/sec; 0.699 sec/batch)
2016-07-15 13:03:25.048651: step 37700, loss = 5.61 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 13:03:31.884467: step 37710, loss = 5.68 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 13:03:37.023192: step 37720, loss = 5.57 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 13:03:42.578358: step 37730, loss = 5.56 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 13:03:47.294425: step 37740, loss = 6.01 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 13:03:52.162527: step 37750, loss = 5.69 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 13:03:56.866739: step 37760, loss = 5.23 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 13:04:01.499955: step 37770, loss = 6.03 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 13:04:06.889355: step 37780, loss = 5.43 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 13:04:12.051514: step 37790, loss = 5.78 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 13:04:16.758095: step 37800, loss = 5.85 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 13:04:23.622318: step 37810, loss = 5.56 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 13:04:29.516848: step 37820, loss = 5.81 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 13:04:34.313440: step 37830, loss = 5.67 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 13:04:39.111858: step 37840, loss = 5.68 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 13:04:43.825571: step 37850, loss = 5.62 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 13:04:48.703188: step 37860, loss = 5.55 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 13:04:53.464503: step 37870, loss = 5.69 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 13:04:59.241863: step 37880, loss = 5.88 (185.4 examples/sec; 0.690 sec/batch)
2016-07-15 13:05:05.325529: step 37890, loss = 5.80 (253.9 examples/sec; 0.504 sec/batch)
2016-07-15 13:05:10.856281: step 37900, loss = 5.77 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 13:05:17.537112: step 37910, loss = 5.45 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 13:05:23.000923: step 37920, loss = 5.52 (253.5 examples/sec; 0.505 sec/batch)
2016-07-15 13:05:27.697457: step 37930, loss = 5.65 (283.4 examples/sec; 0.452 sec/batch)
2016-07-15 13:05:32.617122: step 37940, loss = 5.73 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 13:05:37.381193: step 37950, loss = 5.70 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 13:05:43.441169: step 37960, loss = 5.70 (178.8 examples/sec; 0.716 sec/batch)
2016-07-15 13:05:50.051201: step 37970, loss = 5.75 (239.0 examples/sec; 0.536 sec/batch)
2016-07-15 13:05:55.721113: step 37980, loss = 5.42 (201.1 examples/sec; 0.637 sec/batch)
2016-07-15 13:06:00.859201: step 37990, loss = 5.65 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 13:06:06.502219: step 38000, loss = 5.77 (250.8 examples/sec; 0.510 sec/batch)
2016-07-15 13:06:12.293137: step 38010, loss = 5.71 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 13:06:17.594430: step 38020, loss = 5.73 (189.2 examples/sec; 0.676 sec/batch)
2016-07-15 13:06:24.084534: step 38030, loss = 5.68 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 13:06:28.956731: step 38040, loss = 5.94 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 13:06:33.660030: step 38050, loss = 5.67 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 13:06:38.458157: step 38060, loss = 5.80 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 13:06:43.281380: step 38070, loss = 5.65 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 13:06:49.613212: step 38080, loss = 5.88 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 13:06:55.070815: step 38090, loss = 5.87 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 13:06:59.812553: step 38100, loss = 5.62 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 13:07:06.361423: step 38110, loss = 5.39 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 13:07:12.657943: step 38120, loss = 5.43 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 13:07:17.935415: step 38130, loss = 5.48 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 13:07:23.353847: step 38140, loss = 5.65 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 13:07:29.076562: step 38150, loss = 5.78 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 13:07:33.896548: step 38160, loss = 5.80 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 13:07:38.769577: step 38170, loss = 5.69 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 13:07:45.453292: step 38180, loss = 5.67 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 13:07:50.978469: step 38190, loss = 5.92 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 13:07:55.749095: step 38200, loss = 5.81 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 13:08:01.615807: step 38210, loss = 5.67 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 13:08:06.395257: step 38220, loss = 5.57 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 13:08:12.459951: step 38230, loss = 5.81 (194.1 examples/sec; 0.660 sec/batch)
2016-07-15 13:08:18.186375: step 38240, loss = 5.67 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 13:08:22.997407: step 38250, loss = 5.72 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 13:08:27.820101: step 38260, loss = 5.63 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 13:08:32.588430: step 38270, loss = 5.71 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 13:08:38.042318: step 38280, loss = 5.66 (169.4 examples/sec; 0.755 sec/batch)
2016-07-15 13:08:44.485093: step 38290, loss = 5.94 (219.4 examples/sec; 0.583 sec/batch)
2016-07-15 13:08:49.774471: step 38300, loss = 5.45 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 13:08:56.527250: step 38310, loss = 5.63 (243.0 examples/sec; 0.527 sec/batch)
2016-07-15 13:09:02.320021: step 38320, loss = 5.64 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 13:09:07.106793: step 38330, loss = 5.85 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 13:09:11.910746: step 38340, loss = 5.63 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 13:09:18.399941: step 38350, loss = 5.42 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 13:09:23.716339: step 38360, loss = 5.65 (255.2 examples/sec; 0.501 sec/batch)
2016-07-15 13:09:28.487594: step 38370, loss = 5.75 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 13:09:33.730893: step 38380, loss = 5.44 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 13:09:40.201553: step 38390, loss = 5.63 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 13:09:45.117832: step 38400, loss = 5.47 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 13:09:50.825945: step 38410, loss = 5.78 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 13:09:55.637162: step 38420, loss = 5.72 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 13:10:00.551055: step 38430, loss = 5.86 (234.3 examples/sec; 0.546 sec/batch)
2016-07-15 13:10:07.174875: step 38440, loss = 5.66 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 13:10:12.327306: step 38450, loss = 5.57 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 13:10:17.056541: step 38460, loss = 5.76 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 13:10:22.595908: step 38470, loss = 5.66 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 13:10:28.882626: step 38480, loss = 5.83 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 13:10:33.736102: step 38490, loss = 5.68 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 13:10:38.553135: step 38500, loss = 5.71 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 13:10:44.312844: step 38510, loss = 5.73 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 13:10:48.939996: step 38520, loss = 5.82 (268.6 examples/sec; 0.476 sec/batch)
2016-07-15 13:10:54.076942: step 38530, loss = 5.66 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 13:10:59.514070: step 38540, loss = 5.68 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 13:11:05.269057: step 38550, loss = 5.82 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 13:11:10.676271: step 38560, loss = 5.64 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 13:11:16.069646: step 38570, loss = 5.77 (238.2 examples/sec; 0.537 sec/batch)
2016-07-15 13:11:20.821578: step 38580, loss = 5.87 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 13:11:26.246796: step 38590, loss = 5.75 (184.9 examples/sec; 0.692 sec/batch)
2016-07-15 13:11:32.617863: step 38600, loss = 5.69 (225.3 examples/sec; 0.568 sec/batch)
2016-07-15 13:11:38.520216: step 38610, loss = 5.68 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 13:11:43.173256: step 38620, loss = 5.66 (283.1 examples/sec; 0.452 sec/batch)
2016-07-15 13:11:47.880036: step 38630, loss = 5.55 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 13:11:53.701214: step 38640, loss = 5.63 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 13:11:58.463039: step 38650, loss = 5.45 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 13:12:03.250856: step 38660, loss = 5.78 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 13:12:09.440814: step 38670, loss = 5.72 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 13:12:15.098753: step 38680, loss = 6.06 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 13:12:20.830396: step 38690, loss = 5.79 (243.4 examples/sec; 0.526 sec/batch)
2016-07-15 13:12:26.066633: step 38700, loss = 5.50 (209.1 examples/sec; 0.612 sec/batch)
2016-07-15 13:12:32.741614: step 38710, loss = 5.53 (238.7 examples/sec; 0.536 sec/batch)
2016-07-15 13:12:38.455584: step 38720, loss = 5.44 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 13:12:43.209004: step 38730, loss = 5.61 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 13:12:48.054048: step 38740, loss = 5.63 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 13:12:52.767280: step 38750, loss = 5.84 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 13:12:58.167679: step 38760, loss = 5.76 (183.2 examples/sec; 0.699 sec/batch)
2016-07-15 13:13:04.630546: step 38770, loss = 5.86 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 13:13:09.448863: step 38780, loss = 5.31 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 13:13:14.188709: step 38790, loss = 5.57 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 13:13:20.024180: step 38800, loss = 5.70 (190.9 examples/sec; 0.670 sec/batch)
2016-07-15 13:13:27.268594: step 38810, loss = 5.33 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 13:13:31.987566: step 38820, loss = 5.73 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 13:13:36.978982: step 38830, loss = 5.73 (209.5 examples/sec; 0.611 sec/batch)
2016-07-15 13:13:43.512804: step 38840, loss = 5.53 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 13:13:48.606315: step 38850, loss = 5.70 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 13:13:53.351839: step 38860, loss = 5.60 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 13:13:59.019494: step 38870, loss = 5.52 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 13:14:05.236014: step 38880, loss = 5.48 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 13:14:10.569079: step 38890, loss = 5.51 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 13:14:15.941092: step 38900, loss = 5.75 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 13:14:21.755555: step 38910, loss = 5.73 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 13:14:27.393823: step 38920, loss = 5.21 (187.8 examples/sec; 0.681 sec/batch)
2016-07-15 13:14:33.513818: step 38930, loss = 5.85 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 13:14:38.338402: step 38940, loss = 5.54 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 13:14:43.134967: step 38950, loss = 5.65 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 13:14:47.894956: step 38960, loss = 5.76 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 13:14:52.702275: step 38970, loss = 5.50 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 13:14:59.299515: step 38980, loss = 5.47 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 13:15:04.502071: step 38990, loss = 5.45 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 13:15:09.292621: step 39000, loss = 5.70 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 13:15:16.247310: step 39010, loss = 5.69 (186.4 examples/sec; 0.687 sec/batch)
2016-07-15 13:15:22.169017: step 39020, loss = 5.51 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 13:15:26.943892: step 39030, loss = 5.61 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 13:15:31.749991: step 39040, loss = 5.83 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 13:15:38.075518: step 39050, loss = 5.80 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 13:15:43.550383: step 39060, loss = 5.61 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 13:15:48.282121: step 39070, loss = 5.48 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 13:15:53.202602: step 39080, loss = 5.49 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 13:15:57.983627: step 39090, loss = 5.79 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 13:16:03.758522: step 39100, loss = 5.65 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 13:16:11.064268: step 39110, loss = 5.77 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 13:16:16.862554: step 39120, loss = 5.57 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 13:16:22.058195: step 39130, loss = 5.71 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 13:16:27.541760: step 39140, loss = 5.38 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 13:16:32.260476: step 39150, loss = 5.58 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 13:16:36.971413: step 39160, loss = 5.62 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 13:16:42.004677: step 39170, loss = 5.60 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 13:16:47.509775: step 39180, loss = 5.62 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 13:16:53.344222: step 39190, loss = 5.65 (257.8 examples/sec; 0.497 sec/batch)
2016-07-15 13:16:58.674226: step 39200, loss = 5.45 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 13:17:05.444095: step 39210, loss = 5.60 (201.5 examples/sec; 0.635 sec/batch)
2016-07-15 13:17:11.046109: step 39220, loss = 5.88 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 13:17:16.897724: step 39230, loss = 5.73 (209.8 examples/sec; 0.610 sec/batch)
2016-07-15 13:17:22.126890: step 39240, loss = 5.98 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 13:17:27.597526: step 39250, loss = 5.81 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 13:17:33.421849: step 39260, loss = 5.39 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 13:17:38.736867: step 39270, loss = 5.38 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 13:17:44.087619: step 39280, loss = 5.39 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 13:17:49.919780: step 39290, loss = 5.43 (246.4 examples/sec; 0.520 sec/batch)
2016-07-15 13:17:54.815038: step 39300, loss = 5.48 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 13:18:00.427199: step 39310, loss = 5.71 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 13:18:05.210531: step 39320, loss = 5.63 (222.7 examples/sec; 0.575 sec/batch)
2016-07-15 13:18:10.897976: step 39330, loss = 5.58 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 13:18:16.595000: step 39340, loss = 5.46 (209.8 examples/sec; 0.610 sec/batch)
2016-07-15 13:18:21.712333: step 39350, loss = 5.71 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 13:18:27.344044: step 39360, loss = 5.71 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 13:18:32.070897: step 39370, loss = 5.73 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 13:18:36.991047: step 39380, loss = 5.43 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 13:18:41.689567: step 39390, loss = 5.73 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 13:18:47.334064: step 39400, loss = 5.44 (186.0 examples/sec; 0.688 sec/batch)
2016-07-15 13:18:53.670904: step 39410, loss = 5.73 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 13:18:58.980037: step 39420, loss = 5.62 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 13:19:04.249525: step 39430, loss = 5.61 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 13:19:08.944806: step 39440, loss = 5.63 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 13:19:13.800273: step 39450, loss = 5.66 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 13:19:18.584462: step 39460, loss = 5.46 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 13:19:24.501230: step 39470, loss = 5.50 (189.8 examples/sec; 0.674 sec/batch)
2016-07-15 13:19:30.389061: step 39480, loss = 5.74 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 13:19:35.201841: step 39490, loss = 5.60 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 13:19:40.025350: step 39500, loss = 5.74 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 13:19:45.789138: step 39510, loss = 5.37 (281.8 examples/sec; 0.454 sec/batch)
2016-07-15 13:19:50.448625: step 39520, loss = 5.72 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 13:19:55.857643: step 39530, loss = 5.58 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 13:20:01.017715: step 39540, loss = 5.40 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 13:20:06.788456: step 39550, loss = 5.52 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 13:20:11.544579: step 39560, loss = 5.53 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 13:20:16.342896: step 39570, loss = 5.75 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 13:20:21.053863: step 39580, loss = 5.83 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 13:20:26.407334: step 39590, loss = 5.37 (187.8 examples/sec; 0.682 sec/batch)
2016-07-15 13:20:32.848717: step 39600, loss = 5.87 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 13:20:38.701429: step 39610, loss = 5.49 (282.5 examples/sec; 0.453 sec/batch)
2016-07-15 13:20:43.521302: step 39620, loss = 5.59 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 13:20:48.249351: step 39630, loss = 5.56 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 13:20:53.155570: step 39640, loss = 5.78 (236.5 examples/sec; 0.541 sec/batch)
2016-07-15 13:20:59.726871: step 39650, loss = 5.70 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 13:21:04.904679: step 39660, loss = 5.77 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 13:21:09.629238: step 39670, loss = 5.69 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 13:21:15.103350: step 39680, loss = 5.68 (187.6 examples/sec; 0.682 sec/batch)
2016-07-15 13:21:21.415781: step 39690, loss = 5.84 (249.2 examples/sec; 0.514 sec/batch)
2016-07-15 13:21:26.680307: step 39700, loss = 5.67 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 13:21:33.392974: step 39710, loss = 5.69 (221.1 examples/sec; 0.579 sec/batch)
2016-07-15 13:21:39.172229: step 39720, loss = 5.54 (250.0 examples/sec; 0.512 sec/batch)
2016-07-15 13:21:44.919201: step 39730, loss = 5.71 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 13:21:49.850987: step 39740, loss = 5.72 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 13:21:54.550207: step 39750, loss = 5.52 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 13:21:59.354947: step 39760, loss = 5.67 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 13:22:04.172121: step 39770, loss = 5.73 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 13:22:10.569287: step 39780, loss = 5.80 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 13:22:16.018589: step 39790, loss = 5.37 (244.8 examples/sec; 0.523 sec/batch)
2016-07-15 13:22:20.737441: step 39800, loss = 5.49 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 13:22:27.335255: step 39810, loss = 5.44 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 13:22:33.573828: step 39820, loss = 5.67 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 13:22:38.430823: step 39830, loss = 5.59 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 13:22:43.225764: step 39840, loss = 5.68 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 13:22:49.295728: step 39850, loss = 5.80 (191.2 examples/sec; 0.670 sec/batch)
2016-07-15 13:22:55.004681: step 39860, loss = 5.38 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 13:22:59.805959: step 39870, loss = 5.51 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 13:23:04.621574: step 39880, loss = 5.61 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 13:23:09.337749: step 39890, loss = 5.30 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 13:23:14.709560: step 39900, loss = 5.31 (186.2 examples/sec; 0.688 sec/batch)
2016-07-15 13:23:22.423678: step 39910, loss = 5.72 (254.8 examples/sec; 0.502 sec/batch)
2016-07-15 13:23:27.931063: step 39920, loss = 5.70 (199.0 examples/sec; 0.643 sec/batch)
2016-07-15 13:23:33.104098: step 39930, loss = 5.63 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 13:23:37.818354: step 39940, loss = 5.29 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 13:23:43.410247: step 39950, loss = 5.39 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 13:23:49.650698: step 39960, loss = 5.54 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 13:23:54.524818: step 39970, loss = 5.51 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 13:23:59.327526: step 39980, loss = 5.60 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 13:24:05.476393: step 39990, loss = 5.48 (195.3 examples/sec; 0.655 sec/batch)
2016-07-15 13:24:11.198872: step 40000, loss = 5.45 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 13:24:18.003748: step 40010, loss = 5.50 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 13:24:23.366878: step 40020, loss = 5.56 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 13:24:28.692785: step 40030, loss = 5.48 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 13:24:33.417715: step 40040, loss = 5.46 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 13:24:38.254875: step 40050, loss = 5.49 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 13:24:43.069265: step 40060, loss = 5.71 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 13:24:47.845091: step 40070, loss = 5.58 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 13:24:52.669322: step 40080, loss = 5.41 (257.3 examples/sec; 0.498 sec/batch)
2016-07-15 13:24:57.407764: step 40090, loss = 5.58 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 13:25:02.315318: step 40100, loss = 5.55 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 13:25:08.131468: step 40110, loss = 5.56 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 13:25:12.862552: step 40120, loss = 5.79 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 13:25:17.743895: step 40130, loss = 5.36 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 13:25:22.553353: step 40140, loss = 5.45 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 13:25:28.329377: step 40150, loss = 5.36 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 13:25:34.362772: step 40160, loss = 5.56 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 13:25:39.204853: step 40170, loss = 5.39 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 13:25:43.984899: step 40180, loss = 5.59 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 13:25:50.205372: step 40190, loss = 5.58 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 13:25:55.776788: step 40200, loss = 5.83 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 13:26:02.619010: step 40210, loss = 5.87 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 13:26:07.409362: step 40220, loss = 5.55 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 13:26:12.228156: step 40230, loss = 5.39 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 13:26:16.939843: step 40240, loss = 5.42 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 13:26:22.016687: step 40250, loss = 5.75 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 13:26:28.510723: step 40260, loss = 5.60 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 13:26:33.698607: step 40270, loss = 5.51 (213.1 examples/sec; 0.601 sec/batch)
2016-07-15 13:26:39.416107: step 40280, loss = 5.34 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 13:26:44.259245: step 40290, loss = 5.64 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 13:26:49.071825: step 40300, loss = 5.63 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 13:26:57.212380: step 40310, loss = 5.58 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 13:27:02.107684: step 40320, loss = 5.53 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 13:27:06.849092: step 40330, loss = 5.56 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 13:27:11.634365: step 40340, loss = 5.48 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 13:27:16.445700: step 40350, loss = 5.57 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 13:27:22.790877: step 40360, loss = 5.32 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 13:27:28.234487: step 40370, loss = 5.44 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 13:27:33.017626: step 40380, loss = 5.48 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 13:27:37.846581: step 40390, loss = 5.28 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 13:27:42.536636: step 40400, loss = 5.35 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 13:27:49.653582: step 40410, loss = 5.79 (194.0 examples/sec; 0.660 sec/batch)
2016-07-15 13:27:55.389648: step 40420, loss = 5.34 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 13:28:00.200090: step 40430, loss = 5.31 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 13:28:05.033869: step 40440, loss = 5.25 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 13:28:09.733833: step 40450, loss = 5.46 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 13:28:15.131954: step 40460, loss = 5.46 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 13:28:21.516489: step 40470, loss = 5.39 (214.5 examples/sec; 0.597 sec/batch)
2016-07-15 13:28:26.736736: step 40480, loss = 5.48 (203.7 examples/sec; 0.629 sec/batch)
2016-07-15 13:28:32.281605: step 40490, loss = 5.36 (239.6 examples/sec; 0.534 sec/batch)
2016-07-15 13:28:37.084676: step 40500, loss = 5.57 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 13:28:42.964236: step 40510, loss = 5.67 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 13:28:47.750312: step 40520, loss = 5.73 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 13:28:52.539711: step 40530, loss = 5.59 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 13:28:57.346907: step 40540, loss = 5.45 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 13:29:02.071515: step 40550, loss = 5.67 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 13:29:07.645335: step 40560, loss = 5.42 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 13:29:13.961454: step 40570, loss = 5.29 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 13:29:18.803402: step 40580, loss = 5.44 (281.1 examples/sec; 0.455 sec/batch)
2016-07-15 13:29:23.610311: step 40590, loss = 5.59 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 13:29:29.734443: step 40600, loss = 5.52 (188.6 examples/sec; 0.679 sec/batch)
2016-07-15 13:29:36.677399: step 40610, loss = 5.57 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 13:29:42.454615: step 40620, loss = 5.35 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 13:29:47.268958: step 40630, loss = 5.32 (270.9 examples/sec; 0.473 sec/batch)
2016-07-15 13:29:52.066271: step 40640, loss = 5.58 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 13:29:58.286039: step 40650, loss = 5.71 (209.8 examples/sec; 0.610 sec/batch)
2016-07-15 13:30:03.868948: step 40660, loss = 5.66 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 13:30:08.626962: step 40670, loss = 5.39 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 13:30:13.565365: step 40680, loss = 5.26 (252.7 examples/sec; 0.506 sec/batch)
2016-07-15 13:30:20.140613: step 40690, loss = 5.07 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 13:30:25.326411: step 40700, loss = 5.48 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 13:30:32.356430: step 40710, loss = 5.55 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 13:30:37.040187: step 40720, loss = 5.33 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 13:30:41.898254: step 40730, loss = 5.82 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 13:30:46.604947: step 40740, loss = 5.78 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 13:30:52.257931: step 40750, loss = 5.43 (185.0 examples/sec; 0.692 sec/batch)
2016-07-15 13:30:58.350452: step 40760, loss = 5.70 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 13:31:03.740797: step 40770, loss = 5.45 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 13:31:09.031092: step 40780, loss = 5.69 (268.6 examples/sec; 0.476 sec/batch)
2016-07-15 13:31:13.739997: step 40790, loss = 5.54 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 13:31:19.181173: step 40800, loss = 5.45 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 13:31:26.882141: step 40810, loss = 5.67 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 13:31:32.439319: step 40820, loss = 5.72 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 13:31:37.638018: step 40830, loss = 5.61 (250.5 examples/sec; 0.511 sec/batch)
2016-07-15 13:31:43.368828: step 40840, loss = 5.70 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 13:31:49.025782: step 40850, loss = 5.79 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 13:31:54.063361: step 40860, loss = 5.67 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 13:31:58.809359: step 40870, loss = 5.82 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 13:32:03.578400: step 40880, loss = 5.86 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 13:32:08.354420: step 40890, loss = 5.58 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 13:32:14.609883: step 40900, loss = 5.47 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 13:32:21.361186: step 40910, loss = 5.64 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 13:32:27.155495: step 40920, loss = 5.56 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 13:32:31.952747: step 40930, loss = 5.74 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 13:32:36.854907: step 40940, loss = 5.65 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 13:32:43.285056: step 40950, loss = 5.37 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 13:32:48.633532: step 40960, loss = 5.41 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 13:32:53.392933: step 40970, loss = 5.41 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 13:32:58.628826: step 40980, loss = 5.73 (183.9 examples/sec; 0.696 sec/batch)
2016-07-15 13:33:05.103797: step 40990, loss = 5.71 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 13:33:10.119462: step 41000, loss = 5.74 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 13:33:15.918484: step 41010, loss = 5.69 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 13:33:22.016162: step 41020, loss = 5.38 (196.6 examples/sec; 0.651 sec/batch)
2016-07-15 13:33:27.710309: step 41030, loss = 5.47 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 13:33:32.478616: step 41040, loss = 5.52 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 13:33:37.375211: step 41050, loss = 5.21 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 13:33:43.878430: step 41060, loss = 5.24 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 13:33:49.142151: step 41070, loss = 5.47 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 13:33:53.875996: step 41080, loss = 5.67 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 13:33:59.184207: step 41090, loss = 5.60 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 13:34:05.641534: step 41100, loss = 5.41 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 13:34:11.530723: step 41110, loss = 5.43 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 13:34:16.320868: step 41120, loss = 5.65 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 13:34:22.474920: step 41130, loss = 5.55 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 13:34:28.054029: step 41140, loss = 5.59 (257.8 examples/sec; 0.497 sec/batch)
2016-07-15 13:34:32.808108: step 41150, loss = 5.52 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 13:34:37.652855: step 41160, loss = 5.56 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 13:34:42.375536: step 41170, loss = 5.65 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 13:34:47.981201: step 41180, loss = 5.27 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 13:34:54.273748: step 41190, loss = 5.72 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 13:34:59.157125: step 41200, loss = 5.57 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 13:35:04.902839: step 41210, loss = 5.47 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 13:35:09.698493: step 41220, loss = 5.81 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 13:35:14.325315: step 41230, loss = 5.55 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 13:35:19.508396: step 41240, loss = 5.25 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 13:35:24.928999: step 41250, loss = 5.57 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 13:35:29.602430: step 41260, loss = 5.43 (283.5 examples/sec; 0.452 sec/batch)
2016-07-15 13:35:34.844709: step 41270, loss = 5.42 (189.8 examples/sec; 0.674 sec/batch)
2016-07-15 13:35:41.406398: step 41280, loss = 5.71 (198.2 examples/sec; 0.646 sec/batch)
2016-07-15 13:35:46.587953: step 41290, loss = 5.38 (208.9 examples/sec; 0.613 sec/batch)
2016-07-15 13:35:52.276902: step 41300, loss = 5.64 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 13:35:58.098841: step 41310, loss = 5.49 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 13:36:03.483744: step 41320, loss = 5.51 (187.0 examples/sec; 0.684 sec/batch)
2016-07-15 13:36:09.882928: step 41330, loss = 5.43 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 13:36:14.740136: step 41340, loss = 5.47 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 13:36:19.577489: step 41350, loss = 5.58 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 13:36:25.558363: step 41360, loss = 5.65 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 13:36:31.441046: step 41370, loss = 5.60 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 13:36:37.087638: step 41380, loss = 5.61 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 13:36:42.159026: step 41390, loss = 5.61 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 13:36:46.890404: step 41400, loss = 5.25 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 13:36:54.037007: step 41410, loss = 5.72 (193.8 examples/sec; 0.660 sec/batch)
2016-07-15 13:36:59.732543: step 41420, loss = 5.54 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 13:37:04.495034: step 41430, loss = 5.45 (282.5 examples/sec; 0.453 sec/batch)
2016-07-15 13:37:09.330769: step 41440, loss = 5.43 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 13:37:14.055517: step 41450, loss = 5.61 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 13:37:19.477961: step 41460, loss = 5.46 (179.6 examples/sec; 0.713 sec/batch)
2016-07-15 13:37:25.926415: step 41470, loss = 5.45 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 13:37:30.768741: step 41480, loss = 5.52 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 13:37:35.489689: step 41490, loss = 5.30 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 13:37:41.465365: step 41500, loss = 5.60 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 13:37:48.569738: step 41510, loss = 5.60 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 13:37:53.291878: step 41520, loss = 5.43 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 13:37:58.271448: step 41530, loss = 5.40 (214.7 examples/sec; 0.596 sec/batch)
2016-07-15 13:38:04.844158: step 41540, loss = 5.40 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 13:38:10.021813: step 41550, loss = 5.26 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 13:38:14.752193: step 41560, loss = 5.42 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 13:38:20.382455: step 41570, loss = 5.78 (187.8 examples/sec; 0.682 sec/batch)
2016-07-15 13:38:26.590701: step 41580, loss = 5.49 (245.5 examples/sec; 0.521 sec/batch)
2016-07-15 13:38:31.927184: step 41590, loss = 5.53 (200.5 examples/sec; 0.638 sec/batch)
2016-07-15 13:38:37.269019: step 41600, loss = 5.33 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 13:38:43.002785: step 41610, loss = 5.35 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 13:38:48.701993: step 41620, loss = 5.42 (187.0 examples/sec; 0.684 sec/batch)
2016-07-15 13:38:54.834406: step 41630, loss = 5.45 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 13:38:59.648093: step 41640, loss = 5.53 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 13:39:04.440231: step 41650, loss = 5.37 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 13:39:09.201341: step 41660, loss = 5.59 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 13:39:14.160544: step 41670, loss = 5.57 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 13:39:18.957362: step 41680, loss = 5.65 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 13:39:23.763363: step 41690, loss = 5.68 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 13:39:28.556241: step 41700, loss = 5.50 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 13:39:36.075357: step 41710, loss = 5.75 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 13:39:41.416693: step 41720, loss = 5.32 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 13:39:46.208154: step 41730, loss = 5.41 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 13:39:51.533007: step 41740, loss = 5.55 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 13:39:58.040057: step 41750, loss = 5.50 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 13:40:02.959723: step 41760, loss = 5.67 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 13:40:07.725101: step 41770, loss = 5.79 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 13:40:13.666271: step 41780, loss = 5.32 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 13:40:19.575342: step 41790, loss = 5.36 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 13:40:24.329437: step 41800, loss = 5.78 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 13:40:30.187646: step 41810, loss = 5.75 (242.8 examples/sec; 0.527 sec/batch)
2016-07-15 13:40:36.782240: step 41820, loss = 5.62 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 13:40:41.958314: step 41830, loss = 5.33 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 13:40:46.689192: step 41840, loss = 5.36 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 13:40:52.111885: step 41850, loss = 5.09 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 13:40:58.426292: step 41860, loss = 5.31 (251.0 examples/sec; 0.510 sec/batch)
2016-07-15 13:41:03.704097: step 41870, loss = 5.48 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 13:41:09.175217: step 41880, loss = 5.38 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 13:41:14.924616: step 41890, loss = 5.57 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 13:41:20.280023: step 41900, loss = 5.78 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 13:41:27.085617: step 41910, loss = 5.55 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 13:41:32.669230: step 41920, loss = 5.61 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 13:41:38.418882: step 41930, loss = 5.37 (242.6 examples/sec; 0.528 sec/batch)
2016-07-15 13:41:43.670863: step 41940, loss = 5.57 (199.8 examples/sec; 0.641 sec/batch)
2016-07-15 13:41:49.120128: step 41950, loss = 5.51 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 13:41:53.861515: step 41960, loss = 5.49 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 13:41:58.733778: step 41970, loss = 5.60 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 13:42:03.441646: step 41980, loss = 5.31 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 13:42:09.305644: step 41990, loss = 5.66 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 13:42:15.300203: step 42000, loss = 5.42 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 13:42:22.104848: step 42010, loss = 5.66 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 13:42:26.976904: step 42020, loss = 5.62 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 13:42:31.698542: step 42030, loss = 5.90 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 13:42:36.519064: step 42040, loss = 5.94 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 13:42:41.370057: step 42050, loss = 5.57 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 13:42:47.719817: step 42060, loss = 5.45 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 13:42:53.123548: step 42070, loss = 5.65 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 13:42:57.831670: step 42080, loss = 5.38 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 13:43:03.039407: step 42090, loss = 5.31 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 13:43:09.588794: step 42100, loss = 5.58 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 13:43:15.609370: step 42110, loss = 5.35 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 13:43:20.348675: step 42120, loss = 5.29 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 13:43:25.137741: step 42130, loss = 5.29 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 13:43:29.971144: step 42140, loss = 5.75 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 13:43:36.531530: step 42150, loss = 5.62 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 13:43:41.818429: step 42160, loss = 5.77 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 13:43:46.579066: step 42170, loss = 5.53 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 13:43:51.432182: step 42180, loss = 5.58 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 13:43:56.193946: step 42190, loss = 5.53 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 13:44:00.988176: step 42200, loss = 5.40 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 13:44:07.111958: step 42210, loss = 5.18 (188.9 examples/sec; 0.677 sec/batch)
2016-07-15 13:44:13.648622: step 42220, loss = 5.23 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 13:44:18.653331: step 42230, loss = 5.49 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 13:44:23.427591: step 42240, loss = 5.47 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 13:44:29.049789: step 42250, loss = 5.59 (184.7 examples/sec; 0.693 sec/batch)
2016-07-15 13:44:35.152734: step 42260, loss = 5.25 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 13:44:39.980344: step 42270, loss = 5.24 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 13:44:44.760876: step 42280, loss = 5.43 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 13:44:49.550711: step 42290, loss = 5.62 (252.5 examples/sec; 0.507 sec/batch)
2016-07-15 13:44:54.406617: step 42300, loss = 5.84 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 13:45:00.089954: step 42310, loss = 5.54 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 13:45:04.784348: step 42320, loss = 5.44 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 13:45:10.522915: step 42330, loss = 5.64 (221.6 examples/sec; 0.578 sec/batch)
2016-07-15 13:45:15.369651: step 42340, loss = 5.24 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 13:45:20.093453: step 42350, loss = 5.64 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 13:45:24.734757: step 42360, loss = 5.54 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 13:45:30.477443: step 42370, loss = 5.36 (221.4 examples/sec; 0.578 sec/batch)
2016-07-15 13:45:35.665573: step 42380, loss = 5.88 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 13:45:41.098118: step 42390, loss = 5.51 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 13:45:46.886609: step 42400, loss = 5.40 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 13:45:52.703702: step 42410, loss = 5.69 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 13:45:57.538852: step 42420, loss = 5.43 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 13:46:03.911784: step 42430, loss = 5.43 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 13:46:09.326744: step 42440, loss = 5.54 (252.2 examples/sec; 0.507 sec/batch)
2016-07-15 13:46:14.114948: step 42450, loss = 5.32 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 13:46:19.257898: step 42460, loss = 5.45 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 13:46:25.771457: step 42470, loss = 5.56 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 13:46:30.768926: step 42480, loss = 5.64 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 13:46:35.550919: step 42490, loss = 5.49 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 13:46:40.353262: step 42500, loss = 5.37 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 13:46:46.196784: step 42510, loss = 5.55 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 13:46:50.979676: step 42520, loss = 5.63 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 13:46:56.450042: step 42530, loss = 5.57 (191.7 examples/sec; 0.668 sec/batch)
2016-07-15 13:47:02.768476: step 42540, loss = 5.45 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 13:47:08.026362: step 42550, loss = 5.60 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 13:47:13.432293: step 42560, loss = 5.17 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 13:47:18.142489: step 42570, loss = 5.61 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 13:47:23.068008: step 42580, loss = 5.54 (253.2 examples/sec; 0.506 sec/batch)
2016-07-15 13:47:27.792054: step 42590, loss = 5.27 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 13:47:32.417274: step 42600, loss = 5.56 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 13:47:39.275416: step 42610, loss = 5.58 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 13:47:44.699352: step 42620, loss = 5.46 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 13:47:49.992200: step 42630, loss = 5.51 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 13:47:55.797071: step 42640, loss = 5.57 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 13:48:01.328241: step 42650, loss = 5.36 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 13:48:06.478821: step 42660, loss = 5.35 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 13:48:11.194440: step 42670, loss = 5.25 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 13:48:16.756152: step 42680, loss = 5.33 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 13:48:23.000605: step 42690, loss = 5.61 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 13:48:27.885502: step 42700, loss = 5.42 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 13:48:33.670263: step 42710, loss = 5.19 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 13:48:38.352464: step 42720, loss = 5.24 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 13:48:43.528715: step 42730, loss = 5.82 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 13:48:50.019984: step 42740, loss = 5.39 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 13:48:55.020446: step 42750, loss = 5.48 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 13:48:59.729951: step 42760, loss = 5.22 (253.1 examples/sec; 0.506 sec/batch)
2016-07-15 13:49:05.484437: step 42770, loss = 5.38 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 13:49:11.599186: step 42780, loss = 5.39 (255.2 examples/sec; 0.501 sec/batch)
2016-07-15 13:49:16.407667: step 42790, loss = 5.47 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 13:49:21.197597: step 42800, loss = 5.29 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 13:49:28.821436: step 42810, loss = 5.46 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 13:49:34.114079: step 42820, loss = 5.45 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 13:49:39.850042: step 42830, loss = 5.61 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 13:49:45.389932: step 42840, loss = 5.43 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 13:49:50.544694: step 42850, loss = 5.60 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 13:49:55.289887: step 42860, loss = 5.36 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 13:50:00.839301: step 42870, loss = 5.29 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 13:50:07.108454: step 42880, loss = 5.75 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 13:50:11.923894: step 42890, loss = 5.37 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 13:50:16.692952: step 42900, loss = 5.67 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 13:50:24.117069: step 42910, loss = 5.48 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 13:50:29.532104: step 42920, loss = 5.42 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 13:50:34.278911: step 42930, loss = 5.68 (254.8 examples/sec; 0.502 sec/batch)
2016-07-15 13:50:39.396427: step 42940, loss = 5.32 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 13:50:45.915205: step 42950, loss = 5.34 (196.9 examples/sec; 0.650 sec/batch)
2016-07-15 13:50:50.968062: step 42960, loss = 5.79 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 13:50:55.673268: step 42970, loss = 5.26 (251.2 examples/sec; 0.510 sec/batch)
2016-07-15 13:51:00.536787: step 42980, loss = 5.51 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 13:51:05.309905: step 42990, loss = 5.56 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 13:51:11.502573: step 43000, loss = 5.45 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 13:51:18.358203: step 43010, loss = 5.17 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 13:51:24.104549: step 43020, loss = 5.28 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 13:51:29.674274: step 43030, loss = 5.40 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 13:51:34.922201: step 43040, loss = 5.42 (233.0 examples/sec; 0.549 sec/batch)
2016-07-15 13:51:40.645936: step 43050, loss = 5.47 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 13:51:45.488684: step 43060, loss = 5.89 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 13:51:50.382656: step 43070, loss = 5.18 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 13:51:56.839002: step 43080, loss = 5.70 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 13:52:02.189403: step 43090, loss = 5.30 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 13:52:06.927048: step 43100, loss = 5.48 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 13:52:13.741431: step 43110, loss = 5.46 (184.6 examples/sec; 0.693 sec/batch)
2016-07-15 13:52:19.805572: step 43120, loss = 5.41 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 13:52:25.267964: step 43130, loss = 5.52 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 13:52:30.518869: step 43140, loss = 5.29 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 13:52:36.307062: step 43150, loss = 5.54 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 13:52:41.871008: step 43160, loss = 5.21 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 13:52:47.056288: step 43170, loss = 5.52 (229.3 examples/sec; 0.558 sec/batch)
2016-07-15 13:52:52.758764: step 43180, loss = 5.47 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 13:52:57.492474: step 43190, loss = 5.58 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 13:53:02.402928: step 43200, loss = 5.48 (247.5 examples/sec; 0.517 sec/batch)
2016-07-15 13:53:10.486045: step 43210, loss = 5.56 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 13:53:15.639553: step 43220, loss = 5.39 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 13:53:21.264762: step 43230, loss = 5.60 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 13:53:26.001416: step 43240, loss = 5.33 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 13:53:30.913593: step 43250, loss = 5.22 (239.8 examples/sec; 0.534 sec/batch)
2016-07-15 13:53:37.471476: step 43260, loss = 5.56 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 13:53:42.753929: step 43270, loss = 5.59 (250.6 examples/sec; 0.511 sec/batch)
2016-07-15 13:53:47.494127: step 43280, loss = 5.42 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 13:53:53.053522: step 43290, loss = 5.58 (185.6 examples/sec; 0.690 sec/batch)
2016-07-15 13:53:59.356673: step 43300, loss = 5.34 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 13:54:05.143993: step 43310, loss = 5.63 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 13:54:09.927891: step 43320, loss = 5.73 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 13:54:16.276321: step 43330, loss = 5.53 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 13:54:21.829020: step 43340, loss = 5.30 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 13:54:26.694635: step 43350, loss = 5.52 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 13:54:31.958340: step 43360, loss = 5.38 (191.1 examples/sec; 0.670 sec/batch)
2016-07-15 13:54:38.423618: step 43370, loss = 5.40 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 13:54:43.424691: step 43380, loss = 5.42 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 13:54:48.227370: step 43390, loss = 5.29 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 13:54:52.992184: step 43400, loss = 5.56 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 13:54:58.890453: step 43410, loss = 5.48 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 13:55:05.475136: step 43420, loss = 5.07 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 13:55:10.662707: step 43430, loss = 5.33 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 13:55:16.479620: step 43440, loss = 5.48 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 13:55:22.043437: step 43450, loss = 5.54 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 13:55:27.153492: step 43460, loss = 5.69 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 13:55:31.835474: step 43470, loss = 5.40 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 13:55:36.646849: step 43480, loss = 5.36 (282.4 examples/sec; 0.453 sec/batch)
2016-07-15 13:55:41.434860: step 43490, loss = 5.76 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 13:55:47.620863: step 43500, loss = 5.43 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 13:55:54.474643: step 43510, loss = 5.56 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 13:56:00.228242: step 43520, loss = 5.53 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 13:56:05.714582: step 43530, loss = 5.34 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 13:56:10.876408: step 43540, loss = 5.84 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 13:56:15.562326: step 43550, loss = 5.44 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 13:56:21.076165: step 43560, loss = 5.22 (185.6 examples/sec; 0.690 sec/batch)
2016-07-15 13:56:27.372776: step 43570, loss = 5.46 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 13:56:32.232270: step 43580, loss = 5.61 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 13:56:37.037859: step 43590, loss = 5.16 (260.4 examples/sec; 0.491 sec/batch)
2016-07-15 13:56:43.092261: step 43600, loss = 5.39 (194.0 examples/sec; 0.660 sec/batch)
2016-07-15 13:56:50.041389: step 43610, loss = 5.30 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 13:56:55.788594: step 43620, loss = 5.30 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 13:57:01.153193: step 43630, loss = 5.33 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 13:57:06.478434: step 43640, loss = 5.69 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 13:57:11.225090: step 43650, loss = 5.44 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 13:57:16.559834: step 43660, loss = 5.29 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 13:57:23.003746: step 43670, loss = 5.67 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 13:57:27.964298: step 43680, loss = 5.44 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 13:57:32.751054: step 43690, loss = 5.43 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 13:57:38.701892: step 43700, loss = 5.26 (185.9 examples/sec; 0.689 sec/batch)
2016-07-15 13:57:45.790939: step 43710, loss = 5.46 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 13:57:51.562216: step 43720, loss = 5.04 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 13:57:56.409624: step 43730, loss = 5.49 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 13:58:01.188115: step 43740, loss = 5.59 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 13:58:05.948435: step 43750, loss = 5.27 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 13:58:10.858571: step 43760, loss = 5.43 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 13:58:15.587907: step 43770, loss = 5.63 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 13:58:20.453677: step 43780, loss = 5.38 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 13:58:25.211301: step 43790, loss = 5.57 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 13:58:29.938073: step 43800, loss = 5.60 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 13:58:35.600339: step 43810, loss = 5.20 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 13:58:40.702777: step 43820, loss = 5.37 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 13:58:46.145130: step 43830, loss = 5.36 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 13:58:51.884475: step 43840, loss = 5.37 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 13:58:57.274628: step 43850, loss = 5.50 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 13:59:02.592016: step 43860, loss = 5.46 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 13:59:07.337886: step 43870, loss = 5.49 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 13:59:12.717811: step 43880, loss = 5.32 (183.6 examples/sec; 0.697 sec/batch)
2016-07-15 13:59:19.193310: step 43890, loss = 5.61 (214.7 examples/sec; 0.596 sec/batch)
2016-07-15 13:59:24.406070: step 43900, loss = 4.89 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 13:59:31.093597: step 43910, loss = 5.34 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 13:59:35.861979: step 43920, loss = 5.23 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 13:59:41.511281: step 43930, loss = 5.29 (184.9 examples/sec; 0.692 sec/batch)
2016-07-15 13:59:47.742607: step 43940, loss = 5.63 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 13:59:53.071192: step 43950, loss = 5.44 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 13:59:58.386436: step 43960, loss = 5.18 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 14:00:04.218070: step 43970, loss = 5.25 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 14:00:09.694956: step 43980, loss = 5.28 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 14:00:14.900851: step 43990, loss = 5.36 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 14:00:20.731514: step 44000, loss = 5.60 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 14:00:27.566260: step 44010, loss = 5.49 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 14:00:32.840066: step 44020, loss = 5.23 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 14:00:38.253482: step 44030, loss = 5.24 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 14:00:44.000454: step 44040, loss = 5.68 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 14:00:49.439650: step 44050, loss = 5.39 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 14:00:54.703435: step 44060, loss = 5.64 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 14:00:59.419762: step 44070, loss = 5.24 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 14:01:04.833786: step 44080, loss = 5.52 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 14:01:11.290239: step 44090, loss = 5.64 (205.6 examples/sec; 0.622 sec/batch)
2016-07-15 14:01:16.495042: step 44100, loss = 5.41 (197.6 examples/sec; 0.648 sec/batch)
2016-07-15 14:01:23.319367: step 44110, loss = 5.32 (245.9 examples/sec; 0.521 sec/batch)
2016-07-15 14:01:29.113387: step 44120, loss = 5.64 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 14:01:33.875556: step 44130, loss = 5.05 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 14:01:38.750395: step 44140, loss = 5.54 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 14:01:45.206074: step 44150, loss = 5.44 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 14:01:50.509592: step 44160, loss = 5.36 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 14:01:56.299880: step 44170, loss = 5.35 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 14:02:01.111081: step 44180, loss = 5.44 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 14:02:05.975622: step 44190, loss = 5.56 (252.2 examples/sec; 0.507 sec/batch)
2016-07-15 14:02:12.271660: step 44200, loss = 5.41 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 14:02:19.007699: step 44210, loss = 5.63 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 14:02:24.804477: step 44220, loss = 5.69 (250.5 examples/sec; 0.511 sec/batch)
2016-07-15 14:02:29.638399: step 44230, loss = 5.14 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 14:02:34.528469: step 44240, loss = 5.71 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 14:02:40.969467: step 44250, loss = 5.27 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 14:02:46.333575: step 44260, loss = 5.48 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 14:02:52.092626: step 44270, loss = 5.53 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 14:02:57.503184: step 44280, loss = 5.57 (200.1 examples/sec; 0.640 sec/batch)
2016-07-15 14:03:02.831243: step 44290, loss = 5.38 (247.7 examples/sec; 0.517 sec/batch)
2016-07-15 14:03:08.583743: step 44300, loss = 5.38 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 14:03:15.395136: step 44310, loss = 5.52 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 14:03:20.609175: step 44320, loss = 5.27 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 14:03:26.117663: step 44330, loss = 5.55 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 14:03:31.930447: step 44340, loss = 5.39 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 14:03:37.242028: step 44350, loss = 5.66 (198.5 examples/sec; 0.645 sec/batch)
2016-07-15 14:03:42.545499: step 44360, loss = 5.48 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 14:03:48.305368: step 44370, loss = 5.32 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 14:03:53.144505: step 44380, loss = 5.47 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 14:03:58.029825: step 44390, loss = 5.48 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 14:04:04.260637: step 44400, loss = 5.36 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 14:04:11.060463: step 44410, loss = 5.24 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 14:04:16.885233: step 44420, loss = 5.54 (250.6 examples/sec; 0.511 sec/batch)
2016-07-15 14:04:21.674531: step 44430, loss = 5.37 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 14:04:26.478509: step 44440, loss = 5.45 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 14:04:32.809067: step 44450, loss = 5.33 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 14:04:38.292093: step 44460, loss = 5.33 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 14:04:43.028933: step 44470, loss = 5.37 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 14:04:48.234790: step 44480, loss = 5.41 (184.5 examples/sec; 0.694 sec/batch)
2016-07-15 14:04:54.802651: step 44490, loss = 5.31 (205.6 examples/sec; 0.622 sec/batch)
2016-07-15 14:04:59.800704: step 44500, loss = 5.61 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 14:05:05.635189: step 44510, loss = 5.48 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 14:05:11.736696: step 44520, loss = 5.32 (195.3 examples/sec; 0.655 sec/batch)
2016-07-15 14:05:17.485642: step 44530, loss = 5.37 (253.9 examples/sec; 0.504 sec/batch)
2016-07-15 14:05:23.235553: step 44540, loss = 5.43 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 14:05:28.444298: step 44550, loss = 5.32 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 14:05:34.012156: step 44560, loss = 5.66 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 14:05:39.793679: step 44570, loss = 5.18 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 14:05:45.096175: step 44580, loss = 5.41 (210.4 examples/sec; 0.608 sec/batch)
2016-07-15 14:05:50.502098: step 44590, loss = 5.40 (249.9 examples/sec; 0.512 sec/batch)
2016-07-15 14:05:56.274259: step 44600, loss = 5.37 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 14:06:02.052116: step 44610, loss = 5.59 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 14:06:06.911600: step 44620, loss = 5.57 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 14:06:11.707051: step 44630, loss = 5.33 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 14:06:17.078599: step 44640, loss = 5.25 (187.0 examples/sec; 0.685 sec/batch)
2016-07-15 14:06:23.504707: step 44650, loss = 5.67 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 14:06:28.333421: step 44660, loss = 5.40 (270.9 examples/sec; 0.473 sec/batch)
2016-07-15 14:06:33.073797: step 44670, loss = 5.51 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 14:06:38.980651: step 44680, loss = 5.34 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 14:06:44.879269: step 44690, loss = 5.27 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 14:06:50.455628: step 44700, loss = 5.35 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 14:06:57.123862: step 44710, loss = 5.39 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 14:07:02.561592: step 44720, loss = 5.13 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 14:07:08.351085: step 44730, loss = 5.26 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 14:07:13.705882: step 44740, loss = 5.56 (208.3 examples/sec; 0.615 sec/batch)
2016-07-15 14:07:19.004083: step 44750, loss = 5.07 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 14:07:23.709091: step 44760, loss = 5.34 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 14:07:29.029381: step 44770, loss = 5.36 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 14:07:35.455269: step 44780, loss = 5.37 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 14:07:40.460281: step 44790, loss = 5.64 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 14:07:45.231970: step 44800, loss = 5.35 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 14:07:52.453831: step 44810, loss = 5.45 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 14:07:58.026227: step 44820, loss = 5.19 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 14:08:02.772287: step 44830, loss = 5.65 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 14:08:07.644977: step 44840, loss = 5.40 (249.2 examples/sec; 0.514 sec/batch)
2016-07-15 14:08:14.189838: step 44850, loss = 5.36 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 14:08:19.310582: step 44860, loss = 5.16 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 14:08:25.109284: step 44870, loss = 5.45 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 14:08:29.903026: step 44880, loss = 5.46 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 14:08:34.693808: step 44890, loss = 5.36 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 14:08:39.469592: step 44900, loss = 5.41 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 14:08:46.069921: step 44910, loss = 5.21 (190.6 examples/sec; 0.671 sec/batch)
2016-07-15 14:08:52.290281: step 44920, loss = 5.36 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 14:08:57.119023: step 44930, loss = 5.40 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 14:09:01.879070: step 44940, loss = 5.42 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 14:09:08.051443: step 44950, loss = 5.32 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 14:09:13.736622: step 44960, loss = 5.43 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 14:09:18.528988: step 44970, loss = 5.37 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 14:09:23.384145: step 44980, loss = 5.60 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 14:09:29.881854: step 44990, loss = 5.28 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 14:09:35.172450: step 45000, loss = 5.21 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 14:09:42.132213: step 45010, loss = 5.41 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 14:09:46.909198: step 45020, loss = 5.45 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 14:09:51.758870: step 45030, loss = 5.29 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 14:09:56.506097: step 45040, loss = 5.31 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 14:10:01.362051: step 45050, loss = 5.42 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 14:10:06.172847: step 45060, loss = 4.88 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 14:10:10.944637: step 45070, loss = 5.63 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 14:10:15.816493: step 45080, loss = 5.27 (249.4 examples/sec; 0.513 sec/batch)
2016-07-15 14:10:20.515051: step 45090, loss = 5.18 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 14:10:26.080842: step 45100, loss = 5.34 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 14:10:33.577832: step 45110, loss = 5.31 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 14:10:38.391873: step 45120, loss = 5.19 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 14:10:43.213212: step 45130, loss = 5.21 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 14:10:49.639353: step 45140, loss = 5.27 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 14:10:55.004766: step 45150, loss = 5.27 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 14:10:59.703710: step 45160, loss = 5.60 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 14:11:04.575213: step 45170, loss = 5.38 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 14:11:09.320804: step 45180, loss = 5.36 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 14:11:15.173712: step 45190, loss = 5.20 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 14:11:21.086697: step 45200, loss = 5.32 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 14:11:26.869530: step 45210, loss = 5.23 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 14:11:31.769217: step 45220, loss = 5.40 (244.4 examples/sec; 0.524 sec/batch)
2016-07-15 14:11:38.420945: step 45230, loss = 5.21 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 14:11:43.619627: step 45240, loss = 5.31 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 14:11:48.340278: step 45250, loss = 5.57 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 14:11:53.899915: step 45260, loss = 5.23 (186.3 examples/sec; 0.687 sec/batch)
2016-07-15 14:12:00.240092: step 45270, loss = 5.31 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 14:12:05.063863: step 45280, loss = 5.57 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 14:12:09.791180: step 45290, loss = 5.54 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 14:12:15.841246: step 45300, loss = 5.44 (192.6 examples/sec; 0.664 sec/batch)
2016-07-15 14:12:22.861599: step 45310, loss = 5.31 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 14:12:28.611651: step 45320, loss = 5.32 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 14:12:34.029470: step 45330, loss = 5.15 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 14:12:39.293733: step 45340, loss = 5.42 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 14:12:45.080397: step 45350, loss = 5.53 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 14:12:50.651678: step 45360, loss = 5.35 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 14:12:55.829466: step 45370, loss = 5.29 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 14:13:01.572420: step 45380, loss = 4.97 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 14:13:07.210436: step 45390, loss = 5.39 (200.8 examples/sec; 0.637 sec/batch)
2016-07-15 14:13:12.238166: step 45400, loss = 5.27 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 14:13:18.051665: step 45410, loss = 5.34 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 14:13:24.135262: step 45420, loss = 5.44 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 14:13:29.845486: step 45430, loss = 5.51 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 14:13:34.665123: step 45440, loss = 5.38 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 14:13:39.572615: step 45450, loss = 5.37 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 14:13:46.062554: step 45460, loss = 5.32 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 14:13:51.303541: step 45470, loss = 5.44 (265.8 examples/sec; 0.481 sec/batch)
2016-07-15 14:13:56.063115: step 45480, loss = 5.23 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 14:14:01.466997: step 45490, loss = 5.42 (183.7 examples/sec; 0.697 sec/batch)
2016-07-15 14:14:07.903133: step 45500, loss = 5.42 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 14:14:14.402391: step 45510, loss = 5.58 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 14:14:19.657403: step 45520, loss = 5.50 (257.8 examples/sec; 0.497 sec/batch)
2016-07-15 14:14:25.477195: step 45530, loss = 5.20 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 14:14:31.082760: step 45540, loss = 5.38 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 14:14:36.212676: step 45550, loss = 5.38 (225.4 examples/sec; 0.568 sec/batch)
2016-07-15 14:14:41.945570: step 45560, loss = 5.27 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 14:14:46.698066: step 45570, loss = 5.29 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 14:14:51.587256: step 45580, loss = 5.43 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 14:14:58.112251: step 45590, loss = 5.61 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 14:15:03.392030: step 45600, loss = 5.45 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 14:15:10.301548: step 45610, loss = 5.30 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 14:15:16.107304: step 45620, loss = 5.34 (208.2 examples/sec; 0.615 sec/batch)
2016-07-15 14:15:21.360042: step 45630, loss = 5.48 (201.7 examples/sec; 0.634 sec/batch)
2016-07-15 14:15:26.840538: step 45640, loss = 5.46 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 14:15:31.583718: step 45650, loss = 5.25 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 14:15:36.408531: step 45660, loss = 5.62 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 14:15:41.163269: step 45670, loss = 5.33 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 14:15:46.897820: step 45680, loss = 5.17 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 14:15:52.941292: step 45690, loss = 5.37 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 14:15:57.774301: step 45700, loss = 5.32 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 14:16:03.604797: step 45710, loss = 5.36 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 14:16:10.160794: step 45720, loss = 5.46 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 14:16:15.460091: step 45730, loss = 5.21 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 14:16:20.158712: step 45740, loss = 5.40 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 14:16:25.505627: step 45750, loss = 5.44 (185.9 examples/sec; 0.689 sec/batch)
2016-07-15 14:16:31.963462: step 45760, loss = 5.27 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 14:16:36.803347: step 45770, loss = 5.29 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 14:16:41.558174: step 45780, loss = 5.58 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 14:16:47.468503: step 45790, loss = 5.35 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 14:16:53.397876: step 45800, loss = 5.56 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 14:16:59.175860: step 45810, loss = 5.47 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 14:17:04.070414: step 45820, loss = 5.20 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 14:17:10.639449: step 45830, loss = 5.54 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 14:17:15.849824: step 45840, loss = 5.42 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 14:17:21.641136: step 45850, loss = 5.31 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 14:17:27.225739: step 45860, loss = 5.36 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 14:17:32.368151: step 45870, loss = 5.27 (228.2 examples/sec; 0.561 sec/batch)
2016-07-15 14:17:38.073947: step 45880, loss = 5.14 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 14:17:42.840542: step 45890, loss = 5.35 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 14:17:47.706409: step 45900, loss = 5.44 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 14:17:55.773686: step 45910, loss = 5.07 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 14:18:00.997226: step 45920, loss = 5.38 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 14:18:06.563445: step 45930, loss = 5.23 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 14:18:11.303397: step 45940, loss = 5.39 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 14:18:16.242592: step 45950, loss = 5.57 (238.9 examples/sec; 0.536 sec/batch)
2016-07-15 14:18:22.827942: step 45960, loss = 5.39 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 14:18:28.011706: step 45970, loss = 5.22 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 14:18:32.686707: step 45980, loss = 5.32 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 14:18:37.505452: step 45990, loss = 5.57 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 14:18:42.303053: step 46000, loss = 5.43 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 14:18:49.802732: step 46010, loss = 5.40 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 14:18:55.204511: step 46020, loss = 5.14 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 14:19:00.953569: step 46030, loss = 5.43 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 14:19:06.320579: step 46040, loss = 5.26 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 14:19:11.652041: step 46050, loss = 5.44 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 14:19:16.357777: step 46060, loss = 5.31 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 14:19:21.730754: step 46070, loss = 5.23 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 14:19:28.193485: step 46080, loss = 5.36 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 14:19:33.089711: step 46090, loss = 5.27 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 14:19:37.910810: step 46100, loss = 5.50 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 14:19:45.244337: step 46110, loss = 5.33 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 14:19:50.802930: step 46120, loss = 5.42 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 14:19:56.552012: step 46130, loss = 5.35 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 14:20:01.811933: step 46140, loss = 5.38 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 14:20:07.315144: step 46150, loss = 5.31 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 14:20:13.082210: step 46160, loss = 5.19 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 14:20:17.954004: step 46170, loss = 5.31 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 14:20:22.711893: step 46180, loss = 5.05 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 14:20:27.471030: step 46190, loss = 5.43 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 14:20:32.399451: step 46200, loss = 5.36 (230.6 examples/sec; 0.555 sec/batch)
2016-07-15 14:20:40.545838: step 46210, loss = 5.42 (243.1 examples/sec; 0.526 sec/batch)
2016-07-15 14:20:45.461434: step 46220, loss = 5.01 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 14:20:50.188446: step 46230, loss = 5.45 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 14:20:54.976716: step 46240, loss = 5.10 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 14:20:59.873437: step 46250, loss = 5.20 (253.1 examples/sec; 0.506 sec/batch)
2016-07-15 14:21:06.428364: step 46260, loss = 5.21 (199.9 examples/sec; 0.640 sec/batch)
2016-07-15 14:21:11.787938: step 46270, loss = 5.18 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 14:21:17.528188: step 46280, loss = 5.31 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 14:21:23.017418: step 46290, loss = 5.07 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 14:21:28.228273: step 46300, loss = 5.26 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 14:21:33.968687: step 46310, loss = 5.22 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 14:21:39.907515: step 46320, loss = 5.45 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 14:21:45.782005: step 46330, loss = 5.13 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 14:21:50.617714: step 46340, loss = 5.36 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 14:21:55.453508: step 46350, loss = 5.16 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 14:22:01.877674: step 46360, loss = 5.40 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 14:22:07.338481: step 46370, loss = 5.17 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 14:22:13.121653: step 46380, loss = 5.32 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 14:22:18.481436: step 46390, loss = 5.42 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 14:22:23.812775: step 46400, loss = 5.54 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 14:22:30.668613: step 46410, loss = 5.45 (250.9 examples/sec; 0.510 sec/batch)
2016-07-15 14:22:36.457910: step 46420, loss = 5.21 (212.0 examples/sec; 0.604 sec/batch)
2016-07-15 14:22:41.616013: step 46430, loss = 5.27 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 14:22:47.136249: step 46440, loss = 5.45 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 14:22:52.959194: step 46450, loss = 5.35 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 14:22:58.268044: step 46460, loss = 5.32 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 14:23:03.609069: step 46470, loss = 5.51 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 14:23:09.398980: step 46480, loss = 5.53 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 14:23:14.226545: step 46490, loss = 5.14 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 14:23:19.017762: step 46500, loss = 5.26 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 14:23:26.576924: step 46510, loss = 5.40 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 14:23:31.931615: step 46520, loss = 5.34 (246.7 examples/sec; 0.519 sec/batch)
2016-07-15 14:23:37.668927: step 46530, loss = 5.44 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 14:23:43.274657: step 46540, loss = 5.05 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 14:23:48.435584: step 46550, loss = 5.35 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 14:23:54.200741: step 46560, loss = 5.15 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 14:23:59.922773: step 46570, loss = 5.20 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 14:24:05.065488: step 46580, loss = 5.30 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 14:24:10.671577: step 46590, loss = 5.39 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 14:24:16.444884: step 46600, loss = 5.36 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 14:24:22.299654: step 46610, loss = 5.19 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 14:24:27.147308: step 46620, loss = 5.25 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 14:24:33.383072: step 46630, loss = 5.57 (208.3 examples/sec; 0.614 sec/batch)
2016-07-15 14:24:38.958557: step 46640, loss = 5.12 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 14:24:43.678310: step 46650, loss = 5.48 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 14:24:48.557523: step 46660, loss = 5.31 (250.6 examples/sec; 0.511 sec/batch)
2016-07-15 14:24:55.170946: step 46670, loss = 5.63 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 14:25:00.318662: step 46680, loss = 5.37 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 14:25:04.993502: step 46690, loss = 5.41 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 14:25:09.835161: step 46700, loss = 5.20 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 14:25:15.560483: step 46710, loss = 5.37 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 14:25:21.911531: step 46720, loss = 5.18 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 14:25:27.394519: step 46730, loss = 5.26 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 14:25:32.118182: step 46740, loss = 5.27 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 14:25:37.301104: step 46750, loss = 5.63 (184.5 examples/sec; 0.694 sec/batch)
2016-07-15 14:25:43.811238: step 46760, loss = 5.39 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 14:25:48.853150: step 46770, loss = 5.64 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 14:25:53.555354: step 46780, loss = 5.06 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 14:25:58.384164: step 46790, loss = 5.31 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 14:26:03.223632: step 46800, loss = 5.32 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 14:26:10.837803: step 46810, loss = 5.18 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 14:26:16.146193: step 46820, loss = 5.54 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 14:26:21.933378: step 46830, loss = 5.35 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 14:26:27.455505: step 46840, loss = 5.22 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 14:26:32.605841: step 46850, loss = 5.48 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 14:26:38.351522: step 46860, loss = 5.19 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 14:26:43.981086: step 46870, loss = 4.94 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 14:26:49.042692: step 46880, loss = 5.32 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 14:26:53.824652: step 46890, loss = 5.09 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 14:26:59.561737: step 46900, loss = 5.14 (187.8 examples/sec; 0.682 sec/batch)
2016-07-15 14:27:06.951257: step 46910, loss = 5.16 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 14:27:11.708744: step 46920, loss = 5.26 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 14:27:16.677832: step 46930, loss = 5.27 (249.8 examples/sec; 0.512 sec/batch)
2016-07-15 14:27:23.248485: step 46940, loss = 5.53 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 14:27:28.395734: step 46950, loss = 5.47 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 14:27:33.126938: step 46960, loss = 5.27 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 14:27:37.999091: step 46970, loss = 5.45 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 14:27:42.744164: step 46980, loss = 5.35 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 14:27:48.870831: step 46990, loss = 5.27 (192.6 examples/sec; 0.664 sec/batch)
2016-07-15 14:27:54.650877: step 47000, loss = 5.21 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 14:28:01.416300: step 47010, loss = 5.21 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 14:28:06.831882: step 47020, loss = 5.46 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 14:28:12.162474: step 47030, loss = 5.40 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 14:28:16.900705: step 47040, loss = 5.09 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 14:28:22.208804: step 47050, loss = 5.31 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 14:28:28.669560: step 47060, loss = 5.23 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 14:28:33.577854: step 47070, loss = 5.05 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 14:28:38.352931: step 47080, loss = 5.35 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 14:28:44.314650: step 47090, loss = 5.09 (191.9 examples/sec; 0.667 sec/batch)
2016-07-15 14:28:50.210403: step 47100, loss = 5.11 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 14:28:57.011318: step 47110, loss = 5.52 (245.6 examples/sec; 0.521 sec/batch)
2016-07-15 14:29:02.287075: step 47120, loss = 5.52 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 14:29:07.707980: step 47130, loss = 5.23 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 14:29:12.432691: step 47140, loss = 5.23 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 14:29:17.636400: step 47150, loss = 5.49 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 14:29:24.182191: step 47160, loss = 5.34 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 14:29:29.359543: step 47170, loss = 5.41 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 14:29:34.997676: step 47180, loss = 4.94 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 14:29:40.809713: step 47190, loss = 5.27 (211.6 examples/sec; 0.605 sec/batch)
2016-07-15 14:29:45.666786: step 47200, loss = 5.12 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 14:29:51.527302: step 47210, loss = 5.40 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 14:29:57.810972: step 47220, loss = 5.32 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 14:30:03.344032: step 47230, loss = 5.34 (260.4 examples/sec; 0.491 sec/batch)
2016-07-15 14:30:09.093550: step 47240, loss = 5.29 (252.7 examples/sec; 0.507 sec/batch)
2016-07-15 14:30:14.394452: step 47250, loss = 5.44 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 14:30:19.814343: step 47260, loss = 5.31 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 14:30:25.550153: step 47270, loss = 5.19 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 14:30:30.918563: step 47280, loss = 5.45 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 14:30:36.243493: step 47290, loss = 5.60 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 14:30:40.940009: step 47300, loss = 5.36 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 14:30:46.792478: step 47310, loss = 5.13 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 14:30:51.484618: step 47320, loss = 5.15 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 14:30:56.119062: step 47330, loss = 5.27 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 14:31:01.864972: step 47340, loss = 5.25 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 14:31:07.333688: step 47350, loss = 5.39 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 14:31:12.532570: step 47360, loss = 5.25 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 14:31:18.358663: step 47370, loss = 5.45 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 14:31:23.975492: step 47380, loss = 5.05 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 14:31:29.082024: step 47390, loss = 5.30 (233.4 examples/sec; 0.548 sec/batch)
2016-07-15 14:31:34.838338: step 47400, loss = 5.20 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 14:31:41.696414: step 47410, loss = 5.19 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 14:31:47.153950: step 47420, loss = 5.28 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 14:31:52.388124: step 47430, loss = 5.12 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 14:31:58.163717: step 47440, loss = 5.20 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 14:32:03.761364: step 47450, loss = 4.94 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 14:32:08.878360: step 47460, loss = 5.05 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 14:32:14.613118: step 47470, loss = 5.17 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 14:32:20.272236: step 47480, loss = 5.23 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 14:32:25.300925: step 47490, loss = 5.28 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 14:32:30.077492: step 47500, loss = 4.96 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 14:32:37.305718: step 47510, loss = 5.14 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 14:32:42.958076: step 47520, loss = 5.37 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 14:32:48.695723: step 47530, loss = 5.13 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 14:32:53.814714: step 47540, loss = 5.30 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 14:32:59.404035: step 47550, loss = 5.29 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 14:33:04.147529: step 47560, loss = 5.42 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 14:33:09.104626: step 47570, loss = 5.31 (229.1 examples/sec; 0.559 sec/batch)
2016-07-15 14:33:15.683899: step 47580, loss = 5.14 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 14:33:20.828534: step 47590, loss = 5.14 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 14:33:25.590909: step 47600, loss = 5.19 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 14:33:32.561407: step 47610, loss = 5.24 (191.2 examples/sec; 0.669 sec/batch)
2016-07-15 14:33:38.506062: step 47620, loss = 5.23 (241.4 examples/sec; 0.530 sec/batch)
2016-07-15 14:33:44.086309: step 47630, loss = 5.35 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 14:33:49.322561: step 47640, loss = 5.63 (209.2 examples/sec; 0.612 sec/batch)
2016-07-15 14:33:55.047066: step 47650, loss = 5.24 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 14:34:00.776876: step 47660, loss = 5.28 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 14:34:05.906369: step 47670, loss = 5.35 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 14:34:11.438016: step 47680, loss = 5.23 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 14:34:16.188365: step 47690, loss = 5.49 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 14:34:21.089089: step 47700, loss = 5.11 (236.1 examples/sec; 0.542 sec/batch)
2016-07-15 14:34:29.241029: step 47710, loss = 5.35 (230.6 examples/sec; 0.555 sec/batch)
2016-07-15 14:34:34.442968: step 47720, loss = 5.29 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 14:34:39.858190: step 47730, loss = 5.11 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 14:34:44.567003: step 47740, loss = 5.21 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 14:34:49.394692: step 47750, loss = 5.28 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 14:34:54.178672: step 47760, loss = 5.38 (240.5 examples/sec; 0.532 sec/batch)
2016-07-15 14:34:59.875478: step 47770, loss = 5.28 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 14:35:05.960364: step 47780, loss = 5.15 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 14:35:10.819940: step 47790, loss = 5.17 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 14:35:15.586525: step 47800, loss = 5.18 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 14:35:23.150367: step 47810, loss = 5.33 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 14:35:28.431852: step 47820, loss = 5.22 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 14:35:34.264390: step 47830, loss = 5.60 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 14:35:39.730608: step 47840, loss = 5.14 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 14:35:44.923849: step 47850, loss = 5.30 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 14:35:49.621968: step 47860, loss = 5.04 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 14:35:55.108013: step 47870, loss = 5.27 (186.2 examples/sec; 0.687 sec/batch)
2016-07-15 14:36:01.403365: step 47880, loss = 5.39 (251.1 examples/sec; 0.510 sec/batch)
2016-07-15 14:36:06.682563: step 47890, loss = 5.25 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 14:36:12.102473: step 47900, loss = 5.36 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 14:36:17.893900: step 47910, loss = 5.67 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 14:36:23.515489: step 47920, loss = 5.29 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 14:36:29.687590: step 47930, loss = 5.37 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 14:36:35.095110: step 47940, loss = 4.87 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 14:36:40.375993: step 47950, loss = 5.27 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 14:36:46.188456: step 47960, loss = 5.28 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 14:36:51.612387: step 47970, loss = 5.43 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 14:36:56.891848: step 47980, loss = 5.38 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 14:37:01.627312: step 47990, loss = 5.26 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 14:37:06.453680: step 48000, loss = 5.26 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 14:37:12.159778: step 48010, loss = 5.46 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 14:37:18.486016: step 48020, loss = 5.21 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 14:37:23.965076: step 48030, loss = 4.96 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 14:37:28.712734: step 48040, loss = 5.36 (271.5 examples/sec; 0.472 sec/batch)
2016-07-15 14:37:33.731912: step 48050, loss = 4.95 (190.7 examples/sec; 0.671 sec/batch)
2016-07-15 14:37:40.262361: step 48060, loss = 5.16 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 14:37:45.394290: step 48070, loss = 5.43 (237.5 examples/sec; 0.539 sec/batch)
2016-07-15 14:37:51.107610: step 48080, loss = 5.21 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 14:37:55.876049: step 48090, loss = 5.30 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 14:38:00.733988: step 48100, loss = 5.24 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 14:38:08.785483: step 48110, loss = 5.27 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 14:38:13.746833: step 48120, loss = 5.29 (282.5 examples/sec; 0.453 sec/batch)
2016-07-15 14:38:18.500899: step 48130, loss = 5.37 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 14:38:24.263310: step 48140, loss = 5.28 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 14:38:30.291611: step 48150, loss = 5.10 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 14:38:35.795168: step 48160, loss = 5.14 (205.0 examples/sec; 0.625 sec/batch)
2016-07-15 14:38:40.985837: step 48170, loss = 5.30 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 14:38:46.777582: step 48180, loss = 5.21 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 14:38:51.634326: step 48190, loss = 5.21 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 14:38:56.533778: step 48200, loss = 5.08 (237.4 examples/sec; 0.539 sec/batch)
2016-07-15 14:39:04.455130: step 48210, loss = 5.23 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 14:39:09.533551: step 48220, loss = 5.29 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 14:39:14.213837: step 48230, loss = 4.90 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 14:39:19.027032: step 48240, loss = 5.24 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 14:39:23.880538: step 48250, loss = 5.10 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 14:39:30.175125: step 48260, loss = 5.03 (205.0 examples/sec; 0.625 sec/batch)
2016-07-15 14:39:35.784032: step 48270, loss = 5.39 (246.6 examples/sec; 0.519 sec/batch)
2016-07-15 14:39:40.548022: step 48280, loss = 5.08 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 14:39:45.490005: step 48290, loss = 5.29 (227.3 examples/sec; 0.563 sec/batch)
2016-07-15 14:39:52.082300: step 48300, loss = 5.19 (200.1 examples/sec; 0.640 sec/batch)
2016-07-15 14:39:58.752942: step 48310, loss = 5.47 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 14:40:04.184346: step 48320, loss = 5.27 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 14:40:08.909459: step 48330, loss = 5.30 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 14:40:13.994524: step 48340, loss = 4.82 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 14:40:20.569284: step 48350, loss = 5.38 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 14:40:25.706598: step 48360, loss = 5.16 (216.6 examples/sec; 0.591 sec/batch)
2016-07-15 14:40:31.389828: step 48370, loss = 5.22 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 14:40:37.118646: step 48380, loss = 5.21 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 14:40:42.214830: step 48390, loss = 5.04 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 14:40:47.827398: step 48400, loss = 5.14 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 14:40:54.625987: step 48410, loss = 5.38 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 14:41:00.170810: step 48420, loss = 5.28 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 14:41:05.337840: step 48430, loss = 5.21 (253.0 examples/sec; 0.506 sec/batch)
2016-07-15 14:41:09.972956: step 48440, loss = 5.36 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 14:41:15.458962: step 48450, loss = 5.48 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 14:41:21.763078: step 48460, loss = 5.31 (239.2 examples/sec; 0.535 sec/batch)
2016-07-15 14:41:26.581512: step 48470, loss = 5.40 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 14:41:31.354232: step 48480, loss = 4.99 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 14:41:36.162546: step 48490, loss = 5.07 (241.1 examples/sec; 0.531 sec/batch)
2016-07-15 14:41:41.072245: step 48500, loss = 5.22 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 14:41:49.189331: step 48510, loss = 5.33 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 14:41:54.349904: step 48520, loss = 5.13 (201.1 examples/sec; 0.636 sec/batch)
2016-07-15 14:41:59.919813: step 48530, loss = 5.20 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 14:42:05.684855: step 48540, loss = 5.02 (237.6 examples/sec; 0.539 sec/batch)
2016-07-15 14:42:10.892819: step 48550, loss = 5.16 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 14:42:16.316538: step 48560, loss = 5.08 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 14:42:21.042383: step 48570, loss = 5.21 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 14:42:26.160813: step 48580, loss = 5.34 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 14:42:32.718572: step 48590, loss = 4.86 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 14:42:37.795856: step 48600, loss = 5.32 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 14:42:43.515263: step 48610, loss = 4.98 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 14:42:49.599130: step 48620, loss = 5.17 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 14:42:55.331349: step 48630, loss = 5.16 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 14:43:00.182386: step 48640, loss = 5.19 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 14:43:05.072688: step 48650, loss = 5.06 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 14:43:11.565404: step 48660, loss = 5.08 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 14:43:16.968250: step 48670, loss = 5.01 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 14:43:22.740262: step 48680, loss = 5.38 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 14:43:28.209322: step 48690, loss = 5.08 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 14:43:33.418751: step 48700, loss = 5.22 (247.4 examples/sec; 0.517 sec/batch)
2016-07-15 14:43:39.226345: step 48710, loss = 5.25 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 14:43:45.237791: step 48720, loss = 5.23 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 14:43:51.099955: step 48730, loss = 5.41 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 14:43:56.698045: step 48740, loss = 5.06 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 14:44:01.814940: step 48750, loss = 5.01 (227.8 examples/sec; 0.562 sec/batch)
2016-07-15 14:44:07.512894: step 48760, loss = 5.11 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 14:44:12.267652: step 48770, loss = 5.14 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 14:44:17.107826: step 48780, loss = 5.52 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 14:44:23.564133: step 48790, loss = 5.29 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 14:44:28.855175: step 48800, loss = 5.32 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 14:44:34.694839: step 48810, loss = 5.20 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 14:44:40.401092: step 48820, loss = 5.22 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 14:44:46.470451: step 48830, loss = 5.30 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 14:44:51.298062: step 48840, loss = 5.13 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 14:44:56.147031: step 48850, loss = 5.40 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 14:45:02.336791: step 48860, loss = 4.99 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 14:45:07.901773: step 48870, loss = 5.41 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 14:45:13.777745: step 48880, loss = 5.30 (216.3 examples/sec; 0.592 sec/batch)
2016-07-15 14:45:18.991983: step 48890, loss = 5.60 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 14:45:24.447073: step 48900, loss = 5.36 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 14:45:31.272529: step 48910, loss = 5.36 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 14:45:36.106488: step 48920, loss = 5.47 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 14:45:40.805658: step 48930, loss = 5.29 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 14:45:45.476659: step 48940, loss = 5.21 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 14:45:51.287660: step 48950, loss = 5.06 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 14:45:56.889501: step 48960, loss = 5.03 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 14:46:02.066476: step 48970, loss = 5.16 (209.2 examples/sec; 0.612 sec/batch)
2016-07-15 14:46:07.775027: step 48980, loss = 5.02 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 14:46:12.523761: step 48990, loss = 5.21 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 14:46:17.389798: step 49000, loss = 5.18 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 14:46:25.518057: step 49010, loss = 5.15 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 14:46:30.709258: step 49020, loss = 5.08 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 14:46:36.262669: step 49030, loss = 5.03 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 14:46:41.991943: step 49040, loss = 5.27 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 14:46:47.255089: step 49050, loss = 5.16 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 14:46:52.757038: step 49060, loss = 5.35 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 14:46:58.558856: step 49070, loss = 5.04 (253.7 examples/sec; 0.505 sec/batch)
2016-07-15 14:47:03.408747: step 49080, loss = 5.27 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 14:47:08.210448: step 49090, loss = 5.39 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 14:47:14.416194: step 49100, loss = 5.30 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 14:47:21.294419: step 49110, loss = 5.29 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 14:47:27.076078: step 49120, loss = 5.23 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 14:47:31.877774: step 49130, loss = 5.44 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 14:47:36.724197: step 49140, loss = 5.28 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 14:47:41.414131: step 49150, loss = 5.31 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 14:47:46.554497: step 49160, loss = 5.14 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 14:47:53.053566: step 49170, loss = 5.02 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 14:47:58.194024: step 49180, loss = 5.33 (211.2 examples/sec; 0.606 sec/batch)
2016-07-15 14:48:03.873951: step 49190, loss = 5.02 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 14:48:09.668475: step 49200, loss = 5.31 (203.7 examples/sec; 0.629 sec/batch)
2016-07-15 14:48:15.520736: step 49210, loss = 5.05 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 14:48:20.361954: step 49220, loss = 5.27 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 14:48:26.634979: step 49230, loss = 5.47 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 14:48:32.229744: step 49240, loss = 5.13 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 14:48:37.001165: step 49250, loss = 5.21 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 14:48:41.905521: step 49260, loss = 5.18 (232.2 examples/sec; 0.551 sec/batch)
2016-07-15 14:48:48.482165: step 49270, loss = 5.19 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 14:48:53.694527: step 49280, loss = 5.06 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 14:48:58.453555: step 49290, loss = 5.16 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 14:49:03.308637: step 49300, loss = 5.23 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 14:49:09.167472: step 49310, loss = 5.00 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 14:49:13.856008: step 49320, loss = 5.09 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 14:49:18.745215: step 49330, loss = 5.18 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 14:49:23.491615: step 49340, loss = 5.05 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 14:49:28.279499: step 49350, loss = 5.09 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 14:49:33.132629: step 49360, loss = 5.05 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 14:49:39.559830: step 49370, loss = 5.23 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 14:49:44.973033: step 49380, loss = 5.16 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 14:49:49.696367: step 49390, loss = 5.21 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 14:49:54.998929: step 49400, loss = 5.22 (183.3 examples/sec; 0.698 sec/batch)
2016-07-15 14:50:02.940379: step 49410, loss = 5.12 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 14:50:08.394470: step 49420, loss = 4.94 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 14:50:13.596301: step 49430, loss = 5.37 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 14:50:19.409138: step 49440, loss = 5.21 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 14:50:25.051343: step 49450, loss = 5.13 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 14:50:30.144126: step 49460, loss = 5.18 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 14:50:34.957940: step 49470, loss = 5.34 (257.8 examples/sec; 0.497 sec/batch)
2016-07-15 14:50:40.601220: step 49480, loss = 5.23 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 14:50:46.755541: step 49490, loss = 5.11 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 14:50:52.121610: step 49500, loss = 5.12 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 14:50:58.775359: step 49510, loss = 5.29 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 14:51:04.348359: step 49520, loss = 5.20 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 14:51:09.093082: step 49530, loss = 5.22 (270.9 examples/sec; 0.473 sec/batch)
2016-07-15 14:51:14.009121: step 49540, loss = 5.15 (242.9 examples/sec; 0.527 sec/batch)
2016-07-15 14:51:20.560006: step 49550, loss = 5.01 (199.9 examples/sec; 0.640 sec/batch)
2016-07-15 14:51:25.697459: step 49560, loss = 5.15 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 14:51:31.495972: step 49570, loss = 5.15 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 14:51:37.083610: step 49580, loss = 5.49 (203.3 examples/sec; 0.629 sec/batch)
2016-07-15 14:51:42.153633: step 49590, loss = 4.91 (240.7 examples/sec; 0.532 sec/batch)
2016-07-15 14:51:47.872954: step 49600, loss = 5.32 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 14:51:53.636012: step 49610, loss = 5.41 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 14:51:58.720451: step 49620, loss = 5.37 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 14:52:05.275245: step 49630, loss = 5.37 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 14:52:10.411135: step 49640, loss = 4.99 (210.4 examples/sec; 0.608 sec/batch)
2016-07-15 14:52:16.061925: step 49650, loss = 5.00 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 14:52:21.828379: step 49660, loss = 5.17 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 14:52:27.009962: step 49670, loss = 5.23 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 14:52:32.553884: step 49680, loss = 5.27 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 14:52:38.363838: step 49690, loss = 5.11 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 14:52:43.704246: step 49700, loss = 5.08 (197.8 examples/sec; 0.647 sec/batch)
2016-07-15 14:52:50.429509: step 49710, loss = 5.29 (210.7 examples/sec; 0.607 sec/batch)
2016-07-15 14:52:56.126913: step 49720, loss = 5.14 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 14:53:00.911068: step 49730, loss = 5.13 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 14:53:05.604626: step 49740, loss = 5.43 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 14:53:10.335876: step 49750, loss = 4.80 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 14:53:14.955267: step 49760, loss = 5.20 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 14:53:20.143507: step 49770, loss = 5.16 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 14:53:25.501051: step 49780, loss = 5.13 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 14:53:31.287832: step 49790, loss = 5.46 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 14:53:36.788427: step 49800, loss = 4.97 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 14:53:43.418347: step 49810, loss = 5.11 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 14:53:48.835982: step 49820, loss = 4.92 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 14:53:53.578653: step 49830, loss = 4.99 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 14:53:58.413682: step 49840, loss = 5.24 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 14:54:03.189735: step 49850, loss = 5.05 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 14:54:08.017473: step 49860, loss = 5.01 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 14:54:12.863898: step 49870, loss = 5.35 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 14:54:19.150283: step 49880, loss = 5.30 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 14:54:24.678140: step 49890, loss = 4.93 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 14:54:30.424908: step 49900, loss = 5.00 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 14:54:36.935754: step 49910, loss = 4.93 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 14:54:42.129756: step 49920, loss = 5.13 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 14:54:47.876659: step 49930, loss = 5.14 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 14:54:53.503125: step 49940, loss = 5.48 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 14:54:58.511534: step 49950, loss = 5.38 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 14:55:03.277737: step 49960, loss = 5.35 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 14:55:09.004491: step 49970, loss = 4.97 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 14:55:15.082153: step 49980, loss = 5.01 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 14:55:19.867957: step 49990, loss = 4.91 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 14:55:24.729433: step 50000, loss = 5.26 (248.7 examples/sec; 0.515 sec/batch)
2016-07-15 14:55:32.328391: step 50010, loss = 5.17 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 14:55:37.660164: step 50020, loss = 4.94 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 14:55:43.434841: step 50030, loss = 5.41 (250.1 examples/sec; 0.512 sec/batch)
2016-07-15 14:55:48.981146: step 50040, loss = 5.19 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 14:55:54.143195: step 50050, loss = 5.28 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 14:55:58.822693: step 50060, loss = 5.38 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 14:56:04.327478: step 50070, loss = 5.50 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 14:56:10.591131: step 50080, loss = 5.21 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 14:56:15.892756: step 50090, loss = 4.98 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 14:56:21.321638: step 50100, loss = 5.08 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 14:56:27.012891: step 50110, loss = 5.16 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 14:56:32.667570: step 50120, loss = 5.14 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 14:56:38.856691: step 50130, loss = 5.10 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 14:56:43.672085: step 50140, loss = 4.94 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 14:56:48.528063: step 50150, loss = 5.29 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 14:56:54.715590: step 50160, loss = 4.92 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 14:57:00.390486: step 50170, loss = 5.10 (242.7 examples/sec; 0.527 sec/batch)
2016-07-15 14:57:05.183971: step 50180, loss = 5.09 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 14:57:10.074717: step 50190, loss = 5.41 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 14:57:16.638228: step 50200, loss = 5.02 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 14:57:22.863523: step 50210, loss = 5.12 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 14:57:27.997995: step 50220, loss = 5.20 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 14:57:33.381528: step 50230, loss = 5.17 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 14:57:39.178264: step 50240, loss = 5.29 (253.0 examples/sec; 0.506 sec/batch)
2016-07-15 14:57:44.009531: step 50250, loss = 5.35 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 14:57:48.797149: step 50260, loss = 5.18 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 14:57:54.991462: step 50270, loss = 4.98 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 14:58:00.615337: step 50280, loss = 5.22 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 14:58:06.418225: step 50290, loss = 5.08 (219.9 examples/sec; 0.582 sec/batch)
2016-07-15 14:58:11.290768: step 50300, loss = 5.01 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 14:58:17.116575: step 50310, loss = 5.11 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 14:58:23.417985: step 50320, loss = 5.02 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 14:58:28.956658: step 50330, loss = 5.18 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 14:58:34.702001: step 50340, loss = 5.19 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 14:58:39.537830: step 50350, loss = 4.98 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 14:58:44.305469: step 50360, loss = 5.27 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 14:58:49.069485: step 50370, loss = 5.13 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 14:58:53.910119: step 50380, loss = 5.01 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 14:59:00.447018: step 50390, loss = 5.09 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 14:59:05.747790: step 50400, loss = 5.39 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 14:59:12.642051: step 50410, loss = 5.18 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 14:59:17.430306: step 50420, loss = 5.11 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 14:59:22.312088: step 50430, loss = 5.10 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 14:59:27.064540: step 50440, loss = 5.04 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 14:59:32.621318: step 50450, loss = 5.02 (191.4 examples/sec; 0.669 sec/batch)
2016-07-15 14:59:38.891531: step 50460, loss = 5.22 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 14:59:43.727341: step 50470, loss = 5.27 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 14:59:48.524631: step 50480, loss = 5.13 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 14:59:54.643916: step 50490, loss = 5.28 (195.0 examples/sec; 0.656 sec/batch)
2016-07-15 15:00:00.337485: step 50500, loss = 5.38 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 15:00:07.144835: step 50510, loss = 4.78 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 15:00:12.596274: step 50520, loss = 5.08 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 15:00:17.897636: step 50530, loss = 5.02 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 15:00:23.697070: step 50540, loss = 5.25 (251.0 examples/sec; 0.510 sec/batch)
2016-07-15 15:00:29.208148: step 50550, loss = 4.98 (208.8 examples/sec; 0.613 sec/batch)
2016-07-15 15:00:34.399296: step 50560, loss = 5.08 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 15:00:39.147635: step 50570, loss = 5.21 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 15:00:44.734525: step 50580, loss = 5.13 (184.9 examples/sec; 0.692 sec/batch)
2016-07-15 15:00:50.997543: step 50590, loss = 5.12 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 15:00:56.298270: step 50600, loss = 5.27 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 15:01:03.078676: step 50610, loss = 5.04 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 15:01:08.728791: step 50620, loss = 5.16 (245.8 examples/sec; 0.521 sec/batch)
2016-07-15 15:01:13.563490: step 50630, loss = 5.36 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 15:01:18.417108: step 50640, loss = 5.21 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 15:01:23.220318: step 50650, loss = 5.02 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 15:01:28.767912: step 50660, loss = 4.96 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 15:01:35.023936: step 50670, loss = 5.37 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 15:01:39.827896: step 50680, loss = 5.24 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 15:01:44.632942: step 50690, loss = 5.06 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 15:01:49.392917: step 50700, loss = 5.15 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 15:01:55.034600: step 50710, loss = 5.04 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 15:02:00.223899: step 50720, loss = 5.13 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 15:02:05.626270: step 50730, loss = 5.02 (265.8 examples/sec; 0.481 sec/batch)
2016-07-15 15:02:10.368711: step 50740, loss = 5.38 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 15:02:15.621053: step 50750, loss = 5.01 (191.5 examples/sec; 0.668 sec/batch)
2016-07-15 15:02:22.114206: step 50760, loss = 5.32 (208.8 examples/sec; 0.613 sec/batch)
2016-07-15 15:02:27.277817: step 50770, loss = 5.22 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 15:02:32.845516: step 50780, loss = 5.13 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 15:02:38.650744: step 50790, loss = 5.13 (221.2 examples/sec; 0.579 sec/batch)
2016-07-15 15:02:43.834488: step 50800, loss = 5.06 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 15:02:50.566781: step 50810, loss = 5.01 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 15:02:56.340089: step 50820, loss = 5.19 (241.3 examples/sec; 0.531 sec/batch)
2016-07-15 15:03:01.987831: step 50830, loss = 5.14 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 15:03:06.974250: step 50840, loss = 5.24 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 15:03:11.703493: step 50850, loss = 5.22 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 15:03:17.430356: step 50860, loss = 5.08 (190.6 examples/sec; 0.671 sec/batch)
2016-07-15 15:03:23.486560: step 50870, loss = 5.03 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 15:03:28.344329: step 50880, loss = 4.92 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 15:03:33.102349: step 50890, loss = 5.52 (254.2 examples/sec; 0.503 sec/batch)
2016-07-15 15:03:39.281556: step 50900, loss = 5.12 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 15:03:46.198709: step 50910, loss = 4.80 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 15:03:51.979716: step 50920, loss = 5.08 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 15:03:56.758772: step 50930, loss = 5.12 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 15:04:01.584949: step 50940, loss = 5.07 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 15:04:06.336140: step 50950, loss = 5.30 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 15:04:11.482703: step 50960, loss = 4.97 (184.7 examples/sec; 0.693 sec/batch)
2016-07-15 15:04:18.016190: step 50970, loss = 5.14 (206.6 examples/sec; 0.619 sec/batch)
2016-07-15 15:04:23.326078: step 50980, loss = 5.14 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 15:04:28.918942: step 50990, loss = 5.06 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 15:04:33.725116: step 51000, loss = 5.08 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 15:04:39.977760: step 51010, loss = 5.05 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 15:04:46.473069: step 51020, loss = 4.90 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 15:04:51.580510: step 51030, loss = 5.32 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 15:04:57.157888: step 51040, loss = 5.29 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 15:05:02.951029: step 51050, loss = 4.93 (232.5 examples/sec; 0.551 sec/batch)
2016-07-15 15:05:08.236925: step 51060, loss = 5.00 (200.8 examples/sec; 0.637 sec/batch)
2016-07-15 15:05:13.676140: step 51070, loss = 5.10 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 15:05:18.468441: step 51080, loss = 5.29 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 15:05:23.589478: step 51090, loss = 5.28 (187.0 examples/sec; 0.685 sec/batch)
2016-07-15 15:05:30.104670: step 51100, loss = 5.12 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 15:05:36.088765: step 51110, loss = 5.33 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 15:05:40.894497: step 51120, loss = 5.30 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 15:05:46.943965: step 51130, loss = 4.92 (191.8 examples/sec; 0.667 sec/batch)
2016-07-15 15:05:52.718903: step 51140, loss = 4.96 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 15:05:57.524787: step 51150, loss = 4.83 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 15:06:02.370304: step 51160, loss = 5.26 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 15:06:08.904421: step 51170, loss = 5.10 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 15:06:14.199471: step 51180, loss = 5.20 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 15:06:18.918554: step 51190, loss = 5.24 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 15:06:24.334179: step 51200, loss = 5.03 (181.5 examples/sec; 0.705 sec/batch)
2016-07-15 15:06:32.068638: step 51210, loss = 5.33 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 15:06:36.827210: step 51220, loss = 5.27 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 15:06:41.403806: step 51230, loss = 5.34 (287.5 examples/sec; 0.445 sec/batch)
2016-07-15 15:06:46.056330: step 51240, loss = 5.02 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 15:06:51.856899: step 51250, loss = 5.05 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 15:06:57.359775: step 51260, loss = 4.95 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 15:07:02.536441: step 51270, loss = 4.76 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 15:07:08.349267: step 51280, loss = 4.91 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 15:07:13.898609: step 51290, loss = 5.06 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 15:07:19.140972: step 51300, loss = 5.31 (210.9 examples/sec; 0.607 sec/batch)
2016-07-15 15:07:26.021960: step 51310, loss = 5.19 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 15:07:31.826088: step 51320, loss = 4.87 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 15:07:36.667044: step 51330, loss = 5.03 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 15:07:41.511571: step 51340, loss = 5.06 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 15:07:47.787333: step 51350, loss = 5.37 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 15:07:53.338359: step 51360, loss = 5.09 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 15:07:59.118285: step 51370, loss = 5.01 (244.8 examples/sec; 0.523 sec/batch)
2016-07-15 15:08:04.450187: step 51380, loss = 5.39 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 15:08:09.872955: step 51390, loss = 4.93 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 15:08:15.611570: step 51400, loss = 5.15 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 15:08:22.318457: step 51410, loss = 5.33 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 15:08:27.447935: step 51420, loss = 5.08 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 15:08:33.134605: step 51430, loss = 4.97 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 15:08:38.927111: step 51440, loss = 5.18 (210.4 examples/sec; 0.609 sec/batch)
2016-07-15 15:08:44.066668: step 51450, loss = 5.14 (209.7 examples/sec; 0.610 sec/batch)
2016-07-15 15:08:49.577516: step 51460, loss = 5.04 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 15:08:55.384177: step 51470, loss = 5.08 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 15:09:00.236571: step 51480, loss = 5.35 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 15:09:05.009746: step 51490, loss = 5.07 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 15:09:09.754884: step 51500, loss = 5.07 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 15:09:15.994639: step 51510, loss = 4.73 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 15:09:22.529397: step 51520, loss = 5.24 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 15:09:27.711401: step 51530, loss = 4.96 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 15:09:33.282362: step 51540, loss = 4.95 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 15:09:39.104022: step 51550, loss = 4.98 (220.7 examples/sec; 0.580 sec/batch)
2016-07-15 15:09:43.947894: step 51560, loss = 5.34 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 15:09:48.627195: step 51570, loss = 5.21 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 15:09:53.268173: step 51580, loss = 5.24 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 15:09:58.994450: step 51590, loss = 4.88 (211.0 examples/sec; 0.607 sec/batch)
2016-07-15 15:10:04.183108: step 51600, loss = 5.09 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 15:10:10.921835: step 51610, loss = 5.03 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 15:10:16.722706: step 51620, loss = 5.23 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 15:10:22.416300: step 51630, loss = 5.18 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 15:10:27.599411: step 51640, loss = 5.11 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 15:10:33.200701: step 51650, loss = 5.22 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 15:10:39.042755: step 51660, loss = 5.33 (211.4 examples/sec; 0.606 sec/batch)
2016-07-15 15:10:43.859924: step 51670, loss = 5.31 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 15:10:48.633090: step 51680, loss = 5.11 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 15:10:54.590932: step 51690, loss = 5.09 (190.3 examples/sec; 0.673 sec/batch)
2016-07-15 15:11:00.466883: step 51700, loss = 5.52 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 15:11:06.226332: step 51710, loss = 4.91 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 15:11:11.053284: step 51720, loss = 5.02 (250.0 examples/sec; 0.512 sec/batch)
2016-07-15 15:11:17.637832: step 51730, loss = 5.12 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 15:11:22.870569: step 51740, loss = 4.90 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 15:11:27.597140: step 51750, loss = 5.12 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 15:11:33.072842: step 51760, loss = 5.24 (192.6 examples/sec; 0.665 sec/batch)
2016-07-15 15:11:39.426959: step 51770, loss = 5.42 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 15:11:44.294200: step 51780, loss = 4.88 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 15:11:49.086017: step 51790, loss = 4.95 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 15:11:55.124291: step 51800, loss = 5.03 (192.5 examples/sec; 0.665 sec/batch)
2016-07-15 15:12:02.204993: step 51810, loss = 5.13 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 15:12:07.989937: step 51820, loss = 4.83 (253.7 examples/sec; 0.505 sec/batch)
2016-07-15 15:12:12.798526: step 51830, loss = 5.29 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 15:12:17.633773: step 51840, loss = 5.26 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 15:12:23.755305: step 51850, loss = 5.05 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 15:12:29.364491: step 51860, loss = 5.10 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 15:12:34.170376: step 51870, loss = 4.95 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 15:12:39.027184: step 51880, loss = 5.00 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 15:12:43.744208: step 51890, loss = 4.82 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 15:12:49.285289: step 51900, loss = 5.11 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 15:12:55.703837: step 51910, loss = 4.86 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 15:13:00.841796: step 51920, loss = 5.25 (210.9 examples/sec; 0.607 sec/batch)
2016-07-15 15:13:06.209780: step 51930, loss = 5.30 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 15:13:10.939421: step 51940, loss = 5.01 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 15:13:16.210459: step 51950, loss = 5.10 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 15:13:22.701921: step 51960, loss = 4.85 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 15:13:27.925495: step 51970, loss = 4.94 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 15:13:33.519167: step 51980, loss = 5.00 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 15:13:38.327413: step 51990, loss = 4.97 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 15:13:43.303588: step 52000, loss = 5.18 (232.5 examples/sec; 0.550 sec/batch)
2016-07-15 15:13:51.396715: step 52010, loss = 5.05 (231.9 examples/sec; 0.552 sec/batch)
2016-07-15 15:13:56.342425: step 52020, loss = 5.05 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 15:14:01.101323: step 52030, loss = 5.05 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 15:14:05.921535: step 52040, loss = 5.08 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 15:14:10.790853: step 52050, loss = 5.41 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 15:14:17.276817: step 52060, loss = 5.17 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 15:14:22.600681: step 52070, loss = 4.75 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 15:14:27.351405: step 52080, loss = 5.41 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 15:14:32.244823: step 52090, loss = 4.84 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 15:14:37.022277: step 52100, loss = 5.10 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 15:14:42.752573: step 52110, loss = 5.23 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 15:14:47.925628: step 52120, loss = 4.87 (189.8 examples/sec; 0.674 sec/batch)
2016-07-15 15:14:54.465897: step 52130, loss = 5.01 (199.5 examples/sec; 0.642 sec/batch)
2016-07-15 15:14:59.584550: step 52140, loss = 5.11 (217.3 examples/sec; 0.589 sec/batch)
2016-07-15 15:15:05.271360: step 52150, loss = 5.00 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 15:15:11.043025: step 52160, loss = 5.19 (199.3 examples/sec; 0.642 sec/batch)
2016-07-15 15:15:16.157740: step 52170, loss = 5.15 (209.2 examples/sec; 0.612 sec/batch)
2016-07-15 15:15:21.733618: step 52180, loss = 5.08 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 15:15:26.503488: step 52190, loss = 5.33 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 15:15:31.461571: step 52200, loss = 5.19 (224.4 examples/sec; 0.570 sec/batch)
2016-07-15 15:15:39.626299: step 52210, loss = 4.95 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 15:15:44.486504: step 52220, loss = 5.00 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 15:15:49.292861: step 52230, loss = 4.94 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 15:15:55.396134: step 52240, loss = 5.10 (193.2 examples/sec; 0.663 sec/batch)
2016-07-15 15:16:01.122664: step 52250, loss = 5.20 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 15:16:06.838662: step 52260, loss = 5.03 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 15:16:12.020619: step 52270, loss = 4.93 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 15:16:17.620109: step 52280, loss = 5.26 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 15:16:23.365691: step 52290, loss = 5.00 (215.9 examples/sec; 0.593 sec/batch)
2016-07-15 15:16:28.542669: step 52300, loss = 5.15 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 15:16:35.258287: step 52310, loss = 4.97 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 15:16:39.988718: step 52320, loss = 5.12 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 15:16:44.788245: step 52330, loss = 5.08 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 15:16:49.602581: step 52340, loss = 5.17 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 15:16:55.779176: step 52350, loss = 5.37 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 15:17:01.437914: step 52360, loss = 5.03 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 15:17:07.230879: step 52370, loss = 5.34 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 15:17:12.150213: step 52380, loss = 5.03 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 15:17:16.983905: step 52390, loss = 5.28 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 15:17:22.953226: step 52400, loss = 5.34 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 15:17:30.086479: step 52410, loss = 5.00 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 15:17:35.906301: step 52420, loss = 4.86 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 15:17:41.219177: step 52430, loss = 4.72 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 15:17:46.548638: step 52440, loss = 4.92 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 15:17:52.323338: step 52450, loss = 5.02 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 15:17:57.131769: step 52460, loss = 5.46 (283.7 examples/sec; 0.451 sec/batch)
2016-07-15 15:18:01.996473: step 52470, loss = 5.14 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 15:18:08.252918: step 52480, loss = 5.15 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 15:18:13.873462: step 52490, loss = 4.98 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 15:18:18.656712: step 52500, loss = 5.37 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 15:18:25.086575: step 52510, loss = 5.23 (188.6 examples/sec; 0.679 sec/batch)
2016-07-15 15:18:31.483722: step 52520, loss = 4.99 (225.9 examples/sec; 0.567 sec/batch)
2016-07-15 15:18:36.345183: step 52530, loss = 4.98 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 15:18:41.099789: step 52540, loss = 5.30 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 15:18:47.056791: step 52550, loss = 4.81 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 15:18:52.928226: step 52560, loss = 5.17 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 15:18:57.738637: step 52570, loss = 5.10 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 15:19:02.614926: step 52580, loss = 5.05 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 15:19:08.988616: step 52590, loss = 5.09 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 15:19:14.425334: step 52600, loss = 5.39 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 15:19:21.210700: step 52610, loss = 5.16 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 15:19:26.875240: step 52620, loss = 5.01 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 15:19:31.884969: step 52630, loss = 4.98 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 15:19:36.630702: step 52640, loss = 5.25 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 15:19:42.347833: step 52650, loss = 5.03 (188.5 examples/sec; 0.679 sec/batch)
2016-07-15 15:19:48.389168: step 52660, loss = 4.83 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 15:19:53.213602: step 52670, loss = 5.11 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 15:19:57.970316: step 52680, loss = 5.08 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 15:20:04.161963: step 52690, loss = 4.99 (209.3 examples/sec; 0.612 sec/batch)
2016-07-15 15:20:09.790585: step 52700, loss = 4.96 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 15:20:15.535963: step 52710, loss = 5.05 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 15:20:20.380177: step 52720, loss = 5.37 (282.6 examples/sec; 0.453 sec/batch)
2016-07-15 15:20:25.139540: step 52730, loss = 5.13 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 15:20:31.024442: step 52740, loss = 5.19 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 15:20:36.951678: step 52750, loss = 4.94 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 15:20:41.764557: step 52760, loss = 5.03 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 15:20:46.596674: step 52770, loss = 5.10 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 15:20:52.933301: step 52780, loss = 5.06 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 15:20:58.377487: step 52790, loss = 5.15 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 15:21:03.092800: step 52800, loss = 5.29 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 15:21:09.569226: step 52810, loss = 5.02 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 15:21:15.861612: step 52820, loss = 5.23 (247.2 examples/sec; 0.518 sec/batch)
2016-07-15 15:21:21.131924: step 52830, loss = 5.24 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 15:21:26.560564: step 52840, loss = 4.98 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 15:21:31.232538: step 52850, loss = 4.99 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 15:21:36.412968: step 52860, loss = 5.38 (186.5 examples/sec; 0.686 sec/batch)
2016-07-15 15:21:42.938171: step 52870, loss = 5.12 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 15:21:47.982648: step 52880, loss = 5.35 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 15:21:52.763187: step 52890, loss = 5.23 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 15:21:58.519295: step 52900, loss = 5.19 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 15:22:05.823765: step 52910, loss = 5.06 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 15:22:11.612795: step 52920, loss = 5.16 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 15:22:16.412361: step 52930, loss = 5.18 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 15:22:21.223425: step 52940, loss = 4.91 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 15:22:27.073038: step 52950, loss = 5.02 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 15:22:32.962624: step 52960, loss = 4.97 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 15:22:38.502497: step 52970, loss = 5.26 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 15:22:43.659320: step 52980, loss = 5.10 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 15:22:49.390903: step 52990, loss = 5.42 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 15:22:55.082720: step 53000, loss = 5.14 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 15:23:01.175041: step 53010, loss = 5.13 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 15:23:05.823451: step 53020, loss = 5.06 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 15:23:10.471897: step 53030, loss = 4.85 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 15:23:16.232686: step 53040, loss = 5.09 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 15:23:21.644286: step 53050, loss = 4.92 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 15:23:26.948139: step 53060, loss = 5.15 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 15:23:32.740388: step 53070, loss = 5.11 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 15:23:37.503424: step 53080, loss = 5.24 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 15:23:42.335792: step 53090, loss = 4.72 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 15:23:48.571082: step 53100, loss = 4.88 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 15:23:55.394940: step 53110, loss = 5.04 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 15:24:01.197251: step 53120, loss = 4.80 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 15:24:06.011954: step 53130, loss = 5.15 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 15:24:10.823780: step 53140, loss = 4.97 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 15:24:17.197707: step 53150, loss = 4.96 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 15:24:22.740263: step 53160, loss = 5.14 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 15:24:28.498592: step 53170, loss = 4.87 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 15:24:33.871494: step 53180, loss = 5.12 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 15:24:39.236313: step 53190, loss = 4.98 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 15:24:44.988974: step 53200, loss = 5.05 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 15:24:51.784810: step 53210, loss = 5.33 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 15:24:56.683554: step 53220, loss = 5.03 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 15:25:01.476866: step 53230, loss = 4.71 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 15:25:07.382678: step 53240, loss = 5.40 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 15:25:13.264431: step 53250, loss = 4.87 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 15:25:18.052388: step 53260, loss = 5.15 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 15:25:22.897110: step 53270, loss = 5.29 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 15:25:27.607999: step 53280, loss = 4.91 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 15:25:32.807336: step 53290, loss = 4.89 (186.0 examples/sec; 0.688 sec/batch)
2016-07-15 15:25:39.326095: step 53300, loss = 4.84 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 15:25:45.854910: step 53310, loss = 5.08 (196.5 examples/sec; 0.651 sec/batch)
2016-07-15 15:25:51.146018: step 53320, loss = 5.10 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 15:25:55.880146: step 53330, loss = 4.98 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 15:26:01.196622: step 53340, loss = 5.12 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 15:26:07.639102: step 53350, loss = 5.13 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 15:26:12.809425: step 53360, loss = 5.26 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 15:26:18.368479: step 53370, loss = 5.07 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 15:26:24.185955: step 53380, loss = 4.94 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 15:26:29.019386: step 53390, loss = 4.80 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 15:26:33.716312: step 53400, loss = 5.13 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 15:26:39.461489: step 53410, loss = 5.06 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 15:26:44.387789: step 53420, loss = 5.07 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 15:26:49.066654: step 53430, loss = 4.98 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 15:26:53.712270: step 53440, loss = 5.15 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 15:26:59.280086: step 53450, loss = 4.70 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 15:27:04.265729: step 53460, loss = 4.96 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 15:27:09.029186: step 53470, loss = 5.12 (251.9 examples/sec; 0.508 sec/batch)
2016-07-15 15:27:14.764117: step 53480, loss = 4.89 (186.4 examples/sec; 0.687 sec/batch)
2016-07-15 15:27:20.843059: step 53490, loss = 5.13 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 15:27:25.650313: step 53500, loss = 4.95 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 15:27:31.463633: step 53510, loss = 5.35 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 15:27:37.952702: step 53520, loss = 5.05 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 15:27:43.287504: step 53530, loss = 5.12 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 15:27:48.029253: step 53540, loss = 4.97 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 15:27:53.355912: step 53550, loss = 5.23 (183.2 examples/sec; 0.699 sec/batch)
2016-07-15 15:27:59.867159: step 53560, loss = 5.22 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 15:28:04.754596: step 53570, loss = 4.98 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 15:28:09.566534: step 53580, loss = 4.88 (251.1 examples/sec; 0.510 sec/batch)
2016-07-15 15:28:15.529687: step 53590, loss = 4.94 (184.6 examples/sec; 0.693 sec/batch)
2016-07-15 15:28:21.414353: step 53600, loss = 4.89 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 15:28:27.191471: step 53610, loss = 5.03 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 15:28:32.045208: step 53620, loss = 5.21 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 15:28:36.786921: step 53630, loss = 5.11 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 15:28:41.619945: step 53640, loss = 4.97 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 15:28:46.390938: step 53650, loss = 4.92 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 15:28:51.231352: step 53660, loss = 5.14 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 15:28:56.170376: step 53670, loss = 5.20 (235.6 examples/sec; 0.543 sec/batch)
2016-07-15 15:29:02.745053: step 53680, loss = 5.08 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 15:29:07.925440: step 53690, loss = 4.79 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 15:29:12.693049: step 53700, loss = 5.16 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 15:29:19.708315: step 53710, loss = 4.90 (188.1 examples/sec; 0.680 sec/batch)
2016-07-15 15:29:25.549956: step 53720, loss = 5.13 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 15:29:31.127969: step 53730, loss = 4.96 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 15:29:36.283158: step 53740, loss = 5.15 (223.3 examples/sec; 0.573 sec/batch)
2016-07-15 15:29:41.995586: step 53750, loss = 5.09 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 15:29:46.793830: step 53760, loss = 5.36 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 15:29:51.620692: step 53770, loss = 4.76 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 15:29:56.371703: step 53780, loss = 5.02 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 15:30:01.844033: step 53790, loss = 5.18 (186.0 examples/sec; 0.688 sec/batch)
2016-07-15 15:30:08.243390: step 53800, loss = 4.92 (222.1 examples/sec; 0.576 sec/batch)
2016-07-15 15:30:14.752607: step 53810, loss = 4.95 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 15:30:19.908205: step 53820, loss = 4.79 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 15:30:24.562020: step 53830, loss = 5.10 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 15:30:30.099459: step 53840, loss = 5.07 (192.3 examples/sec; 0.666 sec/batch)
2016-07-15 15:30:36.398608: step 53850, loss = 4.83 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 15:30:41.259655: step 53860, loss = 5.00 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 15:30:46.005351: step 53870, loss = 4.89 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 15:30:50.785352: step 53880, loss = 5.17 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 15:30:55.696011: step 53890, loss = 5.19 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 15:31:00.451308: step 53900, loss = 4.63 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 15:31:06.362537: step 53910, loss = 5.00 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 15:31:11.191530: step 53920, loss = 4.96 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 15:31:17.490514: step 53930, loss = 5.05 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 15:31:22.919353: step 53940, loss = 5.04 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 15:31:28.682611: step 53950, loss = 5.06 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 15:31:33.980919: step 53960, loss = 5.04 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 15:31:39.304368: step 53970, loss = 5.02 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 15:31:45.083436: step 53980, loss = 5.01 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 15:31:49.913138: step 53990, loss = 5.00 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 15:31:54.727318: step 54000, loss = 5.25 (253.2 examples/sec; 0.505 sec/batch)
2016-07-15 15:32:02.270639: step 54010, loss = 5.08 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 15:32:07.551622: step 54020, loss = 5.20 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 15:32:12.259308: step 54030, loss = 4.97 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 15:32:17.556623: step 54040, loss = 5.08 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 15:32:24.046265: step 54050, loss = 5.11 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 15:32:28.954125: step 54060, loss = 5.18 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 15:32:33.685412: step 54070, loss = 5.14 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 15:32:38.517895: step 54080, loss = 5.00 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 15:32:43.377230: step 54090, loss = 5.33 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 15:32:49.805570: step 54100, loss = 4.99 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 15:32:56.525893: step 54110, loss = 5.04 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 15:33:01.287975: step 54120, loss = 4.89 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 15:33:06.148787: step 54130, loss = 5.20 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 15:33:10.983601: step 54140, loss = 5.06 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 15:33:17.244337: step 54150, loss = 5.30 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 15:33:22.772936: step 54160, loss = 5.21 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 15:33:28.525470: step 54170, loss = 4.72 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 15:33:33.755211: step 54180, loss = 5.01 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 15:33:39.167576: step 54190, loss = 4.93 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 15:33:43.883329: step 54200, loss = 4.79 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 15:33:50.302161: step 54210, loss = 5.01 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 15:33:56.562674: step 54220, loss = 5.19 (253.4 examples/sec; 0.505 sec/batch)
2016-07-15 15:34:01.817894: step 54230, loss = 5.06 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 15:34:07.284255: step 54240, loss = 5.27 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 15:34:13.083718: step 54250, loss = 4.58 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 15:34:18.514502: step 54260, loss = 4.85 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 15:34:23.814793: step 54270, loss = 5.03 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 15:34:29.553683: step 54280, loss = 5.28 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 15:34:35.126216: step 54290, loss = 4.84 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 15:34:40.262086: step 54300, loss = 4.94 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 15:34:46.028623: step 54310, loss = 4.95 (254.0 examples/sec; 0.504 sec/batch)
2016-07-15 15:34:51.988260: step 54320, loss = 5.12 (186.3 examples/sec; 0.687 sec/batch)
2016-07-15 15:34:57.863123: step 54330, loss = 4.85 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 15:35:03.487550: step 54340, loss = 4.99 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 15:35:08.553394: step 54350, loss = 4.88 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 15:35:13.287554: step 54360, loss = 4.88 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 15:35:18.077298: step 54370, loss = 5.21 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 15:35:22.851002: step 54380, loss = 5.04 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 15:35:29.083349: step 54390, loss = 4.86 (200.8 examples/sec; 0.638 sec/batch)
2016-07-15 15:35:34.740105: step 54400, loss = 4.95 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 15:35:40.533798: step 54410, loss = 4.91 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 15:35:45.875134: step 54420, loss = 5.08 (189.5 examples/sec; 0.676 sec/batch)
2016-07-15 15:35:52.369656: step 54430, loss = 5.15 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 15:35:57.273890: step 54440, loss = 4.91 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 15:36:02.029587: step 54450, loss = 4.77 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 15:36:07.966679: step 54460, loss = 4.89 (185.6 examples/sec; 0.690 sec/batch)
2016-07-15 15:36:13.823504: step 54470, loss = 4.97 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 15:36:18.669101: step 54480, loss = 4.81 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 15:36:23.521842: step 54490, loss = 4.93 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 15:36:29.922772: step 54500, loss = 5.11 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 15:36:36.641677: step 54510, loss = 5.15 (234.5 examples/sec; 0.546 sec/batch)
2016-07-15 15:36:42.372100: step 54520, loss = 5.07 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 15:36:47.153165: step 54530, loss = 4.97 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 15:36:52.023135: step 54540, loss = 4.83 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 15:36:58.559977: step 54550, loss = 4.81 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 15:37:03.935234: step 54560, loss = 5.11 (251.2 examples/sec; 0.510 sec/batch)
2016-07-15 15:37:09.703339: step 54570, loss = 5.02 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 15:37:15.229299: step 54580, loss = 4.87 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 15:37:20.385449: step 54590, loss = 4.84 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 15:37:25.114398: step 54600, loss = 4.90 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 15:37:32.053626: step 54610, loss = 4.82 (185.0 examples/sec; 0.692 sec/batch)
2016-07-15 15:37:37.942194: step 54620, loss = 5.02 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 15:37:42.762000: step 54630, loss = 4.99 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 15:37:47.589164: step 54640, loss = 4.98 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 15:37:52.276909: step 54650, loss = 4.93 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 15:37:57.203882: step 54660, loss = 5.04 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 15:38:01.941506: step 54670, loss = 4.82 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 15:38:06.736528: step 54680, loss = 4.80 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 15:38:11.596003: step 54690, loss = 5.05 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 15:38:16.352481: step 54700, loss = 4.65 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 15:38:22.334352: step 54710, loss = 5.31 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 15:38:27.174078: step 54720, loss = 5.14 (251.4 examples/sec; 0.509 sec/batch)
2016-07-15 15:38:33.386742: step 54730, loss = 4.72 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 15:38:38.978556: step 54740, loss = 4.68 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 15:38:44.804184: step 54750, loss = 4.79 (242.7 examples/sec; 0.527 sec/batch)
2016-07-15 15:38:50.104941: step 54760, loss = 4.82 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 15:38:55.565002: step 54770, loss = 4.89 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 15:39:01.346781: step 54780, loss = 4.91 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 15:39:06.157144: step 54790, loss = 4.92 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 15:39:10.944060: step 54800, loss = 4.95 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 15:39:16.674101: step 54810, loss = 4.84 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 15:39:21.554480: step 54820, loss = 5.00 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 15:39:26.348726: step 54830, loss = 5.05 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 15:39:32.369628: step 54840, loss = 5.15 (184.9 examples/sec; 0.692 sec/batch)
2016-07-15 15:39:38.265205: step 54850, loss = 4.79 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 15:39:43.967190: step 54860, loss = 5.07 (192.3 examples/sec; 0.666 sec/batch)
2016-07-15 15:39:48.946033: step 54870, loss = 4.87 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 15:39:53.653996: step 54880, loss = 4.85 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 15:39:59.404799: step 54890, loss = 5.03 (190.3 examples/sec; 0.673 sec/batch)
2016-07-15 15:40:05.496238: step 54900, loss = 5.02 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 15:40:11.332217: step 54910, loss = 4.90 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 15:40:16.170521: step 54920, loss = 5.05 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 15:40:22.675075: step 54930, loss = 4.86 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 15:40:27.975020: step 54940, loss = 4.90 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 15:40:32.678324: step 54950, loss = 5.17 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 15:40:37.965897: step 54960, loss = 5.00 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 15:40:44.438854: step 54970, loss = 4.99 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 15:40:49.587116: step 54980, loss = 5.01 (209.5 examples/sec; 0.611 sec/batch)
2016-07-15 15:40:55.160984: step 54990, loss = 4.74 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 15:41:00.926571: step 55000, loss = 4.76 (244.3 examples/sec; 0.524 sec/batch)
2016-07-15 15:41:06.812459: step 55010, loss = 4.83 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 15:41:11.648404: step 55020, loss = 4.85 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 15:41:16.407874: step 55030, loss = 4.91 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 15:41:21.603175: step 55040, loss = 5.18 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 15:41:28.130587: step 55050, loss = 5.25 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 15:41:33.073454: step 55060, loss = 4.71 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 15:41:37.797940: step 55070, loss = 5.09 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 15:41:43.538121: step 55080, loss = 4.96 (184.3 examples/sec; 0.695 sec/batch)
2016-07-15 15:41:49.611634: step 55090, loss = 5.00 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 15:41:54.395901: step 55100, loss = 5.08 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 15:42:00.167190: step 55110, loss = 5.16 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 15:42:04.939678: step 55120, loss = 5.06 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 15:42:10.248765: step 55130, loss = 5.09 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 15:42:16.747474: step 55140, loss = 5.04 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 15:42:21.954009: step 55150, loss = 4.91 (209.0 examples/sec; 0.612 sec/batch)
2016-07-15 15:42:27.422962: step 55160, loss = 5.12 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 15:42:33.196078: step 55170, loss = 5.13 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 15:42:38.104371: step 55180, loss = 5.26 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 15:42:42.887482: step 55190, loss = 5.23 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 15:42:48.984652: step 55200, loss = 4.98 (197.2 examples/sec; 0.649 sec/batch)
2016-07-15 15:42:55.977539: step 55210, loss = 5.01 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 15:43:01.753245: step 55220, loss = 5.01 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 15:43:06.525987: step 55230, loss = 5.10 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 15:43:11.416232: step 55240, loss = 5.11 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 15:43:17.601704: step 55250, loss = 5.14 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 15:43:23.230541: step 55260, loss = 4.81 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 15:43:29.065948: step 55270, loss = 4.81 (241.4 examples/sec; 0.530 sec/batch)
2016-07-15 15:43:34.349829: step 55280, loss = 4.81 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 15:43:39.746265: step 55290, loss = 5.03 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 15:43:44.514557: step 55300, loss = 5.10 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 15:43:51.201980: step 55310, loss = 5.25 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 15:43:57.443643: step 55320, loss = 5.10 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 15:44:02.815954: step 55330, loss = 4.76 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 15:44:08.168692: step 55340, loss = 4.89 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 15:44:12.817163: step 55350, loss = 4.95 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 15:44:17.716631: step 55360, loss = 4.68 (282.6 examples/sec; 0.453 sec/batch)
2016-07-15 15:44:22.493210: step 55370, loss = 4.79 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 15:44:28.385352: step 55380, loss = 4.92 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 15:44:34.299460: step 55390, loss = 5.10 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 15:44:39.111830: step 55400, loss = 4.99 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 15:44:45.072020: step 55410, loss = 4.64 (228.7 examples/sec; 0.560 sec/batch)
2016-07-15 15:44:51.652096: step 55420, loss = 4.85 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 15:44:56.799259: step 55430, loss = 4.94 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 15:45:01.552496: step 55440, loss = 4.99 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 15:45:07.088330: step 55450, loss = 4.89 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 15:45:13.376717: step 55460, loss = 4.94 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 15:45:18.270265: step 55470, loss = 4.93 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 15:45:23.065778: step 55480, loss = 5.26 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 15:45:29.140115: step 55490, loss = 4.79 (194.6 examples/sec; 0.658 sec/batch)
2016-07-15 15:45:34.860148: step 55500, loss = 5.15 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 15:45:40.657574: step 55510, loss = 4.94 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 15:45:45.818794: step 55520, loss = 4.99 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 15:45:52.341112: step 55530, loss = 5.09 (209.9 examples/sec; 0.610 sec/batch)
2016-07-15 15:45:57.351326: step 55540, loss = 4.95 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 15:46:02.062813: step 55550, loss = 4.91 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 15:46:06.860448: step 55560, loss = 5.05 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 15:46:11.699671: step 55570, loss = 5.05 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 15:46:18.004951: step 55580, loss = 4.94 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 15:46:23.518035: step 55590, loss = 5.26 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 15:46:29.270895: step 55600, loss = 4.90 (249.4 examples/sec; 0.513 sec/batch)
2016-07-15 15:46:35.104530: step 55610, loss = 4.76 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 15:46:39.916408: step 55620, loss = 4.67 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 15:46:44.657449: step 55630, loss = 4.91 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 15:46:49.537647: step 55640, loss = 4.75 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 15:46:54.271203: step 55650, loss = 4.95 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 15:47:00.108792: step 55660, loss = 4.93 (176.0 examples/sec; 0.727 sec/batch)
2016-07-15 15:47:06.134986: step 55670, loss = 5.24 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 15:47:10.936700: step 55680, loss = 5.04 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 15:47:15.723806: step 55690, loss = 5.03 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 15:47:22.013831: step 55700, loss = 5.05 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 15:47:28.754730: step 55710, loss = 4.74 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 15:47:33.476425: step 55720, loss = 5.02 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 15:47:38.338012: step 55730, loss = 5.00 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 15:47:43.150944: step 55740, loss = 4.96 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 15:47:49.259852: step 55750, loss = 4.76 (193.1 examples/sec; 0.663 sec/batch)
2016-07-15 15:47:55.016477: step 55760, loss = 5.05 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 15:47:59.763393: step 55770, loss = 4.95 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 15:48:04.567505: step 55780, loss = 4.98 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 15:48:11.027037: step 55790, loss = 4.93 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 15:48:16.358216: step 55800, loss = 4.94 (258.3 examples/sec; 0.495 sec/batch)
2016-07-15 15:48:22.112411: step 55810, loss = 4.85 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 15:48:26.970304: step 55820, loss = 4.98 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 15:48:31.756083: step 55830, loss = 4.71 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 15:48:37.980680: step 55840, loss = 4.74 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 15:48:43.532031: step 55850, loss = 4.82 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 15:48:48.331456: step 55860, loss = 4.66 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 15:48:53.285809: step 55870, loss = 4.72 (226.4 examples/sec; 0.565 sec/batch)
2016-07-15 15:48:59.879438: step 55880, loss = 5.17 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 15:49:05.040877: step 55890, loss = 4.81 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 15:49:09.828287: step 55900, loss = 5.02 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 15:49:16.828986: step 55910, loss = 4.72 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 15:49:22.681311: step 55920, loss = 4.90 (253.9 examples/sec; 0.504 sec/batch)
2016-07-15 15:49:27.494638: step 55930, loss = 4.84 (275.0 examples/sec; 0.466 sec/batch)
2016-07-15 15:49:32.276492: step 55940, loss = 4.81 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 15:49:38.640726: step 55950, loss = 4.83 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 15:49:44.056463: step 55960, loss = 4.82 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 15:49:48.801105: step 55970, loss = 5.00 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 15:49:53.963701: step 55980, loss = 5.14 (185.4 examples/sec; 0.690 sec/batch)
2016-07-15 15:50:00.471245: step 55990, loss = 4.94 (201.1 examples/sec; 0.636 sec/batch)
2016-07-15 15:50:05.535706: step 56000, loss = 5.09 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 15:50:11.213576: step 56010, loss = 4.77 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 15:50:17.294134: step 56020, loss = 4.96 (196.9 examples/sec; 0.650 sec/batch)
2016-07-15 15:50:23.032284: step 56030, loss = 4.77 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 15:50:27.807449: step 56040, loss = 4.84 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 15:50:32.638042: step 56050, loss = 4.92 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 15:50:37.357044: step 56060, loss = 5.03 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 15:50:42.752998: step 56070, loss = 4.98 (182.3 examples/sec; 0.702 sec/batch)
2016-07-15 15:50:49.210872: step 56080, loss = 5.28 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 15:50:54.455359: step 56090, loss = 4.81 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 15:50:59.893122: step 56100, loss = 4.74 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 15:51:06.730860: step 56110, loss = 5.02 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 15:51:11.485073: step 56120, loss = 5.08 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 15:51:16.343142: step 56130, loss = 4.95 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 15:51:22.713919: step 56140, loss = 4.84 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 15:51:28.188763: step 56150, loss = 4.72 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 15:51:33.049560: step 56160, loss = 4.54 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 15:51:37.753925: step 56170, loss = 4.91 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 15:51:42.933478: step 56180, loss = 4.75 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 15:51:48.337172: step 56190, loss = 5.01 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 15:51:54.143741: step 56200, loss = 5.04 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 15:52:00.009219: step 56210, loss = 5.09 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 15:52:04.882572: step 56220, loss = 4.86 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 15:52:11.397789: step 56230, loss = 4.98 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 15:52:16.717431: step 56240, loss = 4.95 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 15:52:21.537559: step 56250, loss = 4.89 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 15:52:26.933214: step 56260, loss = 4.89 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 15:52:33.363283: step 56270, loss = 4.80 (210.2 examples/sec; 0.609 sec/batch)
2016-07-15 15:52:38.549431: step 56280, loss = 4.82 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 15:52:44.047220: step 56290, loss = 4.87 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 15:52:49.842495: step 56300, loss = 5.14 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 15:52:55.644250: step 56310, loss = 5.00 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 15:53:00.511546: step 56320, loss = 5.20 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 15:53:06.909548: step 56330, loss = 4.89 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 15:53:12.321582: step 56340, loss = 5.29 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 15:53:18.096972: step 56350, loss = 5.23 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 15:53:23.527771: step 56360, loss = 4.94 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 15:53:28.801566: step 56370, loss = 4.91 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 15:53:33.528942: step 56380, loss = 5.03 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 15:53:38.936559: step 56390, loss = 4.72 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 15:53:45.374697: step 56400, loss = 4.64 (207.6 examples/sec; 0.617 sec/batch)
2016-07-15 15:53:51.866297: step 56410, loss = 4.95 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 15:53:57.087921: step 56420, loss = 4.77 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 15:54:02.865310: step 56430, loss = 4.72 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 15:54:07.629726: step 56440, loss = 4.82 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 15:54:12.403307: step 56450, loss = 5.09 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 15:54:18.703085: step 56460, loss = 5.05 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 15:54:24.211875: step 56470, loss = 5.17 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 15:54:28.946875: step 56480, loss = 4.88 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 15:54:33.797094: step 56490, loss = 5.04 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 15:54:38.572194: step 56500, loss = 5.01 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 15:54:45.793850: step 56510, loss = 4.95 (198.4 examples/sec; 0.645 sec/batch)
2016-07-15 15:54:51.467552: step 56520, loss = 4.55 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 15:54:57.189092: step 56530, loss = 4.57 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 15:55:02.156627: step 56540, loss = 4.87 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 15:55:06.879070: step 56550, loss = 5.00 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 15:55:11.728456: step 56560, loss = 4.97 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 15:55:16.581521: step 56570, loss = 4.86 (245.3 examples/sec; 0.522 sec/batch)
2016-07-15 15:55:22.952693: step 56580, loss = 4.71 (200.3 examples/sec; 0.639 sec/batch)
2016-07-15 15:55:28.382925: step 56590, loss = 4.81 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 15:55:33.111672: step 56600, loss = 5.02 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 15:55:39.588286: step 56610, loss = 4.86 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 15:55:45.862422: step 56620, loss = 4.97 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 15:55:50.736017: step 56630, loss = 4.87 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 15:55:55.497882: step 56640, loss = 5.00 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 15:56:01.551999: step 56650, loss = 4.61 (190.3 examples/sec; 0.673 sec/batch)
2016-07-15 15:56:07.259899: step 56660, loss = 5.06 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 15:56:12.960124: step 56670, loss = 4.97 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 15:56:18.183818: step 56680, loss = 4.80 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 15:56:23.813986: step 56690, loss = 5.18 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 15:56:29.584102: step 56700, loss = 5.02 (221.2 examples/sec; 0.579 sec/batch)
2016-07-15 15:56:36.057969: step 56710, loss = 4.66 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 15:56:41.238034: step 56720, loss = 5.05 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 15:56:47.023716: step 56730, loss = 4.91 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 15:56:51.818103: step 56740, loss = 4.77 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 15:56:56.688208: step 56750, loss = 5.17 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 15:57:03.062881: step 56760, loss = 4.78 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 15:57:08.531552: step 56770, loss = 4.92 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 15:57:14.337172: step 56780, loss = 4.93 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 15:57:19.744673: step 56790, loss = 5.09 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 15:57:25.042679: step 56800, loss = 4.69 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 15:57:31.948245: step 56810, loss = 4.83 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 15:57:36.749899: step 56820, loss = 5.05 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 15:57:41.595248: step 56830, loss = 5.15 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 15:57:48.169806: step 56840, loss = 5.05 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 15:57:53.394833: step 56850, loss = 5.07 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 15:57:58.131668: step 56860, loss = 4.98 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 15:58:03.647194: step 56870, loss = 4.55 (188.4 examples/sec; 0.680 sec/batch)
2016-07-15 15:58:09.933299: step 56880, loss = 4.97 (253.4 examples/sec; 0.505 sec/batch)
2016-07-15 15:58:15.247056: step 56890, loss = 4.95 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 15:58:20.642299: step 56900, loss = 4.73 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 15:58:26.402194: step 56910, loss = 4.75 (283.5 examples/sec; 0.452 sec/batch)
2016-07-15 15:58:31.032078: step 56920, loss = 4.94 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 15:58:36.537927: step 56930, loss = 4.89 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 15:58:41.686784: step 56940, loss = 4.96 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 15:58:47.407007: step 56950, loss = 4.96 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 15:58:52.235291: step 56960, loss = 4.75 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 15:58:57.068887: step 56970, loss = 4.94 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 15:59:01.800027: step 56980, loss = 4.67 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 15:59:07.183237: step 56990, loss = 4.86 (185.7 examples/sec; 0.689 sec/batch)
2016-07-15 15:59:13.638679: step 57000, loss = 5.02 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 15:59:19.456776: step 57010, loss = 4.82 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 15:59:24.235131: step 57020, loss = 4.64 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 15:59:30.500805: step 57030, loss = 4.82 (198.5 examples/sec; 0.645 sec/batch)
2016-07-15 15:59:36.063079: step 57040, loss = 5.16 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 15:59:40.827516: step 57050, loss = 4.84 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 15:59:45.762966: step 57060, loss = 4.91 (246.9 examples/sec; 0.518 sec/batch)
2016-07-15 15:59:52.354202: step 57070, loss = 4.77 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 15:59:57.558123: step 57080, loss = 4.79 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 16:00:02.295275: step 57090, loss = 4.75 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 16:00:07.761705: step 57100, loss = 4.87 (192.6 examples/sec; 0.664 sec/batch)
2016-07-15 16:00:14.196262: step 57110, loss = 4.91 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 16:00:19.351776: step 57120, loss = 4.85 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 16:00:24.768806: step 57130, loss = 4.88 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 16:00:30.526136: step 57140, loss = 4.98 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 16:00:35.893036: step 57150, loss = 5.01 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 16:00:41.218231: step 57160, loss = 5.02 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 16:00:46.964465: step 57170, loss = 4.80 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 16:00:52.497779: step 57180, loss = 4.82 (200.4 examples/sec; 0.639 sec/batch)
2016-07-15 16:00:57.726083: step 57190, loss = 5.27 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 16:01:03.478551: step 57200, loss = 4.83 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 16:01:10.265316: step 57210, loss = 4.81 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 16:01:15.124178: step 57220, loss = 4.99 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 16:01:19.894216: step 57230, loss = 5.05 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 16:01:26.041581: step 57240, loss = 5.25 (193.8 examples/sec; 0.661 sec/batch)
2016-07-15 16:01:31.788357: step 57250, loss = 4.71 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 16:01:37.530109: step 57260, loss = 4.82 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 16:01:42.628185: step 57270, loss = 5.13 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 16:01:48.237626: step 57280, loss = 4.76 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 16:01:53.050603: step 57290, loss = 4.95 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 16:01:58.021118: step 57300, loss = 5.05 (233.6 examples/sec; 0.548 sec/batch)
2016-07-15 16:02:06.146455: step 57310, loss = 4.70 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 16:02:11.031249: step 57320, loss = 4.49 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 16:02:15.791186: step 57330, loss = 4.73 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 16:02:21.875093: step 57340, loss = 4.78 (191.8 examples/sec; 0.668 sec/batch)
2016-07-15 16:02:27.633598: step 57350, loss = 4.77 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 16:02:32.409981: step 57360, loss = 4.92 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 16:02:37.246260: step 57370, loss = 5.05 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 16:02:41.956807: step 57380, loss = 4.74 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 16:02:47.269468: step 57390, loss = 5.07 (185.9 examples/sec; 0.688 sec/batch)
2016-07-15 16:02:53.719104: step 57400, loss = 4.93 (201.4 examples/sec; 0.635 sec/batch)
2016-07-15 16:02:59.570091: step 57410, loss = 4.87 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 16:03:04.450610: step 57420, loss = 4.89 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 16:03:10.662605: step 57430, loss = 4.87 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 16:03:16.222837: step 57440, loss = 4.87 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 16:03:20.992664: step 57450, loss = 4.71 (264.7 examples/sec; 0.483 sec/batch)
2016-07-15 16:03:25.879044: step 57460, loss = 5.05 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 16:03:32.482676: step 57470, loss = 4.76 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 16:03:37.695493: step 57480, loss = 5.00 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 16:03:42.453953: step 57490, loss = 4.97 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 16:03:47.967240: step 57500, loss = 5.09 (185.6 examples/sec; 0.690 sec/batch)
2016-07-15 16:03:55.603095: step 57510, loss = 5.22 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 16:04:01.303000: step 57520, loss = 4.97 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 16:04:06.337331: step 57530, loss = 4.86 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 16:04:11.057958: step 57540, loss = 5.00 (251.9 examples/sec; 0.508 sec/batch)
2016-07-15 16:04:16.833639: step 57550, loss = 4.80 (188.4 examples/sec; 0.680 sec/batch)
2016-07-15 16:04:22.936186: step 57560, loss = 4.66 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 16:04:27.857411: step 57570, loss = 5.05 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 16:04:32.690995: step 57580, loss = 5.09 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 16:04:39.038094: step 57590, loss = 5.00 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 16:04:44.558739: step 57600, loss = 4.82 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 16:04:50.222007: step 57610, loss = 5.03 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 16:04:55.136119: step 57620, loss = 5.10 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 16:04:59.968766: step 57630, loss = 5.16 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 16:05:04.754364: step 57640, loss = 4.84 (281.9 examples/sec; 0.454 sec/batch)
2016-07-15 16:05:09.573131: step 57650, loss = 4.90 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 16:05:14.279514: step 57660, loss = 4.90 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 16:05:19.666500: step 57670, loss = 4.87 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 16:05:26.061968: step 57680, loss = 5.01 (210.0 examples/sec; 0.610 sec/batch)
2016-07-15 16:05:31.279181: step 57690, loss = 4.86 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 16:05:36.716953: step 57700, loss = 4.90 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 16:05:42.442136: step 57710, loss = 4.86 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 16:05:47.910447: step 57720, loss = 5.04 (184.1 examples/sec; 0.695 sec/batch)
2016-07-15 16:05:54.257254: step 57730, loss = 4.60 (230.5 examples/sec; 0.555 sec/batch)
2016-07-15 16:05:59.474590: step 57740, loss = 4.61 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 16:06:04.964928: step 57750, loss = 4.80 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 16:06:09.698130: step 57760, loss = 4.98 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 16:06:14.826456: step 57770, loss = 5.28 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 16:06:21.327580: step 57780, loss = 4.87 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 16:06:26.345393: step 57790, loss = 4.86 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 16:06:31.110576: step 57800, loss = 5.09 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 16:06:38.308298: step 57810, loss = 4.59 (197.0 examples/sec; 0.650 sec/batch)
2016-07-15 16:06:44.064980: step 57820, loss = 5.11 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 16:06:48.799104: step 57830, loss = 4.97 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 16:06:53.660719: step 57840, loss = 4.65 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 16:06:58.379316: step 57850, loss = 5.20 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 16:07:03.862751: step 57860, loss = 5.00 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 16:07:10.237278: step 57870, loss = 4.80 (218.9 examples/sec; 0.585 sec/batch)
2016-07-15 16:07:15.406530: step 57880, loss = 4.80 (209.8 examples/sec; 0.610 sec/batch)
2016-07-15 16:07:20.833275: step 57890, loss = 4.87 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 16:07:26.652010: step 57900, loss = 4.70 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 16:07:33.284960: step 57910, loss = 5.03 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 16:07:38.466616: step 57920, loss = 4.79 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 16:07:43.248194: step 57930, loss = 4.84 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 16:07:49.050996: step 57940, loss = 4.90 (184.2 examples/sec; 0.695 sec/batch)
2016-07-15 16:07:55.146348: step 57950, loss = 5.19 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 16:08:00.593541: step 57960, loss = 4.92 (196.8 examples/sec; 0.650 sec/batch)
2016-07-15 16:08:05.812859: step 57970, loss = 4.79 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 16:08:11.610992: step 57980, loss = 5.05 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 16:08:16.430098: step 57990, loss = 4.87 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 16:08:21.303389: step 58000, loss = 5.01 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 16:08:26.986484: step 58010, loss = 4.99 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 16:08:31.801510: step 58020, loss = 5.00 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 16:08:36.582517: step 58030, loss = 4.60 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 16:08:41.412344: step 58040, loss = 4.95 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 16:08:46.372397: step 58050, loss = 4.87 (232.0 examples/sec; 0.552 sec/batch)
2016-07-15 16:08:52.935737: step 58060, loss = 4.58 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 16:08:58.137786: step 58070, loss = 4.95 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 16:09:02.900053: step 58080, loss = 4.91 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 16:09:08.429175: step 58090, loss = 4.91 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 16:09:14.677435: step 58100, loss = 4.91 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 16:09:21.370414: step 58110, loss = 5.02 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 16:09:26.511598: step 58120, loss = 4.88 (219.5 examples/sec; 0.583 sec/batch)
2016-07-15 16:09:32.180684: step 58130, loss = 4.90 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 16:09:36.901655: step 58140, loss = 4.66 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 16:09:41.733158: step 58150, loss = 4.70 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 16:09:48.242608: step 58160, loss = 5.02 (205.6 examples/sec; 0.622 sec/batch)
2016-07-15 16:09:53.553576: step 58170, loss = 4.89 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 16:09:58.233514: step 58180, loss = 4.91 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 16:10:03.575277: step 58190, loss = 4.84 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 16:10:10.082261: step 58200, loss = 4.99 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 16:10:15.940480: step 58210, loss = 4.87 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 16:10:20.683534: step 58220, loss = 4.89 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 16:10:25.446334: step 58230, loss = 4.71 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 16:10:30.285358: step 58240, loss = 4.85 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 16:10:36.865532: step 58250, loss = 4.85 (203.7 examples/sec; 0.629 sec/batch)
2016-07-15 16:10:42.048601: step 58260, loss = 4.81 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 16:10:47.823226: step 58270, loss = 4.68 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 16:10:53.462650: step 58280, loss = 4.69 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 16:10:58.640313: step 58290, loss = 4.79 (214.9 examples/sec; 0.596 sec/batch)
2016-07-15 16:11:04.362034: step 58300, loss = 4.76 (252.3 examples/sec; 0.507 sec/batch)
2016-07-15 16:11:11.200397: step 58310, loss = 4.94 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 16:11:16.048110: step 58320, loss = 5.03 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 16:11:20.842717: step 58330, loss = 4.75 (264.7 examples/sec; 0.483 sec/batch)
2016-07-15 16:11:26.997652: step 58340, loss = 4.71 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 16:11:32.663986: step 58350, loss = 5.15 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 16:11:37.421920: step 58360, loss = 4.86 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 16:11:42.260295: step 58370, loss = 5.12 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 16:11:47.019925: step 58380, loss = 4.75 (253.2 examples/sec; 0.506 sec/batch)
2016-07-15 16:11:52.579455: step 58390, loss = 5.05 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 16:11:58.838842: step 58400, loss = 5.01 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 16:12:05.502106: step 58410, loss = 4.70 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 16:12:10.640524: step 58420, loss = 4.66 (225.1 examples/sec; 0.569 sec/batch)
2016-07-15 16:12:16.386124: step 58430, loss = 5.18 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 16:12:22.092054: step 58440, loss = 4.67 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 16:12:27.236262: step 58450, loss = 4.88 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 16:12:32.861587: step 58460, loss = 5.02 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 16:12:37.643436: step 58470, loss = 4.82 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 16:12:42.589819: step 58480, loss = 4.92 (226.7 examples/sec; 0.565 sec/batch)
2016-07-15 16:12:49.163289: step 58490, loss = 4.96 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 16:12:54.357652: step 58500, loss = 4.83 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 16:13:00.070058: step 58510, loss = 4.73 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 16:13:05.965305: step 58520, loss = 4.90 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 16:13:11.889735: step 58530, loss = 5.24 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 16:13:16.686775: step 58540, loss = 4.62 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 16:13:21.537458: step 58550, loss = 5.08 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 16:13:26.240213: step 58560, loss = 4.80 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 16:13:31.462445: step 58570, loss = 4.86 (190.9 examples/sec; 0.671 sec/batch)
2016-07-15 16:13:37.970440: step 58580, loss = 5.00 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 16:13:43.155777: step 58590, loss = 4.96 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 16:13:48.783074: step 58600, loss = 4.75 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 16:13:54.541949: step 58610, loss = 4.74 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 16:13:59.832279: step 58620, loss = 4.93 (183.4 examples/sec; 0.698 sec/batch)
2016-07-15 16:14:06.531741: step 58630, loss = 4.69 (212.2 examples/sec; 0.603 sec/batch)
2016-07-15 16:14:11.764566: step 58640, loss = 5.00 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 16:14:17.279095: step 58650, loss = 4.98 (238.0 examples/sec; 0.538 sec/batch)
2016-07-15 16:14:21.987216: step 58660, loss = 5.01 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 16:14:26.816371: step 58670, loss = 4.92 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 16:14:31.542739: step 58680, loss = 4.85 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 16:14:37.216644: step 58690, loss = 4.86 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 16:14:43.352049: step 58700, loss = 4.80 (255.7 examples/sec; 0.500 sec/batch)
2016-07-15 16:14:49.179452: step 58710, loss = 4.69 (268.6 examples/sec; 0.476 sec/batch)
2016-07-15 16:14:54.014506: step 58720, loss = 4.69 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 16:15:00.500114: step 58730, loss = 4.92 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 16:15:05.819292: step 58740, loss = 5.11 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 16:15:10.557565: step 58750, loss = 4.89 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 16:15:15.388786: step 58760, loss = 4.94 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 16:15:20.168395: step 58770, loss = 4.74 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 16:15:26.169884: step 58780, loss = 4.92 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 16:15:31.997717: step 58790, loss = 4.92 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 16:15:36.802489: step 58800, loss = 4.94 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 16:15:42.533210: step 58810, loss = 4.66 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 16:15:49.116742: step 58820, loss = 4.87 (209.9 examples/sec; 0.610 sec/batch)
2016-07-15 16:15:54.291774: step 58830, loss = 4.79 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 16:16:00.107984: step 58840, loss = 4.79 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 16:16:04.859534: step 58850, loss = 4.91 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 16:16:09.726503: step 58860, loss = 4.77 (239.6 examples/sec; 0.534 sec/batch)
2016-07-15 16:16:16.092640: step 58870, loss = 5.01 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 16:16:21.560868: step 58880, loss = 4.71 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 16:16:26.318297: step 58890, loss = 4.85 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 16:16:31.146680: step 58900, loss = 5.00 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 16:16:36.890720: step 58910, loss = 4.78 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 16:16:43.013262: step 58920, loss = 5.19 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 16:16:48.753550: step 58930, loss = 5.09 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 16:16:54.513806: step 58940, loss = 4.91 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 16:16:59.419453: step 58950, loss = 4.81 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 16:17:04.181799: step 58960, loss = 5.11 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 16:17:10.050889: step 58970, loss = 4.91 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 16:17:16.007740: step 58980, loss = 4.85 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 16:17:20.840031: step 58990, loss = 4.78 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 16:17:25.662172: step 59000, loss = 4.81 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 16:17:31.405179: step 59010, loss = 5.22 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 16:17:37.043047: step 59020, loss = 4.87 (189.8 examples/sec; 0.674 sec/batch)
2016-07-15 16:17:43.251680: step 59030, loss = 4.72 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 16:17:48.619278: step 59040, loss = 4.86 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 16:17:53.969740: step 59050, loss = 4.88 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 16:17:59.847476: step 59060, loss = 4.59 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 16:18:05.436589: step 59070, loss = 4.78 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 16:18:10.625202: step 59080, loss = 4.71 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 16:18:15.317365: step 59090, loss = 4.80 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 16:18:20.237926: step 59100, loss = 5.03 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 16:18:26.097472: step 59110, loss = 5.04 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 16:18:30.773186: step 59120, loss = 4.85 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 16:18:36.088074: step 59130, loss = 4.69 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 16:18:42.538720: step 59140, loss = 5.12 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 16:18:47.437450: step 59150, loss = 4.83 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 16:18:52.234859: step 59160, loss = 4.81 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 16:18:58.091279: step 59170, loss = 4.70 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 16:19:04.038489: step 59180, loss = 4.75 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 16:19:09.561520: step 59190, loss = 4.99 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 16:19:14.725214: step 59200, loss = 4.86 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 16:19:21.819050: step 59210, loss = 4.90 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 16:19:27.622070: step 59220, loss = 4.61 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 16:19:32.431529: step 59230, loss = 4.90 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 16:19:37.232347: step 59240, loss = 4.92 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 16:19:41.990207: step 59250, loss = 4.86 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 16:19:46.950154: step 59260, loss = 4.93 (234.5 examples/sec; 0.546 sec/batch)
2016-07-15 16:19:53.563569: step 59270, loss = 5.03 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 16:19:58.747510: step 59280, loss = 4.96 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 16:20:04.493035: step 59290, loss = 5.10 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 16:20:10.054546: step 59300, loss = 4.84 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 16:20:16.735734: step 59310, loss = 4.83 (206.6 examples/sec; 0.619 sec/batch)
2016-07-15 16:20:22.063728: step 59320, loss = 4.93 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 16:20:27.857251: step 59330, loss = 4.94 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 16:20:33.342897: step 59340, loss = 4.91 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 16:20:38.535226: step 59350, loss = 4.78 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 16:20:44.340720: step 59360, loss = 4.94 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 16:20:49.976935: step 59370, loss = 4.33 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 16:20:55.130444: step 59380, loss = 4.88 (224.1 examples/sec; 0.571 sec/batch)
2016-07-15 16:21:00.835646: step 59390, loss = 4.59 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 16:21:06.557897: step 59400, loss = 4.46 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 16:21:12.998881: step 59410, loss = 4.95 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 16:21:18.257230: step 59420, loss = 4.83 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 16:21:23.037799: step 59430, loss = 4.63 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 16:21:28.403480: step 59440, loss = 4.77 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 16:21:34.814956: step 59450, loss = 4.79 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 16:21:39.670414: step 59460, loss = 4.80 (282.8 examples/sec; 0.453 sec/batch)
2016-07-15 16:21:44.431034: step 59470, loss = 4.91 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 16:21:50.349651: step 59480, loss = 4.71 (187.3 examples/sec; 0.684 sec/batch)
2016-07-15 16:21:56.250500: step 59490, loss = 4.83 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 16:22:01.096931: step 59500, loss = 4.59 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 16:22:06.987004: step 59510, loss = 5.15 (233.8 examples/sec; 0.547 sec/batch)
2016-07-15 16:22:13.522545: step 59520, loss = 4.76 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 16:22:18.731806: step 59530, loss = 5.09 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 16:22:24.532449: step 59540, loss = 4.69 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 16:22:30.172063: step 59550, loss = 5.01 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 16:22:35.341486: step 59560, loss = 5.05 (208.7 examples/sec; 0.613 sec/batch)
2016-07-15 16:22:41.024890: step 59570, loss = 4.81 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 16:22:45.779864: step 59580, loss = 4.64 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 16:22:50.661208: step 59590, loss = 4.90 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 16:22:57.185334: step 59600, loss = 5.05 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 16:23:03.746890: step 59610, loss = 4.96 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 16:23:08.507235: step 59620, loss = 4.85 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 16:23:14.393611: step 59630, loss = 4.76 (184.9 examples/sec; 0.692 sec/batch)
2016-07-15 16:23:20.279539: step 59640, loss = 4.77 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 16:23:25.044225: step 59650, loss = 4.93 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 16:23:29.903974: step 59660, loss = 4.95 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 16:23:34.633172: step 59670, loss = 4.85 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 16:23:39.867098: step 59680, loss = 4.69 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 16:23:46.390973: step 59690, loss = 4.43 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 16:23:51.416182: step 59700, loss = 4.86 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 16:23:57.087857: step 59710, loss = 5.01 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 16:24:03.141426: step 59720, loss = 4.77 (196.0 examples/sec; 0.653 sec/batch)
2016-07-15 16:24:08.898973: step 59730, loss = 4.86 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 16:24:13.669781: step 59740, loss = 5.02 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 16:24:18.492689: step 59750, loss = 4.95 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 16:24:24.954865: step 59760, loss = 4.80 (208.7 examples/sec; 0.613 sec/batch)
2016-07-15 16:24:30.263028: step 59770, loss = 4.85 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 16:24:35.045506: step 59780, loss = 4.87 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 16:24:40.365149: step 59790, loss = 4.77 (188.4 examples/sec; 0.680 sec/batch)
2016-07-15 16:24:46.852659: step 59800, loss = 4.89 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 16:24:52.758458: step 59810, loss = 4.76 (247.0 examples/sec; 0.518 sec/batch)
2016-07-15 16:24:57.580609: step 59820, loss = 5.04 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 16:25:03.819831: step 59830, loss = 4.83 (199.7 examples/sec; 0.641 sec/batch)
2016-07-15 16:25:09.403280: step 59840, loss = 4.78 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 16:25:14.195673: step 59850, loss = 4.87 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 16:25:19.143886: step 59860, loss = 4.91 (237.6 examples/sec; 0.539 sec/batch)
2016-07-15 16:25:25.793820: step 59870, loss = 4.88 (199.4 examples/sec; 0.642 sec/batch)
2016-07-15 16:25:31.009207: step 59880, loss = 4.74 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 16:25:35.664404: step 59890, loss = 4.95 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 16:25:41.167557: step 59900, loss = 5.09 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 16:25:48.813399: step 59910, loss = 4.81 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 16:25:54.449407: step 59920, loss = 4.75 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 16:25:59.536044: step 59930, loss = 4.82 (251.1 examples/sec; 0.510 sec/batch)
2016-07-15 16:26:04.304634: step 59940, loss = 4.84 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 16:26:10.041054: step 59950, loss = 4.84 (180.9 examples/sec; 0.707 sec/batch)
2016-07-15 16:26:16.092313: step 59960, loss = 4.70 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 16:26:20.885091: step 59970, loss = 5.08 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 16:26:25.681172: step 59980, loss = 4.87 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 16:26:31.811582: step 59990, loss = 4.76 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 16:26:37.428583: step 60000, loss = 4.94 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 16:26:43.149248: step 60010, loss = 4.96 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 16:26:48.022830: step 60020, loss = 4.74 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 16:26:52.770987: step 60030, loss = 5.04 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 16:26:58.636962: step 60040, loss = 4.57 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 16:27:04.546056: step 60050, loss = 4.83 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 16:27:09.331788: step 60060, loss = 4.83 (282.8 examples/sec; 0.453 sec/batch)
2016-07-15 16:27:14.214494: step 60070, loss = 4.82 (249.1 examples/sec; 0.514 sec/batch)
2016-07-15 16:27:20.526011: step 60080, loss = 4.90 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 16:27:25.975827: step 60090, loss = 5.09 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 16:27:30.681168: step 60100, loss = 4.76 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 16:27:36.525837: step 60110, loss = 4.65 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 16:27:41.262526: step 60120, loss = 5.20 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 16:27:47.349033: step 60130, loss = 4.88 (195.4 examples/sec; 0.655 sec/batch)
2016-07-15 16:27:53.071089: step 60140, loss = 4.86 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 16:27:57.898642: step 60150, loss = 4.71 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 16:28:02.746936: step 60160, loss = 4.75 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 16:28:09.205033: step 60170, loss = 4.76 (205.0 examples/sec; 0.625 sec/batch)
2016-07-15 16:28:14.557316: step 60180, loss = 4.58 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 16:28:19.245858: step 60190, loss = 4.69 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 16:28:24.127162: step 60200, loss = 4.74 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 16:28:29.896380: step 60210, loss = 4.91 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 16:28:36.122394: step 60220, loss = 4.46 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 16:28:41.660415: step 60230, loss = 4.75 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 16:28:46.420792: step 60240, loss = 4.77 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 16:28:51.233798: step 60250, loss = 4.77 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 16:28:57.798405: step 60260, loss = 5.03 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 16:29:02.992700: step 60270, loss = 4.88 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 16:29:08.788130: step 60280, loss = 4.66 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 16:29:14.428416: step 60290, loss = 4.81 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 16:29:19.579817: step 60300, loss = 4.85 (222.8 examples/sec; 0.574 sec/batch)
2016-07-15 16:29:26.469617: step 60310, loss = 4.85 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 16:29:32.289634: step 60320, loss = 4.83 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 16:29:37.778834: step 60330, loss = 4.75 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 16:29:42.994583: step 60340, loss = 4.79 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 16:29:48.810365: step 60350, loss = 4.88 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 16:29:53.639228: step 60360, loss = 4.87 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 16:29:58.518939: step 60370, loss = 4.86 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 16:30:04.992330: step 60380, loss = 4.71 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 16:30:10.333485: step 60390, loss = 4.79 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 16:30:16.099495: step 60400, loss = 4.77 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 16:30:21.874626: step 60410, loss = 4.78 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 16:30:26.754152: step 60420, loss = 4.80 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 16:30:31.461034: step 60430, loss = 4.92 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 16:30:36.814690: step 60440, loss = 4.77 (188.1 examples/sec; 0.681 sec/batch)
2016-07-15 16:30:43.285064: step 60450, loss = 4.99 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 16:30:48.163663: step 60460, loss = 4.75 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 16:30:52.907122: step 60470, loss = 4.70 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 16:30:58.846054: step 60480, loss = 4.69 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 16:31:04.794792: step 60490, loss = 4.65 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 16:31:09.612186: step 60500, loss = 4.76 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 16:31:15.532196: step 60510, loss = 4.70 (227.9 examples/sec; 0.562 sec/batch)
2016-07-15 16:31:22.065295: step 60520, loss = 4.90 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 16:31:27.234729: step 60530, loss = 4.32 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 16:31:31.921913: step 60540, loss = 4.94 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 16:31:37.390804: step 60550, loss = 4.65 (192.3 examples/sec; 0.666 sec/batch)
2016-07-15 16:31:43.695091: step 60560, loss = 4.47 (246.6 examples/sec; 0.519 sec/batch)
2016-07-15 16:31:48.922682: step 60570, loss = 4.78 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 16:31:54.361041: step 60580, loss = 4.49 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 16:31:59.061107: step 60590, loss = 4.90 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 16:32:04.262077: step 60600, loss = 4.58 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 16:32:12.231749: step 60610, loss = 4.77 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 16:32:17.648443: step 60620, loss = 4.79 (197.1 examples/sec; 0.649 sec/batch)
2016-07-15 16:32:22.939843: step 60630, loss = 5.01 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 16:32:28.665873: step 60640, loss = 4.79 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 16:32:34.185866: step 60650, loss = 4.79 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 16:32:39.300121: step 60660, loss = 4.44 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 16:32:44.032775: step 60670, loss = 4.72 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 16:32:49.472697: step 60680, loss = 4.64 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 16:32:55.788517: step 60690, loss = 4.61 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 16:33:00.656160: step 60700, loss = 4.95 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 16:33:06.477364: step 60710, loss = 4.75 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 16:33:12.759024: step 60720, loss = 4.90 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 16:33:18.217466: step 60730, loss = 4.73 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 16:33:24.035110: step 60740, loss = 4.62 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 16:33:28.838525: step 60750, loss = 5.05 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 16:33:33.617035: step 60760, loss = 4.99 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 16:33:39.710329: step 60770, loss = 4.79 (192.4 examples/sec; 0.665 sec/batch)
2016-07-15 16:33:45.414768: step 60780, loss = 5.09 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 16:33:50.179112: step 60790, loss = 4.97 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 16:33:55.020144: step 60800, loss = 4.66 (253.7 examples/sec; 0.505 sec/batch)
2016-07-15 16:34:03.062953: step 60810, loss = 4.85 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 16:34:08.248910: step 60820, loss = 4.87 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 16:34:13.823503: step 60830, loss = 4.85 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 16:34:18.609492: step 60840, loss = 4.74 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 16:34:23.490236: step 60850, loss = 4.46 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 16:34:28.268466: step 60860, loss = 4.84 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 16:34:33.811797: step 60870, loss = 4.63 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 16:34:40.074388: step 60880, loss = 4.71 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 16:34:44.911949: step 60890, loss = 4.90 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 16:34:49.705256: step 60900, loss = 4.82 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 16:34:57.152130: step 60910, loss = 4.72 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 16:35:02.574319: step 60920, loss = 4.74 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 16:35:08.361718: step 60930, loss = 4.75 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 16:35:13.786706: step 60940, loss = 4.79 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 16:35:19.114715: step 60950, loss = 4.67 (251.3 examples/sec; 0.509 sec/batch)
2016-07-15 16:35:24.857095: step 60960, loss = 4.78 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 16:35:30.381265: step 60970, loss = 4.74 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 16:35:35.594507: step 60980, loss = 4.97 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 16:35:41.395983: step 60990, loss = 4.78 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 16:35:47.031011: step 61000, loss = 4.82 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 16:35:53.568994: step 61010, loss = 4.77 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 16:35:58.866719: step 61020, loss = 4.81 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 16:36:04.708781: step 61030, loss = 4.69 (243.1 examples/sec; 0.527 sec/batch)
2016-07-15 16:36:10.172910: step 61040, loss = 4.66 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 16:36:15.379512: step 61050, loss = 4.89 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 16:36:21.171042: step 61060, loss = 5.03 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 16:36:25.972326: step 61070, loss = 4.80 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 16:36:30.789075: step 61080, loss = 5.00 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 16:36:37.154971: step 61090, loss = 4.51 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 16:36:42.613205: step 61100, loss = 4.67 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 16:36:49.417953: step 61110, loss = 4.94 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 16:36:55.088423: step 61120, loss = 4.70 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 16:37:00.105933: step 61130, loss = 5.01 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 16:37:04.825716: step 61140, loss = 4.79 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 16:37:09.653538: step 61150, loss = 4.74 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 16:37:14.271235: step 61160, loss = 4.98 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 16:37:18.907402: step 61170, loss = 4.79 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 16:37:24.631733: step 61180, loss = 4.51 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 16:37:30.045178: step 61190, loss = 4.94 (208.2 examples/sec; 0.615 sec/batch)
2016-07-15 16:37:35.336602: step 61200, loss = 4.61 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 16:37:42.286456: step 61210, loss = 4.82 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 16:37:48.001854: step 61220, loss = 4.72 (230.8 examples/sec; 0.555 sec/batch)
2016-07-15 16:37:53.261123: step 61230, loss = 4.78 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 16:37:58.716393: step 61240, loss = 4.72 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 16:38:04.504035: step 61250, loss = 5.05 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 16:38:09.854729: step 61260, loss = 4.69 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 16:38:15.202645: step 61270, loss = 4.88 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 16:38:20.012221: step 61280, loss = 4.79 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 16:38:25.349796: step 61290, loss = 4.94 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 16:38:31.822276: step 61300, loss = 4.76 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 16:38:38.367836: step 61310, loss = 4.88 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 16:38:43.528613: step 61320, loss = 4.70 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 16:38:49.317821: step 61330, loss = 4.81 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 16:38:54.918834: step 61340, loss = 4.70 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 16:39:00.085389: step 61350, loss = 4.62 (223.8 examples/sec; 0.572 sec/batch)
2016-07-15 16:39:05.824273: step 61360, loss = 4.79 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 16:39:10.606388: step 61370, loss = 4.85 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 16:39:15.441614: step 61380, loss = 4.71 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 16:39:21.939662: step 61390, loss = 4.89 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 16:39:27.293011: step 61400, loss = 4.69 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 16:39:34.072109: step 61410, loss = 4.64 (257.8 examples/sec; 0.497 sec/batch)
2016-07-15 16:39:39.897143: step 61420, loss = 4.99 (207.6 examples/sec; 0.616 sec/batch)
2016-07-15 16:39:45.176769: step 61430, loss = 4.94 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 16:39:50.682411: step 61440, loss = 4.63 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 16:39:55.465029: step 61450, loss = 4.74 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 16:40:00.521211: step 61460, loss = 4.89 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 16:40:07.118674: step 61470, loss = 4.79 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 16:40:12.139221: step 61480, loss = 4.75 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 16:40:16.861603: step 61490, loss = 4.48 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 16:40:21.635187: step 61500, loss = 5.10 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 16:40:27.426598: step 61510, loss = 4.76 (254.0 examples/sec; 0.504 sec/batch)
2016-07-15 16:40:33.889151: step 61520, loss = 4.91 (200.5 examples/sec; 0.639 sec/batch)
2016-07-15 16:40:39.231988: step 61530, loss = 4.74 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 16:40:43.939952: step 61540, loss = 4.71 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 16:40:48.759266: step 61550, loss = 4.76 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 16:40:53.532797: step 61560, loss = 4.71 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 16:40:59.442426: step 61570, loss = 5.01 (188.6 examples/sec; 0.679 sec/batch)
2016-07-15 16:41:05.363553: step 61580, loss = 4.62 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 16:41:10.170387: step 61590, loss = 4.89 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 16:41:15.006970: step 61600, loss = 5.06 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 16:41:22.532401: step 61610, loss = 4.85 (207.6 examples/sec; 0.617 sec/batch)
2016-07-15 16:41:27.706333: step 61620, loss = 4.76 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 16:41:33.519263: step 61630, loss = 4.66 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 16:41:39.160693: step 61640, loss = 4.82 (197.3 examples/sec; 0.649 sec/batch)
2016-07-15 16:41:44.197905: step 61650, loss = 4.61 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 16:41:48.929754: step 61660, loss = 4.84 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 16:41:53.767963: step 61670, loss = 4.84 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 16:41:58.565214: step 61680, loss = 4.91 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 16:42:04.834227: step 61690, loss = 4.73 (209.1 examples/sec; 0.612 sec/batch)
2016-07-15 16:42:10.400443: step 61700, loss = 4.85 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 16:42:17.258738: step 61710, loss = 4.57 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 16:42:22.831911: step 61720, loss = 4.62 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 16:42:27.975073: step 61730, loss = 4.75 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 16:42:33.771173: step 61740, loss = 4.60 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 16:42:39.515498: step 61750, loss = 4.82 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 16:42:44.665838: step 61760, loss = 4.49 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 16:42:50.255500: step 61770, loss = 4.64 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 16:42:56.062758: step 61780, loss = 4.69 (211.7 examples/sec; 0.605 sec/batch)
2016-07-15 16:43:00.933869: step 61790, loss = 4.60 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 16:43:05.716954: step 61800, loss = 4.71 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 16:43:12.963514: step 61810, loss = 4.84 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 16:43:18.529757: step 61820, loss = 4.54 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 16:43:23.276702: step 61830, loss = 4.77 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 16:43:28.227913: step 61840, loss = 4.70 (220.2 examples/sec; 0.581 sec/batch)
2016-07-15 16:43:34.788149: step 61850, loss = 4.82 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 16:43:40.000540: step 61860, loss = 4.83 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 16:43:44.725358: step 61870, loss = 4.88 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 16:43:50.292869: step 61880, loss = 4.75 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 16:43:56.566515: step 61890, loss = 4.64 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 16:44:01.375857: step 61900, loss = 4.62 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 16:44:07.156074: step 61910, loss = 4.92 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 16:44:13.524086: step 61920, loss = 4.75 (198.7 examples/sec; 0.644 sec/batch)
2016-07-15 16:44:18.982852: step 61930, loss = 4.74 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 16:44:23.683393: step 61940, loss = 4.97 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 16:44:28.522569: step 61950, loss = 4.98 (279.2 examples/sec; 0.459 sec/batch)
2016-07-15 16:44:33.270463: step 61960, loss = 4.62 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 16:44:38.983231: step 61970, loss = 4.70 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 16:44:45.079414: step 61980, loss = 4.70 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 16:44:49.879179: step 61990, loss = 4.62 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 16:44:54.724473: step 62000, loss = 4.89 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 16:45:02.268330: step 62010, loss = 4.45 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 16:45:07.553603: step 62020, loss = 4.87 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 16:45:12.233942: step 62030, loss = 4.62 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 16:45:17.516843: step 62040, loss = 4.58 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 16:45:23.992681: step 62050, loss = 4.75 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 16:45:28.867324: step 62060, loss = 4.90 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 16:45:33.612611: step 62070, loss = 4.79 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 16:45:38.398592: step 62080, loss = 4.77 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 16:45:43.251927: step 62090, loss = 4.61 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 16:45:47.988014: step 62100, loss = 4.64 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 16:45:53.961011: step 62110, loss = 4.56 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 16:45:58.765533: step 62120, loss = 4.88 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 16:46:05.002969: step 62130, loss = 4.74 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 16:46:10.656753: step 62140, loss = 5.03 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 16:46:15.379896: step 62150, loss = 4.69 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 16:46:20.276801: step 62160, loss = 4.87 (245.8 examples/sec; 0.521 sec/batch)
2016-07-15 16:46:26.858167: step 62170, loss = 4.65 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 16:46:32.059653: step 62180, loss = 4.72 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 16:46:36.795742: step 62190, loss = 4.64 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 16:46:41.592803: step 62200, loss = 4.70 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 16:46:47.395044: step 62210, loss = 4.80 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 16:46:53.757599: step 62220, loss = 5.06 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 16:46:59.205940: step 62230, loss = 4.82 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 16:47:04.993800: step 62240, loss = 5.06 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 16:47:10.311337: step 62250, loss = 4.62 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 16:47:15.591750: step 62260, loss = 4.61 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 16:47:20.290380: step 62270, loss = 4.79 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 16:47:25.588744: step 62280, loss = 4.88 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 16:47:32.009831: step 62290, loss = 4.74 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 16:47:37.145375: step 62300, loss = 4.82 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 16:47:43.993914: step 62310, loss = 4.63 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 16:47:49.830557: step 62320, loss = 4.95 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 16:47:55.433440: step 62330, loss = 4.68 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 16:48:00.561788: step 62340, loss = 4.46 (222.9 examples/sec; 0.574 sec/batch)
2016-07-15 16:48:06.283886: step 62350, loss = 4.80 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 16:48:11.994186: step 62360, loss = 4.61 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 16:48:17.222147: step 62370, loss = 4.55 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 16:48:22.841557: step 62380, loss = 4.88 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 16:48:27.649175: step 62390, loss = 4.69 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 16:48:32.703186: step 62400, loss = 4.72 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 16:48:40.843107: step 62410, loss = 5.11 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 16:48:45.717139: step 62420, loss = 4.80 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 16:48:50.469168: step 62430, loss = 4.73 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 16:48:55.232470: step 62440, loss = 4.84 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 16:49:00.050910: step 62450, loss = 4.78 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 16:49:06.609120: step 62460, loss = 4.77 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 16:49:11.882322: step 62470, loss = 4.55 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 16:49:17.628425: step 62480, loss = 4.69 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 16:49:23.133730: step 62490, loss = 4.67 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 16:49:28.323934: step 62500, loss = 4.64 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 16:49:34.027722: step 62510, loss = 4.60 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 16:49:38.807131: step 62520, loss = 4.79 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 16:49:43.641610: step 62530, loss = 4.63 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 16:49:50.080431: step 62540, loss = 4.58 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 16:49:55.571346: step 62550, loss = 4.76 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 16:50:01.339284: step 62560, loss = 4.65 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 16:50:06.135329: step 62570, loss = 4.62 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 16:50:10.982181: step 62580, loss = 4.67 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 16:50:17.125722: step 62590, loss = 4.64 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 16:50:22.733258: step 62600, loss = 4.77 (252.7 examples/sec; 0.506 sec/batch)
2016-07-15 16:50:28.554796: step 62610, loss = 4.73 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 16:50:33.901382: step 62620, loss = 4.90 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 16:50:40.363864: step 62630, loss = 4.65 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 16:50:45.215352: step 62640, loss = 4.75 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 16:50:50.000466: step 62650, loss = 4.36 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 16:50:55.865125: step 62660, loss = 4.76 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 16:51:01.786183: step 62670, loss = 4.64 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 16:51:06.564766: step 62680, loss = 4.74 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 16:51:11.351475: step 62690, loss = 4.69 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 16:51:16.083298: step 62700, loss = 4.50 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 16:51:22.666649: step 62710, loss = 4.64 (186.4 examples/sec; 0.687 sec/batch)
2016-07-15 16:51:28.932616: step 62720, loss = 4.83 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 16:51:33.813048: step 62730, loss = 4.91 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 16:51:38.584180: step 62740, loss = 4.61 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 16:51:44.710226: step 62750, loss = 4.84 (194.4 examples/sec; 0.658 sec/batch)
2016-07-15 16:51:50.531633: step 62760, loss = 5.13 (233.1 examples/sec; 0.549 sec/batch)
2016-07-15 16:51:56.513024: step 62770, loss = 4.79 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 16:52:01.369680: step 62780, loss = 4.86 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 16:52:06.116399: step 62790, loss = 4.59 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 16:52:12.014930: step 62800, loss = 4.65 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 16:52:19.144995: step 62810, loss = 4.61 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 16:52:24.929332: step 62820, loss = 4.82 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 16:52:29.772817: step 62830, loss = 4.72 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 16:52:34.572690: step 62840, loss = 4.75 (243.4 examples/sec; 0.526 sec/batch)
2016-07-15 16:52:40.624932: step 62850, loss = 4.59 (191.7 examples/sec; 0.668 sec/batch)
2016-07-15 16:52:46.324079: step 62860, loss = 4.68 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 16:52:52.068770: step 62870, loss = 4.62 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 16:52:57.204728: step 62880, loss = 4.84 (199.7 examples/sec; 0.641 sec/batch)
2016-07-15 16:53:02.824900: step 62890, loss = 4.68 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 16:53:08.593886: step 62900, loss = 4.46 (209.7 examples/sec; 0.610 sec/batch)
2016-07-15 16:53:15.074721: step 62910, loss = 4.73 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 16:53:20.238623: step 62920, loss = 4.41 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 16:53:26.026503: step 62930, loss = 4.59 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 16:53:31.594711: step 62940, loss = 4.63 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 16:53:36.759502: step 62950, loss = 4.64 (239.5 examples/sec; 0.534 sec/batch)
2016-07-15 16:53:42.518125: step 62960, loss = 4.60 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 16:53:47.283366: step 62970, loss = 4.52 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 16:53:52.089526: step 62980, loss = 4.55 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 16:53:56.786710: step 62990, loss = 4.88 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 16:54:02.104208: step 63000, loss = 4.70 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 16:54:09.919191: step 63010, loss = 4.70 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 16:54:15.426926: step 63020, loss = 4.67 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 16:54:20.588510: step 63030, loss = 4.77 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 16:54:25.325159: step 63040, loss = 4.74 (251.4 examples/sec; 0.509 sec/batch)
2016-07-15 16:54:30.850579: step 63050, loss = 4.58 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 16:54:37.108045: step 63060, loss = 4.51 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 16:54:41.961396: step 63070, loss = 4.69 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 16:54:46.715985: step 63080, loss = 4.72 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 16:54:52.776743: step 63090, loss = 4.96 (194.4 examples/sec; 0.658 sec/batch)
2016-07-15 16:54:58.498785: step 63100, loss = 4.68 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 16:55:04.230214: step 63110, loss = 5.04 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 16:55:09.379489: step 63120, loss = 4.53 (187.8 examples/sec; 0.681 sec/batch)
2016-07-15 16:55:15.900774: step 63130, loss = 4.73 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 16:55:20.957406: step 63140, loss = 4.77 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 16:55:25.644361: step 63150, loss = 4.70 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 16:55:31.293631: step 63160, loss = 4.94 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 16:55:37.445478: step 63170, loss = 4.68 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 16:55:42.774315: step 63180, loss = 4.99 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 16:55:48.071623: step 63190, loss = 4.73 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 16:55:52.822611: step 63200, loss = 5.12 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 16:55:59.606822: step 63210, loss = 4.62 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 16:56:05.681702: step 63220, loss = 4.80 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 16:56:10.469296: step 63230, loss = 4.80 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 16:56:15.265304: step 63240, loss = 4.71 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 16:56:21.407951: step 63250, loss = 4.53 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 16:56:26.988343: step 63260, loss = 4.63 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 16:56:32.735291: step 63270, loss = 4.68 (200.4 examples/sec; 0.639 sec/batch)
2016-07-15 16:56:37.611026: step 63280, loss = 4.67 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 16:56:42.420777: step 63290, loss = 4.55 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 16:56:47.204518: step 63300, loss = 4.64 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 16:56:52.786636: step 63310, loss = 4.64 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 16:56:57.947743: step 63320, loss = 4.75 (199.0 examples/sec; 0.643 sec/batch)
2016-07-15 16:57:04.131050: step 63330, loss = 4.52 (234.4 examples/sec; 0.546 sec/batch)
2016-07-15 16:57:08.823403: step 63340, loss = 4.80 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 16:57:14.149508: step 63350, loss = 4.77 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 16:57:20.652346: step 63360, loss = 4.69 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 16:57:25.825118: step 63370, loss = 4.80 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 16:57:31.392768: step 63380, loss = 4.60 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 16:57:37.167164: step 63390, loss = 4.91 (253.5 examples/sec; 0.505 sec/batch)
2016-07-15 16:57:42.436397: step 63400, loss = 4.76 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 16:57:49.081989: step 63410, loss = 4.42 (227.7 examples/sec; 0.562 sec/batch)
2016-07-15 16:57:54.809517: step 63420, loss = 4.69 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 16:57:59.573210: step 63430, loss = 4.71 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 16:58:04.430350: step 63440, loss = 4.85 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 16:58:09.174582: step 63450, loss = 4.75 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 16:58:14.690051: step 63460, loss = 4.74 (183.4 examples/sec; 0.698 sec/batch)
2016-07-15 16:58:21.057409: step 63470, loss = 4.66 (225.6 examples/sec; 0.567 sec/batch)
2016-07-15 16:58:25.871760: step 63480, loss = 4.53 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 16:58:30.680873: step 63490, loss = 4.72 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 16:58:36.693228: step 63500, loss = 4.61 (189.5 examples/sec; 0.676 sec/batch)
2016-07-15 16:58:43.694110: step 63510, loss = 4.80 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 16:58:48.458953: step 63520, loss = 4.48 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 16:58:53.583750: step 63530, loss = 4.61 (184.1 examples/sec; 0.695 sec/batch)
2016-07-15 16:59:00.095988: step 63540, loss = 4.91 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 16:59:05.244751: step 63550, loss = 4.68 (218.4 examples/sec; 0.586 sec/batch)
2016-07-15 16:59:10.909375: step 63560, loss = 4.71 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 16:59:15.677072: step 63570, loss = 4.64 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 16:59:20.572226: step 63580, loss = 4.76 (249.1 examples/sec; 0.514 sec/batch)
2016-07-15 16:59:27.074678: step 63590, loss = 4.72 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 16:59:32.355511: step 63600, loss = 4.72 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 16:59:39.274772: step 63610, loss = 4.66 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 16:59:44.063888: step 63620, loss = 4.49 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 16:59:48.894784: step 63630, loss = 4.61 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 16:59:53.568721: step 63640, loss = 4.79 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 16:59:58.398881: step 63650, loss = 4.63 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 17:00:03.222701: step 63660, loss = 4.62 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 17:00:09.367717: step 63670, loss = 4.62 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 17:00:15.053925: step 63680, loss = 4.61 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 17:00:20.811117: step 63690, loss = 4.80 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 17:00:25.932695: step 63700, loss = 4.69 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 17:00:32.709194: step 63710, loss = 4.59 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 17:00:38.523036: step 63720, loss = 4.51 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 17:00:44.197950: step 63730, loss = 4.51 (208.7 examples/sec; 0.613 sec/batch)
2016-07-15 17:00:49.245017: step 63740, loss = 4.73 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 17:00:53.931192: step 63750, loss = 4.62 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 17:00:58.716394: step 63760, loss = 4.66 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 17:01:03.541408: step 63770, loss = 4.65 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 17:01:09.688537: step 63780, loss = 4.76 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 17:01:15.258901: step 63790, loss = 4.67 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 17:01:21.057158: step 63800, loss = 4.52 (209.7 examples/sec; 0.610 sec/batch)
2016-07-15 17:01:27.551802: step 63810, loss = 4.29 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 17:01:32.741023: step 63820, loss = 4.92 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 17:01:38.535850: step 63830, loss = 4.64 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 17:01:43.281317: step 63840, loss = 4.71 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 17:01:48.096872: step 63850, loss = 4.61 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 17:01:54.361264: step 63860, loss = 4.44 (207.3 examples/sec; 0.618 sec/batch)
2016-07-15 17:01:59.795314: step 63870, loss = 4.65 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 17:02:05.573892: step 63880, loss = 4.69 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 17:02:10.427806: step 63890, loss = 4.84 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 17:02:15.158375: step 63900, loss = 4.57 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 17:02:20.916380: step 63910, loss = 4.98 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 17:02:25.812639: step 63920, loss = 4.98 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 17:02:30.572872: step 63930, loss = 4.72 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 17:02:35.380795: step 63940, loss = 4.85 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 17:02:40.239214: step 63950, loss = 4.53 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 17:02:46.611990: step 63960, loss = 4.52 (199.4 examples/sec; 0.642 sec/batch)
2016-07-15 17:02:52.085304: step 63970, loss = 4.60 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 17:02:56.838463: step 63980, loss = 4.68 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 17:03:02.065190: step 63990, loss = 4.61 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 17:03:08.585434: step 64000, loss = 4.75 (197.8 examples/sec; 0.647 sec/batch)
2016-07-15 17:03:14.615406: step 64010, loss = 4.51 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 17:03:19.319168: step 64020, loss = 4.70 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 17:03:23.993819: step 64030, loss = 4.62 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 17:03:29.761899: step 64040, loss = 4.62 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 17:03:35.117929: step 64050, loss = 4.77 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 17:03:40.408986: step 64060, loss = 4.34 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 17:03:45.115761: step 64070, loss = 4.83 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 17:03:49.998665: step 64080, loss = 4.55 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 17:03:54.750346: step 64090, loss = 4.60 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 17:04:00.606594: step 64100, loss = 4.89 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 17:04:07.794350: step 64110, loss = 4.77 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 17:04:12.558501: step 64120, loss = 4.50 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 17:04:17.626576: step 64130, loss = 4.55 (195.9 examples/sec; 0.653 sec/batch)
2016-07-15 17:04:24.182744: step 64140, loss = 4.48 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 17:04:29.249320: step 64150, loss = 4.73 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 17:04:33.946913: step 64160, loss = 4.63 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 17:04:38.571089: step 64170, loss = 4.54 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 17:04:43.946855: step 64180, loss = 4.80 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 17:04:49.151351: step 64190, loss = 4.79 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 17:04:53.858398: step 64200, loss = 4.59 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 17:05:00.853914: step 64210, loss = 4.53 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 17:05:06.702688: step 64220, loss = 4.51 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 17:05:11.460360: step 64230, loss = 4.69 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 17:05:16.310458: step 64240, loss = 4.69 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 17:05:22.654804: step 64250, loss = 4.98 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 17:05:28.127481: step 64260, loss = 4.68 (254.7 examples/sec; 0.502 sec/batch)
2016-07-15 17:05:32.894538: step 64270, loss = 4.78 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 17:05:37.814151: step 64280, loss = 4.53 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 17:05:42.538175: step 64290, loss = 4.55 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 17:05:47.371550: step 64300, loss = 4.53 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 17:05:53.213236: step 64310, loss = 4.60 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 17:05:57.891219: step 64320, loss = 4.89 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 17:06:02.784882: step 64330, loss = 4.41 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 17:06:07.558284: step 64340, loss = 4.94 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 17:06:13.662506: step 64350, loss = 4.70 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 17:06:19.345058: step 64360, loss = 4.98 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 17:06:25.022109: step 64370, loss = 4.82 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 17:06:30.139464: step 64380, loss = 4.92 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 17:06:35.745373: step 64390, loss = 4.67 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 17:06:40.566084: step 64400, loss = 4.41 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 17:06:46.773915: step 64410, loss = 4.66 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 17:06:53.234325: step 64420, loss = 4.57 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 17:06:58.062265: step 64430, loss = 4.55 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 17:07:02.791758: step 64440, loss = 4.58 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 17:07:07.557817: step 64450, loss = 4.44 (286.2 examples/sec; 0.447 sec/batch)
2016-07-15 17:07:12.341221: step 64460, loss = 4.70 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 17:07:17.053164: step 64470, loss = 4.56 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 17:07:22.162733: step 64480, loss = 4.63 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 17:07:28.671913: step 64490, loss = 4.68 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 17:07:33.735800: step 64500, loss = 4.78 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 17:07:39.567970: step 64510, loss = 4.68 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 17:07:45.691937: step 64520, loss = 4.66 (197.2 examples/sec; 0.649 sec/batch)
2016-07-15 17:07:51.402036: step 64530, loss = 4.83 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 17:07:57.150949: step 64540, loss = 4.50 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 17:08:02.081766: step 64550, loss = 4.73 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 17:08:06.842116: step 64560, loss = 4.85 (246.1 examples/sec; 0.520 sec/batch)
2016-07-15 17:08:11.694740: step 64570, loss = 4.72 (249.7 examples/sec; 0.513 sec/batch)
2016-07-15 17:08:16.546202: step 64580, loss = 4.51 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 17:08:22.935390: step 64590, loss = 4.80 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 17:08:28.381679: step 64600, loss = 4.53 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 17:08:34.137183: step 64610, loss = 4.81 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 17:08:39.665279: step 64620, loss = 4.65 (190.9 examples/sec; 0.670 sec/batch)
2016-07-15 17:08:45.926792: step 64630, loss = 4.60 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 17:08:50.693601: step 64640, loss = 4.50 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 17:08:55.419141: step 64650, loss = 4.69 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 17:09:00.158629: step 64660, loss = 4.48 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 17:09:05.001350: step 64670, loss = 4.68 (256.8 examples/sec; 0.499 sec/batch)
2016-07-15 17:09:11.492821: step 64680, loss = 4.57 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 17:09:16.814873: step 64690, loss = 4.52 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 17:09:21.577261: step 64700, loss = 4.54 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 17:09:27.417091: step 64710, loss = 4.63 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 17:09:32.251656: step 64720, loss = 4.67 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 17:09:36.937333: step 64730, loss = 4.60 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 17:09:41.806392: step 64740, loss = 4.58 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 17:09:46.614101: step 64750, loss = 4.53 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 17:09:51.451531: step 64760, loss = 4.53 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 17:09:56.233354: step 64770, loss = 4.73 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 17:10:02.518155: step 64780, loss = 4.89 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 17:10:08.078841: step 64790, loss = 4.65 (268.6 examples/sec; 0.476 sec/batch)
2016-07-15 17:10:13.864406: step 64800, loss = 4.79 (248.7 examples/sec; 0.515 sec/batch)
2016-07-15 17:10:19.745832: step 64810, loss = 4.43 (281.8 examples/sec; 0.454 sec/batch)
2016-07-15 17:10:24.512312: step 64820, loss = 4.50 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 17:10:29.272984: step 64830, loss = 4.59 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 17:10:34.446454: step 64840, loss = 4.61 (187.5 examples/sec; 0.682 sec/batch)
2016-07-15 17:10:40.925051: step 64850, loss = 4.71 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 17:10:46.155345: step 64860, loss = 4.59 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 17:10:51.732478: step 64870, loss = 4.62 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 17:10:56.478254: step 64880, loss = 4.71 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 17:11:01.314474: step 64890, loss = 4.55 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 17:11:06.050167: step 64900, loss = 4.72 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 17:11:13.000432: step 64910, loss = 4.78 (184.6 examples/sec; 0.693 sec/batch)
2016-07-15 17:11:18.912622: step 64920, loss = 4.59 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 17:11:23.689756: step 64930, loss = 4.60 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 17:11:28.542201: step 64940, loss = 4.84 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 17:11:33.241138: step 64950, loss = 4.68 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 17:11:38.411312: step 64960, loss = 4.49 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 17:11:44.934675: step 64970, loss = 4.62 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 17:11:49.962625: step 64980, loss = 4.86 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 17:11:54.752473: step 64990, loss = 4.83 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 17:11:59.533585: step 65000, loss = 4.62 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 17:12:05.292971: step 65010, loss = 4.66 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 17:12:11.766246: step 65020, loss = 4.53 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 17:12:17.063462: step 65030, loss = 4.62 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 17:12:21.807279: step 65040, loss = 4.71 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 17:12:27.182046: step 65050, loss = 4.79 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 17:12:33.630983: step 65060, loss = 4.42 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 17:12:38.527448: step 65070, loss = 4.92 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 17:12:43.293464: step 65080, loss = 4.67 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 17:12:48.020720: step 65090, loss = 4.55 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 17:12:52.674193: step 65100, loss = 4.61 (281.8 examples/sec; 0.454 sec/batch)
2016-07-15 17:12:58.480055: step 65110, loss = 4.39 (198.8 examples/sec; 0.644 sec/batch)
2016-07-15 17:13:04.060446: step 65120, loss = 4.47 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 17:13:08.772423: step 65130, loss = 4.72 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 17:13:13.665125: step 65140, loss = 4.67 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 17:13:20.233969: step 65150, loss = 4.73 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 17:13:25.474887: step 65160, loss = 4.74 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 17:13:30.224473: step 65170, loss = 4.67 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 17:13:35.007645: step 65180, loss = 4.44 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 17:13:39.795436: step 65190, loss = 4.56 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 17:13:45.806770: step 65200, loss = 4.95 (195.3 examples/sec; 0.656 sec/batch)
2016-07-15 17:13:52.770484: step 65210, loss = 4.94 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 17:13:58.507484: step 65220, loss = 4.33 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 17:14:03.900182: step 65230, loss = 4.65 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 17:14:09.250250: step 65240, loss = 4.64 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 17:14:14.962079: step 65250, loss = 4.57 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 17:14:20.493725: step 65260, loss = 4.92 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 17:14:25.644295: step 65270, loss = 4.51 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 17:14:30.350375: step 65280, loss = 4.65 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 17:14:35.199536: step 65290, loss = 4.61 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 17:14:39.984835: step 65300, loss = 4.52 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 17:14:45.709781: step 65310, loss = 4.60 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 17:14:50.576025: step 65320, loss = 4.52 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 17:14:55.364122: step 65330, loss = 4.45 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 17:15:00.214305: step 65340, loss = 4.82 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 17:15:04.831328: step 65350, loss = 4.66 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 17:15:09.577437: step 65360, loss = 4.36 (249.6 examples/sec; 0.513 sec/batch)
2016-07-15 17:15:14.195347: step 65370, loss = 4.53 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 17:15:19.283929: step 65380, loss = 4.76 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 17:15:24.796543: step 65390, loss = 4.77 (238.1 examples/sec; 0.538 sec/batch)
2016-07-15 17:15:30.536447: step 65400, loss = 4.72 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 17:15:37.239511: step 65410, loss = 4.94 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 17:15:42.210496: step 65420, loss = 4.89 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 17:15:46.920574: step 65430, loss = 4.79 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 17:15:52.647763: step 65440, loss = 4.51 (183.6 examples/sec; 0.697 sec/batch)
2016-07-15 17:15:58.752829: step 65450, loss = 4.49 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 17:16:03.538487: step 65460, loss = 4.35 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 17:16:08.327389: step 65470, loss = 4.74 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 17:16:13.130483: step 65480, loss = 4.64 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 17:16:18.079661: step 65490, loss = 4.81 (223.5 examples/sec; 0.573 sec/batch)
2016-07-15 17:16:24.682478: step 65500, loss = 4.71 (198.6 examples/sec; 0.644 sec/batch)
2016-07-15 17:16:31.430628: step 65510, loss = 4.62 (201.7 examples/sec; 0.634 sec/batch)
2016-07-15 17:16:36.837979: step 65520, loss = 4.84 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 17:16:41.544764: step 65530, loss = 4.85 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 17:16:46.399243: step 65540, loss = 4.88 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 17:16:51.086693: step 65550, loss = 4.65 (281.0 examples/sec; 0.456 sec/batch)
2016-07-15 17:16:55.750453: step 65560, loss = 4.43 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 17:17:00.433707: step 65570, loss = 4.56 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 17:17:05.097774: step 65580, loss = 4.28 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 17:17:10.892460: step 65590, loss = 4.47 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 17:17:15.747398: step 65600, loss = 4.63 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 17:17:21.512235: step 65610, loss = 4.28 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 17:17:26.211669: step 65620, loss = 4.72 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 17:17:31.585301: step 65630, loss = 4.37 (183.8 examples/sec; 0.696 sec/batch)
2016-07-15 17:17:37.983831: step 65640, loss = 4.58 (220.5 examples/sec; 0.581 sec/batch)
2016-07-15 17:17:42.835838: step 65650, loss = 4.54 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 17:17:47.546658: step 65660, loss = 4.78 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 17:17:52.182951: step 65670, loss = 4.43 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 17:17:57.873462: step 65680, loss = 4.48 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 17:18:02.773011: step 65690, loss = 4.87 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 17:18:07.524374: step 65700, loss = 4.33 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 17:18:13.292740: step 65710, loss = 4.68 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 17:18:18.267132: step 65720, loss = 4.79 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 17:18:24.843748: step 65730, loss = 4.60 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 17:18:29.984908: step 65740, loss = 4.43 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 17:18:35.692664: step 65750, loss = 4.76 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 17:18:41.357186: step 65760, loss = 4.67 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 17:18:46.385090: step 65770, loss = 4.47 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 17:18:51.137541: step 65780, loss = 4.70 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 17:18:56.929281: step 65790, loss = 4.86 (180.8 examples/sec; 0.708 sec/batch)
2016-07-15 17:19:02.964062: step 65800, loss = 4.49 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 17:19:09.766336: step 65810, loss = 4.68 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 17:19:14.851410: step 65820, loss = 4.35 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 17:19:20.464535: step 65830, loss = 4.78 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 17:19:25.189753: step 65840, loss = 4.70 (281.0 examples/sec; 0.456 sec/batch)
2016-07-15 17:19:30.104358: step 65850, loss = 4.42 (237.9 examples/sec; 0.538 sec/batch)
2016-07-15 17:19:36.663267: step 65860, loss = 4.63 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 17:19:41.600825: step 65870, loss = 4.86 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 17:19:46.343530: step 65880, loss = 4.61 (222.4 examples/sec; 0.576 sec/batch)
2016-07-15 17:19:52.044590: step 65890, loss = 4.74 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 17:19:57.747880: step 65900, loss = 4.46 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 17:20:03.655989: step 65910, loss = 4.57 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 17:20:08.510238: step 65920, loss = 4.51 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 17:20:13.289609: step 65930, loss = 4.73 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 17:20:17.992106: step 65940, loss = 4.80 (285.1 examples/sec; 0.449 sec/batch)
2016-07-15 17:20:22.924667: step 65950, loss = 4.60 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 17:20:28.502940: step 65960, loss = 4.75 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 17:20:33.291689: step 65970, loss = 4.90 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 17:20:38.153341: step 65980, loss = 4.74 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 17:20:42.846732: step 65990, loss = 4.71 (284.3 examples/sec; 0.450 sec/batch)
2016-07-15 17:20:47.481611: step 66000, loss = 4.76 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 17:20:54.179917: step 66010, loss = 4.83 (221.2 examples/sec; 0.579 sec/batch)
2016-07-15 17:20:59.385803: step 66020, loss = 4.45 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 17:21:04.817477: step 66030, loss = 4.52 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 17:21:10.637340: step 66040, loss = 4.58 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 17:21:16.043815: step 66050, loss = 4.58 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 17:21:21.378887: step 66060, loss = 4.68 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 17:21:26.121499: step 66070, loss = 4.76 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 17:21:31.031834: step 66080, loss = 4.75 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 17:21:35.777662: step 66090, loss = 4.39 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 17:21:41.693342: step 66100, loss = 4.34 (188.6 examples/sec; 0.679 sec/batch)
2016-07-15 17:21:48.825332: step 66110, loss = 4.67 (252.5 examples/sec; 0.507 sec/batch)
2016-07-15 17:21:54.699335: step 66120, loss = 4.50 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 17:21:59.520448: step 66130, loss = 4.46 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 17:22:04.347126: step 66140, loss = 4.86 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 17:22:10.447953: step 66150, loss = 4.78 (199.3 examples/sec; 0.642 sec/batch)
2016-07-15 17:22:16.176650: step 66160, loss = 4.61 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 17:22:21.913061: step 66170, loss = 4.69 (200.8 examples/sec; 0.638 sec/batch)
2016-07-15 17:22:27.055777: step 66180, loss = 4.44 (197.1 examples/sec; 0.649 sec/batch)
2016-07-15 17:22:32.647348: step 66190, loss = 4.54 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 17:22:37.341129: step 66200, loss = 4.39 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 17:22:42.873720: step 66210, loss = 4.92 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 17:22:48.033206: step 66220, loss = 4.48 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 17:22:53.418402: step 66230, loss = 4.69 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 17:22:59.166614: step 66240, loss = 4.66 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 17:23:04.034771: step 66250, loss = 4.58 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 17:23:08.828312: step 66260, loss = 4.55 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 17:23:13.602800: step 66270, loss = 4.52 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 17:23:18.202180: step 66280, loss = 4.50 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 17:23:23.085136: step 66290, loss = 4.54 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 17:23:28.647643: step 66300, loss = 4.58 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 17:23:34.382854: step 66310, loss = 4.64 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 17:23:39.207000: step 66320, loss = 4.60 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 17:23:43.886135: step 66330, loss = 4.57 (281.8 examples/sec; 0.454 sec/batch)
2016-07-15 17:23:48.534356: step 66340, loss = 4.45 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 17:23:54.204430: step 66350, loss = 4.50 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 17:23:59.061213: step 66360, loss = 4.94 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 17:24:03.847371: step 66370, loss = 4.61 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 17:24:08.659463: step 66380, loss = 4.45 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 17:24:13.472917: step 66390, loss = 4.92 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 17:24:19.832527: step 66400, loss = 4.26 (209.2 examples/sec; 0.612 sec/batch)
2016-07-15 17:24:26.587857: step 66410, loss = 4.59 (220.8 examples/sec; 0.580 sec/batch)
2016-07-15 17:24:32.277211: step 66420, loss = 4.79 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 17:24:36.994946: step 66430, loss = 4.43 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 17:24:41.844535: step 66440, loss = 4.57 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 17:24:48.369584: step 66450, loss = 4.56 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 17:24:53.712097: step 66460, loss = 4.40 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 17:24:58.414788: step 66470, loss = 4.60 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 17:25:03.308140: step 66480, loss = 4.57 (278.0 examples/sec; 0.461 sec/batch)
2016-07-15 17:25:07.959824: step 66490, loss = 4.67 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 17:25:12.587195: step 66500, loss = 4.83 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 17:25:19.238517: step 66510, loss = 4.66 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 17:25:24.038587: step 66520, loss = 4.39 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 17:25:28.715803: step 66530, loss = 4.49 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 17:25:33.353528: step 66540, loss = 4.51 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 17:25:37.989143: step 66550, loss = 4.67 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 17:25:42.806755: step 66560, loss = 4.70 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 17:25:48.473364: step 66570, loss = 4.62 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 17:25:54.246999: step 66580, loss = 4.37 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 17:25:59.082154: step 66590, loss = 4.72 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 17:26:03.818890: step 66600, loss = 4.51 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 17:26:09.639818: step 66610, loss = 4.43 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 17:26:14.645688: step 66620, loss = 4.27 (217.7 examples/sec; 0.588 sec/batch)
2016-07-15 17:26:21.187402: step 66630, loss = 4.65 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 17:26:26.370116: step 66640, loss = 4.71 (254.2 examples/sec; 0.503 sec/batch)
2016-07-15 17:26:31.084666: step 66650, loss = 4.68 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 17:26:36.632014: step 66660, loss = 4.82 (188.5 examples/sec; 0.679 sec/batch)
2016-07-15 17:26:42.901934: step 66670, loss = 4.46 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 17:26:47.727476: step 66680, loss = 4.72 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 17:26:52.554991: step 66690, loss = 4.45 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 17:26:57.297529: step 66700, loss = 4.51 (283.6 examples/sec; 0.451 sec/batch)
2016-07-15 17:27:02.867629: step 66710, loss = 4.54 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 17:27:07.941804: step 66720, loss = 4.51 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 17:27:13.390150: step 66730, loss = 4.45 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 17:27:18.131239: step 66740, loss = 4.17 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 17:27:22.989464: step 66750, loss = 4.42 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 17:27:27.672077: step 66760, loss = 4.54 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 17:27:32.278298: step 66770, loss = 4.67 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 17:27:37.793745: step 66780, loss = 4.59 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 17:27:42.799885: step 66790, loss = 4.68 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 17:27:47.527853: step 66800, loss = 4.53 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 17:27:54.573931: step 66810, loss = 4.53 (198.8 examples/sec; 0.644 sec/batch)
2016-07-15 17:27:59.219702: step 66820, loss = 4.77 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 17:28:04.460914: step 66830, loss = 4.47 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 17:28:09.730320: step 66840, loss = 4.49 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 17:28:14.436907: step 66850, loss = 4.93 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 17:28:19.252442: step 66860, loss = 4.45 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 17:28:24.021890: step 66870, loss = 4.58 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 17:28:28.754556: step 66880, loss = 4.44 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 17:28:33.566813: step 66890, loss = 4.48 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 17:28:39.947817: step 66900, loss = 4.67 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 17:28:46.674835: step 66910, loss = 4.62 (229.9 examples/sec; 0.557 sec/batch)
2016-07-15 17:28:52.282497: step 66920, loss = 4.68 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 17:28:56.930421: step 66930, loss = 4.60 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 17:29:01.585017: step 66940, loss = 4.61 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 17:29:06.287812: step 66950, loss = 4.76 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 17:29:12.084239: step 66960, loss = 4.44 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 17:29:16.896656: step 66970, loss = 4.49 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 17:29:21.717821: step 66980, loss = 4.64 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 17:29:28.163047: step 66990, loss = 4.44 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 17:29:33.535447: step 67000, loss = 4.38 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 17:29:39.299490: step 67010, loss = 4.38 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 17:29:44.874704: step 67020, loss = 4.55 (183.9 examples/sec; 0.696 sec/batch)
2016-07-15 17:29:51.421842: step 67030, loss = 4.82 (246.4 examples/sec; 0.520 sec/batch)
2016-07-15 17:29:57.360438: step 67040, loss = 4.74 (183.7 examples/sec; 0.697 sec/batch)
2016-07-15 17:30:02.991995: step 67050, loss = 4.69 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 17:30:08.171408: step 67060, loss = 4.36 (235.7 examples/sec; 0.543 sec/batch)
2016-07-15 17:30:15.011579: step 67070, loss = 4.66 (176.8 examples/sec; 0.724 sec/batch)
2016-07-15 17:30:21.177438: step 67080, loss = 4.47 (247.6 examples/sec; 0.517 sec/batch)
2016-07-15 17:30:25.935069: step 67090, loss = 4.52 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 17:30:30.542108: step 67100, loss = 4.67 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 17:30:36.081665: step 67110, loss = 4.56 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 17:30:40.733666: step 67120, loss = 4.69 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 17:30:45.379042: step 67130, loss = 4.89 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 17:30:51.073698: step 67140, loss = 4.83 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 17:30:55.883670: step 67150, loss = 4.62 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 17:31:00.714279: step 67160, loss = 4.56 (235.9 examples/sec; 0.543 sec/batch)
2016-07-15 17:31:07.549004: step 67170, loss = 4.96 (182.5 examples/sec; 0.701 sec/batch)
2016-07-15 17:31:13.192994: step 67180, loss = 4.88 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 17:31:18.951720: step 67190, loss = 4.90 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 17:31:24.149401: step 67200, loss = 4.74 (200.5 examples/sec; 0.638 sec/batch)
2016-07-15 17:31:30.941263: step 67210, loss = 4.44 (228.0 examples/sec; 0.561 sec/batch)
2016-07-15 17:31:36.659437: step 67220, loss = 4.65 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 17:31:41.392911: step 67230, loss = 4.81 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 17:31:46.188606: step 67240, loss = 4.74 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 17:31:50.853247: step 67250, loss = 4.31 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 17:31:55.637783: step 67260, loss = 4.48 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 17:32:00.373745: step 67270, loss = 4.54 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 17:32:05.110278: step 67280, loss = 4.61 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 17:32:09.947359: step 67290, loss = 4.71 (264.7 examples/sec; 0.483 sec/batch)
2016-07-15 17:32:16.389055: step 67300, loss = 4.82 (198.1 examples/sec; 0.646 sec/batch)
2016-07-15 17:32:22.804462: step 67310, loss = 4.69 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 17:32:27.735159: step 67320, loss = 4.58 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 17:32:33.272918: step 67330, loss = 4.57 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 17:32:39.018708: step 67340, loss = 4.48 (252.3 examples/sec; 0.507 sec/batch)
2016-07-15 17:32:44.213130: step 67350, loss = 4.53 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 17:32:49.623220: step 67360, loss = 4.31 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 17:32:54.307405: step 67370, loss = 4.59 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 17:32:59.426475: step 67380, loss = 4.53 (185.6 examples/sec; 0.690 sec/batch)
2016-07-15 17:33:06.033976: step 67390, loss = 4.74 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 17:33:10.805556: step 67400, loss = 4.08 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 17:33:16.351017: step 67410, loss = 4.49 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 17:33:21.007257: step 67420, loss = 4.51 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 17:33:25.627364: step 67430, loss = 4.35 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 17:33:31.352290: step 67440, loss = 4.42 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 17:33:36.625991: step 67450, loss = 4.48 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 17:33:41.996790: step 67460, loss = 4.45 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 17:33:47.817242: step 67470, loss = 4.61 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 17:33:52.601482: step 67480, loss = 4.43 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 17:33:57.367921: step 67490, loss = 4.60 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 17:34:02.067977: step 67500, loss = 4.56 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 17:34:07.830876: step 67510, loss = 4.52 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 17:34:12.543575: step 67520, loss = 4.84 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 17:34:18.507248: step 67530, loss = 4.61 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 17:34:24.403164: step 67540, loss = 4.43 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 17:34:29.973967: step 67550, loss = 4.89 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 17:34:35.141925: step 67560, loss = 4.33 (218.3 examples/sec; 0.586 sec/batch)
2016-07-15 17:34:40.820118: step 67570, loss = 4.68 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 17:34:45.562187: step 67580, loss = 4.81 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 17:34:50.431926: step 67590, loss = 4.40 (243.8 examples/sec; 0.525 sec/batch)
2016-07-15 17:34:55.090108: step 67600, loss = 4.39 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 17:35:00.651400: step 67610, loss = 4.48 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 17:35:06.214856: step 67620, loss = 4.46 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 17:35:11.219779: step 67630, loss = 4.77 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 17:35:15.912506: step 67640, loss = 4.60 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 17:35:20.700196: step 67650, loss = 4.66 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 17:35:25.867964: step 67660, loss = 4.84 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 17:35:32.417924: step 67670, loss = 4.71 (184.1 examples/sec; 0.695 sec/batch)
2016-07-15 17:35:38.542448: step 67680, loss = 4.73 (242.0 examples/sec; 0.529 sec/batch)
2016-07-15 17:35:44.862162: step 67690, loss = 4.46 (239.1 examples/sec; 0.535 sec/batch)
2016-07-15 17:35:49.836525: step 67700, loss = 4.30 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 17:35:55.932399: step 67710, loss = 4.42 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 17:36:00.742025: step 67720, loss = 4.73 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 17:36:05.706652: step 67730, loss = 4.61 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 17:36:13.128916: step 67740, loss = 4.45 (88.3 examples/sec; 1.449 sec/batch)
2016-07-15 17:36:25.062809: step 67750, loss = 4.77 (88.5 examples/sec; 1.447 sec/batch)
2016-07-15 17:36:38.932546: step 67760, loss = 4.37 (114.2 examples/sec; 1.121 sec/batch)
2016-07-15 17:36:53.391981: step 67770, loss = 4.59 (97.3 examples/sec; 1.315 sec/batch)
2016-07-15 17:37:08.143613: step 67780, loss = 4.75 (75.1 examples/sec; 1.704 sec/batch)
2016-07-15 17:37:19.821690: step 67790, loss = 4.30 (239.4 examples/sec; 0.535 sec/batch)
2016-07-15 17:37:25.874194: step 67800, loss = 4.53 (240.5 examples/sec; 0.532 sec/batch)
2016-07-15 17:37:31.680886: step 67810, loss = 4.59 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 17:37:36.726293: step 67820, loss = 4.94 (254.0 examples/sec; 0.504 sec/batch)
2016-07-15 17:37:41.841214: step 67830, loss = 4.62 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 17:37:48.205636: step 67840, loss = 4.81 (170.0 examples/sec; 0.753 sec/batch)
2016-07-15 17:37:54.854485: step 67850, loss = 4.49 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 17:38:01.027331: step 67860, loss = 4.66 (182.9 examples/sec; 0.700 sec/batch)
2016-07-15 17:38:06.268520: step 67870, loss = 4.88 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 17:38:11.312554: step 67880, loss = 4.52 (229.0 examples/sec; 0.559 sec/batch)
2016-07-15 17:38:18.018175: step 67890, loss = 4.39 (168.4 examples/sec; 0.760 sec/batch)
2016-07-15 17:38:24.051872: step 67900, loss = 4.59 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 17:38:29.782965: step 67910, loss = 4.39 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 17:38:34.644664: step 67920, loss = 4.52 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 17:38:39.389314: step 67930, loss = 4.78 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 17:38:45.220695: step 67940, loss = 4.60 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 17:38:51.141127: step 67950, loss = 4.62 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 17:38:55.981608: step 67960, loss = 4.44 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 17:39:00.802045: step 67970, loss = 4.80 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 17:39:05.533405: step 67980, loss = 4.51 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 17:39:10.412395: step 67990, loss = 4.71 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 17:39:15.080057: step 68000, loss = 4.41 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 17:39:20.670018: step 68010, loss = 4.85 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 17:39:25.359816: step 68020, loss = 4.35 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 17:39:31.095925: step 68030, loss = 4.65 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 17:39:36.567649: step 68040, loss = 4.54 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 17:39:41.769977: step 68050, loss = 4.49 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 17:39:46.519316: step 68060, loss = 4.59 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 17:39:51.335975: step 68070, loss = 4.79 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 17:39:56.164290: step 68080, loss = 4.49 (251.4 examples/sec; 0.509 sec/batch)
2016-07-15 17:40:01.030148: step 68090, loss = 4.51 (257.3 examples/sec; 0.498 sec/batch)
2016-07-15 17:40:06.170143: step 68100, loss = 4.78 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 17:40:11.883390: step 68110, loss = 4.74 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 17:40:16.705524: step 68120, loss = 4.54 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 17:40:21.584171: step 68130, loss = 4.22 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 17:40:28.074847: step 68140, loss = 4.54 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 17:40:33.345978: step 68150, loss = 4.52 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 17:40:38.079151: step 68160, loss = 4.42 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 17:40:42.984686: step 68170, loss = 4.36 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 17:40:47.687729: step 68180, loss = 4.48 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 17:40:52.336380: step 68190, loss = 4.55 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 17:40:58.044355: step 68200, loss = 4.85 (219.8 examples/sec; 0.582 sec/batch)
2016-07-15 17:41:03.873655: step 68210, loss = 4.65 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 17:41:08.603829: step 68220, loss = 4.44 (282.0 examples/sec; 0.454 sec/batch)
2016-07-15 17:41:13.230221: step 68230, loss = 4.67 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 17:41:19.002761: step 68240, loss = 4.47 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 17:41:24.613883: step 68250, loss = 4.45 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 17:41:29.590539: step 68260, loss = 4.55 (287.0 examples/sec; 0.446 sec/batch)
2016-07-15 17:41:34.272883: step 68270, loss = 4.59 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 17:41:38.923521: step 68280, loss = 4.55 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 17:41:44.287895: step 68290, loss = 4.56 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 17:41:49.582673: step 68300, loss = 4.53 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 17:41:56.540776: step 68310, loss = 4.54 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 17:42:02.297419: step 68320, loss = 4.40 (253.4 examples/sec; 0.505 sec/batch)
2016-07-15 17:42:07.141476: step 68330, loss = 4.90 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 17:42:11.765686: step 68340, loss = 4.61 (283.6 examples/sec; 0.451 sec/batch)
2016-07-15 17:42:16.432311: step 68350, loss = 4.77 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 17:42:21.138404: step 68360, loss = 4.59 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 17:42:25.825531: step 68370, loss = 4.70 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 17:42:30.484433: step 68380, loss = 4.63 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 17:42:35.608426: step 68390, loss = 4.56 (201.7 examples/sec; 0.634 sec/batch)
2016-07-15 17:42:41.094095: step 68400, loss = 4.74 (252.2 examples/sec; 0.508 sec/batch)
2016-07-15 17:42:46.731239: step 68410, loss = 4.37 (285.2 examples/sec; 0.449 sec/batch)
2016-07-15 17:42:51.374953: step 68420, loss = 4.48 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 17:42:56.701285: step 68430, loss = 4.58 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 17:43:01.886944: step 68440, loss = 4.72 (241.8 examples/sec; 0.529 sec/batch)
2016-07-15 17:43:06.569501: step 68450, loss = 4.70 (285.6 examples/sec; 0.448 sec/batch)
2016-07-15 17:43:11.217084: step 68460, loss = 4.73 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 17:43:16.737727: step 68470, loss = 4.64 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 17:43:22.106319: step 68480, loss = 4.25 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 17:43:26.790002: step 68490, loss = 4.61 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 17:43:31.567036: step 68500, loss = 4.55 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 17:43:37.152874: step 68510, loss = 4.59 (281.7 examples/sec; 0.454 sec/batch)
2016-07-15 17:43:41.870287: step 68520, loss = 4.39 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 17:43:47.660263: step 68530, loss = 4.53 (254.2 examples/sec; 0.504 sec/batch)
2016-07-15 17:43:53.231843: step 68540, loss = 4.55 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 17:43:58.382814: step 68550, loss = 4.69 (221.9 examples/sec; 0.577 sec/batch)
2016-07-15 17:44:04.100205: step 68560, loss = 4.09 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 17:44:08.845953: step 68570, loss = 4.43 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 17:44:13.755106: step 68580, loss = 4.54 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 17:44:20.275692: step 68590, loss = 4.76 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 17:44:25.602293: step 68600, loss = 4.50 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 17:44:32.310814: step 68610, loss = 4.42 (280.4 examples/sec; 0.457 sec/batch)
2016-07-15 17:44:36.965596: step 68620, loss = 4.50 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 17:44:42.686167: step 68630, loss = 4.49 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 17:44:47.557779: step 68640, loss = 4.38 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 17:44:52.233788: step 68650, loss = 4.56 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 17:44:56.848624: step 68660, loss = 4.39 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 17:45:01.498449: step 68670, loss = 4.54 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 17:45:07.259231: step 68680, loss = 4.75 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 17:45:12.120089: step 68690, loss = 4.65 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 17:45:16.790363: step 68700, loss = 4.39 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 17:45:22.335690: step 68710, loss = 4.92 (251.3 examples/sec; 0.509 sec/batch)
2016-07-15 17:45:28.028907: step 68720, loss = 4.37 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 17:45:32.820566: step 68730, loss = 4.65 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 17:45:37.626825: step 68740, loss = 4.78 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 17:45:42.373145: step 68750, loss = 4.43 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 17:45:47.661872: step 68760, loss = 4.38 (191.1 examples/sec; 0.670 sec/batch)
2016-07-15 17:45:54.150859: step 68770, loss = 4.34 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 17:45:59.027246: step 68780, loss = 4.56 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 17:46:03.754353: step 68790, loss = 4.53 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 17:46:08.543301: step 68800, loss = 4.55 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 17:46:14.327648: step 68810, loss = 4.51 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 17:46:19.048383: step 68820, loss = 4.62 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 17:46:23.642751: step 68830, loss = 4.72 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 17:46:29.076843: step 68840, loss = 4.61 (207.6 examples/sec; 0.617 sec/batch)
2016-07-15 17:46:34.215685: step 68850, loss = 4.39 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 17:46:38.941755: step 68860, loss = 4.21 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 17:46:44.488238: step 68870, loss = 4.61 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 17:46:50.788462: step 68880, loss = 4.60 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 17:46:55.588789: step 68890, loss = 4.35 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 17:47:00.375310: step 68900, loss = 4.44 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 17:47:07.789175: step 68910, loss = 4.67 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 17:47:13.245798: step 68920, loss = 4.22 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 17:47:17.991233: step 68930, loss = 4.56 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 17:47:23.128866: step 68940, loss = 4.57 (184.8 examples/sec; 0.693 sec/batch)
2016-07-15 17:47:29.635841: step 68950, loss = 4.47 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 17:47:34.684853: step 68960, loss = 4.52 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 17:47:39.444633: step 68970, loss = 4.38 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 17:47:44.313110: step 68980, loss = 4.44 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 17:47:48.957087: step 68990, loss = 4.42 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 17:47:53.630296: step 69000, loss = 4.68 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 17:47:59.161105: step 69010, loss = 4.46 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 17:48:04.394669: step 69020, loss = 4.77 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 17:48:09.740586: step 69030, loss = 4.73 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 17:48:14.463010: step 69040, loss = 4.43 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 17:48:19.392499: step 69050, loss = 4.34 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 17:48:24.309850: step 69060, loss = 4.50 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 17:48:29.576671: step 69070, loss = 4.48 (281.6 examples/sec; 0.455 sec/batch)
2016-07-15 17:48:34.265693: step 69080, loss = 4.47 (281.9 examples/sec; 0.454 sec/batch)
2016-07-15 17:48:40.082138: step 69090, loss = 4.29 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 17:48:45.774427: step 69100, loss = 4.36 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 17:48:51.867182: step 69110, loss = 4.53 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 17:48:56.549968: step 69120, loss = 4.45 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 17:49:01.245899: step 69130, loss = 4.64 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 17:49:07.041638: step 69140, loss = 4.65 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 17:49:11.879441: step 69150, loss = 4.68 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 17:49:16.579620: step 69160, loss = 4.29 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 17:49:21.259191: step 69170, loss = 4.63 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 17:49:27.010413: step 69180, loss = 4.39 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 17:49:32.299421: step 69190, loss = 4.30 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 17:49:37.664065: step 69200, loss = 4.52 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 17:49:43.424351: step 69210, loss = 4.50 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 17:49:48.260588: step 69220, loss = 4.60 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 17:49:53.122413: step 69230, loss = 4.81 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 17:49:57.861401: step 69240, loss = 4.27 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 17:50:02.437778: step 69250, loss = 4.50 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 17:50:07.143262: step 69260, loss = 4.64 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 17:50:11.823596: step 69270, loss = 4.48 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 17:50:16.479475: step 69280, loss = 4.52 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 17:50:21.164037: step 69290, loss = 4.64 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 17:50:25.833723: step 69300, loss = 4.33 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 17:50:32.606287: step 69310, loss = 4.54 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 17:50:37.232630: step 69320, loss = 4.45 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 17:50:43.084998: step 69330, loss = 4.44 (239.9 examples/sec; 0.533 sec/batch)
2016-07-15 17:50:48.395599: step 69340, loss = 4.45 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 17:50:53.784495: step 69350, loss = 4.64 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 17:50:58.588272: step 69360, loss = 4.69 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 17:51:03.408432: step 69370, loss = 4.17 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 17:51:08.155704: step 69380, loss = 4.62 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 17:51:12.988604: step 69390, loss = 4.54 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 17:51:17.642353: step 69400, loss = 4.46 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 17:51:23.244214: step 69410, loss = 4.43 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 17:51:28.462697: step 69420, loss = 4.45 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 17:51:33.767177: step 69430, loss = 4.53 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 17:51:38.480788: step 69440, loss = 4.65 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 17:51:43.116551: step 69450, loss = 4.52 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 17:51:47.784941: step 69460, loss = 4.48 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 17:51:52.462046: step 69470, loss = 4.70 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 17:51:58.104264: step 69480, loss = 4.47 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 17:52:03.066880: step 69490, loss = 4.77 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 17:52:07.799273: step 69500, loss = 4.85 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 17:52:13.608513: step 69510, loss = 4.69 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 17:52:18.468568: step 69520, loss = 4.59 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 17:52:23.233327: step 69530, loss = 4.63 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 17:52:28.792409: step 69540, loss = 4.68 (184.9 examples/sec; 0.692 sec/batch)
2016-07-15 17:52:35.069411: step 69550, loss = 4.51 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 17:52:39.946623: step 69560, loss = 4.75 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 17:52:44.721629: step 69570, loss = 4.46 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 17:52:49.482609: step 69580, loss = 4.23 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 17:52:54.316564: step 69590, loss = 4.41 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 17:53:00.872065: step 69600, loss = 4.53 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 17:53:07.052005: step 69610, loss = 4.57 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 17:53:12.108221: step 69620, loss = 4.74 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 17:53:17.611272: step 69630, loss = 4.65 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 17:53:22.396081: step 69640, loss = 4.28 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 17:53:27.596342: step 69650, loss = 4.47 (185.0 examples/sec; 0.692 sec/batch)
2016-07-15 17:53:34.110864: step 69660, loss = 4.37 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 17:53:39.100666: step 69670, loss = 4.26 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 17:53:43.872560: step 69680, loss = 4.27 (268.1 examples/sec; 0.478 sec/batch)
2016-07-15 17:53:49.637384: step 69690, loss = 4.46 (185.5 examples/sec; 0.690 sec/batch)
2016-07-15 17:53:55.714050: step 69700, loss = 4.67 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 17:54:01.572492: step 69710, loss = 4.81 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 17:54:06.250252: step 69720, loss = 4.10 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 17:54:11.097134: step 69730, loss = 4.33 (210.2 examples/sec; 0.609 sec/batch)
2016-07-15 17:54:16.766796: step 69740, loss = 4.45 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 17:54:22.485606: step 69750, loss = 4.74 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 17:54:27.391505: step 69760, loss = 4.38 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 17:54:32.169964: step 69770, loss = 4.39 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 17:54:36.998196: step 69780, loss = 4.40 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 17:54:41.859344: step 69790, loss = 4.53 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 17:54:48.203265: step 69800, loss = 4.41 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 17:54:55.001016: step 69810, loss = 4.54 (225.8 examples/sec; 0.567 sec/batch)
2016-07-15 17:55:00.736250: step 69820, loss = 4.33 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 17:55:05.511728: step 69830, loss = 4.50 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 17:55:10.378604: step 69840, loss = 4.52 (252.2 examples/sec; 0.507 sec/batch)
2016-07-15 17:55:15.096887: step 69850, loss = 4.58 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 17:55:19.790397: step 69860, loss = 4.41 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 17:55:25.080281: step 69870, loss = 4.48 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 17:55:30.336945: step 69880, loss = 4.48 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 17:55:35.031034: step 69890, loss = 4.27 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 17:55:39.702802: step 69900, loss = 4.68 (248.1 examples/sec; 0.516 sec/batch)
2016-07-15 17:55:46.258394: step 69910, loss = 4.51 (200.3 examples/sec; 0.639 sec/batch)
2016-07-15 17:55:51.381010: step 69920, loss = 4.30 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 17:55:57.027235: step 69930, loss = 4.35 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 17:56:01.786902: step 69940, loss = 4.41 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 17:56:06.680367: step 69950, loss = 4.28 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 17:56:11.421705: step 69960, loss = 4.42 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 17:56:16.241854: step 69970, loss = 4.60 (282.6 examples/sec; 0.453 sec/batch)
2016-07-15 17:56:21.079431: step 69980, loss = 4.67 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 17:56:25.787101: step 69990, loss = 4.59 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 17:56:30.361970: step 70000, loss = 4.63 (286.7 examples/sec; 0.446 sec/batch)
2016-07-15 17:56:36.137431: step 70010, loss = 4.34 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 17:56:40.838588: step 70020, loss = 4.30 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 17:56:45.504905: step 70030, loss = 4.54 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 17:56:50.181429: step 70040, loss = 4.53 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 17:56:54.820055: step 70050, loss = 4.44 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 17:56:59.840587: step 70060, loss = 4.32 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 17:57:05.403431: step 70070, loss = 4.37 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 17:57:11.168136: step 70080, loss = 4.46 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 17:57:15.833414: step 70090, loss = 4.45 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 17:57:20.527623: step 70100, loss = 4.59 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 17:57:26.171200: step 70110, loss = 4.36 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 17:57:31.915963: step 70120, loss = 4.43 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 17:57:37.388082: step 70130, loss = 4.48 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 17:57:42.620578: step 70140, loss = 4.29 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 17:57:47.329337: step 70150, loss = 4.33 (285.0 examples/sec; 0.449 sec/batch)
2016-07-15 17:57:51.967126: step 70160, loss = 4.64 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 17:57:56.667816: step 70170, loss = 4.17 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 17:58:01.339695: step 70180, loss = 4.20 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 17:58:06.032673: step 70190, loss = 4.60 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 17:58:10.659890: step 70200, loss = 4.27 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 17:58:16.790785: step 70210, loss = 4.23 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 17:58:22.155522: step 70220, loss = 4.34 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 17:58:27.989830: step 70230, loss = 4.39 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 17:58:33.436387: step 70240, loss = 4.33 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 17:58:38.655897: step 70250, loss = 4.34 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 17:58:43.423364: step 70260, loss = 4.34 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 17:58:48.284526: step 70270, loss = 4.43 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 17:58:53.114639: step 70280, loss = 4.56 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 17:58:57.873580: step 70290, loss = 4.58 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 17:59:02.523830: step 70300, loss = 4.46 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 17:59:08.196716: step 70310, loss = 4.62 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 17:59:12.841521: step 70320, loss = 4.24 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 17:59:18.471467: step 70330, loss = 4.50 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 17:59:23.658933: step 70340, loss = 4.36 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 17:59:29.256289: step 70350, loss = 4.34 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 17:59:35.080818: step 70360, loss = 4.44 (217.3 examples/sec; 0.589 sec/batch)
2016-07-15 17:59:39.990995: step 70370, loss = 4.46 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 17:59:44.730918: step 70380, loss = 4.65 (247.9 examples/sec; 0.516 sec/batch)
2016-07-15 17:59:50.753644: step 70390, loss = 4.70 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 17:59:56.626895: step 70400, loss = 4.44 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 18:00:02.401868: step 70410, loss = 4.41 (280.9 examples/sec; 0.456 sec/batch)
2016-07-15 18:00:07.032136: step 70420, loss = 4.55 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 18:00:12.056757: step 70430, loss = 4.52 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 18:00:17.585833: step 70440, loss = 4.50 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 18:00:22.352294: step 70450, loss = 4.50 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 18:00:26.970709: step 70460, loss = 4.23 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 18:00:31.620358: step 70470, loss = 4.29 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 18:00:36.293594: step 70480, loss = 4.46 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 18:00:40.949808: step 70490, loss = 4.35 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 18:00:46.594989: step 70500, loss = 4.52 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 18:00:52.590576: step 70510, loss = 4.60 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 18:00:57.247504: step 70520, loss = 4.67 (284.9 examples/sec; 0.449 sec/batch)
2016-07-15 18:01:01.940282: step 70530, loss = 4.18 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 18:01:06.627104: step 70540, loss = 4.55 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 18:01:11.310302: step 70550, loss = 4.49 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 18:01:15.982111: step 70560, loss = 4.48 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 18:01:21.251890: step 70570, loss = 4.51 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 18:01:26.576447: step 70580, loss = 4.27 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 18:01:32.374629: step 70590, loss = 4.38 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 18:01:37.205493: step 70600, loss = 4.42 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 18:01:43.069059: step 70610, loss = 4.59 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 18:01:47.785526: step 70620, loss = 4.46 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 18:01:53.393596: step 70630, loss = 4.72 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 18:01:59.636928: step 70640, loss = 4.41 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 18:02:04.445668: step 70650, loss = 4.81 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 18:02:09.186334: step 70660, loss = 4.79 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 18:02:15.249498: step 70670, loss = 4.43 (194.7 examples/sec; 0.658 sec/batch)
2016-07-15 18:02:20.855419: step 70680, loss = 4.40 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 18:02:25.601017: step 70690, loss = 4.52 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 18:02:30.250011: step 70700, loss = 4.44 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 18:02:35.965308: step 70710, loss = 4.56 (282.9 examples/sec; 0.453 sec/batch)
2016-07-15 18:02:40.628716: step 70720, loss = 4.56 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 18:02:45.319524: step 70730, loss = 4.33 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 18:02:50.003823: step 70740, loss = 4.29 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 18:02:55.761660: step 70750, loss = 4.36 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 18:03:01.103480: step 70760, loss = 4.46 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 18:03:06.428331: step 70770, loss = 4.63 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 18:03:11.154775: step 70780, loss = 4.59 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 18:03:16.021924: step 70790, loss = 4.36 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 18:03:20.764727: step 70800, loss = 4.62 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 18:03:26.541712: step 70810, loss = 4.46 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 18:03:31.254212: step 70820, loss = 4.50 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 18:03:36.280015: step 70830, loss = 4.18 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 18:03:41.833186: step 70840, loss = 4.23 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 18:03:46.594078: step 70850, loss = 4.48 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 18:03:51.239204: step 70860, loss = 4.54 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 18:03:56.195158: step 70870, loss = 4.49 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 18:04:01.776639: step 70880, loss = 4.49 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 18:04:07.544761: step 70890, loss = 4.25 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 18:04:12.368195: step 70900, loss = 4.48 (283.1 examples/sec; 0.452 sec/batch)
2016-07-15 18:04:17.945329: step 70910, loss = 4.37 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 18:04:22.648257: step 70920, loss = 4.40 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 18:04:27.303501: step 70930, loss = 4.44 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 18:04:31.982597: step 70940, loss = 4.37 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 18:04:37.200353: step 70950, loss = 4.57 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 18:04:42.554411: step 70960, loss = 4.28 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 18:04:47.321633: step 70970, loss = 4.65 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 18:04:52.176092: step 70980, loss = 4.67 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 18:04:57.001325: step 70990, loss = 4.30 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 18:05:03.003404: step 71000, loss = 4.77 (187.8 examples/sec; 0.682 sec/batch)
2016-07-15 18:05:10.108953: step 71010, loss = 4.47 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 18:05:14.873156: step 71020, loss = 4.40 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 18:05:19.744923: step 71030, loss = 4.59 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 18:05:24.484009: step 71040, loss = 4.43 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 18:05:29.093018: step 71050, loss = 4.40 (282.0 examples/sec; 0.454 sec/batch)
2016-07-15 18:05:34.649436: step 71060, loss = 4.72 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 18:05:39.716583: step 71070, loss = 4.41 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 18:05:44.481660: step 71080, loss = 4.48 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 18:05:50.208884: step 71090, loss = 4.55 (191.9 examples/sec; 0.667 sec/batch)
2016-07-15 18:05:56.284253: step 71100, loss = 4.81 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 18:06:02.089053: step 71110, loss = 4.36 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 18:06:06.933474: step 71120, loss = 4.42 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 18:06:11.635796: step 71130, loss = 4.50 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 18:06:16.537269: step 71140, loss = 4.75 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 18:06:21.330489: step 71150, loss = 4.57 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 18:06:26.120756: step 71160, loss = 4.48 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 18:06:30.976276: step 71170, loss = 4.62 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 18:06:37.508395: step 71180, loss = 4.33 (196.4 examples/sec; 0.652 sec/batch)
2016-07-15 18:06:42.848068: step 71190, loss = 4.65 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 18:06:47.581100: step 71200, loss = 4.46 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 18:06:54.382172: step 71210, loss = 4.47 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 18:07:00.438071: step 71220, loss = 4.21 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 18:07:05.222771: step 71230, loss = 4.52 (281.0 examples/sec; 0.455 sec/batch)
2016-07-15 18:07:10.003662: step 71240, loss = 4.33 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 18:07:14.745970: step 71250, loss = 4.48 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 18:07:19.421647: step 71260, loss = 4.45 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 18:07:24.138058: step 71270, loss = 4.40 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 18:07:28.771588: step 71280, loss = 4.46 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 18:07:34.271290: step 71290, loss = 4.28 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 18:07:39.605656: step 71300, loss = 4.40 (232.9 examples/sec; 0.550 sec/batch)
2016-07-15 18:07:46.654884: step 71310, loss = 4.30 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 18:07:52.477270: step 71320, loss = 4.35 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 18:07:57.975544: step 71330, loss = 4.18 (199.2 examples/sec; 0.643 sec/batch)
2016-07-15 18:08:02.938095: step 71340, loss = 4.14 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 18:08:07.710253: step 71350, loss = 4.72 (228.3 examples/sec; 0.561 sec/batch)
2016-07-15 18:08:13.423984: step 71360, loss = 4.28 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 18:08:18.144819: step 71370, loss = 4.63 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 18:08:23.012659: step 71380, loss = 4.25 (249.7 examples/sec; 0.513 sec/batch)
2016-07-15 18:08:28.979005: step 71390, loss = 4.34 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 18:08:33.674567: step 71400, loss = 4.36 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 18:08:40.521795: step 71410, loss = 4.30 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 18:08:45.351003: step 71420, loss = 4.33 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 18:08:50.180110: step 71430, loss = 4.27 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 18:08:56.407735: step 71440, loss = 4.39 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 18:09:02.008299: step 71450, loss = 4.68 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 18:09:06.788074: step 71460, loss = 4.43 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 18:09:11.706749: step 71470, loss = 4.58 (232.3 examples/sec; 0.551 sec/batch)
2016-07-15 18:09:18.288338: step 71480, loss = 4.47 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 18:09:23.412653: step 71490, loss = 4.40 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 18:09:28.208292: step 71500, loss = 4.68 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 18:09:35.223807: step 71510, loss = 4.30 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 18:09:41.058256: step 71520, loss = 4.40 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 18:09:46.712305: step 71530, loss = 4.49 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 18:09:51.747676: step 71540, loss = 4.46 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 18:09:56.530171: step 71550, loss = 4.80 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 18:10:01.372205: step 71560, loss = 4.31 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 18:10:06.195673: step 71570, loss = 4.29 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 18:10:11.024644: step 71580, loss = 4.60 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 18:10:15.684538: step 71590, loss = 4.29 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 18:10:20.350446: step 71600, loss = 4.35 (287.2 examples/sec; 0.446 sec/batch)
2016-07-15 18:10:25.947121: step 71610, loss = 4.23 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 18:10:31.680468: step 71620, loss = 4.20 (222.2 examples/sec; 0.576 sec/batch)
2016-07-15 18:10:36.859342: step 71630, loss = 4.09 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 18:10:42.328692: step 71640, loss = 4.50 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 18:10:47.097508: step 71650, loss = 4.31 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 18:10:51.976516: step 71660, loss = 4.42 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 18:10:56.728240: step 71670, loss = 4.57 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 18:11:01.513704: step 71680, loss = 4.61 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 18:11:06.332118: step 71690, loss = 4.40 (254.7 examples/sec; 0.502 sec/batch)
2016-07-15 18:11:12.644319: step 71700, loss = 4.52 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 18:11:19.220892: step 71710, loss = 4.53 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 18:11:23.928610: step 71720, loss = 4.39 (281.2 examples/sec; 0.455 sec/batch)
2016-07-15 18:11:28.580395: step 71730, loss = 4.36 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 18:11:33.963263: step 71740, loss = 4.29 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 18:11:39.253381: step 71750, loss = 4.32 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 18:11:43.956566: step 71760, loss = 4.39 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 18:11:48.564756: step 71770, loss = 4.65 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 18:11:53.856779: step 71780, loss = 4.29 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 18:11:59.169358: step 71790, loss = 4.55 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 18:12:04.946326: step 71800, loss = 4.54 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 18:12:10.697725: step 71810, loss = 4.71 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 18:12:15.532329: step 71820, loss = 4.80 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 18:12:20.263672: step 71830, loss = 4.48 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 18:12:25.807315: step 71840, loss = 4.47 (187.6 examples/sec; 0.682 sec/batch)
2016-07-15 18:12:32.285417: step 71850, loss = 4.49 (180.1 examples/sec; 0.711 sec/batch)
2016-07-15 18:12:37.910733: step 71860, loss = 4.21 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 18:12:43.200175: step 71870, loss = 4.23 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 18:12:47.917992: step 71880, loss = 4.45 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 18:12:52.762241: step 71890, loss = 4.67 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 18:12:57.383059: step 71900, loss = 4.44 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 18:13:02.993626: step 71910, loss = 4.38 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 18:13:08.750739: step 71920, loss = 4.48 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 18:13:13.550870: step 71930, loss = 4.59 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 18:13:18.362538: step 71940, loss = 4.70 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 18:13:23.142066: step 71950, loss = 4.38 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 18:13:27.762787: step 71960, loss = 4.49 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 18:13:32.729074: step 71970, loss = 4.48 (201.4 examples/sec; 0.635 sec/batch)
2016-07-15 18:13:38.295150: step 71980, loss = 4.27 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 18:13:42.991950: step 71990, loss = 4.59 (283.5 examples/sec; 0.451 sec/batch)
2016-07-15 18:13:47.824953: step 72000, loss = 4.42 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 18:13:53.583775: step 72010, loss = 4.40 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 18:13:58.381498: step 72020, loss = 4.52 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 18:14:03.288699: step 72030, loss = 4.52 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 18:14:08.035559: step 72040, loss = 4.34 (249.7 examples/sec; 0.513 sec/batch)
2016-07-15 18:14:13.373855: step 72050, loss = 4.26 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 18:14:19.831586: step 72060, loss = 4.44 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 18:14:24.660055: step 72070, loss = 4.61 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 18:14:29.399314: step 72080, loss = 4.84 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 18:14:35.216930: step 72090, loss = 4.48 (186.5 examples/sec; 0.686 sec/batch)
2016-07-15 18:14:41.127810: step 72100, loss = 4.45 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 18:14:46.873575: step 72110, loss = 4.48 (277.4 examples/sec; 0.462 sec/batch)
2016-07-15 18:14:51.711923: step 72120, loss = 4.22 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 18:14:56.487454: step 72130, loss = 4.66 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 18:15:02.042943: step 72140, loss = 4.49 (184.5 examples/sec; 0.694 sec/batch)
2016-07-15 18:15:08.336172: step 72150, loss = 4.34 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 18:15:13.148836: step 72160, loss = 4.76 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 18:15:17.922022: step 72170, loss = 4.60 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 18:15:24.201377: step 72180, loss = 4.29 (192.6 examples/sec; 0.664 sec/batch)
2016-07-15 18:15:29.877919: step 72190, loss = 4.42 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 18:15:34.688655: step 72200, loss = 4.31 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 18:15:40.256250: step 72210, loss = 4.27 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 18:15:44.868732: step 72220, loss = 4.54 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 18:15:49.553517: step 72230, loss = 4.50 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 18:15:54.193393: step 72240, loss = 4.49 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 18:15:59.914627: step 72250, loss = 4.42 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 18:16:04.729444: step 72260, loss = 4.71 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 18:16:09.502005: step 72270, loss = 4.32 (257.3 examples/sec; 0.498 sec/batch)
2016-07-15 18:16:15.382935: step 72280, loss = 4.38 (187.5 examples/sec; 0.683 sec/batch)
2016-07-15 18:16:21.272634: step 72290, loss = 4.59 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 18:16:26.054054: step 72300, loss = 4.47 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 18:16:31.913212: step 72310, loss = 4.55 (253.4 examples/sec; 0.505 sec/batch)
2016-07-15 18:16:38.488991: step 72320, loss = 4.60 (208.9 examples/sec; 0.613 sec/batch)
2016-07-15 18:16:43.754723: step 72330, loss = 4.57 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 18:16:48.468095: step 72340, loss = 4.18 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 18:16:53.272443: step 72350, loss = 4.43 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 18:16:58.065100: step 72360, loss = 4.42 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 18:17:04.125347: step 72370, loss = 4.32 (194.8 examples/sec; 0.657 sec/batch)
2016-07-15 18:17:09.838401: step 72380, loss = 4.44 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 18:17:14.559479: step 72390, loss = 4.45 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 18:17:19.405993: step 72400, loss = 4.45 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 18:17:27.286868: step 72410, loss = 4.46 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 18:17:32.342486: step 72420, loss = 4.45 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 18:17:37.171784: step 72430, loss = 4.32 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 18:17:42.205097: step 72440, loss = 4.42 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 18:17:47.693778: step 72450, loss = 4.26 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 18:17:54.284976: step 72460, loss = 4.34 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 18:17:59.544620: step 72470, loss = 4.52 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 18:18:05.287452: step 72480, loss = 4.47 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 18:18:10.835915: step 72490, loss = 4.48 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 18:18:15.995593: step 72500, loss = 4.19 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 18:18:21.731014: step 72510, loss = 4.67 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 18:18:27.622532: step 72520, loss = 4.32 (187.3 examples/sec; 0.683 sec/batch)
2016-07-15 18:18:33.523468: step 72530, loss = 4.53 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 18:18:38.311469: step 72540, loss = 4.30 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 18:18:43.094471: step 72550, loss = 4.28 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 18:18:49.420119: step 72560, loss = 4.18 (199.3 examples/sec; 0.642 sec/batch)
2016-07-15 18:18:54.907847: step 72570, loss = 4.40 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 18:18:59.640899: step 72580, loss = 4.31 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 18:19:04.469833: step 72590, loss = 4.28 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 18:19:09.265215: step 72600, loss = 4.29 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 18:19:16.468250: step 72610, loss = 4.56 (200.3 examples/sec; 0.639 sec/batch)
2016-07-15 18:19:22.154609: step 72620, loss = 4.54 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 18:19:27.904697: step 72630, loss = 4.67 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 18:19:32.791673: step 72640, loss = 4.37 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 18:19:37.593489: step 72650, loss = 4.30 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 18:19:42.380633: step 72660, loss = 4.66 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 18:19:47.203246: step 72670, loss = 4.32 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 18:19:51.953598: step 72680, loss = 4.67 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 18:19:57.115165: step 72690, loss = 4.33 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 18:20:03.634326: step 72700, loss = 4.56 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 18:20:10.149492: step 72710, loss = 4.86 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 18:20:15.473633: step 72720, loss = 4.28 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 18:20:20.177988: step 72730, loss = 4.28 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 18:20:25.524852: step 72740, loss = 4.57 (185.5 examples/sec; 0.690 sec/batch)
2016-07-15 18:20:32.016752: step 72750, loss = 4.26 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 18:20:36.853143: step 72760, loss = 4.51 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 18:20:41.628303: step 72770, loss = 4.56 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 18:20:46.442218: step 72780, loss = 4.51 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 18:20:51.252342: step 72790, loss = 4.57 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 18:20:57.633938: step 72800, loss = 4.45 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 18:21:04.494624: step 72810, loss = 4.26 (222.2 examples/sec; 0.576 sec/batch)
2016-07-15 18:21:10.204134: step 72820, loss = 4.20 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 18:21:15.960035: step 72830, loss = 4.49 (199.9 examples/sec; 0.640 sec/batch)
2016-07-15 18:21:21.149884: step 72840, loss = 4.62 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 18:21:26.723714: step 72850, loss = 4.29 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 18:21:32.498529: step 72860, loss = 4.54 (244.0 examples/sec; 0.525 sec/batch)
2016-07-15 18:21:37.786325: step 72870, loss = 4.34 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 18:21:43.244053: step 72880, loss = 4.42 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 18:21:49.019086: step 72890, loss = 4.61 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 18:21:54.360446: step 72900, loss = 4.49 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 18:22:01.074244: step 72910, loss = 4.52 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 18:22:06.673936: step 72920, loss = 4.30 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 18:22:11.451712: step 72930, loss = 4.48 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 18:22:16.282438: step 72940, loss = 4.29 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 18:22:22.852694: step 72950, loss = 4.57 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 18:22:28.019440: step 72960, loss = 4.49 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 18:22:33.821794: step 72970, loss = 4.58 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 18:22:38.571716: step 72980, loss = 4.45 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 18:22:43.393759: step 72990, loss = 4.41 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 18:22:48.103262: step 73000, loss = 4.37 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 18:22:54.685502: step 73010, loss = 4.44 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 18:23:00.858897: step 73020, loss = 4.32 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 18:23:05.678781: step 73030, loss = 4.15 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 18:23:10.449024: step 73040, loss = 4.51 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 18:23:15.249920: step 73050, loss = 4.68 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 18:23:19.910477: step 73060, loss = 4.49 (284.0 examples/sec; 0.451 sec/batch)
2016-07-15 18:23:24.733414: step 73070, loss = 4.38 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 18:23:30.443552: step 73080, loss = 4.15 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 18:23:35.211580: step 73090, loss = 4.44 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 18:23:39.835009: step 73100, loss = 4.43 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 18:23:45.931358: step 73110, loss = 4.39 (196.8 examples/sec; 0.651 sec/batch)
2016-07-15 18:23:51.354357: step 73120, loss = 4.49 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 18:23:57.079176: step 73130, loss = 4.38 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 18:24:02.443165: step 73140, loss = 4.36 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 18:24:07.773171: step 73150, loss = 4.44 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 18:24:12.525237: step 73160, loss = 4.51 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 18:24:17.909114: step 73170, loss = 4.45 (186.6 examples/sec; 0.686 sec/batch)
2016-07-15 18:24:24.383935: step 73180, loss = 4.42 (201.5 examples/sec; 0.635 sec/batch)
2016-07-15 18:24:29.262484: step 73190, loss = 4.24 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 18:24:34.050108: step 73200, loss = 4.42 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 18:24:41.265832: step 73210, loss = 4.14 (199.0 examples/sec; 0.643 sec/batch)
2016-07-15 18:24:46.843770: step 73220, loss = 4.34 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 18:24:51.767659: step 73230, loss = 4.68 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 18:24:56.744934: step 73240, loss = 4.80 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 18:25:01.462581: step 73250, loss = 4.39 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 18:25:06.224620: step 73260, loss = 4.32 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 18:25:11.013210: step 73270, loss = 4.59 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 18:25:17.274936: step 73280, loss = 4.47 (201.7 examples/sec; 0.634 sec/batch)
2016-07-15 18:25:22.899243: step 73290, loss = 4.41 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 18:25:27.665399: step 73300, loss = 4.42 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 18:25:33.892911: step 73310, loss = 4.22 (188.4 examples/sec; 0.679 sec/batch)
2016-07-15 18:25:40.359483: step 73320, loss = 4.77 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 18:25:45.229816: step 73330, loss = 4.51 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 18:25:49.904125: step 73340, loss = 4.33 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 18:25:54.599983: step 73350, loss = 4.28 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 18:26:00.264205: step 73360, loss = 4.39 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 18:26:05.169653: step 73370, loss = 4.24 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 18:26:09.920005: step 73380, loss = 4.29 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 18:26:14.756932: step 73390, loss = 4.26 (246.4 examples/sec; 0.519 sec/batch)
2016-07-15 18:26:19.575315: step 73400, loss = 4.29 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 18:26:25.270177: step 73410, loss = 4.41 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 18:26:29.907881: step 73420, loss = 4.67 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 18:26:35.356919: step 73430, loss = 4.36 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 18:26:40.509505: step 73440, loss = 4.34 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 18:26:46.291469: step 73450, loss = 4.63 (247.2 examples/sec; 0.518 sec/batch)
2016-07-15 18:26:51.102597: step 73460, loss = 4.40 (282.9 examples/sec; 0.452 sec/batch)
2016-07-15 18:26:55.882512: step 73470, loss = 4.48 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 18:27:00.622029: step 73480, loss = 4.25 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 18:27:05.879193: step 73490, loss = 4.43 (191.0 examples/sec; 0.670 sec/batch)
2016-07-15 18:27:12.320166: step 73500, loss = 4.46 (209.1 examples/sec; 0.612 sec/batch)
2016-07-15 18:27:18.206773: step 73510, loss = 4.52 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 18:27:23.023465: step 73520, loss = 4.37 (249.6 examples/sec; 0.513 sec/batch)
2016-07-15 18:27:27.838299: step 73530, loss = 4.07 (244.4 examples/sec; 0.524 sec/batch)
2016-07-15 18:27:32.794765: step 73540, loss = 4.41 (221.2 examples/sec; 0.579 sec/batch)
2016-07-15 18:27:39.374212: step 73550, loss = 4.45 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 18:27:44.519088: step 73560, loss = 4.40 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 18:27:49.216970: step 73570, loss = 4.46 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 18:27:54.048150: step 73580, loss = 4.66 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 18:27:58.827513: step 73590, loss = 4.54 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 18:28:04.941799: step 73600, loss = 4.68 (199.3 examples/sec; 0.642 sec/batch)
2016-07-15 18:28:11.832478: step 73610, loss = 4.22 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 18:28:17.663069: step 73620, loss = 4.27 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 18:28:23.107671: step 73630, loss = 4.49 (196.6 examples/sec; 0.651 sec/batch)
2016-07-15 18:28:28.310965: step 73640, loss = 4.31 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 18:28:34.114293: step 73650, loss = 4.44 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 18:28:38.870878: step 73660, loss = 4.22 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 18:28:43.698536: step 73670, loss = 4.49 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 18:28:48.403511: step 73680, loss = 3.96 (281.6 examples/sec; 0.454 sec/batch)
2016-07-15 18:28:53.658963: step 73690, loss = 4.27 (185.5 examples/sec; 0.690 sec/batch)
2016-07-15 18:29:00.154000: step 73700, loss = 4.16 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 18:29:06.713862: step 73710, loss = 4.29 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 18:29:12.028084: step 73720, loss = 4.34 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 18:29:16.721382: step 73730, loss = 4.31 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 18:29:22.167700: step 73740, loss = 4.24 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 18:29:28.645825: step 73750, loss = 4.18 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 18:29:33.482487: step 73760, loss = 4.36 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 18:29:38.219067: step 73770, loss = 4.34 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 18:29:44.051265: step 73780, loss = 4.45 (190.7 examples/sec; 0.671 sec/batch)
2016-07-15 18:29:49.964540: step 73790, loss = 4.28 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 18:29:54.883570: step 73800, loss = 4.36 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 18:30:01.086585: step 73810, loss = 4.20 (190.6 examples/sec; 0.672 sec/batch)
2016-07-15 18:30:08.029352: step 73820, loss = 4.64 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 18:30:13.031851: step 73830, loss = 4.36 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 18:30:17.815792: step 73840, loss = 4.55 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 18:30:23.644395: step 73850, loss = 4.24 (186.4 examples/sec; 0.687 sec/batch)
2016-07-15 18:30:29.723930: step 73860, loss = 4.53 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 18:30:35.214855: step 73870, loss = 4.48 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 18:30:40.513638: step 73880, loss = 4.48 (243.5 examples/sec; 0.526 sec/batch)
2016-07-15 18:30:45.220246: step 73890, loss = 4.21 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 18:30:50.048783: step 73900, loss = 4.50 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 18:30:55.742976: step 73910, loss = 4.22 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 18:31:02.066410: step 73920, loss = 4.43 (197.1 examples/sec; 0.649 sec/batch)
2016-07-15 18:31:07.530765: step 73930, loss = 4.52 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 18:31:12.276886: step 73940, loss = 4.30 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 18:31:17.412440: step 73950, loss = 4.36 (186.5 examples/sec; 0.686 sec/batch)
2016-07-15 18:31:23.972991: step 73960, loss = 4.39 (194.5 examples/sec; 0.658 sec/batch)
2016-07-15 18:31:29.011063: step 73970, loss = 4.30 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 18:31:33.746792: step 73980, loss = 4.44 (251.9 examples/sec; 0.508 sec/batch)
2016-07-15 18:31:38.560218: step 73990, loss = 4.21 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 18:31:43.351600: step 74000, loss = 4.12 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 18:31:50.925330: step 74010, loss = 4.14 (199.0 examples/sec; 0.643 sec/batch)
2016-07-15 18:31:56.271118: step 74020, loss = 4.58 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 18:32:02.054616: step 74030, loss = 4.30 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 18:32:07.621322: step 74040, loss = 4.37 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 18:32:12.850073: step 74050, loss = 4.52 (244.6 examples/sec; 0.523 sec/batch)
2016-07-15 18:32:18.603922: step 74060, loss = 4.58 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 18:32:24.338387: step 74070, loss = 4.32 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 18:32:29.338174: step 74080, loss = 4.34 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 18:32:34.044945: step 74090, loss = 4.40 (283.6 examples/sec; 0.451 sec/batch)
2016-07-15 18:32:38.739737: step 74100, loss = 4.34 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 18:32:45.451257: step 74110, loss = 4.49 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 18:32:50.780542: step 74120, loss = 4.54 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 18:32:56.157069: step 74130, loss = 4.55 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 18:33:00.876516: step 74140, loss = 4.40 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 18:33:06.157761: step 74150, loss = 4.31 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 18:33:12.646913: step 74160, loss = 4.38 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 18:33:17.756955: step 74170, loss = 4.23 (198.9 examples/sec; 0.644 sec/batch)
2016-07-15 18:33:23.328418: step 74180, loss = 4.45 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 18:33:28.140729: step 74190, loss = 4.43 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 18:33:33.115903: step 74200, loss = 4.61 (222.2 examples/sec; 0.576 sec/batch)
2016-07-15 18:33:41.272642: step 74210, loss = 4.60 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 18:33:46.119461: step 74220, loss = 4.41 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 18:33:50.876424: step 74230, loss = 4.45 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 18:33:55.680803: step 74240, loss = 4.22 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 18:34:00.568258: step 74250, loss = 4.36 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 18:34:07.080103: step 74260, loss = 4.46 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 18:34:12.350515: step 74270, loss = 4.42 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 18:34:17.071703: step 74280, loss = 4.52 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 18:34:21.906142: step 74290, loss = 4.33 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 18:34:26.641203: step 74300, loss = 4.20 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 18:34:33.935466: step 74310, loss = 4.41 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 18:34:39.516572: step 74320, loss = 4.45 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 18:34:45.294416: step 74330, loss = 4.22 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 18:34:50.123322: step 74340, loss = 4.48 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 18:34:54.876058: step 74350, loss = 4.26 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 18:34:59.621611: step 74360, loss = 4.33 (285.5 examples/sec; 0.448 sec/batch)
2016-07-15 18:35:04.434161: step 74370, loss = 4.51 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 18:35:10.940198: step 74380, loss = 4.23 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 18:35:16.256961: step 74390, loss = 4.43 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 18:35:20.959833: step 74400, loss = 4.61 (271.5 examples/sec; 0.472 sec/batch)
2016-07-15 18:35:27.722168: step 74410, loss = 4.47 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 18:35:33.810093: step 74420, loss = 4.27 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 18:35:39.236719: step 74430, loss = 4.57 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 18:35:44.576735: step 74440, loss = 4.43 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 18:35:49.298674: step 74450, loss = 4.39 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 18:35:54.712184: step 74460, loss = 4.43 (185.6 examples/sec; 0.690 sec/batch)
2016-07-15 18:36:01.076388: step 74470, loss = 4.39 (217.2 examples/sec; 0.589 sec/batch)
2016-07-15 18:36:06.292341: step 74480, loss = 4.26 (201.5 examples/sec; 0.635 sec/batch)
2016-07-15 18:36:11.732703: step 74490, loss = 4.18 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 18:36:17.513010: step 74500, loss = 4.19 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 18:36:24.241055: step 74510, loss = 4.36 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 18:36:29.253389: step 74520, loss = 4.45 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 18:36:33.967726: step 74530, loss = 4.07 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 18:36:38.751737: step 74540, loss = 4.33 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 18:36:43.400322: step 74550, loss = 4.25 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 18:36:48.036517: step 74560, loss = 4.38 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 18:36:52.608254: step 74570, loss = 4.44 (283.2 examples/sec; 0.452 sec/batch)
2016-07-15 18:36:57.377913: step 74580, loss = 4.33 (221.9 examples/sec; 0.577 sec/batch)
2016-07-15 18:37:03.107653: step 74590, loss = 4.22 (251.6 examples/sec; 0.509 sec/batch)
2016-07-15 18:37:08.801246: step 74600, loss = 4.31 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 18:37:15.366400: step 74610, loss = 4.55 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 18:37:20.632018: step 74620, loss = 4.40 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 18:37:26.391899: step 74630, loss = 4.41 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 18:37:31.224357: step 74640, loss = 4.19 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 18:37:36.044827: step 74650, loss = 4.33 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 18:37:40.745326: step 74660, loss = 4.57 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 18:37:45.913184: step 74670, loss = 4.41 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 18:37:52.420124: step 74680, loss = 4.35 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 18:37:57.406817: step 74690, loss = 4.35 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 18:38:02.125412: step 74700, loss = 4.20 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 18:38:09.187641: step 74710, loss = 4.29 (190.3 examples/sec; 0.673 sec/batch)
2016-07-15 18:38:14.927199: step 74720, loss = 4.26 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 18:38:20.590963: step 74730, loss = 4.44 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 18:38:25.563303: step 74740, loss = 4.59 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 18:38:30.269069: step 74750, loss = 4.43 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 18:38:36.011120: step 74760, loss = 4.55 (187.0 examples/sec; 0.685 sec/batch)
2016-07-15 18:38:42.156053: step 74770, loss = 4.35 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 18:38:46.939919: step 74780, loss = 4.17 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 18:38:51.744965: step 74790, loss = 4.14 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 18:38:57.982590: step 74800, loss = 4.16 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 18:39:04.811408: step 74810, loss = 4.49 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 18:39:10.560898: step 74820, loss = 4.22 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 18:39:15.328074: step 74830, loss = 4.57 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 18:39:20.203490: step 74840, loss = 3.83 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 18:39:26.543974: step 74850, loss = 4.35 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 18:39:32.023144: step 74860, loss = 4.18 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 18:39:36.773093: step 74870, loss = 4.03 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 18:39:41.844527: step 74880, loss = 4.28 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 18:39:48.365917: step 74890, loss = 4.03 (198.2 examples/sec; 0.646 sec/batch)
2016-07-15 18:39:53.565854: step 74900, loss = 4.36 (211.0 examples/sec; 0.607 sec/batch)
2016-07-15 18:40:00.529293: step 74910, loss = 4.22 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 18:40:05.283409: step 74920, loss = 4.41 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 18:40:10.547273: step 74930, loss = 4.32 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 18:40:16.987919: step 74940, loss = 4.41 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 18:40:22.111781: step 74950, loss = 4.34 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 18:40:27.708887: step 74960, loss = 4.28 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 18:40:32.474186: step 74970, loss = 4.05 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 18:40:37.341372: step 74980, loss = 4.19 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 18:40:42.027220: step 74990, loss = 4.32 (286.1 examples/sec; 0.447 sec/batch)
2016-07-15 18:40:46.622369: step 75000, loss = 4.10 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 18:40:53.315274: step 75010, loss = 4.05 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 18:40:58.157160: step 75020, loss = 4.58 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 18:41:02.908066: step 75030, loss = 4.55 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 18:41:08.835575: step 75040, loss = 4.39 (183.6 examples/sec; 0.697 sec/batch)
2016-07-15 18:41:14.728570: step 75050, loss = 4.20 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 18:41:19.586548: step 75060, loss = 4.48 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 18:41:24.418967: step 75070, loss = 4.61 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 18:41:30.762660: step 75080, loss = 4.44 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 18:41:36.252208: step 75090, loss = 4.59 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 18:41:40.940979: step 75100, loss = 4.45 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 18:41:47.506277: step 75110, loss = 4.32 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 18:41:53.785673: step 75120, loss = 4.38 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 18:41:58.728402: step 75130, loss = 4.34 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 18:42:03.524973: step 75140, loss = 4.31 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 18:42:09.642805: step 75150, loss = 4.13 (199.3 examples/sec; 0.642 sec/batch)
2016-07-15 18:42:15.345302: step 75160, loss = 4.02 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 18:42:21.124305: step 75170, loss = 4.50 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 18:42:26.315534: step 75180, loss = 4.28 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 18:42:31.845867: step 75190, loss = 4.49 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 18:42:36.562847: step 75200, loss = 4.27 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 18:42:42.862689: step 75210, loss = 4.44 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 18:42:49.321637: step 75220, loss = 4.45 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 18:42:54.174391: step 75230, loss = 4.24 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 18:42:58.926901: step 75240, loss = 4.31 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 18:43:04.816958: step 75250, loss = 4.46 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 18:43:10.715678: step 75260, loss = 4.28 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 18:43:15.485261: step 75270, loss = 4.29 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 18:43:20.349131: step 75280, loss = 4.23 (244.4 examples/sec; 0.524 sec/batch)
2016-07-15 18:43:25.072981: step 75290, loss = 4.52 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 18:43:30.139568: step 75300, loss = 4.16 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 18:43:38.072332: step 75310, loss = 4.21 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 18:43:42.890142: step 75320, loss = 4.34 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 18:43:47.625945: step 75330, loss = 4.42 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 18:43:52.298812: step 75340, loss = 4.35 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 18:43:58.043017: step 75350, loss = 4.32 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 18:44:03.387469: step 75360, loss = 4.53 (210.0 examples/sec; 0.609 sec/batch)
2016-07-15 18:44:08.683648: step 75370, loss = 4.52 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 18:44:14.493256: step 75380, loss = 4.43 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 18:44:19.299146: step 75390, loss = 4.71 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 18:44:24.097394: step 75400, loss = 4.05 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 18:44:31.673141: step 75410, loss = 4.35 (200.5 examples/sec; 0.638 sec/batch)
2016-07-15 18:44:36.916151: step 75420, loss = 4.40 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 18:44:41.671820: step 75430, loss = 4.18 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 18:44:47.019215: step 75440, loss = 4.54 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 18:44:53.499529: step 75450, loss = 4.29 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 18:44:58.325813: step 75460, loss = 4.37 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 18:45:03.079137: step 75470, loss = 4.49 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 18:45:08.965268: step 75480, loss = 4.17 (188.1 examples/sec; 0.680 sec/batch)
2016-07-15 18:45:14.835542: step 75490, loss = 4.08 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 18:45:19.662589: step 75500, loss = 4.37 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 18:45:25.424986: step 75510, loss = 4.18 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 18:45:30.182485: step 75520, loss = 4.37 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 18:45:35.394784: step 75530, loss = 4.23 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 18:45:40.175784: step 75540, loss = 4.36 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 18:45:46.370271: step 75550, loss = 4.40 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 18:45:51.912329: step 75560, loss = 4.55 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 18:45:56.683655: step 75570, loss = 4.23 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 18:46:01.673965: step 75580, loss = 4.31 (233.0 examples/sec; 0.549 sec/batch)
2016-07-15 18:46:08.267047: step 75590, loss = 4.12 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 18:46:13.411044: step 75600, loss = 4.47 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 18:46:19.126910: step 75610, loss = 4.52 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 18:46:25.052196: step 75620, loss = 4.29 (184.6 examples/sec; 0.693 sec/batch)
2016-07-15 18:46:30.959321: step 75630, loss = 4.32 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 18:46:35.737084: step 75640, loss = 4.25 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 18:46:40.590380: step 75650, loss = 4.17 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 18:46:46.867360: step 75660, loss = 4.18 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 18:46:52.343763: step 75670, loss = 4.19 (241.6 examples/sec; 0.530 sec/batch)
2016-07-15 18:46:58.155102: step 75680, loss = 4.47 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 18:47:03.479244: step 75690, loss = 4.29 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 18:47:08.868188: step 75700, loss = 4.45 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 18:47:14.705326: step 75710, loss = 4.27 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 18:47:19.576488: step 75720, loss = 4.29 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 18:47:24.317396: step 75730, loss = 4.60 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 18:47:30.589958: step 75740, loss = 4.48 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 18:47:36.150015: step 75750, loss = 4.20 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 18:47:41.882670: step 75760, loss = 4.31 (239.7 examples/sec; 0.534 sec/batch)
2016-07-15 18:47:47.138777: step 75770, loss = 4.54 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 18:47:52.612198: step 75780, loss = 4.35 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 18:47:57.309989: step 75790, loss = 4.35 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 18:48:02.199643: step 75800, loss = 4.14 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 18:48:07.911655: step 75810, loss = 4.33 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 18:48:14.054917: step 75820, loss = 4.51 (200.1 examples/sec; 0.640 sec/batch)
2016-07-15 18:48:19.766704: step 75830, loss = 4.38 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 18:48:25.469135: step 75840, loss = 4.51 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 18:48:30.582391: step 75850, loss = 4.28 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 18:48:36.158505: step 75860, loss = 4.32 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 18:48:40.898801: step 75870, loss = 4.26 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 18:48:45.796955: step 75880, loss = 4.65 (235.2 examples/sec; 0.544 sec/batch)
2016-07-15 18:48:52.343993: step 75890, loss = 4.14 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 18:48:57.566422: step 75900, loss = 4.35 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 18:49:03.304041: step 75910, loss = 4.47 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 18:49:08.142564: step 75920, loss = 4.56 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 18:49:12.933137: step 75930, loss = 4.23 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 18:49:19.301067: step 75940, loss = 4.14 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 18:49:24.740656: step 75950, loss = 4.20 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 18:49:30.480114: step 75960, loss = 4.17 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 18:49:35.942602: step 75970, loss = 4.34 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 18:49:41.228290: step 75980, loss = 4.35 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 18:49:46.952241: step 75990, loss = 4.38 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 18:49:52.500728: step 76000, loss = 4.32 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 18:49:59.184772: step 76010, loss = 4.46 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 18:50:04.689954: step 76020, loss = 4.19 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 18:50:09.399117: step 76030, loss = 4.01 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 18:50:14.525483: step 76040, loss = 4.21 (187.0 examples/sec; 0.685 sec/batch)
2016-07-15 18:50:21.030951: step 76050, loss = 4.10 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 18:50:26.033959: step 76060, loss = 4.40 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 18:50:30.798275: step 76070, loss = 4.14 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 18:50:35.723648: step 76080, loss = 4.23 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 18:50:40.736526: step 76090, loss = 4.25 (253.1 examples/sec; 0.506 sec/batch)
2016-07-15 18:50:47.751749: step 76100, loss = 4.15 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 18:50:54.417262: step 76110, loss = 4.31 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 18:51:00.069632: step 76120, loss = 4.29 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 18:51:05.880775: step 76130, loss = 4.00 (209.3 examples/sec; 0.612 sec/batch)
2016-07-15 18:51:11.052463: step 76140, loss = 4.17 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 18:51:16.567704: step 76150, loss = 4.17 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 18:51:22.397194: step 76160, loss = 4.16 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 18:51:27.208963: step 76170, loss = 4.45 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 18:51:32.015504: step 76180, loss = 4.46 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 18:51:38.102716: step 76190, loss = 4.26 (196.0 examples/sec; 0.653 sec/batch)
2016-07-15 18:51:43.811440: step 76200, loss = 4.08 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 18:51:50.597019: step 76210, loss = 4.09 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 18:51:55.972035: step 76220, loss = 4.45 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 18:52:01.266437: step 76230, loss = 4.49 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 18:52:06.004713: step 76240, loss = 4.33 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 18:52:11.275650: step 76250, loss = 4.54 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 18:52:17.739633: step 76260, loss = 4.38 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 18:52:22.631441: step 76270, loss = 4.32 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 18:52:27.349680: step 76280, loss = 4.25 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 18:52:32.183671: step 76290, loss = 4.19 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 18:52:37.033158: step 76300, loss = 4.35 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 18:52:42.692584: step 76310, loss = 4.20 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 18:52:47.329221: step 76320, loss = 4.01 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 18:52:52.805808: step 76330, loss = 4.42 (200.8 examples/sec; 0.637 sec/batch)
2016-07-15 18:52:57.976575: step 76340, loss = 4.20 (246.3 examples/sec; 0.520 sec/batch)
2016-07-15 18:53:03.734762: step 76350, loss = 4.23 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 18:53:09.430108: step 76360, loss = 4.30 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 18:53:14.381267: step 76370, loss = 4.40 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 18:53:19.132114: step 76380, loss = 4.35 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 18:53:24.864419: step 76390, loss = 4.19 (191.4 examples/sec; 0.669 sec/batch)
2016-07-15 18:53:30.945813: step 76400, loss = 4.18 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 18:53:37.738497: step 76410, loss = 4.41 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 18:53:42.884627: step 76420, loss = 4.24 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 18:53:48.490216: step 76430, loss = 4.30 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 18:53:54.290636: step 76440, loss = 4.20 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 18:53:59.186125: step 76450, loss = 4.39 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 18:54:03.966293: step 76460, loss = 4.27 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 18:54:10.109438: step 76470, loss = 4.27 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 18:54:15.809195: step 76480, loss = 4.16 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 18:54:20.582403: step 76490, loss = 4.23 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 18:54:25.433685: step 76500, loss = 4.27 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 18:54:33.162228: step 76510, loss = 4.14 (200.8 examples/sec; 0.638 sec/batch)
2016-07-15 18:54:38.330564: step 76520, loss = 4.40 (214.5 examples/sec; 0.597 sec/batch)
2016-07-15 18:54:44.023232: step 76530, loss = 4.57 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 18:54:49.734300: step 76540, loss = 4.36 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 18:54:54.944839: step 76550, loss = 4.51 (199.5 examples/sec; 0.641 sec/batch)
2016-07-15 18:55:00.546107: step 76560, loss = 4.10 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 18:55:06.292827: step 76570, loss = 4.16 (240.8 examples/sec; 0.531 sec/batch)
2016-07-15 18:55:11.599963: step 76580, loss = 4.42 (206.6 examples/sec; 0.619 sec/batch)
2016-07-15 18:55:17.049368: step 76590, loss = 4.61 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 18:55:22.809962: step 76600, loss = 4.29 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 18:55:28.736452: step 76610, loss = 4.30 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 18:55:33.581670: step 76620, loss = 4.16 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 18:55:40.044928: step 76630, loss = 4.46 (199.9 examples/sec; 0.640 sec/batch)
2016-07-15 18:55:45.370093: step 76640, loss = 4.35 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 18:55:50.119694: step 76650, loss = 4.10 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 18:55:54.951173: step 76660, loss = 4.53 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 18:55:59.736866: step 76670, loss = 4.20 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 18:56:05.683270: step 76680, loss = 4.15 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 18:56:11.579944: step 76690, loss = 4.35 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 18:56:17.179266: step 76700, loss = 4.56 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 18:56:23.288330: step 76710, loss = 4.46 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 18:56:28.039149: step 76720, loss = 4.56 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 18:56:32.785095: step 76730, loss = 4.19 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 18:56:37.663745: step 76740, loss = 4.38 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 18:56:44.155338: step 76750, loss = 4.29 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 18:56:49.457165: step 76760, loss = 4.55 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 18:56:54.157296: step 76770, loss = 4.35 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 18:56:58.995540: step 76780, loss = 4.21 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 18:57:03.774143: step 76790, loss = 4.25 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 18:57:09.693566: step 76800, loss = 4.22 (187.6 examples/sec; 0.682 sec/batch)
2016-07-15 18:57:16.860148: step 76810, loss = 4.26 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 18:57:22.664941: step 76820, loss = 4.15 (251.0 examples/sec; 0.510 sec/batch)
2016-07-15 18:57:28.003996: step 76830, loss = 4.32 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 18:57:33.383045: step 76840, loss = 4.41 (249.3 examples/sec; 0.513 sec/batch)
2016-07-15 18:57:39.186832: step 76850, loss = 4.20 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 18:57:44.006388: step 76860, loss = 4.33 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 18:57:48.815731: step 76870, loss = 4.33 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 18:57:55.068920: step 76880, loss = 4.36 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 18:58:00.641710: step 76890, loss = 4.18 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 18:58:05.368302: step 76900, loss = 4.21 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 18:58:11.740316: step 76910, loss = 4.10 (186.3 examples/sec; 0.687 sec/batch)
2016-07-15 18:58:18.214231: step 76920, loss = 4.32 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 18:58:23.078851: step 76930, loss = 4.40 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 18:58:27.836005: step 76940, loss = 4.33 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 18:58:32.607720: step 76950, loss = 4.31 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 18:58:37.421711: step 76960, loss = 4.09 (252.5 examples/sec; 0.507 sec/batch)
2016-07-15 18:58:43.808121: step 76970, loss = 4.56 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 18:58:49.230607: step 76980, loss = 4.26 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 18:58:53.998412: step 76990, loss = 4.31 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 18:58:59.159259: step 77000, loss = 4.23 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 18:59:07.062713: step 77010, loss = 4.29 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 18:59:11.898442: step 77020, loss = 4.28 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 18:59:16.711235: step 77030, loss = 4.24 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 18:59:21.506849: step 77040, loss = 4.55 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 18:59:26.450945: step 77050, loss = 4.23 (229.9 examples/sec; 0.557 sec/batch)
2016-07-15 18:59:33.056852: step 77060, loss = 4.36 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 18:59:38.181983: step 77070, loss = 4.66 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 18:59:42.925759: step 77080, loss = 4.35 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 18:59:48.479070: step 77090, loss = 4.43 (189.2 examples/sec; 0.676 sec/batch)
2016-07-15 18:59:54.767573: step 77100, loss = 4.18 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 19:00:00.588159: step 77110, loss = 4.30 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 19:00:05.418000: step 77120, loss = 4.11 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 19:00:11.794511: step 77130, loss = 4.42 (198.7 examples/sec; 0.644 sec/batch)
2016-07-15 19:00:17.247535: step 77140, loss = 4.37 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 19:00:21.960821: step 77150, loss = 4.31 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 19:00:27.088323: step 77160, loss = 4.22 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 19:00:33.655476: step 77170, loss = 4.35 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 19:00:38.683769: step 77180, loss = 4.14 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 19:00:43.432566: step 77190, loss = 4.41 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 19:00:49.046259: step 77200, loss = 4.26 (190.9 examples/sec; 0.670 sec/batch)
2016-07-15 19:00:56.410925: step 77210, loss = 4.10 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 19:01:02.082111: step 77220, loss = 4.31 (199.4 examples/sec; 0.642 sec/batch)
2016-07-15 19:01:07.240950: step 77230, loss = 3.98 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 19:01:12.810980: step 77240, loss = 4.15 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 19:01:17.557366: step 77250, loss = 4.25 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 19:01:22.478188: step 77260, loss = 4.15 (239.6 examples/sec; 0.534 sec/batch)
2016-07-15 19:01:29.097531: step 77270, loss = 4.30 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 19:01:34.321239: step 77280, loss = 4.12 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 19:01:39.019781: step 77290, loss = 4.17 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 19:01:44.657538: step 77300, loss = 4.50 (188.1 examples/sec; 0.681 sec/batch)
2016-07-15 19:01:51.110310: step 77310, loss = 4.12 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 19:01:56.271053: step 77320, loss = 4.23 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 19:02:01.639270: step 77330, loss = 4.30 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 19:02:07.459580: step 77340, loss = 4.45 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 19:02:12.963954: step 77350, loss = 4.25 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 19:02:18.157339: step 77360, loss = 4.14 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 19:02:22.827414: step 77370, loss = 4.16 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 19:02:27.759317: step 77380, loss = 4.33 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 19:02:32.826364: step 77390, loss = 4.34 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 19:02:39.188395: step 77400, loss = 4.44 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 19:02:45.920147: step 77410, loss = 4.52 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 19:02:51.675960: step 77420, loss = 4.55 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 19:02:57.201814: step 77430, loss = 4.16 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 19:03:02.336706: step 77440, loss = 4.22 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 19:03:07.096616: step 77450, loss = 4.23 (246.2 examples/sec; 0.520 sec/batch)
2016-07-15 19:03:12.664893: step 77460, loss = 4.18 (188.5 examples/sec; 0.679 sec/batch)
2016-07-15 19:03:18.960014: step 77470, loss = 4.47 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 19:03:23.785668: step 77480, loss = 4.29 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 19:03:28.556122: step 77490, loss = 4.84 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 19:03:34.557060: step 77500, loss = 4.38 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 19:03:41.536117: step 77510, loss = 4.35 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 19:03:47.259567: step 77520, loss = 4.21 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 19:03:52.658714: step 77530, loss = 4.39 (208.8 examples/sec; 0.613 sec/batch)
2016-07-15 19:03:57.951369: step 77540, loss = 4.27 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 19:04:02.607130: step 77550, loss = 4.24 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 19:04:07.982130: step 77560, loss = 4.14 (186.0 examples/sec; 0.688 sec/batch)
2016-07-15 19:04:14.445998: step 77570, loss = 4.21 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 19:04:19.259662: step 77580, loss = 4.17 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 19:04:24.011142: step 77590, loss = 4.08 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 19:04:29.895296: step 77600, loss = 4.10 (188.6 examples/sec; 0.679 sec/batch)
2016-07-15 19:04:37.020072: step 77610, loss = 4.53 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 19:04:41.788586: step 77620, loss = 4.17 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 19:04:46.742293: step 77630, loss = 4.35 (228.5 examples/sec; 0.560 sec/batch)
2016-07-15 19:04:53.286616: step 77640, loss = 4.26 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 19:04:58.451822: step 77650, loss = 4.29 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 19:05:03.158305: step 77660, loss = 4.14 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 19:05:08.811024: step 77670, loss = 4.32 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 19:05:15.053372: step 77680, loss = 4.59 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 19:05:19.893034: step 77690, loss = 4.00 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 19:05:24.690401: step 77700, loss = 4.46 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 19:05:32.013724: step 77710, loss = 4.57 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 19:05:37.425163: step 77720, loss = 4.13 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 19:05:42.153790: step 77730, loss = 4.10 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 19:05:47.256142: step 77740, loss = 4.29 (189.5 examples/sec; 0.676 sec/batch)
2016-07-15 19:05:53.736090: step 77750, loss = 4.64 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 19:05:58.827929: step 77760, loss = 4.25 (235.6 examples/sec; 0.543 sec/batch)
2016-07-15 19:06:04.558753: step 77770, loss = 4.31 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 19:06:09.321306: step 77780, loss = 4.20 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 19:06:14.133468: step 77790, loss = 4.11 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 19:06:20.594109: step 77800, loss = 4.15 (203.3 examples/sec; 0.629 sec/batch)
2016-07-15 19:06:27.170372: step 77810, loss = 4.11 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 19:06:31.851185: step 77820, loss = 4.38 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 19:06:37.605551: step 77830, loss = 3.98 (192.0 examples/sec; 0.667 sec/batch)
2016-07-15 19:06:43.658756: step 77840, loss = 4.14 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 19:06:49.172912: step 77850, loss = 4.34 (200.3 examples/sec; 0.639 sec/batch)
2016-07-15 19:06:54.442075: step 77860, loss = 4.28 (247.0 examples/sec; 0.518 sec/batch)
2016-07-15 19:06:59.147429: step 77870, loss = 4.28 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 19:07:04.034885: step 77880, loss = 4.28 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 19:07:08.819181: step 77890, loss = 4.21 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 19:07:14.912591: step 77900, loss = 4.39 (195.8 examples/sec; 0.654 sec/batch)
2016-07-15 19:07:21.930993: step 77910, loss = 4.26 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 19:07:26.623558: step 77920, loss = 4.35 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 19:07:31.862825: step 77930, loss = 4.27 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 19:07:38.764580: step 77940, loss = 4.14 (196.4 examples/sec; 0.652 sec/batch)
2016-07-15 19:07:44.497881: step 77950, loss = 4.28 (146.9 examples/sec; 0.872 sec/batch)
2016-07-15 19:07:49.983122: step 77960, loss = 4.12 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 19:07:54.733025: step 77970, loss = 4.14 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 19:07:59.576902: step 77980, loss = 4.14 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 19:08:04.288291: step 77990, loss = 4.19 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 19:08:10.240211: step 78000, loss = 4.48 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 19:08:17.372974: step 78010, loss = 4.08 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 19:08:23.168450: step 78020, loss = 4.24 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 19:08:28.027519: step 78030, loss = 4.16 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 19:08:32.791220: step 78040, loss = 4.29 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 19:08:38.976827: step 78050, loss = 4.21 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 19:08:44.703827: step 78060, loss = 4.16 (254.0 examples/sec; 0.504 sec/batch)
2016-07-15 19:08:49.473974: step 78070, loss = 4.32 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 19:08:54.316003: step 78080, loss = 4.15 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 19:09:00.848456: step 78090, loss = 4.27 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 19:09:06.183793: step 78100, loss = 4.31 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 19:09:13.039094: step 78110, loss = 4.34 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 19:09:17.804635: step 78120, loss = 3.93 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 19:09:22.741055: step 78130, loss = 4.34 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 19:09:29.288776: step 78140, loss = 4.24 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 19:09:34.502855: step 78150, loss = 4.50 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 19:09:40.283309: step 78160, loss = 4.23 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 19:09:45.838332: step 78170, loss = 4.03 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 19:09:50.955469: step 78180, loss = 4.27 (242.6 examples/sec; 0.528 sec/batch)
2016-07-15 19:09:56.691625: step 78190, loss = 4.22 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 19:10:01.497226: step 78200, loss = 4.28 (268.1 examples/sec; 0.478 sec/batch)
2016-07-15 19:10:07.552390: step 78210, loss = 4.31 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 19:10:14.052748: step 78220, loss = 4.42 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 19:10:19.217612: step 78230, loss = 4.70 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 19:10:24.893938: step 78240, loss = 4.37 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 19:10:30.602305: step 78250, loss = 4.41 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 19:10:35.774499: step 78260, loss = 4.11 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 19:10:41.369516: step 78270, loss = 4.09 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 19:10:47.122034: step 78280, loss = 4.25 (246.3 examples/sec; 0.520 sec/batch)
2016-07-15 19:10:52.055875: step 78290, loss = 4.21 (247.0 examples/sec; 0.518 sec/batch)
2016-07-15 19:10:56.777163: step 78300, loss = 4.30 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 19:11:02.477074: step 78310, loss = 4.33 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 19:11:07.581208: step 78320, loss = 4.14 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 19:11:14.108665: step 78330, loss = 4.25 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 19:11:19.139907: step 78340, loss = 4.19 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 19:11:23.865759: step 78350, loss = 4.37 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 19:11:29.581607: step 78360, loss = 4.26 (184.6 examples/sec; 0.694 sec/batch)
2016-07-15 19:11:35.685050: step 78370, loss = 4.12 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 19:11:40.570954: step 78380, loss = 4.39 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 19:11:45.382935: step 78390, loss = 4.25 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 19:11:51.544568: step 78400, loss = 4.42 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 19:11:58.439798: step 78410, loss = 4.18 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 19:12:04.223133: step 78420, loss = 4.22 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 19:12:09.027930: step 78430, loss = 4.19 (281.0 examples/sec; 0.456 sec/batch)
2016-07-15 19:12:13.843060: step 78440, loss = 4.05 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 19:12:18.577393: step 78450, loss = 4.20 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 19:12:23.682598: step 78460, loss = 4.35 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 19:12:30.200702: step 78470, loss = 4.27 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 19:12:35.199779: step 78480, loss = 4.12 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 19:12:39.912894: step 78490, loss = 4.52 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 19:12:44.694007: step 78500, loss = 4.27 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 19:12:50.475890: step 78510, loss = 4.29 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 19:12:56.908512: step 78520, loss = 4.18 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 19:13:02.236433: step 78530, loss = 4.11 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 19:13:06.901172: step 78540, loss = 4.35 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 19:13:11.818169: step 78550, loss = 4.27 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 19:13:16.567193: step 78560, loss = 4.38 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 19:13:22.472131: step 78570, loss = 4.18 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 19:13:28.390566: step 78580, loss = 4.30 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 19:13:33.179116: step 78590, loss = 4.12 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 19:13:38.021250: step 78600, loss = 4.29 (249.9 examples/sec; 0.512 sec/batch)
2016-07-15 19:13:43.799713: step 78610, loss = 4.35 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 19:13:48.687783: step 78620, loss = 4.26 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 19:13:53.549288: step 78630, loss = 4.49 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 19:13:59.744021: step 78640, loss = 4.15 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 19:14:05.346091: step 78650, loss = 4.60 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 19:14:10.099392: step 78660, loss = 4.19 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 19:14:14.991958: step 78670, loss = 4.57 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 19:14:21.576106: step 78680, loss = 4.36 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 19:14:26.766965: step 78690, loss = 4.57 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 19:14:31.485277: step 78700, loss = 4.37 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 19:14:38.409905: step 78710, loss = 4.23 (188.4 examples/sec; 0.679 sec/batch)
2016-07-15 19:14:44.362805: step 78720, loss = 4.07 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 19:14:49.185398: step 78730, loss = 4.18 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 19:14:54.005466: step 78740, loss = 4.22 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 19:14:58.770087: step 78750, loss = 4.19 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 19:15:03.902834: step 78760, loss = 4.06 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 19:15:10.426798: step 78770, loss = 4.35 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 19:15:15.465062: step 78780, loss = 4.09 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 19:15:20.194899: step 78790, loss = 4.19 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 19:15:25.893794: step 78800, loss = 4.35 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 19:15:33.230163: step 78810, loss = 4.32 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 19:15:39.019666: step 78820, loss = 4.21 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 19:15:43.855284: step 78830, loss = 4.28 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 19:15:48.677807: step 78840, loss = 4.46 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 19:15:54.594457: step 78850, loss = 4.09 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 19:16:00.465160: step 78860, loss = 4.41 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 19:16:06.028234: step 78870, loss = 4.27 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 19:16:11.283800: step 78880, loss = 4.17 (230.4 examples/sec; 0.556 sec/batch)
2016-07-15 19:16:17.094596: step 78890, loss = 4.08 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 19:16:21.876038: step 78900, loss = 4.39 (273.2 examples/sec; 0.468 sec/batch)
2016-07-15 19:16:28.101445: step 78910, loss = 4.58 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 19:16:34.606212: step 78920, loss = 4.26 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 19:16:39.754481: step 78930, loss = 4.20 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 19:16:45.348505: step 78940, loss = 4.21 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 19:16:51.113557: step 78950, loss = 4.00 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 19:16:55.967785: step 78960, loss = 4.26 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 19:17:00.700339: step 78970, loss = 4.13 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 19:17:05.565750: step 78980, loss = 4.12 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 19:17:10.391299: step 78990, loss = 4.14 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 19:17:16.838855: step 79000, loss = 4.06 (200.5 examples/sec; 0.639 sec/batch)
2016-07-15 19:17:23.515012: step 79010, loss = 4.47 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 19:17:28.255164: step 79020, loss = 4.18 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 19:17:33.957808: step 79030, loss = 4.22 (187.0 examples/sec; 0.684 sec/batch)
2016-07-15 19:17:40.064537: step 79040, loss = 4.20 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 19:17:44.898420: step 79050, loss = 4.12 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 19:17:49.692188: step 79060, loss = 4.28 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 19:17:54.478603: step 79070, loss = 4.21 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 19:17:59.385275: step 79080, loss = 4.18 (232.0 examples/sec; 0.552 sec/batch)
2016-07-15 19:18:05.996726: step 79090, loss = 4.35 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 19:18:11.162526: step 79100, loss = 4.24 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 19:18:16.854342: step 79110, loss = 4.27 (282.4 examples/sec; 0.453 sec/batch)
2016-07-15 19:18:21.519313: step 79120, loss = 4.11 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 19:18:27.179924: step 79130, loss = 4.17 (209.9 examples/sec; 0.610 sec/batch)
2016-07-15 19:18:32.086434: step 79140, loss = 4.20 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 19:18:36.820840: step 79150, loss = 4.11 (263.6 examples/sec; 0.485 sec/batch)
2016-07-15 19:18:42.750423: step 79160, loss = 4.35 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 19:18:48.648730: step 79170, loss = 4.24 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 19:18:53.465569: step 79180, loss = 4.26 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 19:18:58.262629: step 79190, loss = 4.32 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 19:19:04.589134: step 79200, loss = 4.31 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 19:19:11.333437: step 79210, loss = 4.21 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 19:19:16.037430: step 79220, loss = 4.14 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 19:19:21.659179: step 79230, loss = 4.19 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 19:19:27.920210: step 79240, loss = 4.22 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 19:19:33.342350: step 79250, loss = 4.33 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 19:19:38.649779: step 79260, loss = 4.27 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 19:19:43.381920: step 79270, loss = 4.10 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 19:19:48.646202: step 79280, loss = 4.31 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 19:19:55.119101: step 79290, loss = 4.36 (198.5 examples/sec; 0.645 sec/batch)
2016-07-15 19:20:00.245189: step 79300, loss = 4.26 (199.1 examples/sec; 0.643 sec/batch)
2016-07-15 19:20:07.035469: step 79310, loss = 4.22 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 19:20:12.871268: step 79320, loss = 4.19 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 19:20:18.463679: step 79330, loss = 4.07 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 19:20:23.647871: step 79340, loss = 4.20 (218.9 examples/sec; 0.585 sec/batch)
2016-07-15 19:20:29.336887: step 79350, loss = 4.17 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 19:20:34.075643: step 79360, loss = 4.40 (280.4 examples/sec; 0.457 sec/batch)
2016-07-15 19:20:38.874105: step 79370, loss = 4.10 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 19:20:45.340350: step 79380, loss = 4.32 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 19:20:50.712541: step 79390, loss = 4.10 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 19:20:55.409590: step 79400, loss = 4.15 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 19:21:02.235306: step 79410, loss = 4.41 (186.5 examples/sec; 0.686 sec/batch)
2016-07-15 19:21:08.339119: step 79420, loss = 3.96 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 19:21:13.132490: step 79430, loss = 4.05 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 19:21:17.926081: step 79440, loss = 4.31 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 19:21:24.120628: step 79450, loss = 4.19 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 19:21:29.733624: step 79460, loss = 4.19 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 19:21:34.529922: step 79470, loss = 4.41 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 19:21:39.402026: step 79480, loss = 4.19 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 19:21:45.983531: step 79490, loss = 4.04 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 19:21:51.164559: step 79500, loss = 4.19 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 19:21:58.111465: step 79510, loss = 4.06 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 19:22:03.894677: step 79520, loss = 4.28 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 19:22:08.781687: step 79530, loss = 3.95 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 19:22:13.583280: step 79540, loss = 4.31 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 19:22:19.666693: step 79550, loss = 4.14 (198.7 examples/sec; 0.644 sec/batch)
2016-07-15 19:22:25.382095: step 79560, loss = 4.20 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 19:22:30.140189: step 79570, loss = 4.38 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 19:22:35.027407: step 79580, loss = 4.06 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 19:22:41.544955: step 79590, loss = 4.22 (199.8 examples/sec; 0.641 sec/batch)
2016-07-15 19:22:46.857487: step 79600, loss = 3.98 (259.9 examples/sec; 0.493 sec/batch)
2016-07-15 19:22:53.783395: step 79610, loss = 3.94 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 19:22:58.523116: step 79620, loss = 4.31 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 19:23:03.417982: step 79630, loss = 4.13 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 19:23:08.154786: step 79640, loss = 4.10 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 19:23:13.663517: step 79650, loss = 4.34 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 19:23:19.957028: step 79660, loss = 4.23 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 19:23:24.806911: step 79670, loss = 4.10 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 19:23:29.595974: step 79680, loss = 4.07 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 19:23:34.389395: step 79690, loss = 4.10 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 19:23:39.281038: step 79700, loss = 4.29 (246.0 examples/sec; 0.520 sec/batch)
2016-07-15 19:23:47.526274: step 79710, loss = 4.18 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 19:23:52.366771: step 79720, loss = 4.42 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 19:23:57.110752: step 79730, loss = 4.34 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 19:24:03.071924: step 79740, loss = 4.27 (185.9 examples/sec; 0.688 sec/batch)
2016-07-15 19:24:08.938860: step 79750, loss = 4.13 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 19:24:13.758640: step 79760, loss = 4.16 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 19:24:18.545201: step 79770, loss = 4.09 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 19:24:24.838585: step 79780, loss = 4.19 (208.8 examples/sec; 0.613 sec/batch)
2016-07-15 19:24:30.316574: step 79790, loss = 4.17 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 19:24:36.083906: step 79800, loss = 4.45 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 19:24:42.668133: step 79810, loss = 4.53 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 19:24:47.757436: step 79820, loss = 4.24 (238.5 examples/sec; 0.537 sec/batch)
2016-07-15 19:24:53.470450: step 79830, loss = 4.44 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 19:24:58.199096: step 79840, loss = 4.28 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 19:25:03.006516: step 79850, loss = 4.50 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 19:25:07.717170: step 79860, loss = 4.31 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 19:25:13.004573: step 79870, loss = 4.01 (191.6 examples/sec; 0.668 sec/batch)
2016-07-15 19:25:19.432085: step 79880, loss = 3.99 (205.0 examples/sec; 0.625 sec/batch)
2016-07-15 19:25:24.325634: step 79890, loss = 3.97 (272.6 examples/sec; 0.469 sec/batch)
2016-07-15 19:25:29.087267: step 79900, loss = 4.09 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 19:25:36.326471: step 79910, loss = 4.11 (207.6 examples/sec; 0.616 sec/batch)
2016-07-15 19:25:41.943356: step 79920, loss = 4.01 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 19:25:46.748229: step 79930, loss = 4.20 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 19:25:51.673861: step 79940, loss = 4.11 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 19:25:56.406504: step 79950, loss = 4.16 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 19:26:01.970674: step 79960, loss = 4.18 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 19:26:08.255324: step 79970, loss = 4.24 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 19:26:13.132887: step 79980, loss = 4.06 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 19:26:17.951228: step 79990, loss = 4.08 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 19:26:22.706196: step 80000, loss = 4.38 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 19:26:28.332423: step 80010, loss = 3.99 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 19:26:33.573581: step 80020, loss = 4.29 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 19:26:38.914815: step 80030, loss = 4.46 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 19:26:43.638434: step 80040, loss = 4.22 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 19:26:48.512833: step 80050, loss = 4.05 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 19:26:53.246984: step 80060, loss = 4.33 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 19:26:59.193501: step 80070, loss = 4.07 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 19:27:05.168769: step 80080, loss = 4.03 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 19:27:09.993175: step 80090, loss = 4.26 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 19:27:14.860996: step 80100, loss = 4.33 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 19:27:22.649166: step 80110, loss = 4.11 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 19:27:27.888498: step 80120, loss = 4.21 (233.9 examples/sec; 0.547 sec/batch)
2016-07-15 19:27:33.653975: step 80130, loss = 4.02 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 19:27:38.408649: step 80140, loss = 4.30 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 19:27:43.227668: step 80150, loss = 4.18 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 19:27:49.722126: step 80160, loss = 4.11 (201.4 examples/sec; 0.635 sec/batch)
2016-07-15 19:27:55.003195: step 80170, loss = 4.16 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 19:28:00.753495: step 80180, loss = 4.49 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 19:28:06.218984: step 80190, loss = 4.22 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 19:28:11.405962: step 80200, loss = 4.39 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 19:28:18.320821: step 80210, loss = 4.18 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 19:28:24.127926: step 80220, loss = 4.12 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 19:28:28.982735: step 80230, loss = 4.08 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 19:28:33.750760: step 80240, loss = 4.27 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 19:28:39.788648: step 80250, loss = 4.55 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 19:28:45.497424: step 80260, loss = 4.15 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 19:28:51.111279: step 80270, loss = 4.30 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 19:28:56.121300: step 80280, loss = 4.41 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 19:29:00.841806: step 80290, loss = 4.27 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 19:29:06.489417: step 80300, loss = 4.42 (190.3 examples/sec; 0.673 sec/batch)
2016-07-15 19:29:13.901857: step 80310, loss = 4.20 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 19:29:19.687293: step 80320, loss = 4.33 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 19:29:24.510645: step 80330, loss = 4.07 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 19:29:29.250085: step 80340, loss = 4.13 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 19:29:34.114503: step 80350, loss = 4.31 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 19:29:39.184651: step 80360, loss = 4.18 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 19:29:44.167323: step 80370, loss = 4.31 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 19:29:48.799317: step 80380, loss = 4.38 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 19:29:54.157271: step 80390, loss = 4.30 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 19:29:59.429900: step 80400, loss = 3.94 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 19:30:06.349994: step 80410, loss = 4.41 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 19:30:12.152906: step 80420, loss = 4.26 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 19:30:17.477068: step 80430, loss = 4.11 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 19:30:22.970714: step 80440, loss = 4.17 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 19:30:28.870525: step 80450, loss = 4.37 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 19:30:33.710206: step 80460, loss = 4.33 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 19:30:38.467181: step 80470, loss = 4.10 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 19:30:44.743985: step 80480, loss = 4.31 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 19:30:50.442778: step 80490, loss = 4.01 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 19:30:56.220322: step 80500, loss = 4.00 (253.5 examples/sec; 0.505 sec/batch)
2016-07-15 19:31:02.090520: step 80510, loss = 4.23 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 19:31:06.894725: step 80520, loss = 4.43 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 19:31:13.315699: step 80530, loss = 3.94 (183.3 examples/sec; 0.698 sec/batch)
2016-07-15 19:31:18.814572: step 80540, loss = 4.48 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 19:31:23.574146: step 80550, loss = 4.14 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 19:31:28.823503: step 80560, loss = 4.04 (188.5 examples/sec; 0.679 sec/batch)
2016-07-15 19:31:35.452699: step 80570, loss = 4.12 (197.4 examples/sec; 0.648 sec/batch)
2016-07-15 19:31:40.477648: step 80580, loss = 4.11 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 19:31:45.246591: step 80590, loss = 4.03 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 19:31:50.121701: step 80600, loss = 4.05 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 19:31:56.208885: step 80610, loss = 4.21 (196.8 examples/sec; 0.650 sec/batch)
2016-07-15 19:32:02.832851: step 80620, loss = 4.27 (199.8 examples/sec; 0.640 sec/batch)
2016-07-15 19:32:08.068667: step 80630, loss = 4.21 (230.9 examples/sec; 0.554 sec/batch)
2016-07-15 19:32:13.835291: step 80640, loss = 4.20 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 19:32:18.622200: step 80650, loss = 3.92 (280.3 examples/sec; 0.457 sec/batch)
2016-07-15 19:32:23.519694: step 80660, loss = 4.12 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 19:32:29.999043: step 80670, loss = 4.58 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 19:32:35.269075: step 80680, loss = 4.17 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 19:32:40.019244: step 80690, loss = 4.06 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 19:32:44.837390: step 80700, loss = 4.46 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 19:32:50.516544: step 80710, loss = 4.13 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 19:32:55.199197: step 80720, loss = 4.10 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 19:33:00.974396: step 80730, loss = 4.40 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 19:33:05.729737: step 80740, loss = 4.03 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 19:33:10.571353: step 80750, loss = 4.18 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 19:33:15.348183: step 80760, loss = 4.06 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 19:33:20.389643: step 80770, loss = 4.15 (199.2 examples/sec; 0.643 sec/batch)
2016-07-15 19:33:26.919561: step 80780, loss = 4.18 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 19:33:32.094713: step 80790, loss = 4.16 (241.4 examples/sec; 0.530 sec/batch)
2016-07-15 19:33:37.818381: step 80800, loss = 4.27 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 19:33:43.534076: step 80810, loss = 4.30 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 19:33:48.397408: step 80820, loss = 4.21 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 19:33:53.150802: step 80830, loss = 3.91 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 19:33:58.957035: step 80840, loss = 4.04 (185.3 examples/sec; 0.691 sec/batch)
2016-07-15 19:34:05.009415: step 80850, loss = 4.23 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 19:34:09.785027: step 80860, loss = 4.37 (281.7 examples/sec; 0.454 sec/batch)
2016-07-15 19:34:14.616313: step 80870, loss = 4.26 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 19:34:20.794503: step 80880, loss = 4.19 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 19:34:26.398766: step 80890, loss = 3.95 (252.6 examples/sec; 0.507 sec/batch)
2016-07-15 19:34:31.128116: step 80900, loss = 4.11 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 19:34:37.367843: step 80910, loss = 4.05 (190.8 examples/sec; 0.671 sec/batch)
2016-07-15 19:34:43.852975: step 80920, loss = 4.06 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 19:34:49.012846: step 80930, loss = 3.92 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 19:34:54.544288: step 80940, loss = 4.32 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 19:34:59.277269: step 80950, loss = 4.17 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 19:35:04.148701: step 80960, loss = 4.33 (241.5 examples/sec; 0.530 sec/batch)
2016-07-15 19:35:10.698260: step 80970, loss = 4.20 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 19:35:15.883809: step 80980, loss = 3.97 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 19:35:20.622186: step 80990, loss = 4.30 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 19:35:26.099049: step 81000, loss = 3.89 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 19:35:33.769905: step 81010, loss = 4.43 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 19:35:39.349902: step 81020, loss = 4.15 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 19:35:44.502164: step 81030, loss = 4.00 (214.4 examples/sec; 0.597 sec/batch)
2016-07-15 19:35:50.158759: step 81040, loss = 4.08 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 19:35:54.965361: step 81050, loss = 3.81 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 19:35:59.837864: step 81060, loss = 4.02 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 19:36:06.371805: step 81070, loss = 4.24 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 19:36:11.636434: step 81080, loss = 4.27 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 19:36:17.377471: step 81090, loss = 4.17 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 19:36:22.873352: step 81100, loss = 4.25 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 19:36:29.498765: step 81110, loss = 4.09 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 19:36:34.947182: step 81120, loss = 4.04 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 19:36:39.722379: step 81130, loss = 4.25 (251.5 examples/sec; 0.509 sec/batch)
2016-07-15 19:36:44.834169: step 81140, loss = 3.96 (182.9 examples/sec; 0.700 sec/batch)
2016-07-15 19:36:51.325487: step 81150, loss = 3.99 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 19:36:56.423629: step 81160, loss = 4.42 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 19:37:01.182467: step 81170, loss = 3.90 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 19:37:06.880303: step 81180, loss = 4.10 (190.6 examples/sec; 0.672 sec/batch)
2016-07-15 19:37:13.055613: step 81190, loss = 4.00 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 19:37:18.023724: step 81200, loss = 4.13 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 19:37:24.613305: step 81210, loss = 4.02 (199.4 examples/sec; 0.642 sec/batch)
2016-07-15 19:37:31.186316: step 81220, loss = 4.35 (202.4 examples/sec; 0.633 sec/batch)
2016-07-15 19:37:36.383932: step 81230, loss = 4.19 (239.8 examples/sec; 0.534 sec/batch)
2016-07-15 19:37:42.083205: step 81240, loss = 4.02 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 19:37:46.872604: step 81250, loss = 4.19 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 19:37:51.750290: step 81260, loss = 4.35 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 19:37:58.225434: step 81270, loss = 4.14 (196.9 examples/sec; 0.650 sec/batch)
2016-07-15 19:38:03.595949: step 81280, loss = 4.06 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 19:38:08.289259: step 81290, loss = 4.18 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 19:38:13.573096: step 81300, loss = 3.86 (187.5 examples/sec; 0.683 sec/batch)
2016-07-15 19:38:21.525460: step 81310, loss = 4.16 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 19:38:27.077420: step 81320, loss = 4.02 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 19:38:32.227195: step 81330, loss = 4.21 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 19:38:36.943080: step 81340, loss = 4.49 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 19:38:42.485766: step 81350, loss = 4.13 (188.1 examples/sec; 0.680 sec/batch)
2016-07-15 19:38:48.768720: step 81360, loss = 4.32 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 19:38:53.644071: step 81370, loss = 4.03 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 19:38:58.464501: step 81380, loss = 4.33 (243.3 examples/sec; 0.526 sec/batch)
2016-07-15 19:39:04.597094: step 81390, loss = 4.15 (195.9 examples/sec; 0.653 sec/batch)
2016-07-15 19:39:10.335817: step 81400, loss = 4.00 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 19:39:17.127678: step 81410, loss = 4.44 (253.5 examples/sec; 0.505 sec/batch)
2016-07-15 19:39:22.541085: step 81420, loss = 4.12 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 19:39:27.806263: step 81430, loss = 4.25 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 19:39:33.602166: step 81440, loss = 4.38 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 19:39:39.219883: step 81450, loss = 4.28 (201.1 examples/sec; 0.637 sec/batch)
2016-07-15 19:39:44.395104: step 81460, loss = 4.33 (238.6 examples/sec; 0.536 sec/batch)
2016-07-15 19:39:50.150536: step 81470, loss = 4.25 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 19:39:55.842044: step 81480, loss = 4.43 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 19:40:00.996948: step 81490, loss = 4.55 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 19:40:06.579612: step 81500, loss = 4.42 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 19:40:12.406853: step 81510, loss = 4.29 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 19:40:17.695011: step 81520, loss = 4.12 (192.1 examples/sec; 0.666 sec/batch)
2016-07-15 19:40:24.219518: step 81530, loss = 4.18 (199.5 examples/sec; 0.642 sec/batch)
2016-07-15 19:40:29.078915: step 81540, loss = 4.26 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 19:40:33.849639: step 81550, loss = 4.19 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 19:40:38.662425: step 81560, loss = 3.85 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 19:40:43.517483: step 81570, loss = 3.90 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 19:40:49.875333: step 81580, loss = 4.08 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 19:40:55.344562: step 81590, loss = 4.19 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 19:41:00.049383: step 81600, loss = 4.31 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 19:41:06.618929: step 81610, loss = 4.40 (184.5 examples/sec; 0.694 sec/batch)
2016-07-15 19:41:12.872743: step 81620, loss = 4.25 (253.8 examples/sec; 0.504 sec/batch)
2016-07-15 19:41:17.717682: step 81630, loss = 4.16 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 19:41:22.558515: step 81640, loss = 4.22 (251.0 examples/sec; 0.510 sec/batch)
2016-07-15 19:41:28.638698: step 81650, loss = 4.03 (190.0 examples/sec; 0.674 sec/batch)
2016-07-15 19:41:34.388009: step 81660, loss = 3.96 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 19:41:39.124019: step 81670, loss = 3.92 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 19:41:43.996353: step 81680, loss = 3.99 (251.8 examples/sec; 0.508 sec/batch)
2016-07-15 19:41:48.704996: step 81690, loss = 3.98 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 19:41:54.101621: step 81700, loss = 4.51 (187.5 examples/sec; 0.683 sec/batch)
2016-07-15 19:42:01.745607: step 81710, loss = 4.25 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 19:42:06.591154: step 81720, loss = 4.10 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 19:42:11.362637: step 81730, loss = 4.17 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 19:42:16.104739: step 81740, loss = 4.09 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 19:42:20.972516: step 81750, loss = 3.93 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 19:42:25.736890: step 81760, loss = 4.04 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 19:42:31.593470: step 81770, loss = 4.12 (183.6 examples/sec; 0.697 sec/batch)
2016-07-15 19:42:37.574304: step 81780, loss = 4.12 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 19:42:42.402971: step 81790, loss = 3.88 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 19:42:47.245858: step 81800, loss = 4.28 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 19:42:53.085155: step 81810, loss = 4.02 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 19:42:58.584226: step 81820, loss = 4.07 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 19:43:04.871689: step 81830, loss = 4.47 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 19:43:10.171587: step 81840, loss = 4.22 (200.3 examples/sec; 0.639 sec/batch)
2016-07-15 19:43:15.589533: step 81850, loss = 4.07 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 19:43:20.300983: step 81860, loss = 4.49 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 19:43:25.548037: step 81870, loss = 3.96 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 19:43:32.066371: step 81880, loss = 4.23 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 19:43:37.296789: step 81890, loss = 4.33 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 19:43:42.888236: step 81900, loss = 4.13 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 19:43:48.681376: step 81910, loss = 4.19 (268.1 examples/sec; 0.478 sec/batch)
2016-07-15 19:43:53.955435: step 81920, loss = 4.12 (181.9 examples/sec; 0.704 sec/batch)
2016-07-15 19:44:00.455558: step 81930, loss = 4.02 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 19:44:05.366822: step 81940, loss = 4.21 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 19:44:10.132371: step 81950, loss = 4.13 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 19:44:16.011984: step 81960, loss = 4.19 (185.6 examples/sec; 0.690 sec/batch)
2016-07-15 19:44:21.944021: step 81970, loss = 4.26 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 19:44:26.752734: step 81980, loss = 4.32 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 19:44:31.546109: step 81990, loss = 4.41 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 19:44:36.266980: step 82000, loss = 4.11 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 19:44:42.806465: step 82010, loss = 4.15 (186.5 examples/sec; 0.686 sec/batch)
2016-07-15 19:44:49.085264: step 82020, loss = 4.23 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 19:44:53.927617: step 82030, loss = 4.32 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 19:44:58.711557: step 82040, loss = 4.19 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 19:45:04.819642: step 82050, loss = 3.91 (190.6 examples/sec; 0.672 sec/batch)
2016-07-15 19:45:10.587826: step 82060, loss = 4.24 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 19:45:15.359563: step 82070, loss = 4.06 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 19:45:20.241778: step 82080, loss = 4.09 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 19:45:26.747175: step 82090, loss = 4.33 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 19:45:32.105199: step 82100, loss = 4.10 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 19:45:39.088901: step 82110, loss = 4.20 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 19:45:43.874038: step 82120, loss = 4.24 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 19:45:48.858148: step 82130, loss = 4.43 (227.5 examples/sec; 0.563 sec/batch)
2016-07-15 19:45:55.408127: step 82140, loss = 4.11 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 19:46:00.607651: step 82150, loss = 4.27 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 19:46:05.357968: step 82160, loss = 3.97 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 19:46:11.016320: step 82170, loss = 4.13 (184.0 examples/sec; 0.695 sec/batch)
2016-07-15 19:46:17.276666: step 82180, loss = 4.42 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 19:46:22.666475: step 82190, loss = 4.14 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 19:46:28.049486: step 82200, loss = 4.03 (242.9 examples/sec; 0.527 sec/batch)
2016-07-15 19:46:34.954532: step 82210, loss = 3.82 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 19:46:40.718901: step 82220, loss = 4.04 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 19:46:45.550266: step 82230, loss = 4.20 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 19:46:50.370474: step 82240, loss = 4.12 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 19:46:56.305049: step 82250, loss = 4.27 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 19:47:02.163674: step 82260, loss = 4.24 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 19:47:07.020174: step 82270, loss = 4.03 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 19:47:11.879017: step 82280, loss = 4.35 (249.9 examples/sec; 0.512 sec/batch)
2016-07-15 19:47:18.271836: step 82290, loss = 4.17 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 19:47:23.726439: step 82300, loss = 4.16 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 19:47:29.415823: step 82310, loss = 3.99 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 19:47:34.954908: step 82320, loss = 4.47 (185.7 examples/sec; 0.689 sec/batch)
2016-07-15 19:47:41.229093: step 82330, loss = 4.02 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 19:47:46.078050: step 82340, loss = 4.10 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 19:47:50.869322: step 82350, loss = 4.07 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 19:47:57.025312: step 82360, loss = 4.01 (193.0 examples/sec; 0.663 sec/batch)
2016-07-15 19:48:02.851487: step 82370, loss = 4.19 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 19:48:08.581085: step 82380, loss = 4.44 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 19:48:13.711305: step 82390, loss = 4.02 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 19:48:19.319457: step 82400, loss = 4.12 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 19:48:26.186899: step 82410, loss = 4.20 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 19:48:31.012595: step 82420, loss = 4.40 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 19:48:35.875806: step 82430, loss = 4.08 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 19:48:42.231530: step 82440, loss = 4.03 (198.9 examples/sec; 0.643 sec/batch)
2016-07-15 19:48:47.716571: step 82450, loss = 4.19 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 19:48:52.455388: step 82460, loss = 4.00 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 19:48:57.564177: step 82470, loss = 3.99 (184.5 examples/sec; 0.694 sec/batch)
2016-07-15 19:49:04.146599: step 82480, loss = 4.02 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 19:49:09.435429: step 82490, loss = 4.18 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 19:49:15.049897: step 82500, loss = 4.15 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 19:49:21.937986: step 82510, loss = 4.19 (261.0 examples/sec; 0.491 sec/batch)
2016-07-15 19:49:27.454986: step 82520, loss = 4.21 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 19:49:32.616715: step 82530, loss = 3.95 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 19:49:38.392389: step 82540, loss = 4.30 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 19:49:43.980266: step 82550, loss = 4.08 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 19:49:49.126465: step 82560, loss = 4.00 (230.5 examples/sec; 0.555 sec/batch)
2016-07-15 19:49:54.835908: step 82570, loss = 4.07 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 19:49:59.570050: step 82580, loss = 4.35 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 19:50:04.387553: step 82590, loss = 4.03 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 19:50:10.842627: step 82600, loss = 4.24 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 19:50:17.518648: step 82610, loss = 4.32 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 19:50:23.157989: step 82620, loss = 4.17 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 19:50:28.921388: step 82630, loss = 4.10 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 19:50:33.768071: step 82640, loss = 4.49 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 19:50:38.565386: step 82650, loss = 4.41 (246.0 examples/sec; 0.520 sec/batch)
2016-07-15 19:50:43.341412: step 82660, loss = 4.48 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 19:50:48.158991: step 82670, loss = 4.36 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 19:50:54.556315: step 82680, loss = 4.23 (211.1 examples/sec; 0.606 sec/batch)
2016-07-15 19:50:59.948675: step 82690, loss = 4.06 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 19:51:04.699165: step 82700, loss = 4.02 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 19:51:10.610166: step 82710, loss = 4.31 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 19:51:15.390487: step 82720, loss = 3.85 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 19:51:20.154005: step 82730, loss = 4.01 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 19:51:25.121534: step 82740, loss = 4.11 (228.4 examples/sec; 0.560 sec/batch)
2016-07-15 19:51:31.689992: step 82750, loss = 4.21 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 19:51:36.865607: step 82760, loss = 4.29 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 19:51:41.624038: step 82770, loss = 4.35 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 19:51:47.130592: step 82780, loss = 4.20 (190.9 examples/sec; 0.671 sec/batch)
2016-07-15 19:51:53.423126: step 82790, loss = 3.90 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 19:51:58.246117: step 82800, loss = 4.26 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 19:52:04.036735: step 82810, loss = 3.97 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 19:52:10.333206: step 82820, loss = 4.07 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 19:52:15.819079: step 82830, loss = 3.96 (253.7 examples/sec; 0.505 sec/batch)
2016-07-15 19:52:21.615993: step 82840, loss = 4.00 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 19:52:26.946778: step 82850, loss = 4.05 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 19:52:32.289401: step 82860, loss = 4.39 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 19:52:38.091487: step 82870, loss = 3.64 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 19:52:43.550044: step 82880, loss = 3.88 (201.4 examples/sec; 0.635 sec/batch)
2016-07-15 19:52:48.800532: step 82890, loss = 4.09 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 19:52:54.604564: step 82900, loss = 4.47 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 19:53:01.344867: step 82910, loss = 3.88 (247.9 examples/sec; 0.516 sec/batch)
2016-07-15 19:53:06.603958: step 82920, loss = 4.05 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 19:53:12.048210: step 82930, loss = 4.06 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 19:53:17.836465: step 82940, loss = 4.01 (250.4 examples/sec; 0.511 sec/batch)
2016-07-15 19:53:22.710155: step 82950, loss = 3.99 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 19:53:27.500924: step 82960, loss = 4.14 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 19:53:32.276468: step 82970, loss = 4.12 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 19:53:37.152531: step 82980, loss = 4.14 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 19:53:41.890706: step 82990, loss = 4.42 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 19:53:46.562422: step 83000, loss = 4.20 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 19:53:53.227292: step 83010, loss = 3.94 (222.5 examples/sec; 0.575 sec/batch)
2016-07-15 19:53:58.425600: step 83020, loss = 4.49 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 19:54:03.896196: step 83030, loss = 3.86 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 19:54:09.723530: step 83040, loss = 4.18 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 19:54:15.131900: step 83050, loss = 4.31 (200.3 examples/sec; 0.639 sec/batch)
2016-07-15 19:54:20.472437: step 83060, loss = 3.98 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 19:54:25.188049: step 83070, loss = 4.02 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 19:54:30.464711: step 83080, loss = 3.91 (191.5 examples/sec; 0.669 sec/batch)
2016-07-15 19:54:36.946369: step 83090, loss = 4.24 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 19:54:42.116891: step 83100, loss = 4.16 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 19:54:48.897853: step 83110, loss = 4.02 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 19:54:54.737117: step 83120, loss = 4.22 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 19:55:00.326284: step 83130, loss = 4.38 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 19:55:05.442058: step 83140, loss = 4.25 (224.4 examples/sec; 0.570 sec/batch)
2016-07-15 19:55:11.152887: step 83150, loss = 4.28 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 19:55:15.897002: step 83160, loss = 3.98 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 19:55:20.767457: step 83170, loss = 3.73 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 19:55:25.522699: step 83180, loss = 4.19 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 19:55:30.926472: step 83190, loss = 4.16 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 19:55:37.332276: step 83200, loss = 3.97 (220.9 examples/sec; 0.579 sec/batch)
2016-07-15 19:55:43.135724: step 83210, loss = 4.02 (281.2 examples/sec; 0.455 sec/batch)
2016-07-15 19:55:47.995854: step 83220, loss = 4.18 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 19:55:54.285601: step 83230, loss = 3.97 (203.0 examples/sec; 0.631 sec/batch)
2016-07-15 19:55:59.792831: step 83240, loss = 4.10 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 19:56:05.499958: step 83250, loss = 4.33 (251.6 examples/sec; 0.509 sec/batch)
2016-07-15 19:56:10.787446: step 83260, loss = 3.98 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 19:56:16.214760: step 83270, loss = 4.09 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 19:56:21.996567: step 83280, loss = 3.99 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 19:56:27.429500: step 83290, loss = 3.96 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 19:56:32.719530: step 83300, loss = 4.00 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 19:56:39.726806: step 83310, loss = 4.30 (251.1 examples/sec; 0.510 sec/batch)
2016-07-15 19:56:44.518441: step 83320, loss = 4.07 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 19:56:49.570269: step 83330, loss = 4.26 (212.2 examples/sec; 0.603 sec/batch)
2016-07-15 19:56:56.137232: step 83340, loss = 4.36 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 19:57:01.291212: step 83350, loss = 4.07 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 19:57:07.109177: step 83360, loss = 3.98 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 19:57:12.757189: step 83370, loss = 4.12 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 19:57:17.714729: step 83380, loss = 4.06 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 19:57:22.482295: step 83390, loss = 4.12 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 19:57:28.181324: step 83400, loss = 3.95 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 19:57:35.512528: step 83410, loss = 4.31 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 19:57:41.330917: step 83420, loss = 4.22 (208.0 examples/sec; 0.615 sec/batch)
2016-07-15 19:57:46.551683: step 83430, loss = 4.00 (199.2 examples/sec; 0.643 sec/batch)
2016-07-15 19:57:52.026171: step 83440, loss = 4.36 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 19:57:57.851515: step 83450, loss = 4.21 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 19:58:03.168591: step 83460, loss = 4.29 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 19:58:08.532335: step 83470, loss = 3.93 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 19:58:14.360380: step 83480, loss = 4.04 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 19:58:19.194281: step 83490, loss = 4.16 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 19:58:23.976439: step 83500, loss = 3.93 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 19:58:31.496321: step 83510, loss = 4.09 (201.1 examples/sec; 0.636 sec/batch)
2016-07-15 19:58:36.791752: step 83520, loss = 4.10 (251.9 examples/sec; 0.508 sec/batch)
2016-07-15 19:58:41.537447: step 83530, loss = 4.16 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 19:58:46.885488: step 83540, loss = 4.39 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 19:58:53.330817: step 83550, loss = 3.98 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 19:58:58.205194: step 83560, loss = 4.05 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 19:59:02.931717: step 83570, loss = 4.24 (269.8 examples/sec; 0.474 sec/batch)
2016-07-15 19:59:08.755542: step 83580, loss = 3.98 (188.1 examples/sec; 0.681 sec/batch)
2016-07-15 19:59:14.688232: step 83590, loss = 4.08 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 19:59:20.229482: step 83600, loss = 4.22 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 19:59:26.966200: step 83610, loss = 4.14 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 19:59:32.425189: step 83620, loss = 4.03 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 19:59:38.160030: step 83630, loss = 3.98 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 19:59:43.571944: step 83640, loss = 3.90 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 19:59:48.889685: step 83650, loss = 4.03 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 19:59:53.645396: step 83660, loss = 4.13 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 19:59:59.020187: step 83670, loss = 4.35 (184.9 examples/sec; 0.692 sec/batch)
2016-07-15 20:00:05.487782: step 83680, loss = 3.81 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 20:00:10.645607: step 83690, loss = 4.36 (209.6 examples/sec; 0.611 sec/batch)
2016-07-15 20:00:16.157523: step 83700, loss = 3.98 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 20:00:21.895768: step 83710, loss = 4.03 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 20:00:26.722124: step 83720, loss = 3.97 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 20:00:31.480598: step 83730, loss = 4.11 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 20:00:37.535478: step 83740, loss = 4.28 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 20:00:43.270393: step 83750, loss = 4.14 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 20:00:48.085435: step 83760, loss = 4.22 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 20:00:52.949762: step 83770, loss = 3.97 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 20:00:57.699682: step 83780, loss = 4.36 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 20:01:03.085080: step 83790, loss = 4.07 (187.6 examples/sec; 0.682 sec/batch)
2016-07-15 20:01:09.539710: step 83800, loss = 4.29 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 20:01:16.015004: step 83810, loss = 4.15 (207.3 examples/sec; 0.618 sec/batch)
2016-07-15 20:01:21.197233: step 83820, loss = 4.34 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 20:01:26.987837: step 83830, loss = 4.18 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 20:01:31.827079: step 83840, loss = 4.26 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 20:01:36.636760: step 83850, loss = 4.18 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 20:01:41.359956: step 83860, loss = 4.06 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 20:01:46.047153: step 83870, loss = 4.23 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 20:01:51.174777: step 83880, loss = 3.98 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 20:01:56.554797: step 83890, loss = 3.91 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 20:02:02.308840: step 83900, loss = 3.97 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 20:02:09.001767: step 83910, loss = 4.24 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 20:02:14.174615: step 83920, loss = 4.28 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 20:02:19.755131: step 83930, loss = 4.34 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 20:02:25.505972: step 83940, loss = 4.18 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 20:02:30.422639: step 83950, loss = 3.82 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 20:02:35.166665: step 83960, loss = 4.11 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 20:02:39.956046: step 83970, loss = 4.01 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 20:02:44.794381: step 83980, loss = 3.84 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 20:02:51.236741: step 83990, loss = 4.01 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 20:02:56.631659: step 84000, loss = 4.19 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 20:03:02.397256: step 84010, loss = 4.03 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 20:03:08.050874: step 84020, loss = 4.24 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 20:03:14.247076: step 84030, loss = 3.74 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 20:03:19.113779: step 84040, loss = 4.00 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 20:03:23.802886: step 84050, loss = 4.06 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 20:03:28.458399: step 84060, loss = 4.31 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 20:03:34.300065: step 84070, loss = 4.14 (254.2 examples/sec; 0.503 sec/batch)
2016-07-15 20:03:39.586927: step 84080, loss = 4.05 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 20:03:44.871418: step 84090, loss = 3.98 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 20:03:50.700768: step 84100, loss = 4.25 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 20:03:57.577224: step 84110, loss = 3.91 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 20:04:02.478533: step 84120, loss = 4.12 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 20:04:07.234847: step 84130, loss = 4.04 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 20:04:13.198939: step 84140, loss = 4.10 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 20:04:19.125139: step 84150, loss = 4.12 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 20:04:23.891770: step 84160, loss = 4.22 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 20:04:28.722669: step 84170, loss = 4.04 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 20:04:33.458970: step 84180, loss = 4.07 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 20:04:38.119922: step 84190, loss = 4.17 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 20:04:43.196337: step 84200, loss = 3.84 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 20:04:49.923188: step 84210, loss = 4.35 (231.5 examples/sec; 0.553 sec/batch)
2016-07-15 20:04:55.650452: step 84220, loss = 4.09 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 20:05:00.409691: step 84230, loss = 3.95 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 20:05:05.246559: step 84240, loss = 3.97 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 20:05:12.016731: step 84250, loss = 4.07 (178.0 examples/sec; 0.719 sec/batch)
2016-07-15 20:05:17.408724: step 84260, loss = 4.17 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 20:05:23.157889: step 84270, loss = 4.05 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 20:05:27.909519: step 84280, loss = 4.29 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 20:05:32.736623: step 84290, loss = 4.43 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 20:05:39.016254: step 84300, loss = 4.18 (206.6 examples/sec; 0.619 sec/batch)
2016-07-15 20:05:45.720562: step 84310, loss = 4.18 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 20:05:50.354082: step 84320, loss = 3.98 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 20:05:55.291592: step 84330, loss = 3.74 (242.4 examples/sec; 0.528 sec/batch)
2016-07-15 20:05:59.938546: step 84340, loss = 4.15 (286.2 examples/sec; 0.447 sec/batch)
2016-07-15 20:06:04.609800: step 84350, loss = 4.18 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 20:06:10.333989: step 84360, loss = 3.98 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 20:06:15.695780: step 84370, loss = 3.94 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 20:06:21.005879: step 84380, loss = 3.94 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 20:06:26.811581: step 84390, loss = 4.11 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 20:06:31.588444: step 84400, loss = 4.20 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 20:06:37.367419: step 84410, loss = 4.12 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 20:06:43.836692: step 84420, loss = 3.96 (199.4 examples/sec; 0.642 sec/batch)
2016-07-15 20:06:49.172314: step 84430, loss = 4.09 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 20:06:53.853716: step 84440, loss = 4.00 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 20:06:58.679265: step 84450, loss = 4.10 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 20:07:03.486098: step 84460, loss = 3.92 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 20:07:09.327679: step 84470, loss = 4.18 (190.6 examples/sec; 0.672 sec/batch)
2016-07-15 20:07:15.231778: step 84480, loss = 3.85 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 20:07:19.994940: step 84490, loss = 3.79 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 20:07:24.807196: step 84500, loss = 4.22 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 20:07:30.519111: step 84510, loss = 3.98 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 20:07:36.067773: step 84520, loss = 3.93 (172.1 examples/sec; 0.744 sec/batch)
2016-07-15 20:07:42.522570: step 84530, loss = 3.98 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 20:07:47.298925: step 84540, loss = 3.91 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 20:07:51.979239: step 84550, loss = 3.97 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 20:07:56.610859: step 84560, loss = 3.87 (271.5 examples/sec; 0.472 sec/batch)
2016-07-15 20:08:02.316371: step 84570, loss = 3.92 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 20:08:07.198873: step 84580, loss = 4.00 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 20:08:11.993954: step 84590, loss = 4.12 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 20:08:18.079031: step 84600, loss = 4.22 (192.9 examples/sec; 0.663 sec/batch)
2016-07-15 20:08:25.053609: step 84610, loss = 4.22 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 20:08:30.851286: step 84620, loss = 4.15 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 20:08:35.620159: step 84630, loss = 3.87 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 20:08:40.373006: step 84640, loss = 4.15 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 20:08:45.157312: step 84650, loss = 4.06 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 20:08:50.107063: step 84660, loss = 3.93 (219.1 examples/sec; 0.584 sec/batch)
2016-07-15 20:08:56.663907: step 84670, loss = 4.11 (202.5 examples/sec; 0.632 sec/batch)
2016-07-15 20:09:01.865643: step 84680, loss = 4.07 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 20:09:06.572437: step 84690, loss = 4.25 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 20:09:12.061434: step 84700, loss = 4.24 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 20:09:18.486395: step 84710, loss = 3.87 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 20:09:23.595844: step 84720, loss = 4.08 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 20:09:28.957621: step 84730, loss = 4.32 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 20:09:34.709977: step 84740, loss = 3.90 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 20:09:40.065746: step 84750, loss = 4.05 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 20:09:45.387666: step 84760, loss = 4.23 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 20:09:50.090510: step 84770, loss = 3.85 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 20:09:55.466850: step 84780, loss = 3.87 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 20:10:01.918567: step 84790, loss = 4.12 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 20:10:06.776984: step 84800, loss = 3.95 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 20:10:12.471966: step 84810, loss = 4.02 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 20:10:17.339612: step 84820, loss = 4.12 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 20:10:22.356394: step 84830, loss = 4.26 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 20:10:27.706835: step 84840, loss = 3.76 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 20:10:33.737821: step 84850, loss = 4.12 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 20:10:39.528393: step 84860, loss = 4.13 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 20:10:44.294096: step 84870, loss = 4.09 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 20:10:49.132983: step 84880, loss = 4.12 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 20:10:53.834210: step 84890, loss = 3.81 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 20:10:58.723894: step 84900, loss = 4.02 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 20:11:04.533919: step 84910, loss = 4.09 (246.6 examples/sec; 0.519 sec/batch)
2016-07-15 20:11:09.268913: step 84920, loss = 4.17 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 20:11:14.213761: step 84930, loss = 4.02 (225.5 examples/sec; 0.568 sec/batch)
2016-07-15 20:11:20.756457: step 84940, loss = 4.29 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 20:11:25.961577: step 84950, loss = 4.01 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 20:11:30.659518: step 84960, loss = 4.21 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 20:11:36.128940: step 84970, loss = 4.35 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 20:11:42.385964: step 84980, loss = 3.93 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 20:11:47.623968: step 84990, loss = 4.01 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 20:11:53.081763: step 85000, loss = 4.13 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 20:11:58.895675: step 85010, loss = 4.22 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 20:12:03.738428: step 85020, loss = 4.08 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 20:12:08.549639: step 85030, loss = 4.21 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 20:12:13.341509: step 85040, loss = 3.95 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 20:12:18.174213: step 85050, loss = 3.86 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 20:12:22.840793: step 85060, loss = 4.00 (282.6 examples/sec; 0.453 sec/batch)
2016-07-15 20:12:27.496069: step 85070, loss = 4.01 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 20:12:32.990854: step 85080, loss = 4.23 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 20:12:38.219242: step 85090, loss = 4.08 (234.5 examples/sec; 0.546 sec/batch)
2016-07-15 20:12:43.980663: step 85100, loss = 3.84 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 20:12:49.694553: step 85110, loss = 3.82 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 20:12:54.794798: step 85120, loss = 4.12 (184.8 examples/sec; 0.693 sec/batch)
2016-07-15 20:13:01.318638: step 85130, loss = 3.92 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 20:13:06.435815: step 85140, loss = 3.78 (219.5 examples/sec; 0.583 sec/batch)
2016-07-15 20:13:12.130009: step 85150, loss = 3.86 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 20:13:16.950714: step 85160, loss = 4.07 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 20:13:21.836082: step 85170, loss = 4.18 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 20:13:28.387702: step 85180, loss = 3.99 (196.6 examples/sec; 0.651 sec/batch)
2016-07-15 20:13:33.652282: step 85190, loss = 4.01 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 20:13:39.398037: step 85200, loss = 3.83 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 20:13:45.235347: step 85210, loss = 4.10 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 20:13:50.053071: step 85220, loss = 4.22 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 20:13:54.720661: step 85230, loss = 3.93 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 20:14:00.243970: step 85240, loss = 3.94 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 20:14:06.540907: step 85250, loss = 3.98 (253.9 examples/sec; 0.504 sec/batch)
2016-07-15 20:14:11.773096: step 85260, loss = 4.12 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 20:14:17.148468: step 85270, loss = 4.06 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 20:14:21.847588: step 85280, loss = 4.19 (284.0 examples/sec; 0.451 sec/batch)
2016-07-15 20:14:26.773704: step 85290, loss = 4.28 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 20:14:31.537862: step 85300, loss = 4.06 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 20:14:37.301095: step 85310, loss = 4.13 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 20:14:42.084276: step 85320, loss = 4.11 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 20:14:46.774709: step 85330, loss = 3.96 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 20:14:51.579108: step 85340, loss = 4.13 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 20:14:56.335124: step 85350, loss = 4.00 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 20:15:02.386751: step 85360, loss = 3.89 (195.3 examples/sec; 0.655 sec/batch)
2016-07-15 20:15:08.105899: step 85370, loss = 4.17 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 20:15:13.795512: step 85380, loss = 4.05 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 20:15:18.800580: step 85390, loss = 4.10 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 20:15:23.522240: step 85400, loss = 4.00 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 20:15:30.624923: step 85410, loss = 4.04 (202.0 examples/sec; 0.634 sec/batch)
2016-07-15 20:15:36.372712: step 85420, loss = 3.89 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 20:15:42.098586: step 85430, loss = 4.00 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 20:15:46.984366: step 85440, loss = 4.16 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 20:15:51.616764: step 85450, loss = 4.30 (282.0 examples/sec; 0.454 sec/batch)
2016-07-15 20:15:56.241363: step 85460, loss = 4.21 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 20:16:01.882875: step 85470, loss = 4.24 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 20:16:07.108498: step 85480, loss = 4.13 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 20:16:12.663116: step 85490, loss = 4.13 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 20:16:17.401519: step 85500, loss = 4.27 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 20:16:22.961884: step 85510, loss = 4.07 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 20:16:28.089486: step 85520, loss = 3.98 (207.6 examples/sec; 0.617 sec/batch)
2016-07-15 20:16:33.446056: step 85530, loss = 3.91 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 20:16:38.125834: step 85540, loss = 4.01 (281.9 examples/sec; 0.454 sec/batch)
2016-07-15 20:16:43.394904: step 85550, loss = 4.04 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 20:16:49.930975: step 85560, loss = 4.09 (199.8 examples/sec; 0.641 sec/batch)
2016-07-15 20:16:55.140473: step 85570, loss = 4.10 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 20:17:00.704308: step 85580, loss = 4.18 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 20:17:05.478030: step 85590, loss = 4.15 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 20:17:10.328952: step 85600, loss = 3.87 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 20:17:16.031418: step 85610, loss = 3.90 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 20:17:20.690363: step 85620, loss = 3.99 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 20:17:26.438202: step 85630, loss = 4.10 (220.1 examples/sec; 0.582 sec/batch)
2016-07-15 20:17:31.295283: step 85640, loss = 3.86 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 20:17:36.037021: step 85650, loss = 3.78 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 20:17:40.800387: step 85660, loss = 4.21 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 20:17:45.484275: step 85670, loss = 3.81 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 20:17:50.113535: step 85680, loss = 4.24 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 20:17:55.911828: step 85690, loss = 3.91 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 20:18:00.693711: step 85700, loss = 3.85 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 20:18:06.474417: step 85710, loss = 3.95 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 20:18:11.218466: step 85720, loss = 3.95 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 20:18:16.078394: step 85730, loss = 3.90 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 20:18:20.884187: step 85740, loss = 3.96 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 20:18:25.650273: step 85750, loss = 4.16 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 20:18:30.576510: step 85760, loss = 4.16 (233.3 examples/sec; 0.549 sec/batch)
2016-07-15 20:18:37.161526: step 85770, loss = 4.00 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 20:18:42.296362: step 85780, loss = 4.05 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 20:18:47.066032: step 85790, loss = 4.15 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 20:18:51.910175: step 85800, loss = 4.04 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 20:18:57.557443: step 85810, loss = 4.04 (284.7 examples/sec; 0.450 sec/batch)
2016-07-15 20:19:02.204364: step 85820, loss = 3.89 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 20:19:07.992143: step 85830, loss = 4.13 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 20:19:13.571129: step 85840, loss = 4.10 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 20:19:18.722310: step 85850, loss = 4.23 (225.9 examples/sec; 0.567 sec/batch)
2016-07-15 20:19:24.475728: step 85860, loss = 4.26 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 20:19:29.237887: step 85870, loss = 4.09 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 20:19:34.070898: step 85880, loss = 3.77 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 20:19:38.786711: step 85890, loss = 4.24 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 20:19:44.202164: step 85900, loss = 4.03 (188.4 examples/sec; 0.679 sec/batch)
2016-07-15 20:19:51.900581: step 85910, loss = 4.02 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 20:19:56.674771: step 85920, loss = 4.07 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 20:20:01.455852: step 85930, loss = 3.80 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 20:20:06.194551: step 85940, loss = 3.83 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 20:20:11.259104: step 85950, loss = 4.10 (187.5 examples/sec; 0.683 sec/batch)
2016-07-15 20:20:17.773605: step 85960, loss = 3.94 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 20:20:22.822182: step 85970, loss = 4.01 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 20:20:27.554727: step 85980, loss = 4.05 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 20:20:32.379336: step 85990, loss = 4.07 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 20:20:37.175542: step 86000, loss = 4.23 (253.0 examples/sec; 0.506 sec/batch)
2016-07-15 20:20:42.852128: step 86010, loss = 4.07 (283.5 examples/sec; 0.451 sec/batch)
2016-07-15 20:20:47.524767: step 86020, loss = 4.10 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 20:20:52.863324: step 86030, loss = 4.06 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 20:20:58.153651: step 86040, loss = 4.03 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 20:21:02.891326: step 86050, loss = 3.82 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 20:21:08.310840: step 86060, loss = 4.02 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 20:21:14.731713: step 86070, loss = 3.97 (209.0 examples/sec; 0.612 sec/batch)
2016-07-15 20:21:19.578060: step 86080, loss = 4.26 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 20:21:24.354492: step 86090, loss = 3.96 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 20:21:29.204380: step 86100, loss = 4.01 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 20:21:34.859631: step 86110, loss = 4.07 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 20:21:39.821498: step 86120, loss = 4.36 (208.7 examples/sec; 0.613 sec/batch)
2016-07-15 20:21:45.337233: step 86130, loss = 4.33 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 20:21:51.130508: step 86140, loss = 3.93 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 20:21:55.983820: step 86150, loss = 3.99 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 20:22:00.746287: step 86160, loss = 4.18 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 20:22:05.538184: step 86170, loss = 3.89 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 20:22:10.153335: step 86180, loss = 3.94 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 20:22:14.984575: step 86190, loss = 4.13 (216.3 examples/sec; 0.592 sec/batch)
2016-07-15 20:22:20.660631: step 86200, loss = 4.44 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 20:22:26.405529: step 86210, loss = 3.85 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 20:22:31.612531: step 86220, loss = 3.95 (189.7 examples/sec; 0.675 sec/batch)
2016-07-15 20:22:38.099584: step 86230, loss = 3.95 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 20:22:43.236625: step 86240, loss = 4.16 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 20:22:48.868468: step 86250, loss = 4.19 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 20:22:54.624427: step 86260, loss = 3.93 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 20:22:59.527570: step 86270, loss = 4.22 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 20:23:04.286642: step 86280, loss = 3.98 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 20:23:09.170560: step 86290, loss = 3.91 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 20:23:13.787528: step 86300, loss = 4.19 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 20:23:19.738725: step 86310, loss = 3.77 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 20:23:25.319766: step 86320, loss = 4.06 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 20:23:31.058579: step 86330, loss = 4.09 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 20:23:35.861596: step 86340, loss = 4.09 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 20:23:40.642702: step 86350, loss = 4.02 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 20:23:45.382794: step 86360, loss = 3.79 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 20:23:50.245085: step 86370, loss = 4.03 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 20:23:54.988906: step 86380, loss = 4.26 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 20:24:00.324497: step 86390, loss = 4.50 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 20:24:06.764116: step 86400, loss = 4.08 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 20:24:12.693031: step 86410, loss = 3.88 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 20:24:17.357103: step 86420, loss = 4.17 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 20:24:22.039098: step 86430, loss = 3.86 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 20:24:27.808445: step 86440, loss = 4.22 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 20:24:33.320660: step 86450, loss = 4.12 (204.3 examples/sec; 0.626 sec/batch)
2016-07-15 20:24:38.503807: step 86460, loss = 3.95 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 20:24:43.228971: step 86470, loss = 4.37 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 20:24:48.060319: step 86480, loss = 4.11 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 20:24:52.802604: step 86490, loss = 3.89 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 20:24:58.851773: step 86500, loss = 3.94 (192.0 examples/sec; 0.667 sec/batch)
2016-07-15 20:25:05.849511: step 86510, loss = 4.04 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 20:25:11.630347: step 86520, loss = 3.94 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 20:25:16.409218: step 86530, loss = 4.29 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 20:25:21.200311: step 86540, loss = 4.03 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 20:25:25.923682: step 86550, loss = 4.03 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 20:25:30.865290: step 86560, loss = 4.12 (243.9 examples/sec; 0.525 sec/batch)
2016-07-15 20:25:37.462438: step 86570, loss = 4.39 (201.7 examples/sec; 0.635 sec/batch)
2016-07-15 20:25:42.617310: step 86580, loss = 3.91 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 20:25:47.284618: step 86590, loss = 3.91 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 20:25:52.146354: step 86600, loss = 4.20 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 20:25:57.923270: step 86610, loss = 4.37 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 20:26:02.651291: step 86620, loss = 4.20 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 20:26:07.862246: step 86630, loss = 4.14 (178.2 examples/sec; 0.718 sec/batch)
2016-07-15 20:26:14.304349: step 86640, loss = 4.15 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 20:26:19.539195: step 86650, loss = 3.95 (201.5 examples/sec; 0.635 sec/batch)
2016-07-15 20:26:25.134980: step 86660, loss = 4.11 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 20:26:29.868307: step 86670, loss = 3.98 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 20:26:34.736505: step 86680, loss = 3.93 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 20:26:39.399713: step 86690, loss = 4.02 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 20:26:44.883932: step 86700, loss = 4.00 (192.1 examples/sec; 0.666 sec/batch)
2016-07-15 20:26:52.548552: step 86710, loss = 3.96 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 20:26:57.342526: step 86720, loss = 4.02 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 20:27:01.974076: step 86730, loss = 4.11 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 20:27:06.655497: step 86740, loss = 4.12 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 20:27:12.446743: step 86750, loss = 4.01 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 20:27:18.111730: step 86760, loss = 3.88 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 20:27:23.181149: step 86770, loss = 3.97 (276.2 examples/sec; 0.464 sec/batch)
2016-07-15 20:27:27.925934: step 86780, loss = 4.08 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 20:27:32.697602: step 86790, loss = 4.05 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 20:27:37.507962: step 86800, loss = 4.06 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 20:27:43.224254: step 86810, loss = 3.80 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 20:27:47.855132: step 86820, loss = 4.33 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 20:27:53.161682: step 86830, loss = 3.94 (200.5 examples/sec; 0.638 sec/batch)
2016-07-15 20:27:58.478880: step 86840, loss = 4.12 (239.1 examples/sec; 0.535 sec/batch)
2016-07-15 20:28:04.230818: step 86850, loss = 3.90 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 20:28:09.115913: step 86860, loss = 3.93 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 20:28:13.918111: step 86870, loss = 3.85 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 20:28:18.659389: step 86880, loss = 3.95 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 20:28:23.480768: step 86890, loss = 4.08 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 20:28:28.202808: step 86900, loss = 3.93 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 20:28:35.263619: step 86910, loss = 4.01 (198.8 examples/sec; 0.644 sec/batch)
2016-07-15 20:28:40.948188: step 86920, loss = 4.27 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 20:28:45.712343: step 86930, loss = 3.99 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 20:28:50.569660: step 86940, loss = 4.09 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 20:28:55.286871: step 86950, loss = 4.02 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 20:29:00.721352: step 86960, loss = 3.96 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 20:29:07.066291: step 86970, loss = 4.22 (225.6 examples/sec; 0.567 sec/batch)
2016-07-15 20:29:12.204075: step 86980, loss = 3.86 (209.0 examples/sec; 0.612 sec/batch)
2016-07-15 20:29:17.706810: step 86990, loss = 4.14 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 20:29:23.441244: step 87000, loss = 3.87 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 20:29:29.305745: step 87010, loss = 4.13 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 20:29:34.165118: step 87020, loss = 4.42 (250.2 examples/sec; 0.512 sec/batch)
2016-07-15 20:29:40.514925: step 87030, loss = 3.94 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 20:29:45.992985: step 87040, loss = 3.79 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 20:29:50.720937: step 87050, loss = 4.14 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 20:29:55.564542: step 87060, loss = 3.85 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 20:30:00.279474: step 87070, loss = 3.94 (282.7 examples/sec; 0.453 sec/batch)
2016-07-15 20:30:04.894255: step 87080, loss = 4.22 (281.7 examples/sec; 0.454 sec/batch)
2016-07-15 20:30:10.478060: step 87090, loss = 4.20 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 20:30:15.471133: step 87100, loss = 4.13 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 20:30:21.258475: step 87110, loss = 4.06 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 20:30:27.336948: step 87120, loss = 3.88 (194.0 examples/sec; 0.660 sec/batch)
2016-07-15 20:30:33.032581: step 87130, loss = 3.99 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 20:30:37.804575: step 87140, loss = 4.14 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 20:30:42.738013: step 87150, loss = 4.18 (258.3 examples/sec; 0.495 sec/batch)
2016-07-15 20:30:49.203157: step 87160, loss = 4.02 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 20:30:54.494208: step 87170, loss = 4.09 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 20:30:59.227901: step 87180, loss = 3.95 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 20:31:04.505591: step 87190, loss = 3.91 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 20:31:10.932581: step 87200, loss = 4.20 (208.7 examples/sec; 0.613 sec/batch)
2016-07-15 20:31:16.741027: step 87210, loss = 3.84 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 20:31:21.483223: step 87220, loss = 4.01 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 20:31:26.242021: step 87230, loss = 3.98 (281.1 examples/sec; 0.455 sec/batch)
2016-07-15 20:31:31.103497: step 87240, loss = 4.26 (253.2 examples/sec; 0.506 sec/batch)
2016-07-15 20:31:35.813784: step 87250, loss = 3.89 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 20:31:41.319156: step 87260, loss = 4.13 (183.7 examples/sec; 0.697 sec/batch)
2016-07-15 20:31:47.567213: step 87270, loss = 4.07 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 20:31:52.456735: step 87280, loss = 4.20 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 20:31:57.245796: step 87290, loss = 3.97 (268.6 examples/sec; 0.476 sec/batch)
2016-07-15 20:32:03.321358: step 87300, loss = 4.00 (196.2 examples/sec; 0.653 sec/batch)
2016-07-15 20:32:10.338643: step 87310, loss = 3.98 (250.8 examples/sec; 0.510 sec/batch)
2016-07-15 20:32:16.118606: step 87320, loss = 4.14 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 20:32:20.883972: step 87330, loss = 3.78 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 20:32:25.694420: step 87340, loss = 3.77 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 20:32:31.898666: step 87350, loss = 4.04 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 20:32:37.351922: step 87360, loss = 4.19 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 20:32:42.009387: step 87370, loss = 3.97 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 20:32:47.742093: step 87380, loss = 3.99 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 20:32:52.590560: step 87390, loss = 3.75 (272.1 examples/sec; 0.470 sec/batch)
2016-07-15 20:32:57.357619: step 87400, loss = 3.88 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 20:33:04.845797: step 87410, loss = 4.20 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 20:33:10.286891: step 87420, loss = 3.92 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 20:33:16.059680: step 87430, loss = 4.04 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 20:33:20.911484: step 87440, loss = 4.01 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 20:33:25.691396: step 87450, loss = 3.83 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 20:33:30.476344: step 87460, loss = 4.16 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 20:33:35.351351: step 87470, loss = 4.08 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 20:33:40.022467: step 87480, loss = 4.08 (284.8 examples/sec; 0.449 sec/batch)
2016-07-15 20:33:44.618118: step 87490, loss = 4.07 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 20:33:50.108641: step 87500, loss = 3.85 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 20:33:56.247119: step 87510, loss = 3.86 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 20:34:01.580581: step 87520, loss = 3.80 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 20:34:06.847816: step 87530, loss = 3.95 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 20:34:11.496891: step 87540, loss = 4.26 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 20:34:16.054538: step 87550, loss = 4.44 (284.5 examples/sec; 0.450 sec/batch)
2016-07-15 20:34:20.678968: step 87560, loss = 4.10 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 20:34:26.060197: step 87570, loss = 3.99 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 20:34:31.230177: step 87580, loss = 4.15 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 20:34:35.956370: step 87590, loss = 4.06 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 20:34:41.506418: step 87600, loss = 3.93 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 20:34:47.885315: step 87610, loss = 4.10 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 20:34:52.621070: step 87620, loss = 3.80 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 20:34:57.287248: step 87630, loss = 4.38 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 20:35:01.959903: step 87640, loss = 4.18 (281.1 examples/sec; 0.455 sec/batch)
2016-07-15 20:35:06.654744: step 87650, loss = 3.87 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 20:35:12.461657: step 87660, loss = 4.01 (247.1 examples/sec; 0.518 sec/batch)
2016-07-15 20:35:17.272774: step 87670, loss = 3.91 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 20:35:21.949908: step 87680, loss = 3.84 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 20:35:26.580021: step 87690, loss = 3.91 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 20:35:32.334664: step 87700, loss = 3.88 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 20:35:39.091557: step 87710, loss = 4.02 (199.6 examples/sec; 0.641 sec/batch)
2016-07-15 20:35:44.246997: step 87720, loss = 4.04 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 20:35:49.820441: step 87730, loss = 4.18 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 20:35:54.561748: step 87740, loss = 3.86 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 20:35:59.473263: step 87750, loss = 3.86 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 20:36:04.188001: step 87760, loss = 4.09 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 20:36:09.740875: step 87770, loss = 4.03 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 20:36:15.958549: step 87780, loss = 4.07 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 20:36:21.267020: step 87790, loss = 3.69 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 20:36:26.602041: step 87800, loss = 4.06 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 20:36:32.360125: step 87810, loss = 3.72 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 20:36:37.171270: step 87820, loss = 4.01 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 20:36:41.966547: step 87830, loss = 3.96 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 20:36:48.117597: step 87840, loss = 3.78 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 20:36:53.690401: step 87850, loss = 4.08 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 20:36:58.421220: step 87860, loss = 3.99 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 20:37:03.044343: step 87870, loss = 4.06 (280.4 examples/sec; 0.456 sec/batch)
2016-07-15 20:37:07.819805: step 87880, loss = 4.06 (209.8 examples/sec; 0.610 sec/batch)
2016-07-15 20:37:13.546948: step 87890, loss = 4.29 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 20:37:18.279043: step 87900, loss = 4.27 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 20:37:23.847853: step 87910, loss = 3.95 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 20:37:29.060050: step 87920, loss = 3.78 (196.0 examples/sec; 0.653 sec/batch)
2016-07-15 20:37:34.474759: step 87930, loss = 3.97 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 20:37:40.221707: step 87940, loss = 4.07 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 20:37:45.645858: step 87950, loss = 4.05 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 20:37:50.940247: step 87960, loss = 3.97 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 20:37:55.637189: step 87970, loss = 3.77 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 20:38:00.465064: step 87980, loss = 3.99 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 20:38:05.095301: step 87990, loss = 3.66 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 20:38:09.711547: step 88000, loss = 4.03 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 20:38:16.466204: step 88010, loss = 3.93 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 20:38:21.267963: step 88020, loss = 3.84 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 20:38:26.057907: step 88030, loss = 3.78 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 20:38:32.273175: step 88040, loss = 3.96 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 20:38:37.904735: step 88050, loss = 3.87 (248.8 examples/sec; 0.514 sec/batch)
2016-07-15 20:38:42.681330: step 88060, loss = 3.86 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 20:38:47.712418: step 88070, loss = 3.91 (227.6 examples/sec; 0.562 sec/batch)
2016-07-15 20:38:54.263971: step 88080, loss = 3.80 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 20:38:59.400031: step 88090, loss = 4.02 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 20:39:04.127058: step 88100, loss = 3.83 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 20:39:10.042943: step 88110, loss = 3.95 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 20:39:14.654782: step 88120, loss = 3.92 (283.2 examples/sec; 0.452 sec/batch)
2016-07-15 20:39:19.315170: step 88130, loss = 4.28 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 20:39:25.081993: step 88140, loss = 3.82 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 20:39:30.683798: step 88150, loss = 4.20 (200.5 examples/sec; 0.639 sec/batch)
2016-07-15 20:39:35.762173: step 88160, loss = 3.84 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 20:39:40.496280: step 88170, loss = 4.16 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 20:39:45.349664: step 88180, loss = 4.07 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 20:39:50.191685: step 88190, loss = 3.96 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 20:39:54.971691: step 88200, loss = 3.73 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 20:40:00.732839: step 88210, loss = 4.14 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 20:40:05.379434: step 88220, loss = 3.81 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 20:40:10.048971: step 88230, loss = 4.11 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 20:40:14.688253: step 88240, loss = 4.18 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 20:40:19.329909: step 88250, loss = 4.06 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 20:40:25.123702: step 88260, loss = 4.13 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 20:40:30.706718: step 88270, loss = 4.06 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 20:40:35.813691: step 88280, loss = 3.99 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 20:40:40.515869: step 88290, loss = 4.07 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 20:40:45.296228: step 88300, loss = 3.91 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 20:40:51.117099: step 88310, loss = 4.15 (248.9 examples/sec; 0.514 sec/batch)
2016-07-15 20:40:57.570307: step 88320, loss = 3.85 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 20:41:02.871129: step 88330, loss = 4.03 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 20:41:08.641548: step 88340, loss = 3.99 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 20:41:13.502640: step 88350, loss = 3.99 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 20:41:18.309632: step 88360, loss = 4.17 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 20:41:24.541328: step 88370, loss = 4.13 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 20:41:30.099836: step 88380, loss = 3.79 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 20:41:34.800764: step 88390, loss = 3.86 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 20:41:39.631456: step 88400, loss = 4.02 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 20:41:45.345795: step 88410, loss = 3.87 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 20:41:51.251038: step 88420, loss = 4.20 (191.1 examples/sec; 0.670 sec/batch)
2016-07-15 20:41:57.116726: step 88430, loss = 4.11 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 20:42:01.873834: step 88440, loss = 4.04 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 20:42:06.537516: step 88450, loss = 3.97 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 20:42:11.220259: step 88460, loss = 4.04 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 20:42:16.967582: step 88470, loss = 3.90 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 20:42:22.481190: step 88480, loss = 3.90 (198.3 examples/sec; 0.646 sec/batch)
2016-07-15 20:42:27.722467: step 88490, loss = 4.11 (246.1 examples/sec; 0.520 sec/batch)
2016-07-15 20:42:32.506998: step 88500, loss = 4.02 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 20:42:39.431338: step 88510, loss = 4.02 (188.1 examples/sec; 0.680 sec/batch)
2016-07-15 20:42:45.196753: step 88520, loss = 4.19 (283.5 examples/sec; 0.451 sec/batch)
2016-07-15 20:42:49.839065: step 88530, loss = 4.08 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 20:42:55.443159: step 88540, loss = 3.96 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 20:43:00.596473: step 88550, loss = 3.82 (201.7 examples/sec; 0.634 sec/batch)
2016-07-15 20:43:06.178749: step 88560, loss = 3.95 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 20:43:10.980463: step 88570, loss = 4.07 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 20:43:15.881717: step 88580, loss = 4.10 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 20:43:20.570911: step 88590, loss = 3.99 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 20:43:25.438460: step 88600, loss = 3.99 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 20:43:31.063099: step 88610, loss = 3.91 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 20:43:35.750408: step 88620, loss = 4.00 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 20:43:41.471949: step 88630, loss = 4.12 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 20:43:47.118363: step 88640, loss = 3.98 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 20:43:52.125200: step 88650, loss = 4.07 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 20:43:56.850917: step 88660, loss = 3.80 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 20:44:02.557096: step 88670, loss = 3.99 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 20:44:07.578324: step 88680, loss = 4.09 (284.4 examples/sec; 0.450 sec/batch)
2016-07-15 20:44:12.237426: step 88690, loss = 3.99 (272.8 examples/sec; 0.469 sec/batch)
2016-07-15 20:44:17.314260: step 88700, loss = 4.00 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 20:44:24.034337: step 88710, loss = 4.33 (227.5 examples/sec; 0.563 sec/batch)
2016-07-15 20:44:29.770890: step 88720, loss = 3.91 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 20:44:34.546045: step 88730, loss = 3.96 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 20:44:39.378544: step 88740, loss = 4.03 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 20:44:45.351167: step 88750, loss = 4.02 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 20:44:49.975792: step 88760, loss = 3.89 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 20:44:55.616388: step 88770, loss = 3.97 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 20:45:00.715910: step 88780, loss = 4.03 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 20:45:06.317328: step 88790, loss = 3.78 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 20:45:11.088325: step 88800, loss = 3.81 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 20:45:16.858568: step 88810, loss = 3.92 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 20:45:21.567765: step 88820, loss = 3.93 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 20:45:26.239026: step 88830, loss = 4.00 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 20:45:31.946408: step 88840, loss = 3.82 (222.6 examples/sec; 0.575 sec/batch)
2016-07-15 20:45:36.801353: step 88850, loss = 3.97 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 20:45:41.507684: step 88860, loss = 3.79 (282.7 examples/sec; 0.453 sec/batch)
2016-07-15 20:45:46.161693: step 88870, loss = 3.92 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 20:45:51.938824: step 88880, loss = 4.03 (209.9 examples/sec; 0.610 sec/batch)
2016-07-15 20:45:57.140055: step 88890, loss = 3.83 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 20:46:02.654138: step 88900, loss = 3.68 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 20:46:08.443214: step 88910, loss = 3.85 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 20:46:13.299032: step 88920, loss = 4.11 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 20:46:18.213416: step 88930, loss = 3.96 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 20:46:22.942866: step 88940, loss = 4.03 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 20:46:27.768464: step 88950, loss = 3.78 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 20:46:32.553002: step 88960, loss = 4.05 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 20:46:37.406840: step 88970, loss = 4.06 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 20:46:42.152938: step 88980, loss = 3.99 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 20:46:48.287781: step 88990, loss = 4.00 (199.7 examples/sec; 0.641 sec/batch)
2016-07-15 20:46:54.003769: step 89000, loss = 4.06 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 20:46:59.788864: step 89010, loss = 3.94 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 20:47:04.660437: step 89020, loss = 3.96 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 20:47:09.423072: step 89030, loss = 4.00 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 20:47:14.235315: step 89040, loss = 4.01 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 20:47:19.065758: step 89050, loss = 4.18 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 20:47:23.801027: step 89060, loss = 4.09 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 20:47:28.477685: step 89070, loss = 4.19 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 20:47:33.550723: step 89080, loss = 4.03 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 20:47:38.989162: step 89090, loss = 3.77 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 20:47:43.668997: step 89100, loss = 3.88 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 20:47:49.484797: step 89110, loss = 3.94 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 20:47:54.163884: step 89120, loss = 4.06 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 20:47:58.931572: step 89130, loss = 3.89 (252.3 examples/sec; 0.507 sec/batch)
2016-07-15 20:48:03.612832: step 89140, loss = 4.07 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 20:48:08.452950: step 89150, loss = 3.65 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 20:48:14.109342: step 89160, loss = 3.89 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 20:48:19.867513: step 89170, loss = 3.72 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 20:48:24.784108: step 89180, loss = 3.67 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 20:48:29.572369: step 89190, loss = 3.93 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 20:48:34.324179: step 89200, loss = 3.69 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 20:48:40.213941: step 89210, loss = 3.72 (230.6 examples/sec; 0.555 sec/batch)
2016-07-15 20:48:46.817061: step 89220, loss = 3.87 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 20:48:52.014031: step 89230, loss = 3.96 (253.0 examples/sec; 0.506 sec/batch)
2016-07-15 20:48:56.738022: step 89240, loss = 3.88 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 20:49:01.558117: step 89250, loss = 4.04 (282.8 examples/sec; 0.453 sec/batch)
2016-07-15 20:49:06.187746: step 89260, loss = 3.85 (283.3 examples/sec; 0.452 sec/batch)
2016-07-15 20:49:10.862276: step 89270, loss = 3.87 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 20:49:16.559551: step 89280, loss = 3.78 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 20:49:21.383969: step 89290, loss = 3.78 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 20:49:26.170749: step 89300, loss = 3.85 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 20:49:31.948020: step 89310, loss = 4.19 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 20:49:36.590395: step 89320, loss = 3.86 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 20:49:41.684082: step 89330, loss = 4.17 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 20:49:47.064514: step 89340, loss = 4.02 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 20:49:51.815141: step 89350, loss = 4.05 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 20:49:57.067491: step 89360, loss = 3.71 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 20:50:03.606993: step 89370, loss = 4.09 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 20:50:08.327652: step 89380, loss = 3.91 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 20:50:13.313128: step 89390, loss = 4.22 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 20:50:18.817870: step 89400, loss = 4.12 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 20:50:24.554714: step 89410, loss = 3.93 (266.4 examples/sec; 0.480 sec/batch)
2016-07-15 20:50:29.139556: step 89420, loss = 3.99 (273.8 examples/sec; 0.468 sec/batch)
2016-07-15 20:50:33.862380: step 89430, loss = 4.00 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 20:50:38.576442: step 89440, loss = 3.83 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 20:50:43.283272: step 89450, loss = 3.96 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 20:50:47.958735: step 89460, loss = 3.83 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 20:50:53.712026: step 89470, loss = 3.67 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 20:50:58.572350: step 89480, loss = 4.16 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 20:51:03.380138: step 89490, loss = 3.96 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 20:51:09.801829: step 89500, loss = 4.05 (200.4 examples/sec; 0.639 sec/batch)
2016-07-15 20:51:16.396363: step 89510, loss = 3.97 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 20:51:21.038823: step 89520, loss = 4.05 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 20:51:25.689119: step 89530, loss = 4.05 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 20:51:31.173674: step 89540, loss = 3.82 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 20:51:36.252677: step 89550, loss = 3.92 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 20:51:40.926884: step 89560, loss = 4.02 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 20:51:45.543171: step 89570, loss = 3.93 (282.1 examples/sec; 0.454 sec/batch)
2016-07-15 20:51:50.993411: step 89580, loss = 3.83 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 20:51:56.167573: step 89590, loss = 3.70 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 20:52:00.887202: step 89600, loss = 4.07 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 20:52:07.887227: step 89610, loss = 3.94 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 20:52:13.705523: step 89620, loss = 3.87 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 20:52:18.480479: step 89630, loss = 3.91 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 20:52:23.343341: step 89640, loss = 3.89 (253.5 examples/sec; 0.505 sec/batch)
2016-07-15 20:52:28.077509: step 89650, loss = 4.04 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 20:52:32.932591: step 89660, loss = 4.04 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 20:52:37.715670: step 89670, loss = 4.19 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 20:52:42.510075: step 89680, loss = 4.03 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 20:52:47.361898: step 89690, loss = 4.01 (240.7 examples/sec; 0.532 sec/batch)
2016-07-15 20:52:53.728134: step 89700, loss = 4.20 (206.3 examples/sec; 0.620 sec/batch)
2016-07-15 20:53:00.331219: step 89710, loss = 3.80 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 20:53:04.988138: step 89720, loss = 3.96 (283.5 examples/sec; 0.451 sec/batch)
2016-07-15 20:53:09.667879: step 89730, loss = 3.97 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 20:53:15.127144: step 89740, loss = 4.03 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 20:53:20.349743: step 89750, loss = 4.08 (247.2 examples/sec; 0.518 sec/batch)
2016-07-15 20:53:26.044976: step 89760, loss = 3.89 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 20:53:31.711242: step 89770, loss = 3.89 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 20:53:36.794180: step 89780, loss = 4.12 (249.5 examples/sec; 0.513 sec/batch)
2016-07-15 20:53:41.533805: step 89790, loss = 3.82 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 20:53:47.334640: step 89800, loss = 3.68 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 20:53:53.405258: step 89810, loss = 4.10 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 20:53:58.105170: step 89820, loss = 3.76 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 20:54:02.730355: step 89830, loss = 3.93 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 20:54:08.433254: step 89840, loss = 4.07 (222.6 examples/sec; 0.575 sec/batch)
2016-07-15 20:54:13.634233: step 89850, loss = 3.94 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 20:54:19.075335: step 89860, loss = 3.97 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 20:54:24.852986: step 89870, loss = 4.24 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 20:54:29.641519: step 89880, loss = 3.92 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 20:54:34.435627: step 89890, loss = 4.03 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 20:54:40.501123: step 89900, loss = 3.76 (194.4 examples/sec; 0.659 sec/batch)
2016-07-15 20:54:47.475267: step 89910, loss = 4.09 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 20:54:53.203082: step 89920, loss = 3.92 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 20:54:58.102221: step 89930, loss = 3.90 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 20:55:02.868657: step 89940, loss = 4.05 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 20:55:07.600743: step 89950, loss = 3.82 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 20:55:12.452540: step 89960, loss = 3.76 (246.3 examples/sec; 0.520 sec/batch)
2016-07-15 20:55:19.002078: step 89970, loss = 3.87 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 20:55:24.230177: step 89980, loss = 3.92 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 20:55:28.936279: step 89990, loss = 3.95 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 20:55:33.800900: step 90000, loss = 4.06 (282.0 examples/sec; 0.454 sec/batch)
2016-07-15 20:55:39.559733: step 90010, loss = 3.91 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 20:55:45.911019: step 90020, loss = 3.75 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 20:55:51.362848: step 90030, loss = 3.92 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 20:55:56.077357: step 90040, loss = 4.01 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 20:56:01.216400: step 90050, loss = 3.94 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 20:56:07.742438: step 90060, loss = 3.74 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 20:56:12.756193: step 90070, loss = 3.93 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 20:56:17.512616: step 90080, loss = 4.06 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 20:56:22.279666: step 90090, loss = 4.05 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 20:56:27.084659: step 90100, loss = 3.92 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 20:56:34.637050: step 90110, loss = 3.80 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 20:56:39.934888: step 90120, loss = 4.01 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 20:56:44.619353: step 90130, loss = 4.13 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 20:56:49.956649: step 90140, loss = 3.97 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 20:56:56.410322: step 90150, loss = 3.96 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 20:57:01.248068: step 90160, loss = 3.95 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 20:57:06.006218: step 90170, loss = 3.82 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 20:57:10.761518: step 90180, loss = 3.79 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 20:57:15.589874: step 90190, loss = 4.11 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 20:57:21.965798: step 90200, loss = 3.83 (197.6 examples/sec; 0.648 sec/batch)
2016-07-15 20:57:28.728322: step 90210, loss = 4.12 (238.8 examples/sec; 0.536 sec/batch)
2016-07-15 20:57:34.423450: step 90220, loss = 4.11 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 20:57:39.205050: step 90230, loss = 4.14 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 20:57:44.054319: step 90240, loss = 4.10 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 20:57:50.502143: step 90250, loss = 3.83 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 20:57:55.782685: step 90260, loss = 3.73 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 20:58:01.571486: step 90270, loss = 3.80 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 20:58:06.366437: step 90280, loss = 3.78 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 20:58:11.157718: step 90290, loss = 3.82 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 20:58:17.410789: step 90300, loss = 4.01 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 20:58:24.202221: step 90310, loss = 3.42 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 20:58:28.984555: step 90320, loss = 3.94 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 20:58:33.606322: step 90330, loss = 3.71 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 20:58:38.913506: step 90340, loss = 3.60 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 20:58:44.165801: step 90350, loss = 3.75 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 20:58:48.903030: step 90360, loss = 3.59 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 20:58:53.528012: step 90370, loss = 4.02 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 20:58:58.850231: step 90380, loss = 3.73 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 20:59:04.083069: step 90390, loss = 3.91 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 20:59:08.753603: step 90400, loss = 3.90 (269.6 examples/sec; 0.475 sec/batch)
2016-07-15 20:59:15.527156: step 90410, loss = 4.02 (187.2 examples/sec; 0.684 sec/batch)
2016-07-15 20:59:21.613315: step 90420, loss = 3.84 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 20:59:26.465896: step 90430, loss = 3.84 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 20:59:31.251172: step 90440, loss = 3.67 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 20:59:37.456879: step 90450, loss = 3.77 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 20:59:43.040584: step 90460, loss = 3.82 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 20:59:47.791529: step 90470, loss = 3.76 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 20:59:52.629370: step 90480, loss = 4.05 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 20:59:57.334177: step 90490, loss = 4.07 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 21:00:02.130869: step 90500, loss = 3.77 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 21:00:07.949148: step 90510, loss = 4.00 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 21:00:14.346480: step 90520, loss = 3.75 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 21:00:19.740920: step 90530, loss = 4.07 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 21:00:25.512042: step 90540, loss = 4.11 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 21:00:30.371499: step 90550, loss = 4.07 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 21:00:35.149806: step 90560, loss = 4.13 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 21:00:39.896623: step 90570, loss = 4.04 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 21:00:44.873396: step 90580, loss = 3.89 (231.5 examples/sec; 0.553 sec/batch)
2016-07-15 21:00:51.404868: step 90590, loss = 3.94 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 21:00:56.565786: step 90600, loss = 3.75 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 21:01:02.360335: step 90610, loss = 3.92 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 21:01:08.273216: step 90620, loss = 3.72 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 21:01:14.177665: step 90630, loss = 3.59 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 21:01:18.982523: step 90640, loss = 4.11 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 21:01:23.783649: step 90650, loss = 3.81 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 21:01:30.109261: step 90660, loss = 3.95 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 21:01:35.553201: step 90670, loss = 3.87 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 21:01:40.303662: step 90680, loss = 4.00 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 21:01:45.125448: step 90690, loss = 3.81 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 21:01:49.880261: step 90700, loss = 3.80 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 21:01:56.997579: step 90710, loss = 3.93 (193.6 examples/sec; 0.661 sec/batch)
2016-07-15 21:02:02.715775: step 90720, loss = 4.05 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 21:02:07.431980: step 90730, loss = 3.88 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 21:02:12.330375: step 90740, loss = 4.03 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 21:02:18.779936: step 90750, loss = 3.79 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 21:02:24.107558: step 90760, loss = 4.06 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 21:02:28.835338: step 90770, loss = 3.75 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 21:02:34.182021: step 90780, loss = 3.91 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 21:02:40.633916: step 90790, loss = 3.75 (209.3 examples/sec; 0.612 sec/batch)
2016-07-15 21:02:45.766974: step 90800, loss = 3.82 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 21:02:52.591364: step 90810, loss = 3.96 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 21:02:57.242730: step 90820, loss = 3.90 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 21:03:02.133316: step 90830, loss = 4.06 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 21:03:06.896220: step 90840, loss = 4.13 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 21:03:12.936646: step 90850, loss = 4.06 (191.9 examples/sec; 0.667 sec/batch)
2016-07-15 21:03:18.668732: step 90860, loss = 4.23 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 21:03:23.439799: step 90870, loss = 3.68 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 21:03:28.251847: step 90880, loss = 3.81 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 21:03:32.933015: step 90890, loss = 4.09 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 21:03:37.752504: step 90900, loss = 4.13 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 21:03:43.567160: step 90910, loss = 4.06 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 21:03:49.742225: step 90920, loss = 3.91 (209.0 examples/sec; 0.612 sec/batch)
2016-07-15 21:03:55.309752: step 90930, loss = 3.93 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 21:04:00.067886: step 90940, loss = 4.10 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 21:04:04.921721: step 90950, loss = 3.87 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 21:04:11.504130: step 90960, loss = 3.64 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 21:04:16.717223: step 90970, loss = 3.99 (248.0 examples/sec; 0.516 sec/batch)
2016-07-15 21:04:21.430484: step 90980, loss = 3.83 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 21:04:26.253931: step 90990, loss = 3.77 (268.1 examples/sec; 0.478 sec/batch)
2016-07-15 21:04:31.038176: step 91000, loss = 4.06 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 21:04:38.427309: step 91010, loss = 3.98 (199.0 examples/sec; 0.643 sec/batch)
2016-07-15 21:04:43.887736: step 91020, loss = 3.92 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 21:04:48.684762: step 91030, loss = 4.15 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 21:04:53.834414: step 91040, loss = 3.81 (188.9 examples/sec; 0.677 sec/batch)
2016-07-15 21:05:00.415096: step 91050, loss = 3.99 (198.6 examples/sec; 0.644 sec/batch)
2016-07-15 21:05:05.548135: step 91060, loss = 3.73 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 21:05:11.163082: step 91070, loss = 4.02 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 21:05:16.962404: step 91080, loss = 3.97 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 21:05:21.799864: step 91090, loss = 3.87 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 21:05:26.610788: step 91100, loss = 3.90 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 21:05:33.920028: step 91110, loss = 3.91 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 21:05:39.499319: step 91120, loss = 3.74 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 21:05:44.354150: step 91130, loss = 3.69 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 21:05:49.299902: step 91140, loss = 3.82 (209.2 examples/sec; 0.612 sec/batch)
2016-07-15 21:05:55.837099: step 91150, loss = 4.09 (205.6 examples/sec; 0.622 sec/batch)
2016-07-15 21:06:00.972059: step 91160, loss = 3.93 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 21:06:06.782621: step 91170, loss = 3.97 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 21:06:12.467152: step 91180, loss = 3.89 (206.3 examples/sec; 0.621 sec/batch)
2016-07-15 21:06:17.693005: step 91190, loss = 3.83 (202.7 examples/sec; 0.631 sec/batch)
2016-07-15 21:06:23.293379: step 91200, loss = 3.92 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 21:06:29.104098: step 91210, loss = 4.02 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 21:06:33.911117: step 91220, loss = 3.78 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 21:06:38.653920: step 91230, loss = 3.89 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 21:06:44.549599: step 91240, loss = 4.10 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 21:06:50.414997: step 91250, loss = 4.06 (256.8 examples/sec; 0.499 sec/batch)
2016-07-15 21:06:55.236778: step 91260, loss = 3.90 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 21:07:00.116980: step 91270, loss = 3.72 (245.8 examples/sec; 0.521 sec/batch)
2016-07-15 21:07:06.499102: step 91280, loss = 4.20 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 21:07:11.942256: step 91290, loss = 3.99 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 21:07:16.668053: step 91300, loss = 3.97 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 21:07:23.112867: step 91310, loss = 4.05 (191.9 examples/sec; 0.667 sec/batch)
2016-07-15 21:07:29.421643: step 91320, loss = 3.88 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 21:07:34.210008: step 91330, loss = 3.88 (283.4 examples/sec; 0.452 sec/batch)
2016-07-15 21:07:39.078403: step 91340, loss = 3.91 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 21:07:45.175668: step 91350, loss = 4.18 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 21:07:50.946507: step 91360, loss = 3.86 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 21:07:55.688471: step 91370, loss = 3.92 (282.2 examples/sec; 0.454 sec/batch)
2016-07-15 21:08:00.485268: step 91380, loss = 4.28 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 21:08:05.276827: step 91390, loss = 3.88 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 21:08:10.531033: step 91400, loss = 4.00 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 21:08:18.383179: step 91410, loss = 3.82 (252.8 examples/sec; 0.506 sec/batch)
2016-07-15 21:08:23.878466: step 91420, loss = 3.87 (200.1 examples/sec; 0.640 sec/batch)
2016-07-15 21:08:29.042692: step 91430, loss = 3.79 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 21:08:33.745800: step 91440, loss = 3.81 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 21:08:39.286873: step 91450, loss = 3.87 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 21:08:45.556406: step 91460, loss = 3.92 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 21:08:50.357210: step 91470, loss = 3.96 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 21:08:55.234716: step 91480, loss = 3.96 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 21:09:01.246102: step 91490, loss = 3.82 (193.5 examples/sec; 0.662 sec/batch)
2016-07-15 21:09:06.943538: step 91500, loss = 4.12 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 21:09:12.747738: step 91510, loss = 3.83 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 21:09:17.844725: step 91520, loss = 3.72 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 21:09:24.324170: step 91530, loss = 3.99 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 21:09:29.488731: step 91540, loss = 3.66 (218.3 examples/sec; 0.586 sec/batch)
2016-07-15 21:09:35.217208: step 91550, loss = 3.88 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 21:09:40.029927: step 91560, loss = 3.88 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 21:09:44.845282: step 91570, loss = 3.79 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 21:09:51.347950: step 91580, loss = 3.86 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 21:09:56.648860: step 91590, loss = 3.79 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 21:10:01.370315: step 91600, loss = 3.93 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 21:10:08.057896: step 91610, loss = 3.76 (192.1 examples/sec; 0.666 sec/batch)
2016-07-15 21:10:14.171130: step 91620, loss = 3.72 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 21:10:19.010917: step 91630, loss = 3.73 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 21:10:23.817706: step 91640, loss = 4.07 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 21:10:30.022909: step 91650, loss = 3.99 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 21:10:35.620459: step 91660, loss = 3.86 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 21:10:40.368769: step 91670, loss = 3.76 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 21:10:45.224818: step 91680, loss = 3.87 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 21:10:49.952964: step 91690, loss = 3.85 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 21:10:55.493353: step 91700, loss = 3.98 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 21:11:03.035747: step 91710, loss = 3.88 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 21:11:08.684395: step 91720, loss = 4.20 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 21:11:13.663081: step 91730, loss = 3.98 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 21:11:18.472071: step 91740, loss = 3.92 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 21:11:24.250833: step 91750, loss = 3.89 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 21:11:30.282373: step 91760, loss = 3.87 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 21:11:35.782117: step 91770, loss = 3.92 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 21:11:41.051172: step 91780, loss = 4.03 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 21:11:45.749225: step 91790, loss = 3.93 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 21:11:50.570011: step 91800, loss = 4.34 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 21:11:56.442219: step 91810, loss = 3.79 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 21:12:01.126702: step 91820, loss = 3.84 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 21:12:06.280681: step 91830, loss = 3.94 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 21:12:12.769300: step 91840, loss = 3.92 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 21:12:17.967162: step 91850, loss = 3.82 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 21:12:23.571953: step 91860, loss = 3.75 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 21:12:29.401264: step 91870, loss = 3.70 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 21:12:34.296036: step 91880, loss = 3.64 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 21:12:39.096120: step 91890, loss = 3.74 (248.0 examples/sec; 0.516 sec/batch)
2016-07-15 21:12:45.075700: step 91900, loss = 3.88 (178.2 examples/sec; 0.718 sec/batch)
2016-07-15 21:12:52.057186: step 91910, loss = 3.89 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 21:12:57.800838: step 91920, loss = 3.90 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 21:13:02.701609: step 91930, loss = 3.85 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 21:13:07.490282: step 91940, loss = 3.94 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 21:13:13.510358: step 91950, loss = 3.79 (195.0 examples/sec; 0.656 sec/batch)
2016-07-15 21:13:19.250245: step 91960, loss = 3.84 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 21:13:24.037274: step 91970, loss = 3.79 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 21:13:28.898088: step 91980, loss = 3.95 (261.0 examples/sec; 0.491 sec/batch)
2016-07-15 21:13:35.333354: step 91990, loss = 4.19 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 21:13:40.645478: step 92000, loss = 3.78 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 21:13:46.388385: step 92010, loss = 4.00 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 21:13:51.248391: step 92020, loss = 4.06 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 21:13:56.056263: step 92030, loss = 3.63 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 21:14:02.327809: step 92040, loss = 3.74 (203.4 examples/sec; 0.629 sec/batch)
2016-07-15 21:14:07.906335: step 92050, loss = 3.96 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 21:14:12.645214: step 92060, loss = 4.20 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 21:14:17.566600: step 92070, loss = 3.96 (233.8 examples/sec; 0.547 sec/batch)
2016-07-15 21:14:24.128113: step 92080, loss = 3.91 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 21:14:29.295216: step 92090, loss = 3.97 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 21:14:33.974820: step 92100, loss = 3.96 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 21:14:40.873444: step 92110, loss = 3.95 (189.5 examples/sec; 0.676 sec/batch)
2016-07-15 21:14:46.783445: step 92120, loss = 3.66 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 21:14:52.342827: step 92130, loss = 4.12 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 21:14:57.492753: step 92140, loss = 3.77 (253.5 examples/sec; 0.505 sec/batch)
2016-07-15 21:15:03.256280: step 92150, loss = 3.76 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 21:15:08.941913: step 92160, loss = 3.91 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 21:15:14.133197: step 92170, loss = 4.08 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 21:15:19.752022: step 92180, loss = 3.82 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 21:15:25.701586: step 92190, loss = 3.99 (228.0 examples/sec; 0.561 sec/batch)
2016-07-15 21:15:30.553810: step 92200, loss = 3.78 (283.3 examples/sec; 0.452 sec/batch)
2016-07-15 21:15:36.120874: step 92210, loss = 3.89 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 21:15:40.801927: step 92220, loss = 3.83 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 21:15:46.574918: step 92230, loss = 4.12 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 21:15:52.024782: step 92240, loss = 3.96 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 21:15:57.266873: step 92250, loss = 3.79 (251.3 examples/sec; 0.509 sec/batch)
2016-07-15 21:16:03.057952: step 92260, loss = 3.83 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 21:16:08.675003: step 92270, loss = 3.88 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 21:16:13.842538: step 92280, loss = 3.97 (227.0 examples/sec; 0.564 sec/batch)
2016-07-15 21:16:19.550358: step 92290, loss = 3.92 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 21:16:24.379297: step 92300, loss = 3.87 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 21:16:30.541213: step 92310, loss = 4.01 (187.5 examples/sec; 0.683 sec/batch)
2016-07-15 21:16:37.074300: step 92320, loss = 3.85 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 21:16:42.189217: step 92330, loss = 3.95 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 21:16:47.811942: step 92340, loss = 3.89 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 21:16:53.574242: step 92350, loss = 4.06 (197.7 examples/sec; 0.647 sec/batch)
2016-07-15 21:16:58.476847: step 92360, loss = 3.83 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 21:17:03.219779: step 92370, loss = 3.90 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 21:17:09.087479: step 92380, loss = 3.94 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 21:17:15.001455: step 92390, loss = 3.71 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 21:17:19.853531: step 92400, loss = 3.95 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 21:17:25.751635: step 92410, loss = 3.98 (234.5 examples/sec; 0.546 sec/batch)
2016-07-15 21:17:32.332284: step 92420, loss = 3.84 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 21:17:37.480665: step 92430, loss = 3.60 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 21:17:42.176896: step 92440, loss = 4.25 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 21:17:47.668213: step 92450, loss = 3.78 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 21:17:53.940228: step 92460, loss = 3.94 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 21:17:58.800360: step 92470, loss = 3.96 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 21:18:03.457530: step 92480, loss = 3.73 (281.9 examples/sec; 0.454 sec/batch)
2016-07-15 21:18:08.189970: step 92490, loss = 3.88 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 21:18:12.822789: step 92500, loss = 3.81 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 21:18:18.710803: step 92510, loss = 4.12 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 21:18:24.259761: step 92520, loss = 3.81 (239.4 examples/sec; 0.535 sec/batch)
2016-07-15 21:18:30.020312: step 92530, loss = 3.93 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 21:18:35.336541: step 92540, loss = 4.21 (199.8 examples/sec; 0.640 sec/batch)
2016-07-15 21:18:40.688986: step 92550, loss = 3.93 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 21:18:45.409243: step 92560, loss = 3.99 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 21:18:50.536737: step 92570, loss = 3.69 (189.3 examples/sec; 0.676 sec/batch)
2016-07-15 21:18:57.044932: step 92580, loss = 3.63 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 21:19:02.055282: step 92590, loss = 3.82 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 21:19:06.789396: step 92600, loss = 3.81 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 21:19:13.858745: step 92610, loss = 3.88 (194.1 examples/sec; 0.659 sec/batch)
2016-07-15 21:19:19.566822: step 92620, loss = 3.96 (258.8 examples/sec; 0.494 sec/batch)
2016-07-15 21:19:24.341492: step 92630, loss = 3.82 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 21:19:29.182653: step 92640, loss = 3.88 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 21:19:33.985777: step 92650, loss = 3.75 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 21:19:39.345905: step 92660, loss = 3.97 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 21:19:45.777055: step 92670, loss = 3.87 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 21:19:50.653729: step 92680, loss = 3.66 (278.4 examples/sec; 0.460 sec/batch)
2016-07-15 21:19:55.396846: step 92690, loss = 4.02 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 21:20:00.210181: step 92700, loss = 3.82 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 21:20:05.874557: step 92710, loss = 3.84 (283.0 examples/sec; 0.452 sec/batch)
2016-07-15 21:20:10.863215: step 92720, loss = 3.87 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 21:20:16.360523: step 92730, loss = 3.79 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 21:20:22.116271: step 92740, loss = 3.88 (254.8 examples/sec; 0.502 sec/batch)
2016-07-15 21:20:27.346846: step 92750, loss = 3.70 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 21:20:32.746740: step 92760, loss = 3.85 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 21:20:37.479979: step 92770, loss = 4.04 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 21:20:42.595393: step 92780, loss = 3.79 (190.6 examples/sec; 0.672 sec/batch)
2016-07-15 21:20:49.103193: step 92790, loss = 3.83 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 21:20:54.099891: step 92800, loss = 4.03 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 21:20:59.742091: step 92810, loss = 3.95 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 21:21:04.576683: step 92820, loss = 3.83 (281.4 examples/sec; 0.455 sec/batch)
2016-07-15 21:21:09.465790: step 92830, loss = 3.77 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 21:21:15.978745: step 92840, loss = 3.77 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 21:21:21.309385: step 92850, loss = 3.92 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 21:21:26.073169: step 92860, loss = 4.04 (259.4 examples/sec; 0.494 sec/batch)
2016-07-15 21:21:31.449192: step 92870, loss = 4.10 (184.0 examples/sec; 0.696 sec/batch)
2016-07-15 21:21:37.869825: step 92880, loss = 3.87 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 21:21:42.746247: step 92890, loss = 3.85 (251.0 examples/sec; 0.510 sec/batch)
2016-07-15 21:21:47.722492: step 92900, loss = 3.75 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 21:21:53.616099: step 92910, loss = 3.85 (278.5 examples/sec; 0.460 sec/batch)
2016-07-15 21:21:58.788694: step 92920, loss = 3.95 (187.5 examples/sec; 0.683 sec/batch)
2016-07-15 21:22:05.347042: step 92930, loss = 3.84 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 21:22:10.513654: step 92940, loss = 3.99 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 21:22:16.138555: step 92950, loss = 3.87 (258.3 examples/sec; 0.495 sec/batch)
2016-07-15 21:22:21.920384: step 92960, loss = 4.33 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 21:22:26.780546: step 92970, loss = 3.94 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 21:22:31.629578: step 92980, loss = 4.10 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 21:22:37.495951: step 92990, loss = 4.05 (185.9 examples/sec; 0.688 sec/batch)
2016-07-15 21:22:43.360902: step 93000, loss = 4.04 (270.9 examples/sec; 0.473 sec/batch)
2016-07-15 21:22:49.179120: step 93010, loss = 3.91 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 21:22:54.115126: step 93020, loss = 3.92 (235.8 examples/sec; 0.543 sec/batch)
2016-07-15 21:23:00.704494: step 93030, loss = 3.75 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 21:23:05.825966: step 93040, loss = 3.71 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 21:23:10.579523: step 93050, loss = 3.73 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 21:23:16.077295: step 93060, loss = 3.77 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 21:23:22.357336: step 93070, loss = 3.71 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 21:23:27.651945: step 93080, loss = 3.93 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 21:23:33.054079: step 93090, loss = 3.75 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 21:23:38.798207: step 93100, loss = 3.73 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 21:23:44.575067: step 93110, loss = 3.72 (253.2 examples/sec; 0.506 sec/batch)
2016-07-15 21:23:49.366770: step 93120, loss = 3.83 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 21:23:54.104939: step 93130, loss = 3.70 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 21:23:58.978217: step 93140, loss = 3.92 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 21:24:03.752165: step 93150, loss = 4.11 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 21:24:09.678983: step 93160, loss = 3.62 (186.7 examples/sec; 0.685 sec/batch)
2016-07-15 21:24:15.544039: step 93170, loss = 4.05 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 21:24:20.280535: step 93180, loss = 3.72 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 21:24:25.097123: step 93190, loss = 3.84 (267.8 examples/sec; 0.478 sec/batch)
2016-07-15 21:24:31.479311: step 93200, loss = 3.81 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 21:24:38.358854: step 93210, loss = 4.01 (216.9 examples/sec; 0.590 sec/batch)
2016-07-15 21:24:44.068762: step 93220, loss = 4.10 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 21:24:49.760836: step 93230, loss = 3.87 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 21:24:54.826899: step 93240, loss = 3.86 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 21:25:00.356852: step 93250, loss = 3.80 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 21:25:05.114695: step 93260, loss = 3.80 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 21:25:10.054235: step 93270, loss = 3.80 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 21:25:16.600656: step 93280, loss = 3.86 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 21:25:21.731604: step 93290, loss = 3.82 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 21:25:27.494477: step 93300, loss = 3.76 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 21:25:33.244149: step 93310, loss = 3.66 (276.5 examples/sec; 0.463 sec/batch)
2016-07-15 21:25:38.100287: step 93320, loss = 3.97 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 21:25:44.681066: step 93330, loss = 3.87 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 21:25:49.851917: step 93340, loss = 3.78 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 21:25:54.535058: step 93350, loss = 3.85 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 21:25:59.287727: step 93360, loss = 3.83 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 21:26:04.012420: step 93370, loss = 4.09 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 21:26:08.750396: step 93380, loss = 3.84 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 21:26:13.572201: step 93390, loss = 3.80 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 21:26:18.261094: step 93400, loss = 3.97 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 21:26:25.049128: step 93410, loss = 3.68 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 21:26:31.107106: step 93420, loss = 3.84 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 21:26:35.935606: step 93430, loss = 3.76 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 21:26:40.751613: step 93440, loss = 3.75 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 21:26:45.483678: step 93450, loss = 3.82 (281.2 examples/sec; 0.455 sec/batch)
2016-07-15 21:26:50.554421: step 93460, loss = 3.82 (194.5 examples/sec; 0.658 sec/batch)
2016-07-15 21:26:57.322660: step 93470, loss = 4.12 (196.2 examples/sec; 0.652 sec/batch)
2016-07-15 21:27:02.961408: step 93480, loss = 3.93 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 21:27:07.709739: step 93490, loss = 4.00 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 21:27:13.591926: step 93500, loss = 4.01 (187.6 examples/sec; 0.682 sec/batch)
2016-07-15 21:27:20.806455: step 93510, loss = 3.83 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 21:27:26.573308: step 93520, loss = 3.79 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 21:27:31.801971: step 93530, loss = 3.92 (208.2 examples/sec; 0.615 sec/batch)
2016-07-15 21:27:37.202135: step 93540, loss = 3.95 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 21:27:41.985918: step 93550, loss = 3.67 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 21:27:47.226054: step 93560, loss = 3.97 (190.9 examples/sec; 0.671 sec/batch)
2016-07-15 21:27:53.712030: step 93570, loss = 3.90 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 21:27:58.847446: step 93580, loss = 3.83 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 21:28:04.437732: step 93590, loss = 3.93 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 21:28:10.242037: step 93600, loss = 4.07 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 21:28:16.757585: step 93610, loss = 3.80 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 21:28:22.044965: step 93620, loss = 4.03 (240.6 examples/sec; 0.532 sec/batch)
2016-07-15 21:28:26.761123: step 93630, loss = 3.89 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 21:28:32.254098: step 93640, loss = 3.63 (189.8 examples/sec; 0.675 sec/batch)
2016-07-15 21:28:38.576237: step 93650, loss = 3.87 (254.7 examples/sec; 0.502 sec/batch)
2016-07-15 21:28:43.887621: step 93660, loss = 3.86 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 21:28:49.315445: step 93670, loss = 3.84 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 21:28:55.043744: step 93680, loss = 3.77 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 21:29:00.452412: step 93690, loss = 3.88 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 21:29:05.747238: step 93700, loss = 3.56 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 21:29:12.691050: step 93710, loss = 3.76 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 21:29:17.441268: step 93720, loss = 3.96 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 21:29:22.369957: step 93730, loss = 3.78 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 21:29:27.106503: step 93740, loss = 3.69 (251.9 examples/sec; 0.508 sec/batch)
2016-07-15 21:29:32.725492: step 93750, loss = 3.82 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 21:29:38.935832: step 93760, loss = 3.76 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 21:29:43.766734: step 93770, loss = 3.86 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 21:29:48.576003: step 93780, loss = 3.80 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 21:29:54.762199: step 93790, loss = 3.83 (201.2 examples/sec; 0.636 sec/batch)
2016-07-15 21:30:00.552123: step 93800, loss = 3.75 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 21:30:06.285497: step 93810, loss = 3.78 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 21:30:11.133137: step 93820, loss = 3.86 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 21:30:15.895314: step 93830, loss = 3.76 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 21:30:21.740051: step 93840, loss = 3.72 (185.3 examples/sec; 0.691 sec/batch)
2016-07-15 21:30:27.659393: step 93850, loss = 4.04 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 21:30:33.174595: step 93860, loss = 3.82 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 21:30:38.402244: step 93870, loss = 4.03 (251.2 examples/sec; 0.510 sec/batch)
2016-07-15 21:30:44.143648: step 93880, loss = 3.85 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 21:30:49.862040: step 93890, loss = 3.74 (209.0 examples/sec; 0.613 sec/batch)
2016-07-15 21:30:55.026651: step 93900, loss = 3.72 (209.8 examples/sec; 0.610 sec/batch)
2016-07-15 21:31:01.921223: step 93910, loss = 3.62 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 21:31:06.639385: step 93920, loss = 3.73 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 21:31:12.033726: step 93930, loss = 4.02 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 21:31:18.507240: step 93940, loss = 3.81 (209.0 examples/sec; 0.612 sec/batch)
2016-07-15 21:31:23.740813: step 93950, loss = 3.77 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 21:31:29.270018: step 93960, loss = 3.96 (246.6 examples/sec; 0.519 sec/batch)
2016-07-15 21:31:34.007068: step 93970, loss = 3.87 (272.5 examples/sec; 0.470 sec/batch)
2016-07-15 21:31:39.070202: step 93980, loss = 3.80 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 21:31:45.635911: step 93990, loss = 3.61 (197.0 examples/sec; 0.650 sec/batch)
2016-07-15 21:31:50.673263: step 94000, loss = 3.82 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 21:31:56.488486: step 94010, loss = 3.88 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 21:32:02.563614: step 94020, loss = 3.99 (192.1 examples/sec; 0.666 sec/batch)
2016-07-15 21:32:08.324621: step 94030, loss = 3.98 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 21:32:13.099660: step 94040, loss = 3.77 (281.7 examples/sec; 0.454 sec/batch)
2016-07-15 21:32:17.947589: step 94050, loss = 3.83 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 21:32:24.385587: step 94060, loss = 3.74 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 21:32:29.704527: step 94070, loss = 3.92 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 21:32:35.506425: step 94080, loss = 3.60 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 21:32:40.919173: step 94090, loss = 3.86 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 21:32:46.176614: step 94100, loss = 3.77 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 21:32:53.128174: step 94110, loss = 3.77 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 21:32:57.820630: step 94120, loss = 4.16 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 21:33:02.814388: step 94130, loss = 3.82 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 21:33:09.331234: step 94140, loss = 4.23 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 21:33:14.535538: step 94150, loss = 3.91 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 21:33:20.248785: step 94160, loss = 4.13 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 21:33:25.904514: step 94170, loss = 4.01 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 21:33:30.892609: step 94180, loss = 3.74 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 21:33:35.656896: step 94190, loss = 3.67 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 21:33:41.349450: step 94200, loss = 3.54 (193.3 examples/sec; 0.662 sec/batch)
2016-07-15 21:33:48.776530: step 94210, loss = 3.76 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 21:33:54.597697: step 94220, loss = 3.66 (208.7 examples/sec; 0.613 sec/batch)
2016-07-15 21:33:59.847947: step 94230, loss = 3.55 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 21:34:05.334431: step 94240, loss = 3.84 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 21:34:10.117535: step 94250, loss = 3.71 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 21:34:14.997743: step 94260, loss = 3.69 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 21:34:19.750835: step 94270, loss = 3.87 (249.1 examples/sec; 0.514 sec/batch)
2016-07-15 21:34:25.508293: step 94280, loss = 3.74 (184.8 examples/sec; 0.693 sec/batch)
2016-07-15 21:34:31.592430: step 94290, loss = 3.65 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 21:34:36.449941: step 94300, loss = 3.70 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 21:34:42.267000: step 94310, loss = 4.04 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 21:34:48.775154: step 94320, loss = 3.75 (200.1 examples/sec; 0.640 sec/batch)
2016-07-15 21:34:54.127477: step 94330, loss = 3.96 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 21:34:59.874430: step 94340, loss = 3.86 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 21:35:05.464659: step 94350, loss = 3.83 (200.8 examples/sec; 0.638 sec/batch)
2016-07-15 21:35:10.604344: step 94360, loss = 3.65 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 21:35:16.412907: step 94370, loss = 4.01 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 21:35:22.082835: step 94380, loss = 3.82 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 21:35:27.331540: step 94390, loss = 3.66 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 21:35:32.939112: step 94400, loss = 3.86 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 21:35:38.711684: step 94410, loss = 3.72 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 21:35:44.045214: step 94420, loss = 3.71 (186.5 examples/sec; 0.686 sec/batch)
2016-07-15 21:35:50.540501: step 94430, loss = 3.79 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 21:35:55.411135: step 94440, loss = 3.76 (279.2 examples/sec; 0.459 sec/batch)
2016-07-15 21:36:00.171503: step 94450, loss = 3.77 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 21:36:04.955110: step 94460, loss = 3.77 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 21:36:09.723647: step 94470, loss = 3.74 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 21:36:16.157524: step 94480, loss = 3.72 (202.4 examples/sec; 0.633 sec/batch)
2016-07-15 21:36:21.599733: step 94490, loss = 4.06 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 21:36:26.366825: step 94500, loss = 3.80 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 21:36:32.939641: step 94510, loss = 3.77 (186.7 examples/sec; 0.686 sec/batch)
2016-07-15 21:36:39.190118: step 94520, loss = 3.81 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 21:36:44.069978: step 94530, loss = 4.02 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 21:36:48.901607: step 94540, loss = 3.81 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 21:36:54.981398: step 94550, loss = 3.87 (197.8 examples/sec; 0.647 sec/batch)
2016-07-15 21:37:00.686436: step 94560, loss = 4.02 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 21:37:05.457737: step 94570, loss = 3.83 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 21:37:10.311573: step 94580, loss = 4.06 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 21:37:15.031110: step 94590, loss = 3.93 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 21:37:20.394453: step 94600, loss = 3.89 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 21:37:28.113913: step 94610, loss = 3.87 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 21:37:33.004574: step 94620, loss = 3.64 (253.2 examples/sec; 0.506 sec/batch)
2016-07-15 21:37:37.886316: step 94630, loss = 3.89 (244.8 examples/sec; 0.523 sec/batch)
2016-07-15 21:37:44.267721: step 94640, loss = 3.84 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 21:37:49.722868: step 94650, loss = 3.99 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 21:37:54.479175: step 94660, loss = 3.99 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 21:37:59.656011: step 94670, loss = 3.97 (184.9 examples/sec; 0.692 sec/batch)
2016-07-15 21:38:06.203563: step 94680, loss = 3.86 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 21:38:11.235101: step 94690, loss = 3.97 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 21:38:15.987861: step 94700, loss = 3.79 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 21:38:23.048893: step 94710, loss = 3.80 (199.3 examples/sec; 0.642 sec/batch)
2016-07-15 21:38:28.763684: step 94720, loss = 3.91 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 21:38:33.581028: step 94730, loss = 3.92 (250.9 examples/sec; 0.510 sec/batch)
2016-07-15 21:38:38.432917: step 94740, loss = 3.70 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 21:38:43.157953: step 94750, loss = 3.83 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 21:38:48.573048: step 94760, loss = 3.96 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 21:38:54.946488: step 94770, loss = 3.89 (218.2 examples/sec; 0.587 sec/batch)
2016-07-15 21:39:00.187233: step 94780, loss = 3.73 (201.5 examples/sec; 0.635 sec/batch)
2016-07-15 21:39:05.709409: step 94790, loss = 4.04 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 21:39:10.432200: step 94800, loss = 3.63 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 21:39:16.964606: step 94810, loss = 3.76 (189.2 examples/sec; 0.676 sec/batch)
2016-07-15 21:39:23.230652: step 94820, loss = 3.86 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 21:39:28.091164: step 94830, loss = 3.88 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 21:39:32.895287: step 94840, loss = 3.73 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 21:39:38.970744: step 94850, loss = 3.88 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 21:39:44.747656: step 94860, loss = 3.61 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 21:39:49.556985: step 94870, loss = 3.74 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 21:39:54.411508: step 94880, loss = 3.82 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 21:40:00.906007: step 94890, loss = 3.76 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 21:40:06.208932: step 94900, loss = 3.65 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 21:40:12.983712: step 94910, loss = 3.82 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 21:40:18.710949: step 94920, loss = 3.69 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 21:40:23.623850: step 94930, loss = 3.77 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 21:40:28.362414: step 94940, loss = 3.92 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 21:40:33.170632: step 94950, loss = 3.86 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 21:40:37.999539: step 94960, loss = 3.58 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 21:40:44.362302: step 94970, loss = 3.94 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 21:40:49.834308: step 94980, loss = 4.07 (258.1 examples/sec; 0.496 sec/batch)
2016-07-15 21:40:54.551461: step 94990, loss = 3.85 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 21:40:59.671514: step 95000, loss = 3.69 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 21:41:07.660069: step 95010, loss = 3.98 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 21:41:13.029518: step 95020, loss = 3.80 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 21:41:18.389108: step 95030, loss = 3.89 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 21:41:23.116927: step 95040, loss = 4.02 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 21:41:28.496272: step 95050, loss = 3.64 (186.0 examples/sec; 0.688 sec/batch)
2016-07-15 21:41:34.951113: step 95060, loss = 3.86 (208.3 examples/sec; 0.614 sec/batch)
2016-07-15 21:41:40.176248: step 95070, loss = 3.61 (201.4 examples/sec; 0.636 sec/batch)
2016-07-15 21:41:45.706408: step 95080, loss = 3.76 (243.7 examples/sec; 0.525 sec/batch)
2016-07-15 21:41:50.493019: step 95090, loss = 3.82 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 21:41:55.347703: step 95100, loss = 3.60 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 21:42:01.104546: step 95110, loss = 3.76 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 21:42:07.175701: step 95120, loss = 3.92 (194.9 examples/sec; 0.657 sec/batch)
2016-07-15 21:42:12.889114: step 95130, loss = 3.79 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 21:42:17.710401: step 95140, loss = 3.67 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 21:42:22.556001: step 95150, loss = 3.64 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 21:42:29.038648: step 95160, loss = 3.72 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 21:42:34.405617: step 95170, loss = 3.80 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 21:42:39.092510: step 95180, loss = 3.90 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 21:42:44.425497: step 95190, loss = 3.90 (182.4 examples/sec; 0.702 sec/batch)
2016-07-15 21:42:50.894176: step 95200, loss = 3.64 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 21:42:56.731484: step 95210, loss = 3.87 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 21:43:01.522520: step 95220, loss = 3.72 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 21:43:06.254224: step 95230, loss = 3.75 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 21:43:11.120949: step 95240, loss = 3.63 (239.0 examples/sec; 0.536 sec/batch)
2016-07-15 21:43:17.676491: step 95250, loss = 3.89 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 21:43:22.894108: step 95260, loss = 3.70 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 21:43:27.586882: step 95270, loss = 4.06 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 21:43:32.391711: step 95280, loss = 3.75 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 21:43:37.176306: step 95290, loss = 3.67 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 21:43:42.013209: step 95300, loss = 3.88 (248.9 examples/sec; 0.514 sec/batch)
2016-07-15 21:43:47.882426: step 95310, loss = 3.90 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 21:43:52.673498: step 95320, loss = 4.14 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 21:43:58.620990: step 95330, loss = 3.86 (190.6 examples/sec; 0.672 sec/batch)
2016-07-15 21:44:04.547442: step 95340, loss = 3.62 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 21:44:09.361285: step 95350, loss = 3.50 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 21:44:14.187579: step 95360, loss = 3.50 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 21:44:18.928936: step 95370, loss = 3.73 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 21:44:23.828645: step 95380, loss = 3.91 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 21:44:28.563831: step 95390, loss = 3.86 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 21:44:33.339593: step 95400, loss = 3.81 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 21:44:39.187690: step 95410, loss = 3.73 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 21:44:43.912541: step 95420, loss = 3.72 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 21:44:49.529458: step 95430, loss = 3.80 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 21:44:55.717292: step 95440, loss = 3.88 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 21:45:00.577982: step 95450, loss = 3.63 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 21:45:05.345249: step 95460, loss = 3.70 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 21:45:11.439770: step 95470, loss = 3.95 (199.6 examples/sec; 0.641 sec/batch)
2016-07-15 21:45:17.204574: step 95480, loss = 3.58 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 21:45:22.020763: step 95490, loss = 3.79 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 21:45:26.886687: step 95500, loss = 3.75 (254.7 examples/sec; 0.502 sec/batch)
2016-07-15 21:45:35.052350: step 95510, loss = 3.81 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 21:45:39.850846: step 95520, loss = 3.67 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 21:45:44.659956: step 95530, loss = 3.96 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 21:45:49.435914: step 95540, loss = 3.67 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 21:45:54.284788: step 95550, loss = 3.66 (250.0 examples/sec; 0.512 sec/batch)
2016-07-15 21:46:00.640168: step 95560, loss = 3.85 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 21:46:06.091028: step 95570, loss = 3.91 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 21:46:10.809329: step 95580, loss = 3.72 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 21:46:15.938738: step 95590, loss = 3.68 (183.6 examples/sec; 0.697 sec/batch)
2016-07-15 21:46:22.493036: step 95600, loss = 3.80 (195.2 examples/sec; 0.656 sec/batch)
2016-07-15 21:46:28.585665: step 95610, loss = 3.80 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 21:46:33.366507: step 95620, loss = 3.78 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 21:46:38.112254: step 95630, loss = 3.72 (276.2 examples/sec; 0.464 sec/batch)
2016-07-15 21:46:42.936230: step 95640, loss = 3.67 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 21:46:49.456730: step 95650, loss = 4.09 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 21:46:54.741768: step 95660, loss = 3.67 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 21:46:59.482866: step 95670, loss = 3.96 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 21:47:04.912969: step 95680, loss = 3.69 (194.1 examples/sec; 0.659 sec/batch)
2016-07-15 21:47:11.344503: step 95690, loss = 4.01 (220.0 examples/sec; 0.582 sec/batch)
2016-07-15 21:47:16.614000: step 95700, loss = 3.74 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 21:47:23.347706: step 95710, loss = 3.81 (246.5 examples/sec; 0.519 sec/batch)
2016-07-15 21:47:29.077961: step 95720, loss = 4.09 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 21:47:33.833208: step 95730, loss = 3.80 (278.6 examples/sec; 0.460 sec/batch)
2016-07-15 21:47:38.672258: step 95740, loss = 3.90 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 21:47:43.434249: step 95750, loss = 4.00 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 21:47:48.305128: step 95760, loss = 3.85 (271.2 examples/sec; 0.472 sec/batch)
2016-07-15 21:47:53.094450: step 95770, loss = 4.06 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 21:47:57.864751: step 95780, loss = 3.73 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 21:48:02.582094: step 95790, loss = 4.19 (284.9 examples/sec; 0.449 sec/batch)
2016-07-15 21:48:07.262817: step 95800, loss = 3.99 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 21:48:14.299163: step 95810, loss = 3.98 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 21:48:20.066801: step 95820, loss = 3.91 (249.3 examples/sec; 0.513 sec/batch)
2016-07-15 21:48:25.410295: step 95830, loss = 3.81 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 21:48:30.694471: step 95840, loss = 3.82 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 21:48:35.533376: step 95850, loss = 3.91 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 21:48:40.360889: step 95860, loss = 3.83 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 21:48:45.039267: step 95870, loss = 3.52 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 21:48:49.733089: step 95880, loss = 3.62 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 21:48:55.502921: step 95890, loss = 3.60 (221.7 examples/sec; 0.577 sec/batch)
2016-07-15 21:49:00.676517: step 95900, loss = 3.76 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 21:49:07.392467: step 95910, loss = 3.79 (246.1 examples/sec; 0.520 sec/batch)
2016-07-15 21:49:13.124572: step 95920, loss = 3.93 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 21:49:18.774270: step 95930, loss = 3.83 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 21:49:23.784463: step 95940, loss = 3.80 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 21:49:28.557827: step 95950, loss = 3.88 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 21:49:34.308054: step 95960, loss = 3.67 (187.6 examples/sec; 0.682 sec/batch)
2016-07-15 21:49:40.406369: step 95970, loss = 3.65 (248.5 examples/sec; 0.515 sec/batch)
2016-07-15 21:49:45.891836: step 95980, loss = 3.51 (200.5 examples/sec; 0.638 sec/batch)
2016-07-15 21:49:51.127392: step 95990, loss = 3.79 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 21:49:55.908596: step 96000, loss = 3.63 (249.1 examples/sec; 0.514 sec/batch)
2016-07-15 21:50:02.872754: step 96010, loss = 3.96 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 21:50:08.830204: step 96020, loss = 3.66 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 21:50:13.623450: step 96030, loss = 3.75 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 21:50:18.519088: step 96040, loss = 3.74 (241.1 examples/sec; 0.531 sec/batch)
2016-07-15 21:50:23.345251: step 96050, loss = 3.63 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 21:50:27.980184: step 96060, loss = 3.63 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 21:50:33.115563: step 96070, loss = 3.63 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 21:50:38.534918: step 96080, loss = 3.75 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 21:50:43.283541: step 96090, loss = 3.87 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 21:50:47.878728: step 96100, loss = 3.44 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 21:50:54.320855: step 96110, loss = 3.75 (203.7 examples/sec; 0.629 sec/batch)
2016-07-15 21:50:59.440018: step 96120, loss = 3.58 (260.4 examples/sec; 0.491 sec/batch)
2016-07-15 21:51:05.192192: step 96130, loss = 3.95 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 21:51:10.880061: step 96140, loss = 4.08 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 21:51:16.051714: step 96150, loss = 3.76 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 21:51:21.705813: step 96160, loss = 3.68 (244.0 examples/sec; 0.525 sec/batch)
2016-07-15 21:51:27.493424: step 96170, loss = 4.10 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 21:51:32.357529: step 96180, loss = 3.63 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 21:51:37.149321: step 96190, loss = 3.69 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 21:51:43.057928: step 96200, loss = 3.97 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 21:51:50.155676: step 96210, loss = 3.68 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 21:51:55.938578: step 96220, loss = 3.88 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 21:52:00.795073: step 96230, loss = 3.65 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 21:52:05.593304: step 96240, loss = 3.67 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 21:52:11.673725: step 96250, loss = 3.87 (194.9 examples/sec; 0.657 sec/batch)
2016-07-15 21:52:17.408574: step 96260, loss = 3.75 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 21:52:22.186158: step 96270, loss = 3.91 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 21:52:27.024204: step 96280, loss = 3.99 (269.8 examples/sec; 0.475 sec/batch)
2016-07-15 21:52:33.465966: step 96290, loss = 3.98 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 21:52:38.817161: step 96300, loss = 4.02 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 21:52:44.545297: step 96310, loss = 3.61 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 21:52:49.356239: step 96320, loss = 3.83 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 21:52:54.163289: step 96330, loss = 3.91 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 21:53:00.433278: step 96340, loss = 3.92 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 21:53:06.010640: step 96350, loss = 3.85 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 21:53:10.773188: step 96360, loss = 3.66 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 21:53:15.647140: step 96370, loss = 3.72 (243.3 examples/sec; 0.526 sec/batch)
2016-07-15 21:53:22.206692: step 96380, loss = 3.72 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 21:53:27.386706: step 96390, loss = 3.74 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 21:53:32.070501: step 96400, loss = 4.00 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 21:53:39.052732: step 96410, loss = 3.89 (188.5 examples/sec; 0.679 sec/batch)
2016-07-15 21:53:44.923807: step 96420, loss = 3.73 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 21:53:49.729889: step 96430, loss = 3.95 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 21:53:54.574803: step 96440, loss = 3.90 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 21:54:00.913455: step 96450, loss = 3.64 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 21:54:06.326407: step 96460, loss = 3.91 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 21:54:12.137853: step 96470, loss = 3.50 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 21:54:17.058538: step 96480, loss = 3.65 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 21:54:21.838409: step 96490, loss = 3.95 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 21:54:27.990058: step 96500, loss = 3.97 (198.1 examples/sec; 0.646 sec/batch)
2016-07-15 21:54:34.867025: step 96510, loss = 3.67 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 21:54:39.583119: step 96520, loss = 3.43 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 21:54:44.254154: step 96530, loss = 3.65 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 21:54:49.511593: step 96540, loss = 3.81 (202.3 examples/sec; 0.633 sec/batch)
2016-07-15 21:54:54.909161: step 96550, loss = 3.79 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 21:55:00.967213: step 96560, loss = 3.81 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 21:55:05.782948: step 96570, loss = 3.79 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 21:55:10.620478: step 96580, loss = 3.72 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 21:55:15.317429: step 96590, loss = 4.00 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 21:55:20.414187: step 96600, loss = 3.81 (187.8 examples/sec; 0.681 sec/batch)
2016-07-15 21:55:28.397789: step 96610, loss = 3.52 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 21:55:33.835776: step 96620, loss = 3.57 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 21:55:39.093103: step 96630, loss = 3.99 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 21:55:43.895933: step 96640, loss = 3.45 (271.5 examples/sec; 0.471 sec/batch)
2016-07-15 21:55:49.357513: step 96650, loss = 3.81 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 21:55:55.775658: step 96660, loss = 3.56 (209.0 examples/sec; 0.612 sec/batch)
2016-07-15 21:56:00.608044: step 96670, loss = 3.69 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 21:56:05.399861: step 96680, loss = 3.82 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 21:56:11.348076: step 96690, loss = 4.13 (182.1 examples/sec; 0.703 sec/batch)
2016-07-15 21:56:17.251232: step 96700, loss = 3.63 (248.0 examples/sec; 0.516 sec/batch)
2016-07-15 21:56:24.141269: step 96710, loss = 3.59 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 21:56:28.994433: step 96720, loss = 3.89 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 21:56:33.770004: step 96730, loss = 4.04 (270.5 examples/sec; 0.473 sec/batch)
2016-07-15 21:56:38.527240: step 96740, loss = 3.62 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 21:56:43.376811: step 96750, loss = 3.75 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 21:56:49.866891: step 96760, loss = 3.73 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 21:56:55.180094: step 96770, loss = 3.49 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 21:56:59.850849: step 96780, loss = 3.61 (273.2 examples/sec; 0.468 sec/batch)
2016-07-15 21:57:05.155402: step 96790, loss = 3.71 (187.8 examples/sec; 0.681 sec/batch)
2016-07-15 21:57:11.630489: step 96800, loss = 3.67 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 21:57:17.478181: step 96810, loss = 4.04 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 21:57:22.293535: step 96820, loss = 3.83 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 21:57:27.010838: step 96830, loss = 3.89 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 21:57:31.876713: step 96840, loss = 3.66 (244.2 examples/sec; 0.524 sec/batch)
2016-07-15 21:57:38.422879: step 96850, loss = 3.97 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 21:57:43.582538: step 96860, loss = 3.78 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 21:57:48.267812: step 96870, loss = 3.64 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 21:57:53.085240: step 96880, loss = 3.79 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 21:57:57.813016: step 96890, loss = 3.89 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 21:58:02.570219: step 96900, loss = 3.69 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 21:58:08.686344: step 96910, loss = 4.07 (189.8 examples/sec; 0.674 sec/batch)
2016-07-15 21:58:15.171215: step 96920, loss = 3.93 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 21:58:20.152554: step 96930, loss = 3.90 (282.8 examples/sec; 0.453 sec/batch)
2016-07-15 21:58:24.898397: step 96940, loss = 3.74 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 21:58:29.676435: step 96950, loss = 4.04 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 21:58:34.498088: step 96960, loss = 4.13 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 21:58:40.751716: step 96970, loss = 4.04 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 21:58:46.354758: step 96980, loss = 3.88 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 21:58:51.128287: step 96990, loss = 3.80 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 21:58:56.046465: step 97000, loss = 3.71 (235.4 examples/sec; 0.544 sec/batch)
2016-07-15 21:59:04.168203: step 97010, loss = 3.88 (250.3 examples/sec; 0.511 sec/batch)
2016-07-15 21:59:09.054554: step 97020, loss = 3.82 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 21:59:13.818046: step 97030, loss = 3.57 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 21:59:18.588758: step 97040, loss = 3.56 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 21:59:23.401247: step 97050, loss = 3.70 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 21:59:28.099153: step 97060, loss = 3.78 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 21:59:33.499643: step 97070, loss = 3.88 (181.7 examples/sec; 0.704 sec/batch)
2016-07-15 21:59:39.974109: step 97080, loss = 3.93 (199.2 examples/sec; 0.642 sec/batch)
2016-07-15 21:59:44.829967: step 97090, loss = 3.89 (278.9 examples/sec; 0.459 sec/batch)
2016-07-15 21:59:49.608649: step 97100, loss = 3.61 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 21:59:56.812241: step 97110, loss = 3.82 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 22:00:02.343883: step 97120, loss = 3.52 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 22:00:07.157762: step 97130, loss = 3.56 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 22:00:12.035079: step 97140, loss = 3.54 (243.8 examples/sec; 0.525 sec/batch)
2016-07-15 22:00:18.646765: step 97150, loss = 3.68 (202.7 examples/sec; 0.632 sec/batch)
2016-07-15 22:00:23.810492: step 97160, loss = 3.60 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 22:00:28.557050: step 97170, loss = 3.99 (252.9 examples/sec; 0.506 sec/batch)
2016-07-15 22:00:34.004967: step 97180, loss = 3.35 (189.6 examples/sec; 0.675 sec/batch)
2016-07-15 22:00:40.274793: step 97190, loss = 3.46 (243.4 examples/sec; 0.526 sec/batch)
2016-07-15 22:00:45.564560: step 97200, loss = 3.53 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 22:00:52.282204: step 97210, loss = 3.78 (223.1 examples/sec; 0.574 sec/batch)
2016-07-15 22:00:58.023946: step 97220, loss = 3.56 (254.7 examples/sec; 0.503 sec/batch)
2016-07-15 22:01:02.814561: step 97230, loss = 3.70 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 22:01:07.683758: step 97240, loss = 3.73 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 22:01:14.178944: step 97250, loss = 3.71 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 22:01:19.462743: step 97260, loss = 3.48 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 22:01:24.191048: step 97270, loss = 3.88 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 22:01:29.822756: step 97280, loss = 3.76 (183.1 examples/sec; 0.699 sec/batch)
2016-07-15 22:01:36.806028: step 97290, loss = 3.70 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 22:01:42.233439: step 97300, loss = 3.96 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 22:01:48.860024: step 97310, loss = 3.83 (284.2 examples/sec; 0.450 sec/batch)
2016-07-15 22:01:53.636704: step 97320, loss = 3.71 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 22:01:59.527987: step 97330, loss = 4.16 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 22:02:05.434916: step 97340, loss = 3.53 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 22:02:10.238133: step 97350, loss = 3.79 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 22:02:15.015793: step 97360, loss = 3.96 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 22:02:19.691319: step 97370, loss = 3.70 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 22:02:24.733002: step 97380, loss = 3.79 (192.4 examples/sec; 0.665 sec/batch)
2016-07-15 22:02:31.226477: step 97390, loss = 3.55 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 22:02:36.350540: step 97400, loss = 3.98 (229.6 examples/sec; 0.557 sec/batch)
2016-07-15 22:02:43.348652: step 97410, loss = 3.76 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 22:02:48.125898: step 97420, loss = 3.75 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 22:02:52.984003: step 97430, loss = 3.90 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 22:02:57.841522: step 97440, loss = 4.01 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 22:03:03.816047: step 97450, loss = 3.91 (192.1 examples/sec; 0.666 sec/batch)
2016-07-15 22:03:09.626921: step 97460, loss = 3.98 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 22:03:14.414501: step 97470, loss = 3.69 (272.6 examples/sec; 0.470 sec/batch)
2016-07-15 22:03:19.223103: step 97480, loss = 3.41 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 22:03:25.545662: step 97490, loss = 3.79 (208.4 examples/sec; 0.614 sec/batch)
2016-07-15 22:03:31.029176: step 97500, loss = 3.68 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 22:03:36.733657: step 97510, loss = 3.79 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 22:03:42.201400: step 97520, loss = 3.57 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 22:03:48.507310: step 97530, loss = 3.77 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 22:03:53.348292: step 97540, loss = 3.72 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 22:03:58.097965: step 97550, loss = 3.72 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 22:04:02.841781: step 97560, loss = 3.92 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 22:04:07.685024: step 97570, loss = 3.75 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 22:04:14.125526: step 97580, loss = 3.72 (199.7 examples/sec; 0.641 sec/batch)
2016-07-15 22:04:19.435034: step 97590, loss = 3.64 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 22:04:25.201759: step 97600, loss = 3.68 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 22:04:31.964040: step 97610, loss = 3.66 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 22:04:37.069026: step 97620, loss = 3.87 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 22:04:42.716024: step 97630, loss = 3.61 (246.2 examples/sec; 0.520 sec/batch)
2016-07-15 22:04:48.463253: step 97640, loss = 3.75 (234.3 examples/sec; 0.546 sec/batch)
2016-07-15 22:04:53.749623: step 97650, loss = 4.12 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 22:04:59.183233: step 97660, loss = 3.69 (255.1 examples/sec; 0.502 sec/batch)
2016-07-15 22:05:03.908975: step 97670, loss = 3.53 (280.7 examples/sec; 0.456 sec/batch)
2016-07-15 22:05:08.801073: step 97680, loss = 3.66 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 22:05:13.507111: step 97690, loss = 3.82 (271.1 examples/sec; 0.472 sec/batch)
2016-07-15 22:05:18.180422: step 97700, loss = 3.70 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 22:05:24.890116: step 97710, loss = 3.74 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 22:05:30.253609: step 97720, loss = 3.86 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 22:05:35.556497: step 97730, loss = 3.84 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 22:05:40.239329: step 97740, loss = 3.67 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 22:05:45.586011: step 97750, loss = 3.98 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 22:05:52.028096: step 97760, loss = 3.76 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 22:05:57.087709: step 97770, loss = 3.78 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 22:06:02.665314: step 97780, loss = 3.55 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 22:06:07.459812: step 97790, loss = 3.66 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 22:06:12.400843: step 97800, loss = 3.82 (237.0 examples/sec; 0.540 sec/batch)
2016-07-15 22:06:20.525884: step 97810, loss = 3.72 (227.8 examples/sec; 0.562 sec/batch)
2016-07-15 22:06:25.334270: step 97820, loss = 3.54 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 22:06:30.033957: step 97830, loss = 3.65 (281.6 examples/sec; 0.454 sec/batch)
2016-07-15 22:06:34.682786: step 97840, loss = 3.79 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 22:06:40.449211: step 97850, loss = 4.01 (214.1 examples/sec; 0.598 sec/batch)
2016-07-15 22:06:45.617397: step 97860, loss = 3.61 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 22:06:51.280113: step 97870, loss = 3.68 (242.5 examples/sec; 0.528 sec/batch)
2016-07-15 22:06:56.254773: step 97880, loss = 3.94 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 22:07:01.437167: step 97890, loss = 3.55 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 22:07:06.076045: step 97900, loss = 3.82 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 22:07:11.678456: step 97910, loss = 3.66 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 22:07:17.439391: step 97920, loss = 3.96 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 22:07:22.312163: step 97930, loss = 3.56 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 22:07:26.941835: step 97940, loss = 3.63 (284.8 examples/sec; 0.449 sec/batch)
2016-07-15 22:07:31.560422: step 97950, loss = 3.56 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 22:07:37.403179: step 97960, loss = 3.72 (242.4 examples/sec; 0.528 sec/batch)
2016-07-15 22:07:42.827440: step 97970, loss = 3.75 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 22:07:48.037702: step 97980, loss = 4.06 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 22:07:53.849581: step 97990, loss = 3.50 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 22:07:58.659269: step 98000, loss = 4.04 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 22:08:04.471502: step 98010, loss = 3.97 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 22:08:09.114553: step 98020, loss = 3.64 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 22:08:13.782250: step 98030, loss = 3.60 (265.8 examples/sec; 0.481 sec/batch)
2016-07-15 22:08:19.174974: step 98040, loss = 3.76 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 22:08:24.337044: step 98050, loss = 3.95 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 22:08:29.077604: step 98060, loss = 3.66 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 22:08:33.889027: step 98070, loss = 3.98 (281.6 examples/sec; 0.455 sec/batch)
2016-07-15 22:08:38.544478: step 98080, loss = 3.65 (280.6 examples/sec; 0.456 sec/batch)
2016-07-15 22:08:43.233254: step 98090, loss = 4.05 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 22:08:48.979265: step 98100, loss = 3.62 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 22:08:55.549211: step 98110, loss = 4.16 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 22:09:00.637576: step 98120, loss = 3.97 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 22:09:05.279594: step 98130, loss = 4.06 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 22:09:09.931304: step 98140, loss = 3.91 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 22:09:15.358216: step 98150, loss = 3.82 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 22:09:20.521401: step 98160, loss = 3.70 (245.0 examples/sec; 0.523 sec/batch)
2016-07-15 22:09:26.286660: step 98170, loss = 3.62 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 22:09:31.086307: step 98180, loss = 3.90 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 22:09:35.929916: step 98190, loss = 3.76 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 22:09:40.644512: step 98200, loss = 3.79 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 22:09:47.445168: step 98210, loss = 3.77 (187.8 examples/sec; 0.681 sec/batch)
2016-07-15 22:09:53.474299: step 98220, loss = 3.74 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 22:09:58.268261: step 98230, loss = 3.90 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 22:10:03.090084: step 98240, loss = 3.91 (257.4 examples/sec; 0.497 sec/batch)
2016-07-15 22:10:09.298837: step 98250, loss = 3.76 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 22:10:14.865063: step 98260, loss = 3.50 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 22:10:19.658683: step 98270, loss = 3.87 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 22:10:24.534739: step 98280, loss = 3.60 (253.4 examples/sec; 0.505 sec/batch)
2016-07-15 22:10:31.120036: step 98290, loss = 3.64 (199.9 examples/sec; 0.640 sec/batch)
2016-07-15 22:10:36.291722: step 98300, loss = 3.79 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 22:10:42.012339: step 98310, loss = 3.57 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 22:10:46.832746: step 98320, loss = 3.70 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 22:10:51.647440: step 98330, loss = 3.91 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 22:10:56.400831: step 98340, loss = 3.38 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 22:11:01.271266: step 98350, loss = 3.48 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 22:11:05.962664: step 98360, loss = 3.64 (268.5 examples/sec; 0.477 sec/batch)
2016-07-15 22:11:10.763972: step 98370, loss = 4.04 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 22:11:15.591435: step 98380, loss = 3.89 (253.3 examples/sec; 0.505 sec/batch)
2016-07-15 22:11:21.879548: step 98390, loss = 3.84 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 22:11:27.370090: step 98400, loss = 3.63 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 22:11:33.161358: step 98410, loss = 3.59 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 22:11:38.652138: step 98420, loss = 3.65 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 22:11:44.925966: step 98430, loss = 3.87 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 22:11:50.180674: step 98440, loss = 3.62 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 22:11:55.570872: step 98450, loss = 3.80 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 22:12:00.237376: step 98460, loss = 3.96 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 22:12:05.262986: step 98470, loss = 3.82 (191.9 examples/sec; 0.667 sec/batch)
2016-07-15 22:12:11.757221: step 98480, loss = 3.60 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 22:12:16.913205: step 98490, loss = 3.84 (221.2 examples/sec; 0.579 sec/batch)
2016-07-15 22:12:22.566159: step 98500, loss = 3.82 (269.3 examples/sec; 0.475 sec/batch)
2016-07-15 22:12:28.382807: step 98510, loss = 3.83 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 22:12:32.986675: step 98520, loss = 3.96 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 22:12:38.081952: step 98530, loss = 3.59 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 22:12:43.515330: step 98540, loss = 3.58 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 22:12:48.303063: step 98550, loss = 3.65 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 22:12:53.436343: step 98560, loss = 3.76 (185.5 examples/sec; 0.690 sec/batch)
2016-07-15 22:12:59.897968: step 98570, loss = 3.81 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 22:13:04.973269: step 98580, loss = 3.66 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 22:13:09.658747: step 98590, loss = 3.80 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 22:13:14.447717: step 98600, loss = 3.98 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 22:13:20.239651: step 98610, loss = 3.91 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 22:13:24.995388: step 98620, loss = 3.91 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 22:13:30.374075: step 98630, loss = 3.67 (191.7 examples/sec; 0.668 sec/batch)
2016-07-15 22:13:36.805706: step 98640, loss = 3.47 (221.7 examples/sec; 0.577 sec/batch)
2016-07-15 22:13:42.017228: step 98650, loss = 3.87 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 22:13:47.462122: step 98660, loss = 3.76 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 22:13:53.244024: step 98670, loss = 3.55 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 22:13:58.131876: step 98680, loss = 3.64 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 22:14:02.926795: step 98690, loss = 3.48 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 22:14:08.989638: step 98700, loss = 3.65 (197.4 examples/sec; 0.648 sec/batch)
2016-07-15 22:14:15.885664: step 98710, loss = 3.61 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 22:14:21.658810: step 98720, loss = 3.89 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 22:14:26.503696: step 98730, loss = 3.60 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 22:14:31.300496: step 98740, loss = 3.72 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 22:14:37.493098: step 98750, loss = 3.61 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 22:14:43.043760: step 98760, loss = 3.59 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 22:14:47.809593: step 98770, loss = 3.58 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 22:14:52.675281: step 98780, loss = 3.68 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 22:14:57.383261: step 98790, loss = 3.53 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 22:15:02.227235: step 98800, loss = 3.44 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 22:15:07.850439: step 98810, loss = 4.13 (283.0 examples/sec; 0.452 sec/batch)
2016-07-15 22:15:12.505585: step 98820, loss = 3.71 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 22:15:18.331259: step 98830, loss = 3.66 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 22:15:23.962575: step 98840, loss = 3.73 (205.3 examples/sec; 0.623 sec/batch)
2016-07-15 22:15:29.063119: step 98850, loss = 3.63 (227.3 examples/sec; 0.563 sec/batch)
2016-07-15 22:15:34.773526: step 98860, loss = 3.71 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 22:15:39.549732: step 98870, loss = 3.52 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 22:15:44.401676: step 98880, loss = 3.44 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 22:15:49.116205: step 98890, loss = 3.58 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 22:15:54.466728: step 98900, loss = 3.76 (191.6 examples/sec; 0.668 sec/batch)
2016-07-15 22:16:01.088325: step 98910, loss = 3.88 (272.6 examples/sec; 0.469 sec/batch)
2016-07-15 22:16:06.104676: step 98920, loss = 3.79 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 22:16:11.573532: step 98930, loss = 3.79 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 22:16:17.390215: step 98940, loss = 3.58 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 22:16:22.763581: step 98950, loss = 3.65 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 22:16:28.097684: step 98960, loss = 3.75 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 22:16:32.825264: step 98970, loss = 3.55 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 22:16:37.650492: step 98980, loss = 3.75 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 22:16:42.393648: step 98990, loss = 3.78 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 22:16:47.258308: step 99000, loss = 3.86 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 22:16:53.059207: step 99010, loss = 3.94 (232.3 examples/sec; 0.551 sec/batch)
2016-07-15 22:16:59.608595: step 99020, loss = 3.56 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 22:17:04.779779: step 99030, loss = 3.65 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 22:17:09.511951: step 99040, loss = 3.85 (244.1 examples/sec; 0.524 sec/batch)
2016-07-15 22:17:15.001496: step 99050, loss = 3.57 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 22:17:21.264803: step 99060, loss = 3.72 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 22:17:26.111361: step 99070, loss = 3.88 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 22:17:30.845609: step 99080, loss = 3.48 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 22:17:35.589991: step 99090, loss = 3.56 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 22:17:40.400211: step 99100, loss = 3.82 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 22:17:48.397820: step 99110, loss = 3.60 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 22:17:53.558808: step 99120, loss = 3.61 (198.7 examples/sec; 0.644 sec/batch)
2016-07-15 22:17:59.198110: step 99130, loss = 3.69 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 22:18:05.138050: step 99140, loss = 3.89 (226.2 examples/sec; 0.566 sec/batch)
2016-07-15 22:18:10.391135: step 99150, loss = 3.55 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 22:18:15.824397: step 99160, loss = 3.98 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 22:18:20.553686: step 99170, loss = 3.70 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 22:18:25.633109: step 99180, loss = 3.67 (187.7 examples/sec; 0.682 sec/batch)
2016-07-15 22:18:32.147892: step 99190, loss = 3.56 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 22:18:37.280980: step 99200, loss = 3.48 (218.6 examples/sec; 0.586 sec/batch)
2016-07-15 22:18:44.171094: step 99210, loss = 3.96 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 22:18:49.953098: step 99220, loss = 3.77 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 22:18:54.733416: step 99230, loss = 3.74 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 22:18:59.585928: step 99240, loss = 3.59 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 22:19:05.535834: step 99250, loss = 4.06 (282.9 examples/sec; 0.452 sec/batch)
2016-07-15 22:19:10.202075: step 99260, loss = 3.70 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 22:19:15.514012: step 99270, loss = 3.89 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 22:19:20.705332: step 99280, loss = 3.91 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 22:19:26.476358: step 99290, loss = 3.64 (258.2 examples/sec; 0.496 sec/batch)
2016-07-15 22:19:31.232990: step 99300, loss = 3.67 (283.4 examples/sec; 0.452 sec/batch)
2016-07-15 22:19:37.088693: step 99310, loss = 3.77 (238.9 examples/sec; 0.536 sec/batch)
2016-07-15 22:19:43.647870: step 99320, loss = 3.84 (209.5 examples/sec; 0.611 sec/batch)
2016-07-15 22:19:48.812371: step 99330, loss = 3.61 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 22:19:53.504288: step 99340, loss = 3.82 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 22:19:58.356596: step 99350, loss = 3.64 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 22:20:03.122697: step 99360, loss = 3.57 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 22:20:09.206832: step 99370, loss = 3.52 (197.8 examples/sec; 0.647 sec/batch)
2016-07-15 22:20:14.913191: step 99380, loss = 3.60 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 22:20:19.648531: step 99390, loss = 3.88 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 22:20:24.473200: step 99400, loss = 3.92 (260.8 examples/sec; 0.491 sec/batch)
2016-07-15 22:20:32.183669: step 99410, loss = 3.75 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 22:20:37.284271: step 99420, loss = 3.42 (234.1 examples/sec; 0.547 sec/batch)
2016-07-15 22:20:43.042482: step 99430, loss = 3.81 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 22:20:47.810703: step 99440, loss = 3.61 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 22:20:52.626374: step 99450, loss = 3.46 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 22:20:59.069891: step 99460, loss = 3.59 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 22:21:04.363310: step 99470, loss = 3.73 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 22:21:09.068204: step 99480, loss = 3.81 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 22:21:13.951216: step 99490, loss = 3.75 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 22:21:18.702954: step 99500, loss = 3.78 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 22:21:25.995056: step 99510, loss = 3.65 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 22:21:31.530439: step 99520, loss = 3.52 (267.5 examples/sec; 0.479 sec/batch)
2016-07-15 22:21:36.258153: step 99530, loss = 3.81 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 22:21:41.091239: step 99540, loss = 3.67 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 22:21:45.819360: step 99550, loss = 3.58 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 22:21:51.349686: step 99560, loss = 3.58 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 22:21:57.625104: step 99570, loss = 3.79 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 22:22:02.440949: step 99580, loss = 3.63 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 22:22:07.189849: step 99590, loss = 3.92 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 22:22:11.974611: step 99600, loss = 3.90 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 22:22:18.197220: step 99610, loss = 3.77 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 22:22:24.702173: step 99620, loss = 3.83 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 22:22:29.660488: step 99630, loss = 3.88 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 22:22:34.359235: step 99640, loss = 3.74 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 22:22:40.074593: step 99650, loss = 3.64 (191.0 examples/sec; 0.670 sec/batch)
2016-07-15 22:22:46.128912: step 99660, loss = 3.73 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 22:22:50.941064: step 99670, loss = 3.49 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 22:22:55.731849: step 99680, loss = 3.76 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 22:23:01.881828: step 99690, loss = 3.72 (208.8 examples/sec; 0.613 sec/batch)
2016-07-15 22:23:07.485989: step 99700, loss = 3.69 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 22:23:13.185468: step 99710, loss = 3.56 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 22:23:17.997315: step 99720, loss = 3.72 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 22:23:22.709614: step 99730, loss = 3.82 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 22:23:27.500982: step 99740, loss = 3.70 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 22:23:32.283273: step 99750, loss = 3.84 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 22:23:38.625427: step 99760, loss = 4.03 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 22:23:44.033201: step 99770, loss = 3.82 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 22:23:48.754442: step 99780, loss = 3.75 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 22:23:53.609235: step 99790, loss = 3.91 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 22:23:58.334996: step 99800, loss = 3.87 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 22:24:05.376168: step 99810, loss = 3.77 (194.4 examples/sec; 0.659 sec/batch)
2016-07-15 22:24:11.133269: step 99820, loss = 3.77 (269.9 examples/sec; 0.474 sec/batch)
2016-07-15 22:24:15.918247: step 99830, loss = 3.55 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 22:24:20.722282: step 99840, loss = 3.52 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 22:24:25.482475: step 99850, loss = 3.52 (281.9 examples/sec; 0.454 sec/batch)
2016-07-15 22:24:30.135039: step 99860, loss = 3.70 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 22:24:35.329478: step 99870, loss = 3.74 (201.5 examples/sec; 0.635 sec/batch)
2016-07-15 22:24:40.621731: step 99880, loss = 3.76 (255.3 examples/sec; 0.501 sec/batch)
2016-07-15 22:24:45.283275: step 99890, loss = 3.89 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 22:24:50.099537: step 99900, loss = 3.70 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 22:24:55.799309: step 99910, loss = 3.64 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 22:25:00.513530: step 99920, loss = 3.64 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 22:25:05.451451: step 99930, loss = 3.59 (230.8 examples/sec; 0.555 sec/batch)
2016-07-15 22:25:12.036491: step 99940, loss = 3.77 (205.1 examples/sec; 0.624 sec/batch)
2016-07-15 22:25:17.179498: step 99950, loss = 3.48 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 22:25:21.892117: step 99960, loss = 3.41 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 22:25:26.676634: step 99970, loss = 3.84 (270.3 examples/sec; 0.474 sec/batch)
2016-07-15 22:25:31.522302: step 99980, loss = 3.51 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 22:25:37.593063: step 99990, loss = 3.94 (196.7 examples/sec; 0.651 sec/batch)
2016-07-15 22:25:43.265674: step 100000, loss = 3.55 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 22:25:49.062437: step 100010, loss = 3.50 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 22:25:54.214267: step 100020, loss = 3.61 (188.1 examples/sec; 0.681 sec/batch)
2016-07-15 22:26:00.749583: step 100030, loss = 3.43 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 22:26:05.749496: step 100040, loss = 3.44 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 22:26:10.446288: step 100050, loss = 3.75 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 22:26:16.092758: step 100060, loss = 3.72 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 22:26:22.246412: step 100070, loss = 3.77 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 22:26:27.043708: step 100080, loss = 3.68 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 22:26:31.811295: step 100090, loss = 3.58 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 22:26:36.521389: step 100100, loss = 3.77 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 22:26:42.752365: step 100110, loss = 3.71 (183.3 examples/sec; 0.698 sec/batch)
2016-07-15 22:26:49.229966: step 100120, loss = 3.63 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 22:26:54.124023: step 100130, loss = 3.56 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 22:26:58.881394: step 100140, loss = 3.73 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 22:27:03.723430: step 100150, loss = 3.68 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 22:27:08.576252: step 100160, loss = 3.86 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 22:27:14.922133: step 100170, loss = 3.91 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 22:27:20.399574: step 100180, loss = 3.75 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 22:27:25.094636: step 100190, loss = 3.45 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 22:27:30.131745: step 100200, loss = 3.86 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 22:27:38.083045: step 100210, loss = 3.50 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 22:27:42.931217: step 100220, loss = 3.66 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 22:27:47.713814: step 100230, loss = 3.86 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 22:27:53.847272: step 100240, loss = 3.49 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 22:27:59.530481: step 100250, loss = 3.48 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 22:28:05.240262: step 100260, loss = 3.75 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 22:28:10.197549: step 100270, loss = 3.59 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 22:28:14.918552: step 100280, loss = 3.70 (255.4 examples/sec; 0.501 sec/batch)
2016-07-15 22:28:19.674615: step 100290, loss = 3.81 (281.2 examples/sec; 0.455 sec/batch)
2016-07-15 22:28:24.514124: step 100300, loss = 3.59 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 22:28:30.245441: step 100310, loss = 3.33 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 22:28:35.761311: step 100320, loss = 3.81 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 22:28:41.978671: step 100330, loss = 3.54 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 22:28:46.782148: step 100340, loss = 3.57 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 22:28:51.586883: step 100350, loss = 3.69 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 22:28:57.653180: step 100360, loss = 3.46 (195.0 examples/sec; 0.656 sec/batch)
2016-07-15 22:29:03.334731: step 100370, loss = 3.71 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 22:29:08.110954: step 100380, loss = 3.68 (284.8 examples/sec; 0.450 sec/batch)
2016-07-15 22:29:12.918631: step 100390, loss = 3.90 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 22:29:17.661619: step 100400, loss = 3.63 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 22:29:23.503755: step 100410, loss = 3.88 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 22:29:28.370912: step 100420, loss = 3.63 (246.4 examples/sec; 0.519 sec/batch)
2016-07-15 22:29:33.075477: step 100430, loss = 3.91 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 22:29:37.910458: step 100440, loss = 3.97 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 22:29:42.622805: step 100450, loss = 3.58 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 22:29:48.333474: step 100460, loss = 3.64 (186.8 examples/sec; 0.685 sec/batch)
2016-07-15 22:29:54.403028: step 100470, loss = 3.67 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 22:29:59.207846: step 100480, loss = 3.63 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 22:30:03.934614: step 100490, loss = 3.57 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 22:30:08.652125: step 100500, loss = 3.71 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 22:30:14.209482: step 100510, loss = 3.52 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 22:30:18.867436: step 100520, loss = 3.47 (281.0 examples/sec; 0.455 sec/batch)
2016-07-15 22:30:23.482232: step 100530, loss = 3.59 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 22:30:29.022889: step 100540, loss = 3.64 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 22:30:34.028641: step 100550, loss = 3.86 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 22:30:38.736296: step 100560, loss = 3.77 (242.4 examples/sec; 0.528 sec/batch)
2016-07-15 22:30:44.482189: step 100570, loss = 3.53 (194.3 examples/sec; 0.659 sec/batch)
2016-07-15 22:30:50.548026: step 100580, loss = 3.72 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 22:30:55.366390: step 100590, loss = 3.64 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 22:31:00.148614: step 100600, loss = 3.64 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 22:31:05.931566: step 100610, loss = 3.46 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 22:31:11.336843: step 100620, loss = 3.53 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 22:31:17.761827: step 100630, loss = 3.83 (224.3 examples/sec; 0.571 sec/batch)
2016-07-15 22:31:22.633371: step 100640, loss = 3.83 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 22:31:27.430905: step 100650, loss = 3.60 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 22:31:33.406260: step 100660, loss = 3.61 (191.0 examples/sec; 0.670 sec/batch)
2016-07-15 22:31:39.231744: step 100670, loss = 3.50 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 22:31:44.060704: step 100680, loss = 3.70 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 22:31:48.886269: step 100690, loss = 3.78 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 22:31:55.280017: step 100700, loss = 4.08 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 22:32:01.997040: step 100710, loss = 3.60 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 22:32:06.792929: step 100720, loss = 3.57 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 22:32:12.407210: step 100730, loss = 3.80 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 22:32:18.481811: step 100740, loss = 3.60 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 22:32:23.818172: step 100750, loss = 3.90 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 22:32:29.190009: step 100760, loss = 3.86 (251.7 examples/sec; 0.509 sec/batch)
2016-07-15 22:32:33.927123: step 100770, loss = 3.66 (249.4 examples/sec; 0.513 sec/batch)
2016-07-15 22:32:39.214298: step 100780, loss = 3.86 (191.1 examples/sec; 0.670 sec/batch)
2016-07-15 22:32:45.668499: step 100790, loss = 3.70 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 22:32:50.502808: step 100800, loss = 3.52 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 22:32:56.231586: step 100810, loss = 3.81 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 22:33:02.420829: step 100820, loss = 3.65 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 22:33:08.048786: step 100830, loss = 3.48 (255.0 examples/sec; 0.502 sec/batch)
2016-07-15 22:33:13.875048: step 100840, loss = 3.78 (210.8 examples/sec; 0.607 sec/batch)
2016-07-15 22:33:19.067145: step 100850, loss = 3.61 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 22:33:24.505355: step 100860, loss = 3.64 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 22:33:30.344085: step 100870, loss = 3.67 (247.6 examples/sec; 0.517 sec/batch)
2016-07-15 22:33:35.691957: step 100880, loss = 3.92 (198.2 examples/sec; 0.646 sec/batch)
2016-07-15 22:33:41.026003: step 100890, loss = 3.60 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 22:33:46.846035: step 100900, loss = 3.72 (255.2 examples/sec; 0.502 sec/batch)
2016-07-15 22:33:52.672151: step 100910, loss = 3.79 (269.2 examples/sec; 0.476 sec/batch)
2016-07-15 22:33:57.277620: step 100920, loss = 3.95 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 22:34:02.015623: step 100930, loss = 3.80 (239.4 examples/sec; 0.535 sec/batch)
2016-07-15 22:34:07.749206: step 100940, loss = 3.75 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 22:34:12.497492: step 100950, loss = 3.68 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 22:34:17.362005: step 100960, loss = 3.76 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 22:34:23.853045: step 100970, loss = 3.84 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 22:34:29.122500: step 100980, loss = 3.59 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 22:34:33.779521: step 100990, loss = 3.70 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 22:34:38.675245: step 101000, loss = 3.80 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 22:34:44.394309: step 101010, loss = 3.62 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 22:34:50.549353: step 101020, loss = 3.51 (203.7 examples/sec; 0.628 sec/batch)
2016-07-15 22:34:56.137795: step 101030, loss = 3.69 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 22:35:00.915221: step 101040, loss = 3.68 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 22:35:05.828345: step 101050, loss = 3.72 (245.3 examples/sec; 0.522 sec/batch)
2016-07-15 22:35:10.590432: step 101060, loss = 3.61 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 22:35:16.110318: step 101070, loss = 3.52 (189.4 examples/sec; 0.676 sec/batch)
2016-07-15 22:35:22.383699: step 101080, loss = 3.59 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 22:35:27.234786: step 101090, loss = 3.66 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 22:35:31.996670: step 101100, loss = 3.56 (266.5 examples/sec; 0.480 sec/batch)
2016-07-15 22:35:39.456492: step 101110, loss = 3.49 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 22:35:44.875446: step 101120, loss = 3.44 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 22:35:49.578392: step 101130, loss = 3.78 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 22:35:54.449637: step 101140, loss = 3.52 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 22:35:59.218183: step 101150, loss = 3.86 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 22:36:04.035526: step 101160, loss = 3.53 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 22:36:08.886325: step 101170, loss = 3.57 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 22:36:13.626699: step 101180, loss = 3.68 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 22:36:18.753670: step 101190, loss = 3.56 (191.7 examples/sec; 0.668 sec/batch)
2016-07-15 22:36:25.235174: step 101200, loss = 3.69 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 22:36:31.242520: step 101210, loss = 3.65 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 22:36:36.009994: step 101220, loss = 3.75 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 22:36:41.989616: step 101230, loss = 3.72 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 22:36:47.698118: step 101240, loss = 3.66 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 22:36:53.303184: step 101250, loss = 3.61 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 22:36:58.299222: step 101260, loss = 3.70 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 22:37:03.062628: step 101270, loss = 3.68 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 22:37:08.750541: step 101280, loss = 3.70 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 22:37:14.841039: step 101290, loss = 3.49 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 22:37:19.667987: step 101300, loss = 3.58 (277.0 examples/sec; 0.462 sec/batch)
2016-07-15 22:37:25.497866: step 101310, loss = 3.74 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 22:37:31.949220: step 101320, loss = 3.85 (207.6 examples/sec; 0.617 sec/batch)
2016-07-15 22:37:37.268811: step 101330, loss = 3.96 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 22:37:41.927322: step 101340, loss = 3.65 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 22:37:46.530298: step 101350, loss = 3.31 (273.6 examples/sec; 0.468 sec/batch)
2016-07-15 22:37:51.698330: step 101360, loss = 3.82 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 22:37:57.066810: step 101370, loss = 3.54 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 22:38:02.825515: step 101380, loss = 3.64 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 22:38:08.203594: step 101390, loss = 3.84 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 22:38:13.528735: step 101400, loss = 3.88 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 22:38:20.443925: step 101410, loss = 3.49 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 22:38:25.217447: step 101420, loss = 3.71 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 22:38:30.020168: step 101430, loss = 3.49 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 22:38:34.748559: step 101440, loss = 3.68 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 22:38:40.262962: step 101450, loss = 3.72 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 22:38:46.552369: step 101460, loss = 3.74 (252.7 examples/sec; 0.507 sec/batch)
2016-07-15 22:38:51.415875: step 101470, loss = 3.86 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 22:38:56.167612: step 101480, loss = 3.71 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 22:39:02.220855: step 101490, loss = 3.59 (193.9 examples/sec; 0.660 sec/batch)
2016-07-15 22:39:07.930517: step 101500, loss = 3.56 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 22:39:13.654615: step 101510, loss = 3.67 (272.2 examples/sec; 0.470 sec/batch)
2016-07-15 22:39:18.495938: step 101520, loss = 3.41 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 22:39:23.235334: step 101530, loss = 3.62 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 22:39:28.925566: step 101540, loss = 3.65 (187.4 examples/sec; 0.683 sec/batch)
2016-07-15 22:39:35.007795: step 101550, loss = 3.93 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 22:39:39.802522: step 101560, loss = 3.66 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 22:39:44.654386: step 101570, loss = 3.72 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 22:39:49.407874: step 101580, loss = 3.76 (282.2 examples/sec; 0.454 sec/batch)
2016-07-15 22:39:54.463275: step 101590, loss = 3.90 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 22:40:01.044422: step 101600, loss = 3.65 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 22:40:07.723078: step 101610, loss = 3.58 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 22:40:13.141105: step 101620, loss = 3.57 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 22:40:18.905449: step 101630, loss = 3.70 (262.0 examples/sec; 0.488 sec/batch)
2016-07-15 22:40:24.343959: step 101640, loss = 3.52 (204.7 examples/sec; 0.625 sec/batch)
2016-07-15 22:40:29.600686: step 101650, loss = 3.54 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 22:40:34.337856: step 101660, loss = 3.68 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 22:40:39.693316: step 101670, loss = 4.10 (190.7 examples/sec; 0.671 sec/batch)
2016-07-15 22:40:46.116378: step 101680, loss = 3.63 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 22:40:51.001247: step 101690, loss = 3.71 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 22:40:55.729052: step 101700, loss = 3.38 (249.7 examples/sec; 0.513 sec/batch)
2016-07-15 22:41:01.584822: step 101710, loss = 3.87 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 22:41:06.206951: step 101720, loss = 3.74 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 22:41:11.129903: step 101730, loss = 3.63 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 22:41:16.701854: step 101740, loss = 3.68 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 22:41:22.449419: step 101750, loss = 3.63 (245.3 examples/sec; 0.522 sec/batch)
2016-07-15 22:41:27.682622: step 101760, loss = 3.54 (202.6 examples/sec; 0.632 sec/batch)
2016-07-15 22:41:33.117683: step 101770, loss = 3.86 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 22:41:37.817027: step 101780, loss = 3.67 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 22:41:42.990692: step 101790, loss = 3.68 (191.0 examples/sec; 0.670 sec/batch)
2016-07-15 22:41:49.472875: step 101800, loss = 3.71 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 22:41:55.458060: step 101810, loss = 3.68 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 22:42:00.249006: step 101820, loss = 3.61 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 22:42:06.330331: step 101830, loss = 3.46 (193.6 examples/sec; 0.661 sec/batch)
2016-07-15 22:42:12.020130: step 101840, loss = 3.58 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 22:42:17.682280: step 101850, loss = 3.89 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 22:42:22.618245: step 101860, loss = 4.05 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 22:42:27.383195: step 101870, loss = 3.72 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 22:42:33.080897: step 101880, loss = 3.75 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 22:42:39.190843: step 101890, loss = 3.76 (246.1 examples/sec; 0.520 sec/batch)
2016-07-15 22:42:44.013513: step 101900, loss = 3.75 (274.3 examples/sec; 0.467 sec/batch)
2016-07-15 22:42:49.831522: step 101910, loss = 3.84 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 22:42:54.524102: step 101920, loss = 3.79 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 22:42:59.825628: step 101930, loss = 3.50 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 22:43:06.286493: step 101940, loss = 4.00 (204.6 examples/sec; 0.626 sec/batch)
2016-07-15 22:43:11.122788: step 101950, loss = 3.72 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 22:43:15.892367: step 101960, loss = 3.52 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 22:43:21.776089: step 101970, loss = 3.50 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 22:43:27.632906: step 101980, loss = 3.73 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 22:43:33.192895: step 101990, loss = 3.45 (209.4 examples/sec; 0.611 sec/batch)
2016-07-15 22:43:38.317682: step 102000, loss = 3.49 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 22:43:45.402347: step 102010, loss = 3.65 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 22:43:51.157181: step 102020, loss = 3.73 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 22:43:56.504872: step 102030, loss = 3.82 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 22:44:01.789899: step 102040, loss = 3.99 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 22:44:06.477684: step 102050, loss = 3.63 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 22:44:11.760301: step 102060, loss = 3.58 (184.0 examples/sec; 0.696 sec/batch)
2016-07-15 22:44:18.189717: step 102070, loss = 3.91 (208.5 examples/sec; 0.614 sec/batch)
2016-07-15 22:44:23.055257: step 102080, loss = 3.62 (279.9 examples/sec; 0.457 sec/batch)
2016-07-15 22:44:27.810328: step 102090, loss = 3.69 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 22:44:32.600619: step 102100, loss = 3.25 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 22:44:38.350028: step 102110, loss = 3.75 (252.4 examples/sec; 0.507 sec/batch)
2016-07-15 22:44:43.047884: step 102120, loss = 3.61 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 22:44:48.507567: step 102130, loss = 3.73 (188.4 examples/sec; 0.680 sec/batch)
2016-07-15 22:44:54.768006: step 102140, loss = 3.61 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 22:44:59.594200: step 102150, loss = 3.56 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 22:45:04.355911: step 102160, loss = 3.60 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 22:45:09.101731: step 102170, loss = 3.59 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 22:45:13.905075: step 102180, loss = 3.65 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 22:45:20.368086: step 102190, loss = 3.84 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 22:45:25.710540: step 102200, loss = 3.84 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 22:45:31.468616: step 102210, loss = 3.65 (260.6 examples/sec; 0.491 sec/batch)
2016-07-15 22:45:37.162225: step 102220, loss = 3.65 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 22:45:43.259942: step 102230, loss = 3.45 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 22:45:48.686769: step 102240, loss = 3.59 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 22:45:53.906572: step 102250, loss = 3.77 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 22:45:59.679962: step 102260, loss = 3.63 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 22:46:05.153354: step 102270, loss = 3.64 (209.1 examples/sec; 0.612 sec/batch)
2016-07-15 22:46:10.291086: step 102280, loss = 3.53 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 22:46:16.069433: step 102290, loss = 3.67 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 22:46:21.659871: step 102300, loss = 3.44 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 22:46:27.787898: step 102310, loss = 3.74 (276.9 examples/sec; 0.462 sec/batch)
2016-07-15 22:46:32.681510: step 102320, loss = 3.56 (254.6 examples/sec; 0.503 sec/batch)
2016-07-15 22:46:38.742933: step 102330, loss = 3.54 (194.6 examples/sec; 0.658 sec/batch)
2016-07-15 22:46:44.403205: step 102340, loss = 3.48 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 22:46:49.163024: step 102350, loss = 3.55 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 22:46:54.011053: step 102360, loss = 3.67 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 22:47:00.485926: step 102370, loss = 3.61 (191.8 examples/sec; 0.667 sec/batch)
2016-07-15 22:47:05.780769: step 102380, loss = 3.64 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 22:47:10.529232: step 102390, loss = 3.50 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 22:47:15.854287: step 102400, loss = 3.77 (189.2 examples/sec; 0.677 sec/batch)
2016-07-15 22:47:23.641463: step 102410, loss = 3.66 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 22:47:29.145478: step 102420, loss = 3.71 (208.2 examples/sec; 0.615 sec/batch)
2016-07-15 22:47:34.271684: step 102430, loss = 3.68 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 22:47:40.084854: step 102440, loss = 3.84 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 22:47:45.647655: step 102450, loss = 3.48 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 22:47:50.763018: step 102460, loss = 3.81 (230.9 examples/sec; 0.554 sec/batch)
2016-07-15 22:47:56.510500: step 102470, loss = 3.53 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 22:48:01.269439: step 102480, loss = 3.67 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 22:48:06.088843: step 102490, loss = 3.72 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 22:48:12.547499: step 102500, loss = 3.60 (208.3 examples/sec; 0.615 sec/batch)
2016-07-15 22:48:19.347942: step 102510, loss = 3.36 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 22:48:24.931531: step 102520, loss = 3.78 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 22:48:29.688241: step 102530, loss = 3.48 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 22:48:34.561857: step 102540, loss = 3.63 (259.5 examples/sec; 0.493 sec/batch)
2016-07-15 22:48:41.155026: step 102550, loss = 3.86 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 22:48:46.342724: step 102560, loss = 3.85 (257.7 examples/sec; 0.497 sec/batch)
2016-07-15 22:48:52.118573: step 102570, loss = 3.80 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 22:48:57.727968: step 102580, loss = 3.58 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 22:49:02.852745: step 102590, loss = 3.69 (224.8 examples/sec; 0.569 sec/batch)
2016-07-15 22:49:08.560835: step 102600, loss = 3.62 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 22:49:15.372410: step 102610, loss = 3.73 (250.9 examples/sec; 0.510 sec/batch)
2016-07-15 22:49:20.744278: step 102620, loss = 3.75 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 22:49:26.036941: step 102630, loss = 3.57 (268.1 examples/sec; 0.478 sec/batch)
2016-07-15 22:49:30.852444: step 102640, loss = 3.72 (256.0 examples/sec; 0.500 sec/batch)
2016-07-15 22:49:36.188326: step 102650, loss = 3.59 (186.9 examples/sec; 0.685 sec/batch)
2016-07-15 22:49:42.650856: step 102660, loss = 3.54 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 22:49:47.862488: step 102670, loss = 3.76 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 22:49:53.313378: step 102680, loss = 3.79 (265.3 examples/sec; 0.483 sec/batch)
2016-07-15 22:49:59.064504: step 102690, loss = 3.61 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 22:50:04.002463: step 102700, loss = 3.51 (250.8 examples/sec; 0.510 sec/batch)
2016-07-15 22:50:09.721595: step 102710, loss = 3.83 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 22:50:16.086330: step 102720, loss = 3.54 (198.7 examples/sec; 0.644 sec/batch)
2016-07-15 22:50:21.507331: step 102730, loss = 3.72 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 22:50:26.241732: step 102740, loss = 3.78 (271.4 examples/sec; 0.472 sec/batch)
2016-07-15 22:50:31.056212: step 102750, loss = 3.80 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 22:50:35.736373: step 102760, loss = 3.55 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 22:50:40.544683: step 102770, loss = 3.63 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 22:50:45.312100: step 102780, loss = 3.60 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 22:50:51.514403: step 102790, loss = 3.53 (207.8 examples/sec; 0.616 sec/batch)
2016-07-15 22:50:57.077410: step 102800, loss = 3.60 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 22:51:02.792737: step 102810, loss = 3.44 (270.0 examples/sec; 0.474 sec/batch)
2016-07-15 22:51:08.035261: step 102820, loss = 3.63 (190.9 examples/sec; 0.671 sec/batch)
2016-07-15 22:51:14.521409: step 102830, loss = 3.84 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 22:51:19.613207: step 102840, loss = 3.64 (207.4 examples/sec; 0.617 sec/batch)
2016-07-15 22:51:25.185870: step 102850, loss = 3.61 (255.9 examples/sec; 0.500 sec/batch)
2016-07-15 22:51:29.965573: step 102860, loss = 3.45 (274.4 examples/sec; 0.467 sec/batch)
2016-07-15 22:51:34.894470: step 102870, loss = 3.60 (223.2 examples/sec; 0.574 sec/batch)
2016-07-15 22:51:41.416595: step 102880, loss = 3.56 (208.2 examples/sec; 0.615 sec/batch)
2016-07-15 22:51:46.553507: step 102890, loss = 3.83 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 22:51:51.243952: step 102900, loss = 3.64 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 22:51:58.145918: step 102910, loss = 3.68 (184.3 examples/sec; 0.694 sec/batch)
2016-07-15 22:52:04.090493: step 102920, loss = 3.51 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 22:52:09.667803: step 102930, loss = 3.77 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 22:52:14.861328: step 102940, loss = 3.65 (249.5 examples/sec; 0.513 sec/batch)
2016-07-15 22:52:20.629215: step 102950, loss = 3.73 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 22:52:26.310320: step 102960, loss = 3.70 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 22:52:31.439171: step 102970, loss = 3.51 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 22:52:37.041070: step 102980, loss = 3.60 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 22:52:42.834935: step 102990, loss = 3.74 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 22:52:47.689993: step 103000, loss = 3.58 (275.4 examples/sec; 0.465 sec/batch)
2016-07-15 22:52:53.510489: step 103010, loss = 3.82 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 22:52:59.741877: step 103020, loss = 4.00 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 22:53:05.305448: step 103030, loss = 3.64 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 22:53:10.040681: step 103040, loss = 3.64 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 22:53:14.980258: step 103050, loss = 3.72 (233.3 examples/sec; 0.549 sec/batch)
2016-07-15 22:53:21.585685: step 103060, loss = 3.71 (206.6 examples/sec; 0.619 sec/batch)
2016-07-15 22:53:26.751477: step 103070, loss = 3.56 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 22:53:31.472843: step 103080, loss = 3.58 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 22:53:37.041877: step 103090, loss = 3.65 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 22:53:43.305135: step 103100, loss = 3.48 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 22:53:49.920410: step 103110, loss = 3.75 (207.9 examples/sec; 0.616 sec/batch)
2016-07-15 22:53:55.079562: step 103120, loss = 3.47 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 22:53:59.812256: step 103130, loss = 3.64 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 22:54:04.644240: step 103140, loss = 3.50 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 22:54:09.434808: step 103150, loss = 3.53 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 22:54:14.228754: step 103160, loss = 3.56 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 22:54:19.267645: step 103170, loss = 3.45 (196.5 examples/sec; 0.651 sec/batch)
2016-07-15 22:54:25.804610: step 103180, loss = 3.50 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 22:54:30.997482: step 103190, loss = 3.73 (238.5 examples/sec; 0.537 sec/batch)
2016-07-15 22:54:36.689366: step 103200, loss = 3.68 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 22:54:42.516754: step 103210, loss = 3.53 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 22:54:47.577899: step 103220, loss = 3.56 (187.1 examples/sec; 0.684 sec/batch)
2016-07-15 22:54:54.113354: step 103230, loss = 3.71 (203.8 examples/sec; 0.628 sec/batch)
2016-07-15 22:54:59.137951: step 103240, loss = 3.60 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 22:55:03.899680: step 103250, loss = 3.70 (255.6 examples/sec; 0.501 sec/batch)
2016-07-15 22:55:08.660925: step 103260, loss = 3.29 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 22:55:13.437175: step 103270, loss = 3.70 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 22:55:19.557089: step 103280, loss = 3.69 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 22:55:25.156102: step 103290, loss = 3.72 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 22:55:30.959457: step 103300, loss = 3.69 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 22:55:36.871167: step 103310, loss = 3.53 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 22:55:41.589230: step 103320, loss = 3.70 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 22:55:46.279658: step 103330, loss = 3.61 (273.4 examples/sec; 0.468 sec/batch)
2016-07-15 22:55:52.044605: step 103340, loss = 3.55 (245.1 examples/sec; 0.522 sec/batch)
2016-07-15 22:55:57.485764: step 103350, loss = 3.78 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 22:56:02.670390: step 103360, loss = 3.92 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 22:56:08.418670: step 103370, loss = 3.98 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 22:56:13.217116: step 103380, loss = 3.83 (278.3 examples/sec; 0.460 sec/batch)
2016-07-15 22:56:18.004799: step 103390, loss = 3.71 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 22:56:24.291357: step 103400, loss = 3.33 (205.4 examples/sec; 0.623 sec/batch)
2016-07-15 22:56:31.019007: step 103410, loss = 3.76 (254.5 examples/sec; 0.503 sec/batch)
2016-07-15 22:56:36.764200: step 103420, loss = 3.62 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 22:56:42.454988: step 103430, loss = 3.62 (197.1 examples/sec; 0.649 sec/batch)
2016-07-15 22:56:47.606925: step 103440, loss = 3.41 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 22:56:53.219907: step 103450, loss = 3.66 (262.5 examples/sec; 0.488 sec/batch)
2016-07-15 22:56:58.966901: step 103460, loss = 3.38 (206.0 examples/sec; 0.621 sec/batch)
2016-07-15 22:57:03.823984: step 103470, loss = 3.63 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 22:57:08.541091: step 103480, loss = 3.52 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 22:57:13.340223: step 103490, loss = 3.40 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 22:57:18.150181: step 103500, loss = 3.64 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 22:57:25.881090: step 103510, loss = 3.58 (206.1 examples/sec; 0.621 sec/batch)
2016-07-15 22:57:31.020351: step 103520, loss = 3.52 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 22:57:36.809167: step 103530, loss = 3.49 (246.7 examples/sec; 0.519 sec/batch)
2016-07-15 22:57:42.444030: step 103540, loss = 3.61 (207.2 examples/sec; 0.618 sec/batch)
2016-07-15 22:57:47.489986: step 103550, loss = 3.43 (268.6 examples/sec; 0.476 sec/batch)
2016-07-15 22:57:52.216544: step 103560, loss = 3.72 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 22:57:57.023481: step 103570, loss = 3.48 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 22:58:01.689421: step 103580, loss = 3.48 (280.2 examples/sec; 0.457 sec/batch)
2016-07-15 22:58:06.343273: step 103590, loss = 3.43 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 22:58:12.107971: step 103600, loss = 3.36 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 22:58:18.822300: step 103610, loss = 3.51 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 22:58:23.953360: step 103620, loss = 3.74 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 22:58:29.508413: step 103630, loss = 3.39 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 22:58:34.302570: step 103640, loss = 3.82 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 22:58:39.138641: step 103650, loss = 3.64 (254.4 examples/sec; 0.503 sec/batch)
2016-07-15 22:58:45.733666: step 103660, loss = 3.66 (201.0 examples/sec; 0.637 sec/batch)
2016-07-15 22:58:50.926236: step 103670, loss = 3.38 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 22:58:55.604552: step 103680, loss = 3.73 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 22:59:00.452188: step 103690, loss = 3.56 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 22:59:05.234322: step 103700, loss = 3.77 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 22:59:12.581725: step 103710, loss = 3.82 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 22:59:18.008283: step 103720, loss = 3.64 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 22:59:22.703582: step 103730, loss = 3.75 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 22:59:27.564917: step 103740, loss = 3.65 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 22:59:32.325419: step 103750, loss = 3.64 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 22:59:38.031044: step 103760, loss = 3.62 (186.1 examples/sec; 0.688 sec/batch)
2016-07-15 22:59:44.079620: step 103770, loss = 3.59 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 22:59:48.949408: step 103780, loss = 3.32 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 22:59:53.684464: step 103790, loss = 3.89 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 22:59:59.850376: step 103800, loss = 3.62 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 23:00:06.697720: step 103810, loss = 3.70 (257.8 examples/sec; 0.497 sec/batch)
2016-07-15 23:00:12.468007: step 103820, loss = 3.60 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 23:00:17.989318: step 103830, loss = 3.78 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 23:00:23.112277: step 103840, loss = 3.64 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 23:00:27.787727: step 103850, loss = 3.44 (256.7 examples/sec; 0.499 sec/batch)
2016-07-15 23:00:33.311606: step 103860, loss = 3.47 (189.5 examples/sec; 0.676 sec/batch)
2016-07-15 23:00:39.596238: step 103870, loss = 3.73 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 23:00:44.451330: step 103880, loss = 3.43 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 23:00:49.192223: step 103890, loss = 3.52 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 23:00:53.965726: step 103900, loss = 3.34 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 23:01:00.084889: step 103910, loss = 3.77 (190.7 examples/sec; 0.671 sec/batch)
2016-07-15 23:01:06.585561: step 103920, loss = 3.56 (205.3 examples/sec; 0.624 sec/batch)
2016-07-15 23:01:11.575523: step 103930, loss = 3.57 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 23:01:16.302248: step 103940, loss = 3.54 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 23:01:21.967078: step 103950, loss = 3.50 (188.0 examples/sec; 0.681 sec/batch)
2016-07-15 23:01:28.068526: step 103960, loss = 3.63 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 23:01:33.405621: step 103970, loss = 3.59 (206.5 examples/sec; 0.620 sec/batch)
2016-07-15 23:01:38.691268: step 103980, loss = 3.85 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 23:01:43.395825: step 103990, loss = 3.52 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 23:01:48.801361: step 104000, loss = 3.72 (190.1 examples/sec; 0.673 sec/batch)
2016-07-15 23:01:56.496368: step 104010, loss = 3.60 (262.0 examples/sec; 0.489 sec/batch)
2016-07-15 23:02:02.006607: step 104020, loss = 3.42 (205.0 examples/sec; 0.624 sec/batch)
2016-07-15 23:02:07.192497: step 104030, loss = 3.61 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 23:02:11.926541: step 104040, loss = 3.71 (245.7 examples/sec; 0.521 sec/batch)
2016-07-15 23:02:17.413242: step 104050, loss = 3.77 (188.9 examples/sec; 0.678 sec/batch)
2016-07-15 23:02:23.706405: step 104060, loss = 3.61 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 23:02:28.589603: step 104070, loss = 3.60 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 23:02:33.348151: step 104080, loss = 3.38 (265.8 examples/sec; 0.482 sec/batch)
2016-07-15 23:02:39.365184: step 104090, loss = 3.72 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 23:02:45.075861: step 104100, loss = 3.46 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 23:02:50.870696: step 104110, loss = 3.62 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 23:02:55.733794: step 104120, loss = 3.71 (277.5 examples/sec; 0.461 sec/batch)
2016-07-15 23:03:00.552452: step 104130, loss = 3.50 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 23:03:06.309492: step 104140, loss = 3.61 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 23:03:12.351048: step 104150, loss = 3.66 (267.0 examples/sec; 0.479 sec/batch)
2016-07-15 23:03:17.197938: step 104160, loss = 3.59 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 23:03:22.017807: step 104170, loss = 3.65 (254.1 examples/sec; 0.504 sec/batch)
2016-07-15 23:03:28.210521: step 104180, loss = 3.76 (203.0 examples/sec; 0.630 sec/batch)
2016-07-15 23:03:33.756745: step 104190, loss = 3.59 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 23:03:38.517199: step 104200, loss = 3.59 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 23:03:44.113749: step 104210, loss = 3.72 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 23:03:48.812171: step 104220, loss = 3.55 (287.8 examples/sec; 0.445 sec/batch)
2016-07-15 23:03:53.436125: step 104230, loss = 3.57 (279.0 examples/sec; 0.459 sec/batch)
2016-07-15 23:03:58.046801: step 104240, loss = 3.54 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 23:04:02.671790: step 104250, loss = 3.56 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 23:04:08.437807: step 104260, loss = 3.42 (256.1 examples/sec; 0.500 sec/batch)
2016-07-15 23:04:13.298083: step 104270, loss = 3.41 (270.4 examples/sec; 0.473 sec/batch)
2016-07-15 23:04:18.116611: step 104280, loss = 3.51 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 23:04:24.359079: step 104290, loss = 3.54 (201.6 examples/sec; 0.635 sec/batch)
2016-07-15 23:04:29.942781: step 104300, loss = 3.70 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 23:04:36.685307: step 104310, loss = 3.49 (266.3 examples/sec; 0.481 sec/batch)
2016-07-15 23:04:42.195642: step 104320, loss = 3.49 (202.2 examples/sec; 0.633 sec/batch)
2016-07-15 23:04:47.378067: step 104330, loss = 3.48 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 23:04:52.134551: step 104340, loss = 3.63 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 23:04:57.677696: step 104350, loss = 3.59 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 23:05:03.925144: step 104360, loss = 3.48 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 23:05:08.733319: step 104370, loss = 3.39 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 23:05:13.535817: step 104380, loss = 3.66 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 23:05:19.546490: step 104390, loss = 3.74 (193.7 examples/sec; 0.661 sec/batch)
2016-07-15 23:05:25.288745: step 104400, loss = 3.63 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 23:05:31.109650: step 104410, loss = 3.66 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 23:05:35.934920: step 104420, loss = 3.37 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 23:05:40.713221: step 104430, loss = 3.59 (261.2 examples/sec; 0.490 sec/batch)
2016-07-15 23:05:46.446717: step 104440, loss = 3.69 (185.1 examples/sec; 0.692 sec/batch)
2016-07-15 23:05:52.498156: step 104450, loss = 3.60 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 23:05:57.284760: step 104460, loss = 3.69 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 23:06:02.096043: step 104470, loss = 3.68 (256.3 examples/sec; 0.499 sec/batch)
2016-07-15 23:06:08.277524: step 104480, loss = 3.56 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 23:06:13.868549: step 104490, loss = 3.54 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 23:06:18.647234: step 104500, loss = 3.79 (281.5 examples/sec; 0.455 sec/batch)
2016-07-15 23:06:25.000695: step 104510, loss = 3.68 (189.8 examples/sec; 0.674 sec/batch)
2016-07-15 23:06:31.447460: step 104520, loss = 3.75 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 23:06:36.281944: step 104530, loss = 3.73 (283.9 examples/sec; 0.451 sec/batch)
2016-07-15 23:06:41.035852: step 104540, loss = 3.92 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 23:06:45.826305: step 104550, loss = 3.60 (276.6 examples/sec; 0.463 sec/batch)
2016-07-15 23:06:50.592761: step 104560, loss = 3.38 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 23:06:55.324528: step 104570, loss = 3.48 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 23:07:01.111118: step 104580, loss = 3.55 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 23:07:05.879229: step 104590, loss = 3.62 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 23:07:10.657938: step 104600, loss = 3.55 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 23:07:16.334969: step 104610, loss = 3.65 (262.9 examples/sec; 0.487 sec/batch)
2016-07-15 23:07:21.868679: step 104620, loss = 3.16 (185.7 examples/sec; 0.689 sec/batch)
2016-07-15 23:07:28.087162: step 104630, loss = 3.45 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 23:07:32.909598: step 104640, loss = 3.33 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 23:07:37.826588: step 104650, loss = 3.33 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 23:07:43.990520: step 104660, loss = 3.51 (197.4 examples/sec; 0.648 sec/batch)
2016-07-15 23:07:49.645288: step 104670, loss = 3.40 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 23:07:54.400584: step 104680, loss = 3.55 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 23:07:59.260595: step 104690, loss = 3.53 (253.4 examples/sec; 0.505 sec/batch)
2016-07-15 23:08:05.796615: step 104700, loss = 3.58 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 23:08:12.546855: step 104710, loss = 3.54 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 23:08:18.148098: step 104720, loss = 3.60 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 23:08:22.857684: step 104730, loss = 3.50 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 23:08:27.715593: step 104740, loss = 3.49 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 23:08:32.426882: step 104750, loss = 3.47 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 23:08:38.012410: step 104760, loss = 3.36 (180.0 examples/sec; 0.711 sec/batch)
2016-07-15 23:08:44.219406: step 104770, loss = 3.51 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 23:08:49.053002: step 104780, loss = 3.30 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 23:08:53.828853: step 104790, loss = 3.57 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 23:08:59.880502: step 104800, loss = 3.63 (193.3 examples/sec; 0.662 sec/batch)
2016-07-15 23:09:06.813962: step 104810, loss = 3.58 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 23:09:12.662176: step 104820, loss = 3.68 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 23:09:17.521729: step 104830, loss = 3.37 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 23:09:22.328967: step 104840, loss = 3.71 (264.8 examples/sec; 0.483 sec/batch)
2016-07-15 23:09:27.106327: step 104850, loss = 3.68 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 23:09:31.753001: step 104860, loss = 3.76 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 23:09:36.667850: step 104870, loss = 3.66 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 23:09:42.192192: step 104880, loss = 3.73 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 23:09:46.879399: step 104890, loss = 3.47 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 23:09:51.724981: step 104900, loss = 3.55 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 23:09:59.852579: step 104910, loss = 3.43 (219.1 examples/sec; 0.584 sec/batch)
2016-07-15 23:10:05.021861: step 104920, loss = 3.59 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 23:10:10.461538: step 104930, loss = 3.38 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 23:10:16.211634: step 104940, loss = 3.33 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 23:10:21.073135: step 104950, loss = 3.83 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 23:10:25.846858: step 104960, loss = 3.50 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 23:10:31.931830: step 104970, loss = 3.69 (193.5 examples/sec; 0.661 sec/batch)
2016-07-15 23:10:37.681346: step 104980, loss = 3.65 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 23:10:42.433299: step 104990, loss = 3.68 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 23:10:47.239277: step 105000, loss = 3.54 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 23:10:55.204193: step 105010, loss = 3.41 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 23:11:00.170146: step 105020, loss = 3.49 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 23:11:04.946235: step 105030, loss = 3.59 (247.5 examples/sec; 0.517 sec/batch)
2016-07-15 23:11:09.793253: step 105040, loss = 3.45 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 23:11:14.421081: step 105050, loss = 3.43 (283.8 examples/sec; 0.451 sec/batch)
2016-07-15 23:11:19.088550: step 105060, loss = 3.35 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 23:11:24.855184: step 105070, loss = 3.66 (258.7 examples/sec; 0.495 sec/batch)
2016-07-15 23:11:30.316903: step 105080, loss = 3.36 (204.0 examples/sec; 0.628 sec/batch)
2016-07-15 23:11:35.523741: step 105090, loss = 3.52 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 23:11:41.287582: step 105100, loss = 3.52 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 23:11:47.130354: step 105110, loss = 3.54 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 23:11:52.079743: step 105120, loss = 3.68 (224.5 examples/sec; 0.570 sec/batch)
2016-07-15 23:11:58.628823: step 105130, loss = 3.67 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 23:12:03.775608: step 105140, loss = 3.82 (258.8 examples/sec; 0.495 sec/batch)
2016-07-15 23:12:08.423364: step 105150, loss = 3.56 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 23:12:13.232034: step 105160, loss = 3.67 (274.5 examples/sec; 0.466 sec/batch)
2016-07-15 23:12:17.963726: step 105170, loss = 3.81 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 23:12:22.719528: step 105180, loss = 3.43 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 23:12:27.593379: step 105190, loss = 3.41 (260.1 examples/sec; 0.492 sec/batch)
2016-07-15 23:12:34.115584: step 105200, loss = 3.78 (203.9 examples/sec; 0.628 sec/batch)
2016-07-15 23:12:40.302906: step 105210, loss = 3.60 (276.8 examples/sec; 0.462 sec/batch)
2016-07-15 23:12:45.331865: step 105220, loss = 3.58 (207.7 examples/sec; 0.616 sec/batch)
2016-07-15 23:12:50.758916: step 105230, loss = 3.60 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 23:12:55.482109: step 105240, loss = 3.53 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 23:13:00.543308: step 105250, loss = 3.73 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 23:13:07.049608: step 105260, loss = 3.55 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 23:13:12.011990: step 105270, loss = 3.23 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 23:13:16.706192: step 105280, loss = 3.70 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 23:13:22.309652: step 105290, loss = 3.43 (185.2 examples/sec; 0.691 sec/batch)
2016-07-15 23:13:28.477788: step 105300, loss = 3.42 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 23:13:35.181896: step 105310, loss = 3.60 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 23:13:40.186692: step 105320, loss = 3.57 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 23:13:44.880863: step 105330, loss = 3.55 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 23:13:49.669371: step 105340, loss = 3.79 (281.0 examples/sec; 0.456 sec/batch)
2016-07-15 23:13:54.491218: step 105350, loss = 3.54 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 23:14:00.655258: step 105360, loss = 3.58 (200.9 examples/sec; 0.637 sec/batch)
2016-07-15 23:14:06.222641: step 105370, loss = 3.51 (259.4 examples/sec; 0.493 sec/batch)
2016-07-15 23:14:10.944268: step 105380, loss = 3.66 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 23:14:15.505380: step 105390, loss = 3.81 (282.3 examples/sec; 0.453 sec/batch)
2016-07-15 23:14:20.286231: step 105400, loss = 3.56 (220.7 examples/sec; 0.580 sec/batch)
2016-07-15 23:14:27.200933: step 105410, loss = 3.53 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 23:14:33.015022: step 105420, loss = 3.46 (250.0 examples/sec; 0.512 sec/batch)
2016-07-15 23:14:38.467922: step 105430, loss = 3.40 (204.8 examples/sec; 0.625 sec/batch)
2016-07-15 23:14:43.653275: step 105440, loss = 3.47 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 23:14:49.401334: step 105450, loss = 3.49 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 23:14:54.244316: step 105460, loss = 3.56 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 23:14:59.021455: step 105470, loss = 3.66 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 23:15:03.727418: step 105480, loss = 3.53 (279.7 examples/sec; 0.458 sec/batch)
2016-07-15 23:15:08.893131: step 105490, loss = 3.52 (191.9 examples/sec; 0.667 sec/batch)
2016-07-15 23:15:15.402851: step 105500, loss = 3.60 (203.6 examples/sec; 0.629 sec/batch)
2016-07-15 23:15:21.340237: step 105510, loss = 3.60 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 23:15:26.071328: step 105520, loss = 3.53 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 23:15:32.098325: step 105530, loss = 3.31 (191.6 examples/sec; 0.668 sec/batch)
2016-07-15 23:15:37.823544: step 105540, loss = 3.50 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 23:15:43.454775: step 105550, loss = 3.79 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 23:15:48.457455: step 105560, loss = 3.71 (278.0 examples/sec; 0.460 sec/batch)
2016-07-15 23:15:53.201617: step 105570, loss = 3.53 (254.9 examples/sec; 0.502 sec/batch)
2016-07-15 23:15:57.966608: step 105580, loss = 3.36 (279.2 examples/sec; 0.459 sec/batch)
2016-07-15 23:16:02.740445: step 105590, loss = 3.91 (258.3 examples/sec; 0.496 sec/batch)
2016-07-15 23:16:08.943026: step 105600, loss = 3.67 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 23:16:15.804245: step 105610, loss = 3.60 (240.3 examples/sec; 0.533 sec/batch)
2016-07-15 23:16:20.518771: step 105620, loss = 3.66 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 23:16:25.302482: step 105630, loss = 3.70 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 23:16:30.054314: step 105640, loss = 3.39 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 23:16:34.828384: step 105650, loss = 3.46 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 23:16:39.660872: step 105660, loss = 3.83 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 23:16:44.351885: step 105670, loss = 3.49 (287.1 examples/sec; 0.446 sec/batch)
2016-07-15 23:16:48.970105: step 105680, loss = 3.54 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 23:16:54.131899: step 105690, loss = 3.82 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 23:16:59.400863: step 105700, loss = 3.54 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 23:17:05.105620: step 105710, loss = 3.72 (261.3 examples/sec; 0.490 sec/batch)
2016-07-15 23:17:09.954497: step 105720, loss = 3.45 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 23:17:14.743152: step 105730, loss = 3.44 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 23:17:19.469329: step 105740, loss = 3.39 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 23:17:24.483872: step 105750, loss = 3.53 (206.2 examples/sec; 0.621 sec/batch)
2016-07-15 23:17:30.968991: step 105760, loss = 3.41 (203.2 examples/sec; 0.630 sec/batch)
2016-07-15 23:17:36.112511: step 105770, loss = 3.56 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 23:17:41.860942: step 105780, loss = 3.69 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 23:17:47.504560: step 105790, loss = 3.54 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 23:17:52.500905: step 105800, loss = 3.58 (273.2 examples/sec; 0.468 sec/batch)
2016-07-15 23:17:58.105857: step 105810, loss = 3.24 (273.0 examples/sec; 0.469 sec/batch)
2016-07-15 23:18:02.729584: step 105820, loss = 3.43 (279.3 examples/sec; 0.458 sec/batch)
2016-07-15 23:18:08.391766: step 105830, loss = 3.47 (237.5 examples/sec; 0.539 sec/batch)
2016-07-15 23:18:13.646300: step 105840, loss = 3.56 (204.3 examples/sec; 0.626 sec/batch)
2016-07-15 23:18:19.096700: step 105850, loss = 3.33 (244.3 examples/sec; 0.524 sec/batch)
2016-07-15 23:18:23.782564: step 105860, loss = 3.52 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 23:18:28.863531: step 105870, loss = 3.55 (181.4 examples/sec; 0.706 sec/batch)
2016-07-15 23:18:35.375918: step 105880, loss = 3.51 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 23:18:40.506705: step 105890, loss = 3.52 (217.2 examples/sec; 0.589 sec/batch)
2016-07-15 23:18:46.188235: step 105900, loss = 3.49 (263.3 examples/sec; 0.486 sec/batch)
2016-07-15 23:18:53.027792: step 105910, loss = 3.39 (242.5 examples/sec; 0.528 sec/batch)
2016-07-15 23:18:57.882559: step 105920, loss = 3.40 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 23:19:02.657607: step 105930, loss = 3.43 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 23:19:07.432446: step 105940, loss = 3.36 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 23:19:12.284209: step 105950, loss = 3.70 (254.3 examples/sec; 0.503 sec/batch)
2016-07-15 23:19:18.860019: step 105960, loss = 3.66 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 23:19:23.824104: step 105970, loss = 3.38 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 23:19:28.615980: step 105980, loss = 3.34 (214.9 examples/sec; 0.596 sec/batch)
2016-07-15 23:19:34.288120: step 105990, loss = 3.58 (260.7 examples/sec; 0.491 sec/batch)
2016-07-15 23:19:39.993489: step 106000, loss = 3.54 (207.3 examples/sec; 0.617 sec/batch)
2016-07-15 23:19:45.950571: step 106010, loss = 3.67 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 23:19:50.819502: step 106020, loss = 3.59 (262.3 examples/sec; 0.488 sec/batch)
2016-07-15 23:19:57.035312: step 106030, loss = 3.73 (205.9 examples/sec; 0.622 sec/batch)
2016-07-15 23:20:02.456146: step 106040, loss = 3.59 (284.4 examples/sec; 0.450 sec/batch)
2016-07-15 23:20:07.059461: step 106050, loss = 3.46 (280.5 examples/sec; 0.456 sec/batch)
2016-07-15 23:20:12.822995: step 106060, loss = 3.57 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 23:20:17.656794: step 106070, loss = 3.43 (273.1 examples/sec; 0.469 sec/batch)
2016-07-15 23:20:22.452751: step 106080, loss = 3.55 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 23:20:28.508239: step 106090, loss = 3.62 (194.5 examples/sec; 0.658 sec/batch)
2016-07-15 23:20:34.288094: step 106100, loss = 3.67 (247.8 examples/sec; 0.516 sec/batch)
2016-07-15 23:20:39.962544: step 106110, loss = 3.75 (276.1 examples/sec; 0.464 sec/batch)
2016-07-15 23:20:45.048829: step 106120, loss = 3.45 (188.7 examples/sec; 0.678 sec/batch)
2016-07-15 23:20:51.554077: step 106130, loss = 3.61 (206.9 examples/sec; 0.619 sec/batch)
2016-07-15 23:20:56.547141: step 106140, loss = 3.43 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 23:21:01.262144: step 106150, loss = 3.62 (259.1 examples/sec; 0.494 sec/batch)
2016-07-15 23:21:06.049153: step 106160, loss = 3.43 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 23:21:10.711907: step 106170, loss = 3.49 (282.3 examples/sec; 0.453 sec/batch)
2016-07-15 23:21:15.370280: step 106180, loss = 3.35 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 23:21:21.097348: step 106190, loss = 3.30 (252.1 examples/sec; 0.508 sec/batch)
2016-07-15 23:21:26.448506: step 106200, loss = 3.41 (201.8 examples/sec; 0.634 sec/batch)
2016-07-15 23:21:33.218783: step 106210, loss = 3.46 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 23:21:38.782422: step 106220, loss = 3.55 (267.6 examples/sec; 0.478 sec/batch)
2016-07-15 23:21:43.491783: step 106230, loss = 3.66 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 23:21:48.306684: step 106240, loss = 3.42 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 23:21:53.050529: step 106250, loss = 3.42 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 23:21:58.649589: step 106260, loss = 3.46 (187.6 examples/sec; 0.682 sec/batch)
2016-07-15 23:22:04.910314: step 106270, loss = 3.47 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 23:22:09.741876: step 106280, loss = 3.49 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 23:22:14.496539: step 106290, loss = 3.56 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 23:22:20.534304: step 106300, loss = 3.91 (195.2 examples/sec; 0.656 sec/batch)
2016-07-15 23:22:27.573795: step 106310, loss = 3.71 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 23:22:33.371480: step 106320, loss = 3.73 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 23:22:38.134948: step 106330, loss = 3.91 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 23:22:42.908039: step 106340, loss = 3.33 (263.5 examples/sec; 0.486 sec/batch)
2016-07-15 23:22:47.637754: step 106350, loss = 3.36 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 23:22:52.481362: step 106360, loss = 3.76 (266.8 examples/sec; 0.480 sec/batch)
2016-07-15 23:22:57.217448: step 106370, loss = 3.79 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 23:23:02.832847: step 106380, loss = 3.49 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 23:23:09.012330: step 106390, loss = 3.57 (264.2 examples/sec; 0.485 sec/batch)
2016-07-15 23:23:13.819752: step 106400, loss = 3.57 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 23:23:19.509467: step 106410, loss = 3.77 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 23:23:24.330679: step 106420, loss = 3.44 (274.4 examples/sec; 0.466 sec/batch)
2016-07-15 23:23:28.986152: step 106430, loss = 3.35 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 23:23:34.056353: step 106440, loss = 3.69 (208.1 examples/sec; 0.615 sec/batch)
2016-07-15 23:23:39.510772: step 106450, loss = 3.44 (267.4 examples/sec; 0.479 sec/batch)
2016-07-15 23:23:45.251500: step 106460, loss = 3.39 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 23:23:50.616475: step 106470, loss = 3.67 (205.8 examples/sec; 0.622 sec/batch)
2016-07-15 23:23:55.928893: step 106480, loss = 3.75 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 23:24:00.685147: step 106490, loss = 3.55 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 23:24:05.489770: step 106500, loss = 3.69 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 23:24:11.122751: step 106510, loss = 3.49 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 23:24:15.794051: step 106520, loss = 3.65 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 23:24:21.538470: step 106530, loss = 3.30 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 23:24:26.307012: step 106540, loss = 3.67 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 23:24:31.059857: step 106550, loss = 3.71 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 23:24:37.254607: step 106560, loss = 3.44 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 23:24:42.836479: step 106570, loss = 3.59 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 23:24:47.532153: step 106580, loss = 3.66 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 23:24:52.369838: step 106590, loss = 3.76 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 23:24:57.113128: step 106600, loss = 3.69 (260.2 examples/sec; 0.492 sec/batch)
2016-07-15 23:25:02.973562: step 106610, loss = 3.38 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 23:25:07.861454: step 106620, loss = 3.62 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 23:25:12.547596: step 106630, loss = 3.59 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 23:25:17.442338: step 106640, loss = 3.64 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 23:25:22.265091: step 106650, loss = 3.62 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 23:25:28.245144: step 106660, loss = 3.54 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 23:25:34.112062: step 106670, loss = 3.57 (260.3 examples/sec; 0.492 sec/batch)
2016-07-15 23:25:39.700273: step 106680, loss = 3.63 (205.5 examples/sec; 0.623 sec/batch)
2016-07-15 23:25:44.902948: step 106690, loss = 3.60 (221.6 examples/sec; 0.578 sec/batch)
2016-07-15 23:25:50.581442: step 106700, loss = 3.27 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 23:25:56.409503: step 106710, loss = 3.74 (270.7 examples/sec; 0.473 sec/batch)
2016-07-15 23:26:01.589673: step 106720, loss = 3.64 (185.8 examples/sec; 0.689 sec/batch)
2016-07-15 23:26:08.096177: step 106730, loss = 3.54 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 23:26:13.140829: step 106740, loss = 3.38 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 23:26:17.919868: step 106750, loss = 3.89 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 23:26:23.719324: step 106760, loss = 3.59 (188.9 examples/sec; 0.677 sec/batch)
2016-07-15 23:26:29.688330: step 106770, loss = 3.67 (257.0 examples/sec; 0.498 sec/batch)
2016-07-15 23:26:35.159270: step 106780, loss = 3.69 (202.4 examples/sec; 0.632 sec/batch)
2016-07-15 23:26:40.380746: step 106790, loss = 3.53 (263.2 examples/sec; 0.486 sec/batch)
2016-07-15 23:26:45.134299: step 106800, loss = 3.57 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 23:26:52.062491: step 106810, loss = 3.42 (189.9 examples/sec; 0.674 sec/batch)
2016-07-15 23:26:57.935888: step 106820, loss = 3.81 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 23:27:02.748224: step 106830, loss = 3.50 (281.0 examples/sec; 0.456 sec/batch)
2016-07-15 23:27:07.577674: step 106840, loss = 3.57 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 23:27:12.307446: step 106850, loss = 3.86 (267.3 examples/sec; 0.479 sec/batch)
2016-07-15 23:27:17.199986: step 106860, loss = 3.53 (268.7 examples/sec; 0.476 sec/batch)
2016-07-15 23:27:21.945008: step 106870, loss = 3.61 (266.7 examples/sec; 0.480 sec/batch)
2016-07-15 23:27:27.650631: step 106880, loss = 3.45 (190.7 examples/sec; 0.671 sec/batch)
2016-07-15 23:27:33.709714: step 106890, loss = 3.43 (256.2 examples/sec; 0.500 sec/batch)
2016-07-15 23:27:38.527253: step 106900, loss = 3.55 (265.0 examples/sec; 0.483 sec/batch)
2016-07-15 23:27:44.400437: step 106910, loss = 3.44 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 23:27:50.905187: step 106920, loss = 3.53 (202.1 examples/sec; 0.633 sec/batch)
2016-07-15 23:27:56.212659: step 106930, loss = 3.58 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 23:28:01.945656: step 106940, loss = 3.66 (271.6 examples/sec; 0.471 sec/batch)
2016-07-15 23:28:07.482216: step 106950, loss = 3.61 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 23:28:12.647004: step 106960, loss = 3.44 (253.7 examples/sec; 0.505 sec/batch)
2016-07-15 23:28:17.343401: step 106970, loss = 3.79 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 23:28:22.789101: step 106980, loss = 3.48 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 23:28:29.057004: step 106990, loss = 3.28 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 23:28:33.917665: step 107000, loss = 3.77 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 23:28:39.665187: step 107010, loss = 3.31 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 23:28:44.373156: step 107020, loss = 3.48 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 23:28:49.271317: step 107030, loss = 3.47 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 23:28:54.006997: step 107040, loss = 3.47 (263.0 examples/sec; 0.487 sec/batch)
2016-07-15 23:28:58.771135: step 107050, loss = 3.43 (282.7 examples/sec; 0.453 sec/batch)
2016-07-15 23:29:03.395555: step 107060, loss = 3.44 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 23:29:08.002213: step 107070, loss = 3.55 (279.6 examples/sec; 0.458 sec/batch)
2016-07-15 23:29:13.774139: step 107080, loss = 3.68 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 23:29:18.601603: step 107090, loss = 3.49 (278.1 examples/sec; 0.460 sec/batch)
2016-07-15 23:29:23.423938: step 107100, loss = 3.53 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 23:29:31.022485: step 107110, loss = 3.67 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 23:29:36.264649: step 107120, loss = 3.67 (266.1 examples/sec; 0.481 sec/batch)
2016-07-15 23:29:40.988219: step 107130, loss = 3.46 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 23:29:46.274100: step 107140, loss = 3.34 (191.0 examples/sec; 0.670 sec/batch)
2016-07-15 23:29:52.753540: step 107150, loss = 3.43 (203.1 examples/sec; 0.630 sec/batch)
2016-07-15 23:29:57.600904: step 107160, loss = 3.49 (282.8 examples/sec; 0.453 sec/batch)
2016-07-15 23:30:02.361058: step 107170, loss = 3.65 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 23:30:08.245515: step 107180, loss = 3.71 (190.4 examples/sec; 0.672 sec/batch)
2016-07-15 23:30:13.070062: step 107190, loss = 3.45 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 23:30:18.107651: step 107200, loss = 3.56 (203.5 examples/sec; 0.629 sec/batch)
2016-07-15 23:30:24.795330: step 107210, loss = 3.58 (264.4 examples/sec; 0.484 sec/batch)
2016-07-15 23:30:29.509219: step 107220, loss = 3.51 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 23:30:34.301208: step 107230, loss = 3.49 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 23:30:38.936691: step 107240, loss = 3.53 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 23:30:43.626822: step 107250, loss = 3.63 (282.0 examples/sec; 0.454 sec/batch)
2016-07-15 23:30:49.349364: step 107260, loss = 3.58 (256.9 examples/sec; 0.498 sec/batch)
2016-07-15 23:30:54.157423: step 107270, loss = 3.98 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 23:30:58.962032: step 107280, loss = 3.43 (264.5 examples/sec; 0.484 sec/batch)
2016-07-15 23:31:04.967833: step 107290, loss = 3.78 (195.8 examples/sec; 0.654 sec/batch)
2016-07-15 23:31:10.561356: step 107300, loss = 3.44 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 23:31:16.175361: step 107310, loss = 3.61 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 23:31:21.932635: step 107320, loss = 3.46 (264.7 examples/sec; 0.483 sec/batch)
2016-07-15 23:31:27.447475: step 107330, loss = 3.47 (200.2 examples/sec; 0.639 sec/batch)
2016-07-15 23:31:32.587478: step 107340, loss = 3.36 (265.4 examples/sec; 0.482 sec/batch)
2016-07-15 23:31:38.358888: step 107350, loss = 3.27 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 23:31:43.141331: step 107360, loss = 3.39 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 23:31:47.949075: step 107370, loss = 3.55 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 23:31:54.249556: step 107380, loss = 3.38 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 23:31:59.741671: step 107390, loss = 3.65 (258.9 examples/sec; 0.494 sec/batch)
2016-07-15 23:32:04.453393: step 107400, loss = 3.64 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 23:32:10.256583: step 107410, loss = 3.59 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 23:32:15.075084: step 107420, loss = 3.47 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 23:32:21.200648: step 107430, loss = 3.48 (194.3 examples/sec; 0.659 sec/batch)
2016-07-15 23:32:26.885873: step 107440, loss = 3.48 (269.1 examples/sec; 0.476 sec/batch)
2016-07-15 23:32:31.655280: step 107450, loss = 3.55 (273.8 examples/sec; 0.467 sec/batch)
2016-07-15 23:32:36.511020: step 107460, loss = 3.90 (261.4 examples/sec; 0.490 sec/batch)
2016-07-15 23:32:42.948078: step 107470, loss = 3.43 (210.3 examples/sec; 0.609 sec/batch)
2016-07-15 23:32:48.252610: step 107480, loss = 3.69 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 23:32:52.987335: step 107490, loss = 3.38 (271.3 examples/sec; 0.472 sec/batch)
2016-07-15 23:32:57.821664: step 107500, loss = 3.51 (259.8 examples/sec; 0.493 sec/batch)
2016-07-15 23:33:03.632448: step 107510, loss = 3.36 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 23:33:09.834072: step 107520, loss = 3.60 (207.0 examples/sec; 0.618 sec/batch)
2016-07-15 23:33:15.398091: step 107530, loss = 3.88 (259.0 examples/sec; 0.494 sec/batch)
2016-07-15 23:33:20.150920: step 107540, loss = 3.58 (274.1 examples/sec; 0.467 sec/batch)
2016-07-15 23:33:24.984775: step 107550, loss = 3.50 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 23:33:31.559636: step 107560, loss = 3.57 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 23:33:36.739756: step 107570, loss = 3.62 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 23:33:41.471829: step 107580, loss = 3.37 (253.5 examples/sec; 0.505 sec/batch)
2016-07-15 23:33:46.296335: step 107590, loss = 3.49 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 23:33:51.097132: step 107600, loss = 3.34 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 23:33:56.892013: step 107610, loss = 3.38 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 23:34:01.543833: step 107620, loss = 3.32 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 23:34:06.626023: step 107630, loss = 3.55 (204.3 examples/sec; 0.627 sec/batch)
2016-07-15 23:34:12.035158: step 107640, loss = 3.72 (259.3 examples/sec; 0.494 sec/batch)
2016-07-15 23:34:17.787296: step 107650, loss = 3.51 (261.0 examples/sec; 0.490 sec/batch)
2016-07-15 23:34:23.150202: step 107660, loss = 3.58 (209.0 examples/sec; 0.613 sec/batch)
2016-07-15 23:34:28.467092: step 107670, loss = 3.43 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 23:34:33.136661: step 107680, loss = 3.66 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 23:34:38.413301: step 107690, loss = 3.46 (188.8 examples/sec; 0.678 sec/batch)
2016-07-15 23:34:44.871358: step 107700, loss = 3.38 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 23:34:50.735110: step 107710, loss = 3.56 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 23:34:55.479449: step 107720, loss = 3.41 (266.6 examples/sec; 0.480 sec/batch)
2016-07-15 23:35:00.239995: step 107730, loss = 3.42 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 23:35:04.857470: step 107740, loss = 3.48 (281.3 examples/sec; 0.455 sec/batch)
2016-07-15 23:35:09.731052: step 107750, loss = 3.63 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 23:35:15.361088: step 107760, loss = 3.54 (251.6 examples/sec; 0.509 sec/batch)
2016-07-15 23:35:20.188744: step 107770, loss = 3.45 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 23:35:25.043090: step 107780, loss = 3.78 (264.6 examples/sec; 0.484 sec/batch)
2016-07-15 23:35:29.742416: step 107790, loss = 3.45 (263.1 examples/sec; 0.486 sec/batch)
2016-07-15 23:35:34.575007: step 107800, loss = 3.66 (273.2 examples/sec; 0.468 sec/batch)
2016-07-15 23:35:40.351643: step 107810, loss = 3.59 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 23:35:45.082457: step 107820, loss = 3.61 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 23:35:49.891246: step 107830, loss = 3.49 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 23:35:54.704470: step 107840, loss = 3.55 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 23:36:00.610253: step 107850, loss = 3.55 (186.2 examples/sec; 0.687 sec/batch)
2016-07-15 23:36:06.511658: step 107860, loss = 3.31 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 23:36:11.367860: step 107870, loss = 3.55 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 23:36:16.161535: step 107880, loss = 3.60 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 23:36:22.424630: step 107890, loss = 3.47 (209.1 examples/sec; 0.612 sec/batch)
2016-07-15 23:36:27.911659: step 107900, loss = 3.62 (266.9 examples/sec; 0.480 sec/batch)
2016-07-15 23:36:33.644758: step 107910, loss = 3.75 (287.4 examples/sec; 0.445 sec/batch)
2016-07-15 23:36:38.286374: step 107920, loss = 3.47 (277.3 examples/sec; 0.462 sec/batch)
2016-07-15 23:36:42.934141: step 107930, loss = 3.59 (280.8 examples/sec; 0.456 sec/batch)
2016-07-15 23:36:47.626846: step 107940, loss = 3.60 (265.9 examples/sec; 0.481 sec/batch)
2016-07-15 23:36:52.231169: step 107950, loss = 3.31 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 23:36:56.877329: step 107960, loss = 3.58 (270.1 examples/sec; 0.474 sec/batch)
2016-07-15 23:37:02.649565: step 107970, loss = 3.50 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 23:37:07.518335: step 107980, loss = 3.47 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 23:37:12.302302: step 107990, loss = 3.54 (267.1 examples/sec; 0.479 sec/batch)
2016-07-15 23:37:16.981124: step 108000, loss = 3.65 (275.2 examples/sec; 0.465 sec/batch)
2016-07-15 23:37:22.877224: step 108010, loss = 3.85 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 23:37:27.632588: step 108020, loss = 3.54 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 23:37:32.380973: step 108030, loss = 3.56 (274.9 examples/sec; 0.466 sec/batch)
2016-07-15 23:37:37.198047: step 108040, loss = 3.41 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 23:37:43.780151: step 108050, loss = 3.31 (208.6 examples/sec; 0.614 sec/batch)
2016-07-15 23:37:48.930332: step 108060, loss = 3.49 (268.4 examples/sec; 0.477 sec/batch)
2016-07-15 23:37:54.720522: step 108070, loss = 3.37 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 23:37:59.503597: step 108080, loss = 3.34 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 23:38:04.350223: step 108090, loss = 3.50 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 23:38:09.065087: step 108100, loss = 3.53 (279.4 examples/sec; 0.458 sec/batch)
2016-07-15 23:38:14.707174: step 108110, loss = 3.54 (268.0 examples/sec; 0.478 sec/batch)
2016-07-15 23:38:20.159839: step 108120, loss = 3.35 (200.7 examples/sec; 0.638 sec/batch)
2016-07-15 23:38:25.337125: step 108130, loss = 3.64 (243.9 examples/sec; 0.525 sec/batch)
2016-07-15 23:38:31.058845: step 108140, loss = 3.50 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 23:38:36.708638: step 108150, loss = 3.32 (202.9 examples/sec; 0.631 sec/batch)
2016-07-15 23:38:41.728302: step 108160, loss = 3.61 (275.3 examples/sec; 0.465 sec/batch)
2016-07-15 23:38:46.515965: step 108170, loss = 3.50 (244.7 examples/sec; 0.523 sec/batch)
2016-07-15 23:38:52.221342: step 108180, loss = 3.62 (190.6 examples/sec; 0.672 sec/batch)
2016-07-15 23:38:58.295020: step 108190, loss = 3.66 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 23:39:03.069172: step 108200, loss = 3.43 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 23:39:08.843391: step 108210, loss = 3.47 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 23:39:15.292096: step 108220, loss = 3.40 (200.4 examples/sec; 0.639 sec/batch)
2016-07-15 23:39:20.406970: step 108230, loss = 3.43 (283.3 examples/sec; 0.452 sec/batch)
2016-07-15 23:39:25.056845: step 108240, loss = 3.57 (277.1 examples/sec; 0.462 sec/batch)
2016-07-15 23:39:30.817297: step 108250, loss = 3.45 (266.0 examples/sec; 0.481 sec/batch)
2016-07-15 23:39:35.648778: step 108260, loss = 3.46 (264.0 examples/sec; 0.485 sec/batch)
2016-07-15 23:39:40.439676: step 108270, loss = 3.61 (267.5 examples/sec; 0.478 sec/batch)
2016-07-15 23:39:45.169983: step 108280, loss = 3.60 (271.7 examples/sec; 0.471 sec/batch)
2016-07-15 23:39:50.338888: step 108290, loss = 3.49 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 23:39:56.844729: step 108300, loss = 3.28 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 23:40:02.795731: step 108310, loss = 3.55 (269.4 examples/sec; 0.475 sec/batch)
2016-07-15 23:40:07.432107: step 108320, loss = 3.63 (271.0 examples/sec; 0.472 sec/batch)
2016-07-15 23:40:12.166863: step 108330, loss = 3.62 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 23:40:17.921191: step 108340, loss = 3.68 (259.2 examples/sec; 0.494 sec/batch)
2016-07-15 23:40:22.718661: step 108350, loss = 3.43 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 23:40:27.487169: step 108360, loss = 3.52 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 23:40:33.582348: step 108370, loss = 3.36 (192.9 examples/sec; 0.663 sec/batch)
2016-07-15 23:40:39.343093: step 108380, loss = 3.50 (245.9 examples/sec; 0.521 sec/batch)
2016-07-15 23:40:44.115710: step 108390, loss = 3.43 (273.9 examples/sec; 0.467 sec/batch)
2016-07-15 23:40:48.962918: step 108400, loss = 3.60 (258.5 examples/sec; 0.495 sec/batch)
2016-07-15 23:40:56.994205: step 108410, loss = 3.70 (200.8 examples/sec; 0.637 sec/batch)
2016-07-15 23:41:01.986252: step 108420, loss = 3.49 (277.2 examples/sec; 0.462 sec/batch)
2016-07-15 23:41:06.609165: step 108430, loss = 3.92 (281.9 examples/sec; 0.454 sec/batch)
2016-07-15 23:41:11.261026: step 108440, loss = 3.50 (275.8 examples/sec; 0.464 sec/batch)
2016-07-15 23:41:16.776014: step 108450, loss = 3.92 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 23:41:21.792717: step 108460, loss = 3.59 (279.8 examples/sec; 0.458 sec/batch)
2016-07-15 23:41:26.582876: step 108470, loss = 3.71 (257.8 examples/sec; 0.496 sec/batch)
2016-07-15 23:41:31.361407: step 108480, loss = 3.84 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 23:41:36.150253: step 108490, loss = 3.24 (261.7 examples/sec; 0.489 sec/batch)
2016-07-15 23:41:42.455017: step 108500, loss = 3.31 (207.5 examples/sec; 0.617 sec/batch)
2016-07-15 23:41:49.205982: step 108510, loss = 3.46 (260.4 examples/sec; 0.492 sec/batch)
2016-07-15 23:41:53.991277: step 108520, loss = 3.38 (260.9 examples/sec; 0.491 sec/batch)
2016-07-15 23:41:58.810578: step 108530, loss = 3.34 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 23:42:03.555119: step 108540, loss = 3.31 (262.4 examples/sec; 0.488 sec/batch)
2016-07-15 23:42:08.371033: step 108550, loss = 3.59 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 23:42:13.035191: step 108560, loss = 3.39 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 23:42:17.747428: step 108570, loss = 3.33 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 23:42:22.371731: step 108580, loss = 3.43 (275.9 examples/sec; 0.464 sec/batch)
2016-07-15 23:42:27.690937: step 108590, loss = 3.69 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 23:42:32.985377: step 108600, loss = 3.39 (257.9 examples/sec; 0.496 sec/batch)
2016-07-15 23:42:39.851625: step 108610, loss = 3.50 (264.1 examples/sec; 0.485 sec/batch)
2016-07-15 23:42:44.594717: step 108620, loss = 3.87 (275.6 examples/sec; 0.464 sec/batch)
2016-07-15 23:42:49.441031: step 108630, loss = 3.29 (263.4 examples/sec; 0.486 sec/batch)
2016-07-15 23:42:54.149210: step 108640, loss = 3.63 (267.2 examples/sec; 0.479 sec/batch)
2016-07-15 23:42:58.980570: step 108650, loss = 3.27 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 23:43:03.753437: step 108660, loss = 3.47 (267.9 examples/sec; 0.478 sec/batch)
2016-07-15 23:43:09.848226: step 108670, loss = 3.45 (200.0 examples/sec; 0.640 sec/batch)
2016-07-15 23:43:15.520784: step 108680, loss = 3.58 (265.5 examples/sec; 0.482 sec/batch)
2016-07-15 23:43:20.254187: step 108690, loss = 3.44 (276.0 examples/sec; 0.464 sec/batch)
2016-07-15 23:43:25.049126: step 108700, loss = 3.36 (265.7 examples/sec; 0.482 sec/batch)
2016-07-15 23:43:33.053576: step 108710, loss = 3.54 (206.4 examples/sec; 0.620 sec/batch)
2016-07-15 23:43:38.194474: step 108720, loss = 3.39 (204.1 examples/sec; 0.627 sec/batch)
2016-07-15 23:43:43.768782: step 108730, loss = 3.42 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 23:43:48.530099: step 108740, loss = 3.34 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 23:43:53.233973: step 108750, loss = 3.53 (281.9 examples/sec; 0.454 sec/batch)
2016-07-15 23:43:57.913446: step 108760, loss = 3.43 (269.8 examples/sec; 0.475 sec/batch)
2016-07-15 23:44:02.572970: step 108770, loss = 3.37 (272.3 examples/sec; 0.470 sec/batch)
2016-07-15 23:44:07.846554: step 108780, loss = 3.44 (205.6 examples/sec; 0.623 sec/batch)
2016-07-15 23:44:13.084733: step 108790, loss = 3.30 (264.3 examples/sec; 0.484 sec/batch)
2016-07-15 23:44:18.832792: step 108800, loss = 3.65 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 23:44:24.740988: step 108810, loss = 3.44 (270.9 examples/sec; 0.473 sec/batch)
2016-07-15 23:44:29.618007: step 108820, loss = 3.50 (251.4 examples/sec; 0.509 sec/batch)
2016-07-15 23:44:36.206645: step 108830, loss = 3.34 (206.6 examples/sec; 0.620 sec/batch)
2016-07-15 23:44:41.327807: step 108840, loss = 3.66 (262.8 examples/sec; 0.487 sec/batch)
2016-07-15 23:44:45.949184: step 108850, loss = 3.50 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 23:44:50.554541: step 108860, loss = 3.49 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 23:44:55.790199: step 108870, loss = 3.54 (206.8 examples/sec; 0.619 sec/batch)
2016-07-15 23:45:01.166141: step 108880, loss = 3.59 (263.1 examples/sec; 0.487 sec/batch)
2016-07-15 23:45:05.868230: step 108890, loss = 3.68 (266.2 examples/sec; 0.481 sec/batch)
2016-07-15 23:45:11.334394: step 108900, loss = 3.65 (183.0 examples/sec; 0.699 sec/batch)
2016-07-15 23:45:18.905188: step 108910, loss = 3.41 (277.8 examples/sec; 0.461 sec/batch)
2016-07-15 23:45:23.565766: step 108920, loss = 3.40 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 23:45:29.178233: step 108930, loss = 3.42 (206.7 examples/sec; 0.619 sec/batch)
2016-07-15 23:45:34.370801: step 108940, loss = 3.67 (201.3 examples/sec; 0.636 sec/batch)
2016-07-15 23:45:39.934360: step 108950, loss = 3.56 (262.7 examples/sec; 0.487 sec/batch)
2016-07-15 23:45:44.733717: step 108960, loss = 3.64 (275.5 examples/sec; 0.465 sec/batch)
2016-07-15 23:45:49.614949: step 108970, loss = 3.64 (248.0 examples/sec; 0.516 sec/batch)
2016-07-15 23:45:54.361220: step 108980, loss = 3.48 (261.5 examples/sec; 0.490 sec/batch)
2016-07-15 23:45:59.887665: step 108990, loss = 3.61 (191.3 examples/sec; 0.669 sec/batch)
2016-07-15 23:46:06.160475: step 109000, loss = 3.54 (255.8 examples/sec; 0.500 sec/batch)
2016-07-15 23:46:11.955929: step 109010, loss = 3.49 (281.1 examples/sec; 0.455 sec/batch)
2016-07-15 23:46:16.774183: step 109020, loss = 3.38 (253.6 examples/sec; 0.505 sec/batch)
2016-07-15 23:46:21.455793: step 109030, loss = 3.58 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 23:46:26.596116: step 109040, loss = 3.48 (191.8 examples/sec; 0.667 sec/batch)
2016-07-15 23:46:33.055125: step 109050, loss = 3.46 (208.9 examples/sec; 0.613 sec/batch)
2016-07-15 23:46:38.032729: step 109060, loss = 3.58 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 23:46:42.785100: step 109070, loss = 3.64 (259.9 examples/sec; 0.492 sec/batch)
2016-07-15 23:46:48.435011: step 109080, loss = 3.54 (190.2 examples/sec; 0.673 sec/batch)
2016-07-15 23:46:54.500136: step 109090, loss = 3.28 (263.8 examples/sec; 0.485 sec/batch)
2016-07-15 23:46:59.865499: step 109100, loss = 3.51 (203.3 examples/sec; 0.630 sec/batch)
2016-07-15 23:47:06.127177: step 109110, loss = 3.36 (280.0 examples/sec; 0.457 sec/batch)
2016-07-15 23:47:10.719366: step 109120, loss = 3.56 (281.2 examples/sec; 0.455 sec/batch)
2016-07-15 23:47:15.330794: step 109130, loss = 3.53 (272.0 examples/sec; 0.471 sec/batch)
2016-07-15 23:47:20.006971: step 109140, loss = 3.43 (270.9 examples/sec; 0.472 sec/batch)
2016-07-15 23:47:24.653271: step 109150, loss = 3.46 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 23:47:29.309046: step 109160, loss = 3.55 (274.2 examples/sec; 0.467 sec/batch)
2016-07-15 23:47:34.108821: step 109170, loss = 3.13 (217.6 examples/sec; 0.588 sec/batch)
2016-07-15 23:47:39.787536: step 109180, loss = 3.45 (254.0 examples/sec; 0.504 sec/batch)
2016-07-15 23:47:44.536746: step 109190, loss = 3.54 (276.7 examples/sec; 0.463 sec/batch)
2016-07-15 23:47:49.345955: step 109200, loss = 3.38 (258.4 examples/sec; 0.495 sec/batch)
2016-07-15 23:47:55.038884: step 109210, loss = 3.39 (257.3 examples/sec; 0.497 sec/batch)
2016-07-15 23:47:59.879998: step 109220, loss = 3.58 (273.7 examples/sec; 0.468 sec/batch)
2016-07-15 23:48:04.701464: step 109230, loss = 3.38 (256.8 examples/sec; 0.498 sec/batch)
2016-07-15 23:48:09.426364: step 109240, loss = 3.44 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 23:48:14.508148: step 109250, loss = 3.48 (182.2 examples/sec; 0.702 sec/batch)
2016-07-15 23:48:21.039795: step 109260, loss = 3.47 (194.7 examples/sec; 0.658 sec/batch)
2016-07-15 23:48:25.822061: step 109270, loss = 3.29 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 23:48:30.759822: step 109280, loss = 3.57 (201.9 examples/sec; 0.634 sec/batch)
2016-07-15 23:48:36.323563: step 109290, loss = 3.65 (252.0 examples/sec; 0.508 sec/batch)
2016-07-15 23:48:42.072498: step 109300, loss = 3.57 (245.2 examples/sec; 0.522 sec/batch)
2016-07-15 23:48:47.897282: step 109310, loss = 3.49 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 23:48:52.490059: step 109320, loss = 3.66 (278.7 examples/sec; 0.459 sec/batch)
2016-07-15 23:48:57.173606: step 109330, loss = 3.51 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 23:49:02.914573: step 109340, loss = 3.61 (261.8 examples/sec; 0.489 sec/batch)
2016-07-15 23:49:08.350842: step 109350, loss = 3.57 (207.1 examples/sec; 0.618 sec/batch)
2016-07-15 23:49:13.531856: step 109360, loss = 3.34 (260.0 examples/sec; 0.492 sec/batch)
2016-07-15 23:49:19.314198: step 109370, loss = 3.38 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 23:49:24.115446: step 109380, loss = 3.24 (272.9 examples/sec; 0.469 sec/batch)
2016-07-15 23:49:28.874418: step 109390, loss = 3.45 (268.1 examples/sec; 0.477 sec/batch)
2016-07-15 23:49:33.645084: step 109400, loss = 3.46 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 23:49:39.553750: step 109410, loss = 3.40 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 23:49:44.325467: step 109420, loss = 3.38 (265.3 examples/sec; 0.482 sec/batch)
2016-07-15 23:49:49.023433: step 109430, loss = 3.68 (276.3 examples/sec; 0.463 sec/batch)
2016-07-15 23:49:53.895725: step 109440, loss = 3.42 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 23:50:00.434910: step 109450, loss = 3.47 (202.8 examples/sec; 0.631 sec/batch)
2016-07-15 23:50:05.609082: step 109460, loss = 3.17 (261.6 examples/sec; 0.489 sec/batch)
2016-07-15 23:50:10.277721: step 109470, loss = 3.39 (271.9 examples/sec; 0.471 sec/batch)
2016-07-15 23:50:14.875234: step 109480, loss = 3.59 (270.2 examples/sec; 0.474 sec/batch)
2016-07-15 23:50:20.157435: step 109490, loss = 3.60 (204.0 examples/sec; 0.627 sec/batch)
2016-07-15 23:50:25.419417: step 109500, loss = 3.51 (257.6 examples/sec; 0.497 sec/batch)
2016-07-15 23:50:32.308125: step 109510, loss = 3.35 (258.0 examples/sec; 0.496 sec/batch)
2016-07-15 23:50:36.995628: step 109520, loss = 3.49 (264.9 examples/sec; 0.483 sec/batch)
2016-07-15 23:50:41.851683: step 109530, loss = 3.34 (257.1 examples/sec; 0.498 sec/batch)
2016-07-15 23:50:46.555452: step 109540, loss = 3.54 (261.5 examples/sec; 0.489 sec/batch)
2016-07-15 23:50:52.134854: step 109550, loss = 3.30 (191.5 examples/sec; 0.668 sec/batch)
2016-07-15 23:50:58.418155: step 109560, loss = 3.52 (257.2 examples/sec; 0.498 sec/batch)
2016-07-15 23:51:03.215285: step 109570, loss = 3.38 (271.8 examples/sec; 0.471 sec/batch)
2016-07-15 23:51:07.940373: step 109580, loss = 3.55 (268.6 examples/sec; 0.477 sec/batch)
2016-07-15 23:51:12.671164: step 109590, loss = 3.41 (276.2 examples/sec; 0.463 sec/batch)
2016-07-15 23:51:17.511917: step 109600, loss = 3.37 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 23:51:25.535917: step 109610, loss = 3.76 (208.2 examples/sec; 0.615 sec/batch)
2016-07-15 23:51:30.509585: step 109620, loss = 3.65 (268.3 examples/sec; 0.477 sec/batch)
2016-07-15 23:51:35.253429: step 109630, loss = 3.80 (257.5 examples/sec; 0.497 sec/batch)
2016-07-15 23:51:41.000110: step 109640, loss = 3.57 (186.5 examples/sec; 0.686 sec/batch)
2016-07-15 23:51:47.033575: step 109650, loss = 3.31 (256.6 examples/sec; 0.499 sec/batch)
2016-07-15 23:51:51.842022: step 109660, loss = 3.27 (276.4 examples/sec; 0.463 sec/batch)
2016-07-15 23:51:56.637538: step 109670, loss = 3.15 (265.1 examples/sec; 0.483 sec/batch)
2016-07-15 23:52:01.385323: step 109680, loss = 3.28 (279.5 examples/sec; 0.458 sec/batch)
2016-07-15 23:52:05.951955: step 109690, loss = 3.54 (285.8 examples/sec; 0.448 sec/batch)
2016-07-15 23:52:10.881300: step 109700, loss = 3.44 (204.2 examples/sec; 0.627 sec/batch)
2016-07-15 23:52:17.717122: step 109710, loss = 3.48 (259.7 examples/sec; 0.493 sec/batch)
2016-07-15 23:52:23.533305: step 109720, loss = 3.45 (269.0 examples/sec; 0.476 sec/batch)
2016-07-15 23:52:28.299691: step 109730, loss = 3.37 (270.8 examples/sec; 0.473 sec/batch)
2016-07-15 23:52:33.101535: step 109740, loss = 3.55 (262.1 examples/sec; 0.488 sec/batch)
2016-07-15 23:52:37.811637: step 109750, loss = 3.45 (268.2 examples/sec; 0.477 sec/batch)
2016-07-15 23:52:42.951157: step 109760, loss = 3.30 (188.2 examples/sec; 0.680 sec/batch)
2016-07-15 23:52:49.448088: step 109770, loss = 3.33 (205.2 examples/sec; 0.624 sec/batch)
2016-07-15 23:52:54.419945: step 109780, loss = 3.56 (281.0 examples/sec; 0.456 sec/batch)
2016-07-15 23:52:59.163825: step 109790, loss = 3.33 (265.2 examples/sec; 0.483 sec/batch)
2016-07-15 23:53:03.956170: step 109800, loss = 3.43 (273.5 examples/sec; 0.468 sec/batch)
2016-07-15 23:53:09.693746: step 109810, loss = 3.53 (263.6 examples/sec; 0.485 sec/batch)
2016-07-15 23:53:14.438633: step 109820, loss = 3.48 (279.2 examples/sec; 0.458 sec/batch)
2016-07-15 23:53:19.101635: step 109830, loss = 3.33 (278.8 examples/sec; 0.459 sec/batch)
2016-07-15 23:53:23.705956: step 109840, loss = 3.69 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 23:53:28.341933: step 109850, loss = 3.48 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 23:53:34.014830: step 109860, loss = 3.36 (199.1 examples/sec; 0.643 sec/batch)
2016-07-15 23:53:38.841346: step 109870, loss = 3.49 (270.6 examples/sec; 0.473 sec/batch)
2016-07-15 23:53:43.562883: step 109880, loss = 3.58 (256.4 examples/sec; 0.499 sec/batch)
2016-07-15 23:53:48.316111: step 109890, loss = 3.42 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 23:53:53.119099: step 109900, loss = 3.34 (260.5 examples/sec; 0.491 sec/batch)
2016-07-15 23:53:58.858363: step 109910, loss = 3.44 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 23:54:04.380346: step 109920, loss = 3.41 (189.5 examples/sec; 0.675 sec/batch)
2016-07-15 23:54:10.640652: step 109930, loss = 3.44 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 23:54:15.483570: step 109940, loss = 3.32 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 23:54:20.224676: step 109950, loss = 3.39 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 23:54:25.022440: step 109960, loss = 3.70 (268.9 examples/sec; 0.476 sec/batch)
2016-07-15 23:54:29.734335: step 109970, loss = 3.47 (283.1 examples/sec; 0.452 sec/batch)
2016-07-15 23:54:34.322579: step 109980, loss = 3.53 (274.8 examples/sec; 0.466 sec/batch)
2016-07-15 23:54:38.994826: step 109990, loss = 3.46 (279.8 examples/sec; 0.457 sec/batch)
2016-07-15 23:54:43.636549: step 110000, loss = 3.43 (274.6 examples/sec; 0.466 sec/batch)
2016-07-15 23:54:50.294743: step 110010, loss = 3.46 (221.0 examples/sec; 0.579 sec/batch)
2016-07-15 23:54:55.523554: step 110020, loss = 3.48 (204.5 examples/sec; 0.626 sec/batch)
2016-07-15 23:55:00.970340: step 110030, loss = 3.57 (259.6 examples/sec; 0.493 sec/batch)
2016-07-15 23:55:05.673212: step 110040, loss = 3.41 (268.8 examples/sec; 0.476 sec/batch)
2016-07-15 23:55:10.549632: step 110050, loss = 3.67 (277.7 examples/sec; 0.461 sec/batch)
2016-07-15 23:55:15.288549: step 110060, loss = 3.67 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 23:55:20.928038: step 110070, loss = 3.54 (187.9 examples/sec; 0.681 sec/batch)
2016-07-15 23:55:27.033653: step 110080, loss = 3.73 (255.5 examples/sec; 0.501 sec/batch)
2016-07-15 23:55:31.879116: step 110090, loss = 3.47 (272.4 examples/sec; 0.470 sec/batch)
2016-07-15 23:55:36.650851: step 110100, loss = 3.85 (263.7 examples/sec; 0.485 sec/batch)
2016-07-15 23:55:42.337957: step 110110, loss = 3.53 (269.2 examples/sec; 0.475 sec/batch)
2016-07-15 23:55:47.648653: step 110120, loss = 3.64 (189.0 examples/sec; 0.677 sec/batch)
2016-07-15 23:55:54.108941: step 110130, loss = 3.54 (207.3 examples/sec; 0.618 sec/batch)
2016-07-15 23:55:58.934018: step 110140, loss = 3.64 (277.4 examples/sec; 0.461 sec/batch)
2016-07-15 23:56:03.734117: step 110150, loss = 3.51 (245.9 examples/sec; 0.520 sec/batch)
2016-07-15 23:56:08.490239: step 110160, loss = 3.64 (278.6 examples/sec; 0.459 sec/batch)
2016-07-15 23:56:13.262155: step 110170, loss = 3.53 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 23:56:18.001558: step 110180, loss = 3.17 (273.2 examples/sec; 0.469 sec/batch)
2016-07-15 23:56:23.193736: step 110190, loss = 3.32 (191.8 examples/sec; 0.667 sec/batch)
2016-07-15 23:56:29.713458: step 110200, loss = 3.32 (205.7 examples/sec; 0.622 sec/batch)
2016-07-15 23:56:35.709495: step 110210, loss = 3.41 (280.1 examples/sec; 0.457 sec/batch)
2016-07-15 23:56:40.350625: step 110220, loss = 3.40 (275.7 examples/sec; 0.464 sec/batch)
2016-07-15 23:56:44.997966: step 110230, loss = 3.52 (279.1 examples/sec; 0.459 sec/batch)
2016-07-15 23:56:50.682905: step 110240, loss = 3.42 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 23:56:55.496063: step 110250, loss = 3.38 (274.0 examples/sec; 0.467 sec/batch)
2016-07-15 23:57:00.196236: step 110260, loss = 3.65 (267.7 examples/sec; 0.478 sec/batch)
2016-07-15 23:57:04.864134: step 110270, loss = 3.34 (275.0 examples/sec; 0.465 sec/batch)
2016-07-15 23:57:10.558596: step 110280, loss = 3.23 (246.3 examples/sec; 0.520 sec/batch)
2016-07-15 23:57:15.803248: step 110290, loss = 3.15 (204.9 examples/sec; 0.625 sec/batch)
2016-07-15 23:57:21.271563: step 110300, loss = 3.44 (262.6 examples/sec; 0.487 sec/batch)
2016-07-15 23:57:26.993670: step 110310, loss = 3.29 (261.9 examples/sec; 0.489 sec/batch)
2016-07-15 23:57:32.512153: step 110320, loss = 3.51 (189.1 examples/sec; 0.677 sec/batch)
2016-07-15 23:57:38.787327: step 110330, loss = 3.36 (256.5 examples/sec; 0.499 sec/batch)
2016-07-15 23:57:43.613176: step 110340, loss = 3.36 (278.2 examples/sec; 0.460 sec/batch)
2016-07-15 23:57:48.356573: step 110350, loss = 3.37 (258.6 examples/sec; 0.495 sec/batch)
2016-07-15 23:57:54.395611: step 110360, loss = 3.38 (190.5 examples/sec; 0.672 sec/batch)
2016-07-15 23:58:00.101120: step 110370, loss = 3.37 (255.7 examples/sec; 0.501 sec/batch)
2016-07-15 23:58:05.769959: step 110380, loss = 3.39 (200.6 examples/sec; 0.638 sec/batch)
2016-07-15 23:58:10.770816: step 110390, loss = 3.39 (272.7 examples/sec; 0.469 sec/batch)
2016-07-15 23:58:15.454330: step 110400, loss = 3.47 (264.2 examples/sec; 0.484 sec/batch)
2016-07-15 23:58:22.604990: step 110410, loss = 3.50 (196.1 examples/sec; 0.653 sec/batch)
2016-07-15 23:58:28.304537: step 110420, loss = 3.48 (261.1 examples/sec; 0.490 sec/batch)
2016-07-15 23:58:33.128489: step 110430, loss = 3.25 (269.5 examples/sec; 0.475 sec/batch)
2016-07-15 23:58:37.985812: step 110440, loss = 3.58 (265.6 examples/sec; 0.482 sec/batch)
2016-07-15 23:58:42.651582: step 110450, loss = 3.70 (273.3 examples/sec; 0.468 sec/batch)
2016-07-15 23:58:47.299086: step 110460, loss = 3.38 (269.7 examples/sec; 0.475 sec/batch)
2016-07-15 23:58:52.819491: step 110470, loss = 3.40 (204.4 examples/sec; 0.626 sec/batch)
2016-07-15 23:58:58.007023: step 110480, loss = 3.83 (266.4 examples/sec; 0.481 sec/batch)
2016-07-15 23:59:02.726935: step 110490, loss = 3.44 (263.6 examples/sec; 0.486 sec/batch)
2016-07-15 23:59:07.595454: step 110500, loss = 3.38 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 23:59:13.361640: step 110510, loss = 3.67 (264.7 examples/sec; 0.484 sec/batch)
2016-07-15 23:59:18.101592: step 110520, loss = 3.33 (277.9 examples/sec; 0.461 sec/batch)
2016-07-15 23:59:23.256837: step 110530, loss = 3.46 (188.3 examples/sec; 0.680 sec/batch)
2016-07-15 23:59:29.794435: step 110540, loss = 3.23 (194.3 examples/sec; 0.659 sec/batch)
2016-07-15 23:59:34.758322: step 110550, loss = 3.43 (275.1 examples/sec; 0.465 sec/batch)
2016-07-15 23:59:39.517528: step 110560, loss = 3.59 (262.2 examples/sec; 0.488 sec/batch)
2016-07-15 23:59:44.373469: step 110570, loss = 3.40 (263.9 examples/sec; 0.485 sec/batch)
2016-07-15 23:59:49.039970: step 110580, loss = 3.64 (274.7 examples/sec; 0.466 sec/batch)
2016-07-15 23:59:53.669628: step 110590, loss = 3.73 (277.6 examples/sec; 0.461 sec/batch)
2016-07-15 23:59:59.464255: step 110600, loss = 3.50 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 00:00:06.262109: step 110610, loss = 3.32 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 00:00:10.927027: step 110620, loss = 3.33 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 00:00:15.575655: step 110630, loss = 3.42 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 00:00:20.864572: step 110640, loss = 3.44 (207.3 examples/sec; 0.618 sec/batch)
2016-07-16 00:00:26.133857: step 110650, loss = 3.52 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 00:00:31.879107: step 110660, loss = 3.59 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 00:00:36.645027: step 110670, loss = 3.36 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 00:00:41.451057: step 110680, loss = 3.50 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 00:00:48.769479: step 110690, loss = 3.46 (83.7 examples/sec; 1.529 sec/batch)
2016-07-16 00:01:03.001526: step 110700, loss = 3.51 (201.7 examples/sec; 0.635 sec/batch)
2016-07-16 00:01:08.641036: step 110710, loss = 3.33 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 00:01:13.344494: step 110720, loss = 3.48 (251.7 examples/sec; 0.509 sec/batch)
2016-07-16 00:01:17.986675: step 110730, loss = 3.32 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 00:01:22.614025: step 110740, loss = 3.74 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 00:01:27.278983: step 110750, loss = 3.40 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 00:01:32.013524: step 110760, loss = 3.29 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 00:01:37.064289: step 110770, loss = 3.36 (208.3 examples/sec; 0.614 sec/batch)
2016-07-16 00:01:42.524298: step 110780, loss = 3.17 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 00:01:47.291439: step 110790, loss = 3.27 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 00:01:51.927507: step 110800, loss = 3.90 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 00:01:57.551740: step 110810, loss = 3.24 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 00:02:02.190601: step 110820, loss = 3.19 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 00:02:07.915366: step 110830, loss = 3.20 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 00:02:12.509775: step 110840, loss = 3.38 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 00:02:17.149717: step 110850, loss = 3.55 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 00:02:21.782399: step 110860, loss = 3.45 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 00:02:26.421095: step 110870, loss = 3.45 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 00:02:31.063655: step 110880, loss = 3.33 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 00:02:35.683037: step 110890, loss = 3.43 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 00:02:40.283194: step 110900, loss = 3.58 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 00:02:45.863207: step 110910, loss = 3.45 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 00:02:50.526590: step 110920, loss = 3.29 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 00:02:55.173713: step 110930, loss = 3.27 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 00:02:59.830480: step 110940, loss = 3.59 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 00:03:04.406832: step 110950, loss = 3.27 (282.9 examples/sec; 0.453 sec/batch)
2016-07-16 00:03:09.022786: step 110960, loss = 3.50 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 00:03:13.610526: step 110970, loss = 3.36 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 00:03:19.113245: step 110980, loss = 3.48 (199.6 examples/sec; 0.641 sec/batch)
2016-07-16 00:03:23.888990: step 110990, loss = 3.51 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 00:03:28.617874: step 111000, loss = 3.86 (284.0 examples/sec; 0.451 sec/batch)
2016-07-16 00:03:34.265785: step 111010, loss = 3.42 (269.2 examples/sec; 0.476 sec/batch)
2016-07-16 00:03:38.901156: step 111020, loss = 3.35 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 00:03:43.518972: step 111030, loss = 3.39 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 00:03:48.199660: step 111040, loss = 3.63 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 00:03:52.834083: step 111050, loss = 3.33 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 00:03:58.129187: step 111060, loss = 3.38 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 00:04:03.381384: step 111070, loss = 3.40 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 00:04:08.162247: step 111080, loss = 3.20 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 00:04:12.999465: step 111090, loss = 3.38 (253.9 examples/sec; 0.504 sec/batch)
2016-07-16 00:04:17.627566: step 111100, loss = 3.40 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 00:04:23.122515: step 111110, loss = 3.42 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 00:04:27.803626: step 111120, loss = 3.69 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 00:04:32.404848: step 111130, loss = 3.27 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 00:04:37.528677: step 111140, loss = 3.58 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 00:04:42.944503: step 111150, loss = 3.31 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 00:04:47.670833: step 111160, loss = 3.61 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 00:04:52.493837: step 111170, loss = 3.31 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 00:04:57.213435: step 111180, loss = 3.30 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 00:05:01.862229: step 111190, loss = 3.33 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 00:05:07.446954: step 111200, loss = 3.51 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 00:05:13.467335: step 111210, loss = 3.55 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 00:05:18.106238: step 111220, loss = 3.63 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 00:05:22.709337: step 111230, loss = 3.51 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 00:05:27.339531: step 111240, loss = 3.66 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 00:05:33.008187: step 111250, loss = 3.40 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 00:05:37.624343: step 111260, loss = 3.80 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 00:05:42.325787: step 111270, loss = 3.63 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 00:05:46.961356: step 111280, loss = 3.61 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 00:05:51.557782: step 111290, loss = 3.53 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 00:05:56.248899: step 111300, loss = 3.58 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 00:06:01.810601: step 111310, loss = 3.51 (286.4 examples/sec; 0.447 sec/batch)
2016-07-16 00:06:06.426540: step 111320, loss = 3.50 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 00:06:11.258196: step 111330, loss = 3.48 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 00:06:16.112684: step 111340, loss = 3.14 (245.4 examples/sec; 0.522 sec/batch)
2016-07-16 00:06:22.238647: step 111350, loss = 3.25 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 00:06:26.917240: step 111360, loss = 3.32 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 00:06:31.548376: step 111370, loss = 3.42 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 00:06:36.181873: step 111380, loss = 3.65 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 00:06:40.863815: step 111390, loss = 3.68 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 00:06:45.549292: step 111400, loss = 3.56 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 00:06:51.111631: step 111410, loss = 3.51 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 00:06:55.713250: step 111420, loss = 3.53 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 00:07:00.407553: step 111430, loss = 3.32 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 00:07:05.023702: step 111440, loss = 3.28 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 00:07:09.729733: step 111450, loss = 3.22 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 00:07:14.333126: step 111460, loss = 3.48 (281.6 examples/sec; 0.454 sec/batch)
2016-07-16 00:07:19.012104: step 111470, loss = 3.30 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 00:07:23.621753: step 111480, loss = 3.55 (285.7 examples/sec; 0.448 sec/batch)
2016-07-16 00:07:28.375459: step 111490, loss = 3.19 (230.4 examples/sec; 0.556 sec/batch)
2016-07-16 00:07:34.099929: step 111500, loss = 3.11 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 00:07:40.026105: step 111510, loss = 3.46 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 00:07:44.677581: step 111520, loss = 3.34 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 00:07:49.383238: step 111530, loss = 3.27 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 00:07:54.015202: step 111540, loss = 3.27 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 00:07:58.653692: step 111550, loss = 3.43 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 00:08:03.218949: step 111560, loss = 3.50 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 00:08:07.880890: step 111570, loss = 3.09 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 00:08:13.709556: step 111580, loss = 3.72 (241.3 examples/sec; 0.530 sec/batch)
2016-07-16 00:08:18.489656: step 111590, loss = 3.54 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 00:08:23.298891: step 111600, loss = 3.40 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 00:08:28.951645: step 111610, loss = 3.82 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 00:08:33.634830: step 111620, loss = 3.34 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 00:08:38.309091: step 111630, loss = 3.38 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 00:08:42.943886: step 111640, loss = 3.77 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 00:08:47.645194: step 111650, loss = 3.29 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 00:08:52.276646: step 111660, loss = 3.35 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 00:08:56.905332: step 111670, loss = 3.65 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 00:09:02.050900: step 111680, loss = 3.55 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 00:09:07.266158: step 111690, loss = 3.43 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 00:09:11.930365: step 111700, loss = 3.33 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 00:09:18.855956: step 111710, loss = 3.47 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 00:09:23.595780: step 111720, loss = 3.52 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 00:09:28.244450: step 111730, loss = 3.37 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 00:09:32.966508: step 111740, loss = 3.53 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 00:09:37.593453: step 111750, loss = 3.59 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 00:09:42.990181: step 111760, loss = 3.63 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 00:09:48.133721: step 111770, loss = 3.69 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 00:09:52.870954: step 111780, loss = 3.49 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 00:09:57.700709: step 111790, loss = 3.16 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 00:10:02.332360: step 111800, loss = 3.36 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 00:10:07.871914: step 111810, loss = 3.54 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 00:10:12.488068: step 111820, loss = 3.52 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 00:10:17.103218: step 111830, loss = 3.33 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 00:10:21.743046: step 111840, loss = 3.37 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 00:10:26.402340: step 111850, loss = 3.38 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 00:10:31.019312: step 111860, loss = 3.47 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 00:10:36.670747: step 111870, loss = 3.59 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 00:10:41.306378: step 111880, loss = 3.40 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 00:10:45.989388: step 111890, loss = 3.45 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 00:10:51.542407: step 111900, loss = 3.37 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 00:10:57.668205: step 111910, loss = 3.49 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 00:11:02.454080: step 111920, loss = 3.36 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 00:11:07.187965: step 111930, loss = 3.47 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 00:11:11.835056: step 111940, loss = 3.15 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 00:11:16.556213: step 111950, loss = 3.20 (285.4 examples/sec; 0.448 sec/batch)
2016-07-16 00:11:21.203344: step 111960, loss = 3.58 (286.3 examples/sec; 0.447 sec/batch)
2016-07-16 00:11:25.860057: step 111970, loss = 3.44 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 00:11:30.544796: step 111980, loss = 3.35 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 00:11:35.156444: step 111990, loss = 3.45 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 00:11:39.777177: step 112000, loss = 3.40 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 00:11:45.708442: step 112010, loss = 3.50 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 00:11:51.005757: step 112020, loss = 3.48 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 00:11:55.645760: step 112030, loss = 3.51 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 00:12:00.255211: step 112040, loss = 3.46 (282.2 examples/sec; 0.454 sec/batch)
2016-07-16 00:12:04.860601: step 112050, loss = 3.39 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 00:12:09.491765: step 112060, loss = 3.67 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 00:12:14.119836: step 112070, loss = 3.40 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 00:12:18.710136: step 112080, loss = 3.47 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 00:12:23.315656: step 112090, loss = 3.39 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 00:12:27.985352: step 112100, loss = 3.43 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 00:12:34.749597: step 112110, loss = 3.25 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 00:12:39.426931: step 112120, loss = 3.45 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 00:12:44.052201: step 112130, loss = 3.20 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 00:12:48.688731: step 112140, loss = 3.48 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 00:12:53.319079: step 112150, loss = 3.37 (276.2 examples/sec; 0.464 sec/batch)
2016-07-16 00:12:58.007613: step 112160, loss = 3.62 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 00:13:02.619819: step 112170, loss = 3.28 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 00:13:07.234183: step 112180, loss = 3.33 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 00:13:11.905706: step 112190, loss = 3.52 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 00:13:16.528866: step 112200, loss = 3.16 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 00:13:22.193969: step 112210, loss = 3.31 (286.0 examples/sec; 0.447 sec/batch)
2016-07-16 00:13:26.879496: step 112220, loss = 3.30 (247.7 examples/sec; 0.517 sec/batch)
2016-07-16 00:13:31.542684: step 112230, loss = 3.44 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 00:13:36.193222: step 112240, loss = 3.15 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 00:13:40.810850: step 112250, loss = 3.27 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 00:13:45.462804: step 112260, loss = 3.29 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 00:13:50.164382: step 112270, loss = 3.30 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 00:13:54.833858: step 112280, loss = 3.49 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 00:13:59.499387: step 112290, loss = 3.69 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 00:14:04.200297: step 112300, loss = 3.10 (250.5 examples/sec; 0.511 sec/batch)
2016-07-16 00:14:09.682338: step 112310, loss = 3.28 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 00:14:14.308710: step 112320, loss = 3.61 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 00:14:19.845022: step 112330, loss = 3.41 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 00:14:24.838828: step 112340, loss = 3.29 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 00:14:29.576728: step 112350, loss = 3.33 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 00:14:35.272845: step 112360, loss = 3.43 (188.5 examples/sec; 0.679 sec/batch)
2016-07-16 00:14:40.281418: step 112370, loss = 3.36 (286.9 examples/sec; 0.446 sec/batch)
2016-07-16 00:14:44.855683: step 112380, loss = 3.62 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 00:14:49.492842: step 112390, loss = 3.38 (284.2 examples/sec; 0.450 sec/batch)
2016-07-16 00:14:54.170241: step 112400, loss = 3.54 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 00:14:59.839288: step 112410, loss = 3.50 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 00:15:04.438272: step 112420, loss = 3.68 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 00:15:09.128189: step 112430, loss = 3.81 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 00:15:13.741938: step 112440, loss = 3.77 (287.8 examples/sec; 0.445 sec/batch)
2016-07-16 00:15:18.427314: step 112450, loss = 3.61 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 00:15:23.099756: step 112460, loss = 3.49 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 00:15:27.829130: step 112470, loss = 3.38 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 00:15:33.769544: step 112480, loss = 3.19 (242.5 examples/sec; 0.528 sec/batch)
2016-07-16 00:15:38.599411: step 112490, loss = 3.52 (265.8 examples/sec; 0.481 sec/batch)
2016-07-16 00:15:43.234950: step 112500, loss = 3.49 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 00:15:48.859364: step 112510, loss = 3.23 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 00:15:53.478278: step 112520, loss = 3.26 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 00:15:58.693604: step 112530, loss = 3.39 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 00:16:04.039109: step 112540, loss = 3.48 (254.3 examples/sec; 0.503 sec/batch)
2016-07-16 00:16:08.776183: step 112550, loss = 3.71 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 00:16:13.660655: step 112560, loss = 3.45 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 00:16:18.315951: step 112570, loss = 3.31 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 00:16:22.969165: step 112580, loss = 3.46 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 00:16:28.689428: step 112590, loss = 3.20 (223.0 examples/sec; 0.574 sec/batch)
2016-07-16 00:16:33.870786: step 112600, loss = 3.34 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 00:16:40.569246: step 112610, loss = 3.17 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 00:16:45.310292: step 112620, loss = 3.26 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 00:16:50.177320: step 112630, loss = 3.24 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 00:16:54.799568: step 112640, loss = 3.55 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 00:16:59.479520: step 112650, loss = 3.09 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 00:17:04.077720: step 112660, loss = 3.15 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 00:17:08.702201: step 112670, loss = 3.28 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 00:17:13.398175: step 112680, loss = 3.65 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 00:17:18.612208: step 112690, loss = 3.60 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 00:17:23.887275: step 112700, loss = 3.41 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 00:17:29.631943: step 112710, loss = 3.34 (255.4 examples/sec; 0.501 sec/batch)
2016-07-16 00:17:34.489125: step 112720, loss = 3.39 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 00:17:39.312240: step 112730, loss = 3.15 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 00:17:45.526586: step 112740, loss = 3.54 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 00:17:51.100242: step 112750, loss = 3.38 (251.6 examples/sec; 0.509 sec/batch)
2016-07-16 00:17:55.824470: step 112760, loss = 3.50 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 00:18:00.706198: step 112770, loss = 3.61 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 00:18:05.430253: step 112780, loss = 3.41 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 00:18:10.247221: step 112790, loss = 3.20 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 00:18:15.123495: step 112800, loss = 3.42 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 00:18:20.884654: step 112810, loss = 3.19 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 00:18:25.714473: step 112820, loss = 3.47 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 00:18:30.413890: step 112830, loss = 3.59 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 00:18:35.054649: step 112840, loss = 3.36 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 00:18:40.784469: step 112850, loss = 3.38 (218.5 examples/sec; 0.586 sec/batch)
2016-07-16 00:18:45.613705: step 112860, loss = 3.22 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 00:18:50.342842: step 112870, loss = 3.73 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 00:18:55.206659: step 112880, loss = 3.62 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 00:18:59.871655: step 112890, loss = 3.29 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 00:19:04.559141: step 112900, loss = 3.50 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 00:19:11.504980: step 112910, loss = 3.66 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 00:19:16.250814: step 112920, loss = 3.48 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 00:19:21.093140: step 112930, loss = 3.59 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 00:19:25.808009: step 112940, loss = 3.47 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 00:19:31.451964: step 112950, loss = 3.14 (189.1 examples/sec; 0.677 sec/batch)
2016-07-16 00:19:37.569560: step 112960, loss = 3.43 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 00:19:42.401017: step 112970, loss = 3.34 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 00:19:47.194194: step 112980, loss = 3.22 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 00:19:51.978713: step 112990, loss = 3.37 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 00:19:56.892014: step 113000, loss = 3.23 (226.0 examples/sec; 0.566 sec/batch)
2016-07-16 00:20:05.031669: step 113010, loss = 3.36 (243.8 examples/sec; 0.525 sec/batch)
2016-07-16 00:20:10.302955: step 113020, loss = 3.33 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 00:20:15.756328: step 113030, loss = 3.59 (248.1 examples/sec; 0.516 sec/batch)
2016-07-16 00:20:20.515908: step 113040, loss = 3.43 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 00:20:25.684003: step 113050, loss = 3.33 (189.1 examples/sec; 0.677 sec/batch)
2016-07-16 00:20:32.180063: step 113060, loss = 3.26 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 00:20:37.144857: step 113070, loss = 3.31 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 00:20:41.798274: step 113080, loss = 3.42 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 00:20:46.479424: step 113090, loss = 3.37 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 00:20:51.939975: step 113100, loss = 3.44 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 00:20:58.126233: step 113110, loss = 3.40 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 00:21:02.858643: step 113120, loss = 3.74 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 00:21:07.698604: step 113130, loss = 3.50 (249.0 examples/sec; 0.514 sec/batch)
2016-07-16 00:21:12.357947: step 113140, loss = 3.27 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 00:21:17.081399: step 113150, loss = 3.34 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 00:21:21.746185: step 113160, loss = 3.47 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 00:21:26.971603: step 113170, loss = 3.40 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 00:21:32.273291: step 113180, loss = 3.40 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 00:21:36.989456: step 113190, loss = 3.13 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 00:21:42.271623: step 113200, loss = 3.23 (191.0 examples/sec; 0.670 sec/batch)
2016-07-16 00:21:49.985273: step 113210, loss = 3.33 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 00:21:55.478070: step 113220, loss = 3.57 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 00:22:00.639169: step 113230, loss = 3.43 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 00:22:05.292658: step 113240, loss = 3.46 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 00:22:09.944336: step 113250, loss = 3.16 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 00:22:14.617962: step 113260, loss = 3.30 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 00:22:19.279245: step 113270, loss = 3.52 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 00:22:25.015115: step 113280, loss = 3.33 (220.3 examples/sec; 0.581 sec/batch)
2016-07-16 00:22:30.154990: step 113290, loss = 3.50 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 00:22:35.638664: step 113300, loss = 3.28 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 00:22:41.414224: step 113310, loss = 3.50 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 00:22:46.286884: step 113320, loss = 3.64 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 00:22:51.095132: step 113330, loss = 3.32 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 00:22:55.840149: step 113340, loss = 3.33 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 00:23:00.479811: step 113350, loss = 3.49 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 00:23:05.315700: step 113360, loss = 3.17 (208.5 examples/sec; 0.614 sec/batch)
2016-07-16 00:23:11.049113: step 113370, loss = 3.51 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 00:23:15.827871: step 113380, loss = 3.38 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 00:23:20.504090: step 113390, loss = 3.15 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 00:23:25.329304: step 113400, loss = 3.33 (215.7 examples/sec; 0.593 sec/batch)
2016-07-16 00:23:32.220899: step 113410, loss = 3.30 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 00:23:36.930670: step 113420, loss = 3.28 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 00:23:41.757686: step 113430, loss = 3.41 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 00:23:46.576852: step 113440, loss = 3.50 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 00:23:52.466230: step 113450, loss = 3.71 (188.3 examples/sec; 0.680 sec/batch)
2016-07-16 00:23:58.267070: step 113460, loss = 3.01 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 00:24:02.921483: step 113470, loss = 3.69 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 00:24:07.583474: step 113480, loss = 3.37 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 00:24:12.236414: step 113490, loss = 3.31 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 00:24:17.983349: step 113500, loss = 3.39 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 00:24:23.740155: step 113510, loss = 3.30 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 00:24:28.645953: step 113520, loss = 3.62 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 00:24:35.136720: step 113530, loss = 3.34 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 00:24:40.490563: step 113540, loss = 3.54 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 00:24:45.197441: step 113550, loss = 3.27 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 00:24:50.039015: step 113560, loss = 3.68 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 00:24:54.779668: step 113570, loss = 3.31 (250.1 examples/sec; 0.512 sec/batch)
2016-07-16 00:24:59.423698: step 113580, loss = 3.47 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 00:25:04.081162: step 113590, loss = 3.41 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 00:25:08.726321: step 113600, loss = 3.20 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 00:25:15.663412: step 113610, loss = 3.58 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 00:25:20.369017: step 113620, loss = 3.52 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 00:25:25.321033: step 113630, loss = 3.46 (223.1 examples/sec; 0.574 sec/batch)
2016-07-16 00:25:31.840561: step 113640, loss = 3.31 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 00:25:37.002997: step 113650, loss = 3.43 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 00:25:41.734766: step 113660, loss = 3.26 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 00:25:46.569316: step 113670, loss = 3.33 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 00:25:51.375649: step 113680, loss = 3.22 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 00:25:56.116381: step 113690, loss = 3.31 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 00:26:00.968096: step 113700, loss = 3.48 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 00:26:06.663940: step 113710, loss = 3.60 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 00:26:12.580931: step 113720, loss = 3.36 (186.0 examples/sec; 0.688 sec/batch)
2016-07-16 00:26:18.496257: step 113730, loss = 3.21 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 00:26:23.254317: step 113740, loss = 3.55 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 00:26:28.175612: step 113750, loss = 3.24 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 00:26:33.002977: step 113760, loss = 3.22 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 00:26:37.628380: step 113770, loss = 3.45 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 00:26:42.293075: step 113780, loss = 3.42 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 00:26:46.935249: step 113790, loss = 3.44 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 00:26:52.470382: step 113800, loss = 3.24 (200.1 examples/sec; 0.640 sec/batch)
2016-07-16 00:26:58.498060: step 113810, loss = 3.05 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 00:27:03.348395: step 113820, loss = 3.18 (256.8 examples/sec; 0.498 sec/batch)
2016-07-16 00:27:09.488638: step 113830, loss = 3.35 (200.1 examples/sec; 0.640 sec/batch)
2016-07-16 00:27:15.234828: step 113840, loss = 3.65 (252.7 examples/sec; 0.507 sec/batch)
2016-07-16 00:27:20.956984: step 113850, loss = 3.56 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 00:27:26.053492: step 113860, loss = 3.52 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 00:27:31.601692: step 113870, loss = 3.22 (267.5 examples/sec; 0.478 sec/batch)
2016-07-16 00:27:36.346406: step 113880, loss = 3.61 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 00:27:41.229745: step 113890, loss = 3.35 (249.9 examples/sec; 0.512 sec/batch)
2016-07-16 00:27:47.812521: step 113900, loss = 3.46 (200.7 examples/sec; 0.638 sec/batch)
2016-07-16 00:27:53.922595: step 113910, loss = 3.27 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 00:27:59.184879: step 113920, loss = 3.45 (200.8 examples/sec; 0.637 sec/batch)
2016-07-16 00:28:04.515756: step 113930, loss = 3.52 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 00:28:09.223872: step 113940, loss = 3.46 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 00:28:14.046459: step 113950, loss = 3.28 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 00:28:18.780985: step 113960, loss = 3.43 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 00:28:23.616803: step 113970, loss = 3.43 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 00:28:28.427613: step 113980, loss = 3.19 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 00:28:34.846902: step 113990, loss = 3.35 (202.0 examples/sec; 0.634 sec/batch)
2016-07-16 00:28:40.241374: step 114000, loss = 3.42 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 00:28:45.980991: step 114010, loss = 3.40 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 00:28:51.517302: step 114020, loss = 3.17 (187.9 examples/sec; 0.681 sec/batch)
2016-07-16 00:28:57.762635: step 114030, loss = 3.54 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 00:29:02.657963: step 114040, loss = 3.52 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 00:29:07.390965: step 114050, loss = 3.30 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 00:29:12.092590: step 114060, loss = 3.43 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 00:29:17.867706: step 114070, loss = 3.51 (253.1 examples/sec; 0.506 sec/batch)
2016-07-16 00:29:22.701226: step 114080, loss = 3.50 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 00:29:27.530758: step 114090, loss = 3.26 (238.7 examples/sec; 0.536 sec/batch)
2016-07-16 00:29:33.661384: step 114100, loss = 3.38 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 00:29:40.417197: step 114110, loss = 3.00 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 00:29:45.102005: step 114120, loss = 3.40 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 00:29:49.784723: step 114130, loss = 3.39 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 00:29:54.974712: step 114140, loss = 3.21 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 00:30:00.444292: step 114150, loss = 3.29 (244.7 examples/sec; 0.523 sec/batch)
2016-07-16 00:30:05.148344: step 114160, loss = 3.10 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 00:30:09.754086: step 114170, loss = 3.23 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 00:30:14.880433: step 114180, loss = 3.47 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 00:30:20.297015: step 114190, loss = 3.45 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 00:30:25.017144: step 114200, loss = 3.57 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 00:30:31.593156: step 114210, loss = 3.35 (189.6 examples/sec; 0.675 sec/batch)
2016-07-16 00:30:37.851890: step 114220, loss = 3.30 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 00:30:42.693588: step 114230, loss = 3.33 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 00:30:47.422625: step 114240, loss = 3.35 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 00:30:53.457323: step 114250, loss = 3.40 (194.7 examples/sec; 0.658 sec/batch)
2016-07-16 00:30:59.158899: step 114260, loss = 3.23 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 00:31:03.952500: step 114270, loss = 3.07 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 00:31:08.769352: step 114280, loss = 3.45 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 00:31:13.491318: step 114290, loss = 2.96 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 00:31:18.717498: step 114300, loss = 3.41 (189.4 examples/sec; 0.676 sec/batch)
2016-07-16 00:31:26.530926: step 114310, loss = 3.38 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 00:31:31.402615: step 114320, loss = 3.32 (249.0 examples/sec; 0.514 sec/batch)
2016-07-16 00:31:36.021219: step 114330, loss = 3.34 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 00:31:40.638128: step 114340, loss = 3.21 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 00:31:46.380391: step 114350, loss = 3.15 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 00:31:51.177743: step 114360, loss = 3.31 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 00:31:55.937676: step 114370, loss = 3.32 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 00:32:02.108408: step 114380, loss = 3.36 (207.3 examples/sec; 0.618 sec/batch)
2016-07-16 00:32:07.728556: step 114390, loss = 3.37 (256.2 examples/sec; 0.500 sec/batch)
2016-07-16 00:32:12.497549: step 114400, loss = 3.13 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 00:32:18.331773: step 114410, loss = 3.22 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 00:32:23.089240: step 114420, loss = 3.31 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 00:32:29.059267: step 114430, loss = 3.41 (184.6 examples/sec; 0.693 sec/batch)
2016-07-16 00:32:34.973812: step 114440, loss = 3.17 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 00:32:39.771191: step 114450, loss = 3.35 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 00:32:44.609867: step 114460, loss = 3.31 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 00:32:50.977398: step 114470, loss = 3.55 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 00:32:56.421499: step 114480, loss = 3.45 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 00:33:01.158031: step 114490, loss = 3.42 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 00:33:06.235694: step 114500, loss = 3.11 (186.6 examples/sec; 0.686 sec/batch)
2016-07-16 00:33:13.025832: step 114510, loss = 3.60 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 00:33:17.796741: step 114520, loss = 3.27 (219.2 examples/sec; 0.584 sec/batch)
2016-07-16 00:33:23.493033: step 114530, loss = 3.34 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 00:33:28.245507: step 114540, loss = 3.52 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 00:33:33.082796: step 114550, loss = 3.14 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 00:33:37.806616: step 114560, loss = 3.26 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 00:33:43.144894: step 114570, loss = 3.35 (189.5 examples/sec; 0.675 sec/batch)
2016-07-16 00:33:49.576334: step 114580, loss = 3.29 (211.5 examples/sec; 0.605 sec/batch)
2016-07-16 00:33:54.399346: step 114590, loss = 3.37 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 00:33:59.151181: step 114600, loss = 3.37 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 00:34:06.493721: step 114610, loss = 3.45 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 00:34:12.080177: step 114620, loss = 3.10 (253.3 examples/sec; 0.505 sec/batch)
2016-07-16 00:34:17.807745: step 114630, loss = 3.64 (255.1 examples/sec; 0.502 sec/batch)
2016-07-16 00:34:22.670403: step 114640, loss = 3.30 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 00:34:27.317665: step 114650, loss = 3.36 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 00:34:32.017314: step 114660, loss = 3.24 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 00:34:36.660630: step 114670, loss = 3.17 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 00:34:41.346331: step 114680, loss = 3.34 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 00:34:47.196169: step 114690, loss = 3.27 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 00:34:51.990291: step 114700, loss = 3.57 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 00:34:57.837577: step 114710, loss = 3.33 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 00:35:02.580475: step 114720, loss = 3.63 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 00:35:07.379672: step 114730, loss = 3.29 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 00:35:12.221388: step 114740, loss = 3.56 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 00:35:18.458659: step 114750, loss = 3.48 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 00:35:24.032478: step 114760, loss = 3.30 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 00:35:28.748480: step 114770, loss = 3.33 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 00:35:33.671295: step 114780, loss = 3.46 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 00:35:38.389903: step 114790, loss = 3.36 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 00:35:43.993205: step 114800, loss = 3.31 (186.1 examples/sec; 0.688 sec/batch)
2016-07-16 00:35:50.325268: step 114810, loss = 3.43 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 00:35:55.485844: step 114820, loss = 3.23 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 00:36:00.830583: step 114830, loss = 3.26 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 00:36:06.640006: step 114840, loss = 3.15 (262.0 examples/sec; 0.488 sec/batch)
2016-07-16 00:36:11.441952: step 114850, loss = 3.28 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 00:36:16.250141: step 114860, loss = 3.51 (254.5 examples/sec; 0.503 sec/batch)
2016-07-16 00:36:21.037291: step 114870, loss = 3.56 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 00:36:25.702478: step 114880, loss = 3.16 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 00:36:30.639332: step 114890, loss = 3.45 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 00:36:36.214707: step 114900, loss = 3.35 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 00:36:41.972220: step 114910, loss = 3.27 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 00:36:46.829359: step 114920, loss = 3.15 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 00:36:51.584554: step 114930, loss = 3.24 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 00:36:56.340859: step 114940, loss = 3.44 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 00:37:01.220849: step 114950, loss = 3.49 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 00:37:07.626407: step 114960, loss = 3.42 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 00:37:13.004946: step 114970, loss = 3.45 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 00:37:17.709971: step 114980, loss = 3.11 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 00:37:22.572960: step 114990, loss = 3.33 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 00:37:27.326579: step 115000, loss = 3.59 (253.7 examples/sec; 0.504 sec/batch)
2016-07-16 00:37:33.040828: step 115010, loss = 3.40 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 00:37:37.969927: step 115020, loss = 3.24 (226.0 examples/sec; 0.566 sec/batch)
2016-07-16 00:37:44.508510: step 115030, loss = 3.22 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 00:37:49.698244: step 115040, loss = 3.51 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 00:37:54.412810: step 115050, loss = 3.28 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 00:37:59.973603: step 115060, loss = 3.46 (188.4 examples/sec; 0.679 sec/batch)
2016-07-16 00:38:06.258883: step 115070, loss = 3.57 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 00:38:11.090276: step 115080, loss = 3.31 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 00:38:15.821058: step 115090, loss = 3.55 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 00:38:21.843596: step 115100, loss = 3.26 (191.0 examples/sec; 0.670 sec/batch)
2016-07-16 00:38:28.798771: step 115110, loss = 3.34 (255.8 examples/sec; 0.500 sec/batch)
2016-07-16 00:38:34.515544: step 115120, loss = 3.37 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 00:38:39.929064: step 115130, loss = 3.48 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 00:38:45.225120: step 115140, loss = 3.15 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 00:38:49.967736: step 115150, loss = 3.34 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 00:38:55.331811: step 115160, loss = 3.35 (185.2 examples/sec; 0.691 sec/batch)
2016-07-16 00:39:01.786089: step 115170, loss = 3.29 (205.3 examples/sec; 0.624 sec/batch)
2016-07-16 00:39:06.607084: step 115180, loss = 3.38 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 00:39:11.327663: step 115190, loss = 3.50 (255.3 examples/sec; 0.501 sec/batch)
2016-07-16 00:39:17.172362: step 115200, loss = 3.16 (191.4 examples/sec; 0.669 sec/batch)
2016-07-16 00:39:24.387345: step 115210, loss = 3.32 (251.8 examples/sec; 0.508 sec/batch)
2016-07-16 00:39:30.122611: step 115220, loss = 3.46 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 00:39:34.989441: step 115230, loss = 3.81 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 00:39:39.768433: step 115240, loss = 3.48 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 00:39:44.579726: step 115250, loss = 3.28 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 00:39:49.378826: step 115260, loss = 3.42 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 00:39:55.834112: step 115270, loss = 3.46 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 00:40:01.170891: step 115280, loss = 3.46 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 00:40:05.922601: step 115290, loss = 3.40 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 00:40:10.561929: step 115300, loss = 3.54 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 00:40:16.994728: step 115310, loss = 3.22 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 00:40:22.143888: step 115320, loss = 3.10 (228.4 examples/sec; 0.560 sec/batch)
2016-07-16 00:40:27.861271: step 115330, loss = 3.28 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 00:40:32.690981: step 115340, loss = 3.42 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 00:40:37.533615: step 115350, loss = 3.46 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 00:40:43.994940: step 115360, loss = 3.47 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 00:40:49.366583: step 115370, loss = 3.34 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 00:40:54.108751: step 115380, loss = 3.28 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 00:40:59.452675: step 115390, loss = 3.09 (192.4 examples/sec; 0.665 sec/batch)
2016-07-16 00:41:05.910786: step 115400, loss = 3.49 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 00:41:11.744335: step 115410, loss = 3.51 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 00:41:16.542462: step 115420, loss = 3.17 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 00:41:22.739876: step 115430, loss = 3.10 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 00:41:28.320260: step 115440, loss = 3.56 (250.8 examples/sec; 0.510 sec/batch)
2016-07-16 00:41:33.078437: step 115450, loss = 3.06 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 00:41:37.668002: step 115460, loss = 3.50 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 00:41:42.554709: step 115470, loss = 3.31 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 00:41:48.183890: step 115480, loss = 3.14 (258.8 examples/sec; 0.494 sec/batch)
2016-07-16 00:41:53.957950: step 115490, loss = 3.30 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 00:41:58.808191: step 115500, loss = 3.28 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 00:42:04.558312: step 115510, loss = 3.13 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 00:42:09.286339: step 115520, loss = 3.23 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 00:42:13.899135: step 115530, loss = 3.37 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 00:42:18.846474: step 115540, loss = 3.44 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 00:42:24.414096: step 115550, loss = 3.19 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 00:42:29.176741: step 115560, loss = 3.20 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 00:42:34.036711: step 115570, loss = 3.38 (253.7 examples/sec; 0.505 sec/batch)
2016-07-16 00:42:40.586134: step 115580, loss = 3.32 (207.3 examples/sec; 0.618 sec/batch)
2016-07-16 00:42:45.781764: step 115590, loss = 3.35 (248.7 examples/sec; 0.515 sec/batch)
2016-07-16 00:42:50.484920: step 115600, loss = 3.11 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 00:42:56.336862: step 115610, loss = 3.39 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 00:43:01.155518: step 115620, loss = 3.42 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 00:43:07.549524: step 115630, loss = 3.52 (200.6 examples/sec; 0.638 sec/batch)
2016-07-16 00:43:12.976553: step 115640, loss = 3.54 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 00:43:17.717820: step 115650, loss = 3.40 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 00:43:22.582959: step 115660, loss = 3.02 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 00:43:27.339959: step 115670, loss = 3.51 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 00:43:32.103545: step 115680, loss = 3.36 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 00:43:36.731389: step 115690, loss = 3.38 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 00:43:41.381272: step 115700, loss = 3.58 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 00:43:48.144980: step 115710, loss = 3.13 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 00:43:53.897048: step 115720, loss = 3.15 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 00:43:58.975903: step 115730, loss = 3.45 (200.7 examples/sec; 0.638 sec/batch)
2016-07-16 00:44:04.648221: step 115740, loss = 3.21 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 00:44:10.401095: step 115750, loss = 3.42 (252.5 examples/sec; 0.507 sec/batch)
2016-07-16 00:44:15.672159: step 115760, loss = 3.38 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 00:44:21.120190: step 115770, loss = 3.23 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 00:44:26.876428: step 115780, loss = 3.52 (253.8 examples/sec; 0.504 sec/batch)
2016-07-16 00:44:32.255625: step 115790, loss = 3.31 (207.6 examples/sec; 0.616 sec/batch)
2016-07-16 00:44:37.616291: step 115800, loss = 3.26 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 00:44:44.555341: step 115810, loss = 3.32 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 00:44:49.309455: step 115820, loss = 3.33 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 00:44:54.257610: step 115830, loss = 3.10 (230.9 examples/sec; 0.554 sec/batch)
2016-07-16 00:45:00.845960: step 115840, loss = 3.26 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 00:45:06.050230: step 115850, loss = 3.35 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 00:45:10.719111: step 115860, loss = 3.62 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 00:45:15.523364: step 115870, loss = 3.28 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 00:45:20.300307: step 115880, loss = 3.39 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 00:45:26.401707: step 115890, loss = 3.31 (194.5 examples/sec; 0.658 sec/batch)
2016-07-16 00:45:32.036099: step 115900, loss = 3.61 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 00:45:37.835621: step 115910, loss = 3.65 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 00:45:42.869172: step 115920, loss = 3.29 (191.3 examples/sec; 0.669 sec/batch)
2016-07-16 00:45:49.337792: step 115930, loss = 3.32 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 00:45:54.444637: step 115940, loss = 3.32 (222.7 examples/sec; 0.575 sec/batch)
2016-07-16 00:46:00.147580: step 115950, loss = 3.28 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 00:46:04.900708: step 115960, loss = 3.27 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 00:46:09.789077: step 115970, loss = 3.34 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 00:46:16.268027: step 115980, loss = 3.73 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 00:46:21.614665: step 115990, loss = 3.38 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 00:46:26.343631: step 116000, loss = 3.52 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 00:46:32.983112: step 116010, loss = 3.30 (192.2 examples/sec; 0.666 sec/batch)
2016-07-16 00:46:39.033235: step 116020, loss = 3.39 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 00:46:44.429895: step 116030, loss = 3.51 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 00:46:49.735774: step 116040, loss = 3.21 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 00:46:55.525772: step 116050, loss = 3.40 (251.3 examples/sec; 0.509 sec/batch)
2016-07-16 00:47:01.078662: step 116060, loss = 3.28 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 00:47:06.262794: step 116070, loss = 3.22 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 00:47:12.004183: step 116080, loss = 3.59 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 00:47:17.672545: step 116090, loss = 3.43 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 00:47:22.701328: step 116100, loss = 3.18 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 00:47:28.390269: step 116110, loss = 3.37 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 00:47:34.460953: step 116120, loss = 3.18 (193.5 examples/sec; 0.662 sec/batch)
2016-07-16 00:47:40.216351: step 116130, loss = 3.33 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 00:47:44.962199: step 116140, loss = 3.09 (275.0 examples/sec; 0.466 sec/batch)
2016-07-16 00:47:49.812482: step 116150, loss = 3.30 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 00:47:56.325237: step 116160, loss = 3.37 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 00:48:01.481913: step 116170, loss = 3.70 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 00:48:06.159185: step 116180, loss = 3.40 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 00:48:11.999034: step 116190, loss = 3.39 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 00:48:16.789605: step 116200, loss = 3.45 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 00:48:22.368651: step 116210, loss = 3.33 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 00:48:27.350322: step 116220, loss = 3.58 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 00:48:32.882238: step 116230, loss = 3.50 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 00:48:38.645440: step 116240, loss = 3.31 (254.0 examples/sec; 0.504 sec/batch)
2016-07-16 00:48:43.874540: step 116250, loss = 3.54 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 00:48:49.347116: step 116260, loss = 3.43 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 00:48:55.152532: step 116270, loss = 3.19 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 00:49:00.480691: step 116280, loss = 3.27 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 00:49:05.763289: step 116290, loss = 3.36 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 00:49:10.475651: step 116300, loss = 3.22 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 00:49:17.157629: step 116310, loss = 3.42 (192.4 examples/sec; 0.665 sec/batch)
2016-07-16 00:49:23.295867: step 116320, loss = 3.36 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 00:49:28.142000: step 116330, loss = 3.35 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 00:49:32.919553: step 116340, loss = 3.36 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 00:49:39.076262: step 116350, loss = 3.60 (209.4 examples/sec; 0.611 sec/batch)
2016-07-16 00:49:44.688131: step 116360, loss = 3.25 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 00:49:49.420147: step 116370, loss = 3.36 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 00:49:54.059315: step 116380, loss = 3.49 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 00:49:58.825270: step 116390, loss = 3.35 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 00:50:03.448228: step 116400, loss = 3.42 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 00:50:09.020586: step 116410, loss = 3.41 (286.3 examples/sec; 0.447 sec/batch)
2016-07-16 00:50:13.721532: step 116420, loss = 3.38 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 00:50:19.455442: step 116430, loss = 3.44 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 00:50:24.258870: step 116440, loss = 3.47 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 00:50:29.043804: step 116450, loss = 3.39 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 00:50:35.261904: step 116460, loss = 3.35 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 00:50:40.810603: step 116470, loss = 3.42 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 00:50:45.601403: step 116480, loss = 3.32 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 00:50:50.550024: step 116490, loss = 3.46 (241.0 examples/sec; 0.531 sec/batch)
2016-07-16 00:50:57.111053: step 116500, loss = 3.35 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 00:51:03.418748: step 116510, loss = 3.33 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 00:51:08.170932: step 116520, loss = 3.45 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 00:51:14.162031: step 116530, loss = 3.31 (191.7 examples/sec; 0.668 sec/batch)
2016-07-16 00:51:20.006689: step 116540, loss = 3.29 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 00:51:24.782268: step 116550, loss = 3.24 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 00:51:29.626105: step 116560, loss = 3.33 (257.0 examples/sec; 0.498 sec/batch)
2016-07-16 00:51:35.966658: step 116570, loss = 3.25 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 00:51:41.401340: step 116580, loss = 3.40 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 00:51:46.115525: step 116590, loss = 3.32 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 00:51:50.981269: step 116600, loss = 3.26 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 00:51:56.672437: step 116610, loss = 3.16 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 00:52:02.764598: step 116620, loss = 3.43 (193.4 examples/sec; 0.662 sec/batch)
2016-07-16 00:52:08.478822: step 116630, loss = 3.32 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 00:52:13.258267: step 116640, loss = 3.18 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 00:52:18.161628: step 116650, loss = 3.21 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 00:52:22.833814: step 116660, loss = 3.17 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 00:52:27.469971: step 116670, loss = 3.34 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 00:52:32.172560: step 116680, loss = 3.08 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 00:52:36.808112: step 116690, loss = 3.29 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 00:52:42.487459: step 116700, loss = 3.20 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 00:52:48.382619: step 116710, loss = 3.33 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 00:52:53.198571: step 116720, loss = 3.21 (249.4 examples/sec; 0.513 sec/batch)
2016-07-16 00:52:59.444763: step 116730, loss = 3.41 (198.9 examples/sec; 0.644 sec/batch)
2016-07-16 00:53:05.060676: step 116740, loss = 3.35 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 00:53:09.783341: step 116750, loss = 3.36 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 00:53:14.694050: step 116760, loss = 3.18 (241.1 examples/sec; 0.531 sec/batch)
2016-07-16 00:53:21.280488: step 116770, loss = 3.23 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 00:53:26.490300: step 116780, loss = 3.18 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 00:53:31.164573: step 116790, loss = 3.38 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 00:53:35.985090: step 116800, loss = 3.30 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 00:53:41.764037: step 116810, loss = 3.30 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 00:53:48.126481: step 116820, loss = 3.03 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 00:53:53.597372: step 116830, loss = 3.48 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 00:53:58.310029: step 116840, loss = 3.21 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 00:54:03.143761: step 116850, loss = 3.33 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 00:54:07.895791: step 116860, loss = 3.47 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 00:54:13.661278: step 116870, loss = 3.57 (193.2 examples/sec; 0.663 sec/batch)
2016-07-16 00:54:19.685363: step 116880, loss = 3.28 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 00:54:24.469903: step 116890, loss = 3.31 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 00:54:29.280303: step 116900, loss = 3.28 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 00:54:36.798235: step 116910, loss = 3.33 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 00:54:42.120169: step 116920, loss = 3.44 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 00:54:46.811536: step 116930, loss = 3.30 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 00:54:52.104431: step 116940, loss = 3.40 (189.7 examples/sec; 0.675 sec/batch)
2016-07-16 00:54:58.610048: step 116950, loss = 3.34 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 00:55:03.473459: step 116960, loss = 3.33 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 00:55:08.269485: step 116970, loss = 3.27 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 00:55:14.132189: step 116980, loss = 3.46 (190.3 examples/sec; 0.673 sec/batch)
2016-07-16 00:55:20.000841: step 116990, loss = 3.38 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 00:55:25.486215: step 117000, loss = 3.34 (207.7 examples/sec; 0.616 sec/batch)
2016-07-16 00:55:31.577189: step 117010, loss = 3.24 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 00:55:36.799995: step 117020, loss = 3.61 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 00:55:42.143721: step 117030, loss = 3.27 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 00:55:46.866344: step 117040, loss = 3.37 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 00:55:51.737846: step 117050, loss = 3.36 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 00:55:56.492502: step 117060, loss = 3.52 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 00:56:02.427037: step 117070, loss = 3.25 (187.0 examples/sec; 0.685 sec/batch)
2016-07-16 00:56:08.308501: step 117080, loss = 3.25 (252.4 examples/sec; 0.507 sec/batch)
2016-07-16 00:56:13.897546: step 117090, loss = 3.11 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 00:56:19.019034: step 117100, loss = 3.24 (228.9 examples/sec; 0.559 sec/batch)
2016-07-16 00:56:26.025641: step 117110, loss = 3.30 (245.9 examples/sec; 0.520 sec/batch)
2016-07-16 00:56:30.732851: step 117120, loss = 3.16 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 00:56:35.584386: step 117130, loss = 3.25 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 00:56:40.355275: step 117140, loss = 3.68 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 00:56:46.183405: step 117150, loss = 3.35 (188.4 examples/sec; 0.680 sec/batch)
2016-07-16 00:56:52.073895: step 117160, loss = 3.41 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 00:56:56.919200: step 117170, loss = 2.98 (253.1 examples/sec; 0.506 sec/batch)
2016-07-16 00:57:01.807095: step 117180, loss = 3.10 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 00:57:06.494371: step 117190, loss = 3.25 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 00:57:11.611366: step 117200, loss = 3.62 (191.2 examples/sec; 0.670 sec/batch)
2016-07-16 00:57:19.607824: step 117210, loss = 3.42 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 00:57:24.991147: step 117220, loss = 3.31 (204.6 examples/sec; 0.625 sec/batch)
2016-07-16 00:57:30.352000: step 117230, loss = 3.19 (242.9 examples/sec; 0.527 sec/batch)
2016-07-16 00:57:36.090022: step 117240, loss = 3.53 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 00:57:41.666635: step 117250, loss = 3.36 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 00:57:46.768684: step 117260, loss = 3.30 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 00:57:51.516365: step 117270, loss = 3.42 (255.8 examples/sec; 0.500 sec/batch)
2016-07-16 00:57:56.345241: step 117280, loss = 3.20 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 00:58:01.116336: step 117290, loss = 3.16 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 00:58:05.852750: step 117300, loss = 3.45 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 00:58:11.679101: step 117310, loss = 3.27 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 00:58:16.431097: step 117320, loss = 3.55 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 00:58:22.243412: step 117330, loss = 3.66 (187.0 examples/sec; 0.685 sec/batch)
2016-07-16 00:58:28.154580: step 117340, loss = 3.33 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 00:58:33.667150: step 117350, loss = 3.36 (208.6 examples/sec; 0.614 sec/batch)
2016-07-16 00:58:38.815141: step 117360, loss = 3.37 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 00:58:43.531143: step 117370, loss = 3.40 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 00:58:49.047400: step 117380, loss = 3.13 (188.5 examples/sec; 0.679 sec/batch)
2016-07-16 00:58:55.300466: step 117390, loss = 3.16 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 00:59:00.148672: step 117400, loss = 3.44 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 00:59:06.013050: step 117410, loss = 3.58 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 00:59:10.679775: step 117420, loss = 3.44 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 00:59:15.917732: step 117430, loss = 3.23 (187.1 examples/sec; 0.684 sec/batch)
2016-07-16 00:59:22.441414: step 117440, loss = 3.30 (201.1 examples/sec; 0.637 sec/batch)
2016-07-16 00:59:27.390009: step 117450, loss = 3.34 (272.6 examples/sec; 0.469 sec/batch)
2016-07-16 00:59:32.094883: step 117460, loss = 3.09 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 00:59:36.894217: step 117470, loss = 3.14 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 00:59:41.545701: step 117480, loss = 3.23 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 00:59:46.206930: step 117490, loss = 3.23 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 00:59:51.964236: step 117500, loss = 3.36 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 00:59:58.781844: step 117510, loss = 3.55 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 01:00:03.663111: step 117520, loss = 3.36 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 01:00:08.431186: step 117530, loss = 3.22 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 01:00:13.189783: step 117540, loss = 3.61 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 01:00:18.024600: step 117550, loss = 3.24 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 01:00:24.346180: step 117560, loss = 3.32 (208.4 examples/sec; 0.614 sec/batch)
2016-07-16 01:00:29.768124: step 117570, loss = 2.97 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 01:00:35.546962: step 117580, loss = 3.40 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 01:00:40.347054: step 117590, loss = 3.22 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 01:00:45.165167: step 117600, loss = 3.37 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 01:00:52.510446: step 117610, loss = 3.29 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 01:00:58.021374: step 117620, loss = 3.26 (257.8 examples/sec; 0.497 sec/batch)
2016-07-16 01:01:03.795591: step 117630, loss = 3.35 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 01:01:09.163120: step 117640, loss = 3.24 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 01:01:14.428036: step 117650, loss = 3.30 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 01:01:19.099545: step 117660, loss = 3.50 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 01:01:23.760826: step 117670, loss = 3.52 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 01:01:28.957603: step 117680, loss = 3.65 (196.0 examples/sec; 0.653 sec/batch)
2016-07-16 01:01:34.321971: step 117690, loss = 3.49 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 01:01:39.036506: step 117700, loss = 3.42 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 01:01:44.860716: step 117710, loss = 3.24 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 01:01:49.678180: step 117720, loss = 3.39 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 01:01:54.491626: step 117730, loss = 3.29 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:01:59.148569: step 117740, loss = 3.41 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 01:02:04.143163: step 117750, loss = 3.20 (201.6 examples/sec; 0.635 sec/batch)
2016-07-16 01:02:09.697983: step 117760, loss = 3.29 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 01:02:15.475394: step 117770, loss = 3.10 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 01:02:20.287572: step 117780, loss = 3.24 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 01:02:24.981503: step 117790, loss = 3.21 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 01:02:29.748312: step 117800, loss = 3.09 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 01:02:35.826004: step 117810, loss = 3.12 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 01:02:42.344498: step 117820, loss = 3.27 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 01:02:47.345581: step 117830, loss = 3.39 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 01:02:52.087402: step 117840, loss = 3.17 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 01:02:56.992620: step 117850, loss = 3.27 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 01:03:01.827982: step 117860, loss = 3.22 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 01:03:08.070846: step 117870, loss = 3.44 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 01:03:13.623466: step 117880, loss = 3.10 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 01:03:18.316629: step 117890, loss = 3.13 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 01:03:23.208917: step 117900, loss = 3.06 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 01:03:28.916534: step 117910, loss = 3.43 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 01:03:34.833012: step 117920, loss = 3.25 (189.4 examples/sec; 0.676 sec/batch)
2016-07-16 01:03:40.645939: step 117930, loss = 3.39 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 01:03:45.419542: step 117940, loss = 3.23 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 01:03:50.249871: step 117950, loss = 3.38 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 01:03:56.594111: step 117960, loss = 3.34 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 01:04:02.057527: step 117970, loss = 3.33 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 01:04:06.814842: step 117980, loss = 3.04 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 01:04:11.872324: step 117990, loss = 3.48 (188.2 examples/sec; 0.680 sec/batch)
2016-07-16 01:04:18.388208: step 118000, loss = 3.47 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 01:04:25.063704: step 118010, loss = 3.28 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 01:04:30.377923: step 118020, loss = 3.53 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 01:04:36.118061: step 118030, loss = 3.47 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 01:04:40.968803: step 118040, loss = 3.50 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 01:04:45.685530: step 118050, loss = 3.23 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 01:04:50.446038: step 118060, loss = 3.36 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 01:04:55.405556: step 118070, loss = 3.36 (234.5 examples/sec; 0.546 sec/batch)
2016-07-16 01:05:01.986599: step 118080, loss = 3.36 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 01:05:07.141488: step 118090, loss = 3.13 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 01:05:11.865512: step 118100, loss = 3.48 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 01:05:17.681193: step 118110, loss = 3.38 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 01:05:22.499148: step 118120, loss = 3.16 (271.5 examples/sec; 0.472 sec/batch)
2016-07-16 01:05:28.886476: step 118130, loss = 3.31 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 01:05:34.281932: step 118140, loss = 3.50 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 01:05:40.036493: step 118150, loss = 3.42 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 01:05:45.433963: step 118160, loss = 3.13 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 01:05:50.725525: step 118170, loss = 3.16 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 01:05:55.450653: step 118180, loss = 3.24 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 01:06:00.287841: step 118190, loss = 3.16 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:06:05.070692: step 118200, loss = 3.11 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 01:06:12.322440: step 118210, loss = 3.38 (200.8 examples/sec; 0.638 sec/batch)
2016-07-16 01:06:17.857203: step 118220, loss = 3.37 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 01:06:23.571680: step 118230, loss = 3.31 (253.6 examples/sec; 0.505 sec/batch)
2016-07-16 01:06:28.761347: step 118240, loss = 3.26 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 01:06:34.226865: step 118250, loss = 3.20 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 01:06:39.033869: step 118260, loss = 3.17 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 01:06:43.865587: step 118270, loss = 3.34 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 01:06:48.568430: step 118280, loss = 3.34 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 01:06:54.349657: step 118290, loss = 3.53 (189.5 examples/sec; 0.675 sec/batch)
2016-07-16 01:07:00.388334: step 118300, loss = 3.23 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 01:07:07.159767: step 118310, loss = 3.37 (201.7 examples/sec; 0.634 sec/batch)
2016-07-16 01:07:12.234313: step 118320, loss = 3.24 (208.3 examples/sec; 0.615 sec/batch)
2016-07-16 01:07:17.798629: step 118330, loss = 3.19 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 01:07:22.536918: step 118340, loss = 3.25 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 01:07:27.395493: step 118350, loss = 3.19 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 01:07:33.988229: step 118360, loss = 3.41 (201.2 examples/sec; 0.636 sec/batch)
2016-07-16 01:07:39.313021: step 118370, loss = 3.39 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 01:07:44.016092: step 118380, loss = 3.39 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 01:07:49.463234: step 118390, loss = 3.11 (183.0 examples/sec; 0.699 sec/batch)
2016-07-16 01:07:55.743932: step 118400, loss = 3.28 (253.8 examples/sec; 0.504 sec/batch)
2016-07-16 01:08:01.551325: step 118410, loss = 3.40 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 01:08:06.360148: step 118420, loss = 3.44 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 01:08:11.088965: step 118430, loss = 3.38 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 01:08:15.945727: step 118440, loss = 3.19 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 01:08:20.723475: step 118450, loss = 3.26 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 01:08:25.501959: step 118460, loss = 3.46 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 01:08:30.340933: step 118470, loss = 3.23 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 01:08:35.058913: step 118480, loss = 3.36 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 01:08:40.132449: step 118490, loss = 3.27 (186.1 examples/sec; 0.688 sec/batch)
2016-07-16 01:08:46.664626: step 118500, loss = 3.29 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 01:08:52.777308: step 118510, loss = 3.34 (269.2 examples/sec; 0.476 sec/batch)
2016-07-16 01:08:57.600412: step 118520, loss = 3.36 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 01:09:02.329272: step 118530, loss = 3.37 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 01:09:07.171893: step 118540, loss = 3.08 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 01:09:13.691136: step 118550, loss = 3.25 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 01:09:18.983527: step 118560, loss = 3.46 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 01:09:24.716379: step 118570, loss = 3.10 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 01:09:30.219466: step 118580, loss = 3.03 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 01:09:35.373334: step 118590, loss = 3.34 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 01:09:40.072280: step 118600, loss = 3.07 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 01:09:45.891751: step 118610, loss = 3.20 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 01:09:50.690432: step 118620, loss = 3.12 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 01:09:57.039435: step 118630, loss = 3.05 (208.1 examples/sec; 0.615 sec/batch)
2016-07-16 01:10:02.472611: step 118640, loss = 3.04 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 01:10:07.195662: step 118650, loss = 3.24 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:10:12.088246: step 118660, loss = 3.39 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 01:10:16.915819: step 118670, loss = 3.30 (237.1 examples/sec; 0.540 sec/batch)
2016-07-16 01:10:21.702847: step 118680, loss = 3.14 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:10:26.523146: step 118690, loss = 3.34 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 01:10:32.851011: step 118700, loss = 3.45 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 01:10:39.550694: step 118710, loss = 3.27 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 01:10:45.276166: step 118720, loss = 3.36 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 01:10:50.962635: step 118730, loss = 2.97 (202.4 examples/sec; 0.632 sec/batch)
2016-07-16 01:10:55.981587: step 118740, loss = 3.36 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 01:11:00.746252: step 118750, loss = 3.35 (245.6 examples/sec; 0.521 sec/batch)
2016-07-16 01:11:06.503975: step 118760, loss = 3.32 (185.6 examples/sec; 0.690 sec/batch)
2016-07-16 01:11:12.559176: step 118770, loss = 3.50 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 01:11:17.391432: step 118780, loss = 3.33 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:11:22.168396: step 118790, loss = 3.30 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 01:11:28.404878: step 118800, loss = 3.29 (207.6 examples/sec; 0.617 sec/batch)
2016-07-16 01:11:35.194854: step 118810, loss = 3.24 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 01:11:39.877795: step 118820, loss = 3.39 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 01:11:44.497766: step 118830, loss = 3.53 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 01:11:49.766538: step 118840, loss = 3.48 (202.4 examples/sec; 0.632 sec/batch)
2016-07-16 01:11:55.113918: step 118850, loss = 3.57 (242.7 examples/sec; 0.527 sec/batch)
2016-07-16 01:11:59.836679: step 118860, loss = 3.44 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 01:12:04.690642: step 118870, loss = 3.04 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 01:12:09.427781: step 118880, loss = 3.17 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 01:12:15.462620: step 118890, loss = 3.27 (180.6 examples/sec; 0.709 sec/batch)
2016-07-16 01:12:21.291721: step 118900, loss = 3.29 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 01:12:28.172942: step 118910, loss = 3.16 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 01:12:33.008626: step 118920, loss = 3.28 (272.6 examples/sec; 0.469 sec/batch)
2016-07-16 01:12:37.778347: step 118930, loss = 3.05 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 01:12:42.531664: step 118940, loss = 3.17 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 01:12:47.351919: step 118950, loss = 3.20 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 01:12:53.864433: step 118960, loss = 3.06 (195.7 examples/sec; 0.654 sec/batch)
2016-07-16 01:12:59.233378: step 118970, loss = 3.28 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 01:13:03.982382: step 118980, loss = 3.12 (262.0 examples/sec; 0.488 sec/batch)
2016-07-16 01:13:09.409217: step 118990, loss = 3.25 (188.7 examples/sec; 0.678 sec/batch)
2016-07-16 01:13:15.811847: step 119000, loss = 3.14 (225.0 examples/sec; 0.569 sec/batch)
2016-07-16 01:13:21.715644: step 119010, loss = 3.21 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 01:13:26.349507: step 119020, loss = 3.13 (283.5 examples/sec; 0.452 sec/batch)
2016-07-16 01:13:31.079529: step 119030, loss = 3.32 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 01:13:35.702986: step 119040, loss = 3.09 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 01:13:41.093618: step 119050, loss = 3.18 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 01:13:46.531425: step 119060, loss = 3.07 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 01:13:51.218676: step 119070, loss = 3.09 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:13:55.858796: step 119080, loss = 3.12 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 01:14:00.566415: step 119090, loss = 3.07 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 01:14:05.161257: step 119100, loss = 3.42 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 01:14:11.827254: step 119110, loss = 3.45 (218.0 examples/sec; 0.587 sec/batch)
2016-07-16 01:14:16.671762: step 119120, loss = 3.24 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 01:14:21.409151: step 119130, loss = 3.34 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 01:14:26.181766: step 119140, loss = 3.11 (286.1 examples/sec; 0.447 sec/batch)
2016-07-16 01:14:30.960666: step 119150, loss = 3.47 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 01:14:37.339723: step 119160, loss = 3.34 (199.7 examples/sec; 0.641 sec/batch)
2016-07-16 01:14:42.892965: step 119170, loss = 3.42 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 01:14:47.680307: step 119180, loss = 3.34 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 01:14:52.289048: step 119190, loss = 3.33 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 01:14:57.455041: step 119200, loss = 3.07 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 01:15:04.094712: step 119210, loss = 3.34 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 01:15:08.836381: step 119220, loss = 3.29 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 01:15:14.496982: step 119230, loss = 3.32 (185.9 examples/sec; 0.688 sec/batch)
2016-07-16 01:15:20.598317: step 119240, loss = 3.26 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 01:15:25.939998: step 119250, loss = 3.07 (203.0 examples/sec; 0.630 sec/batch)
2016-07-16 01:15:31.221384: step 119260, loss = 3.42 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 01:15:35.955977: step 119270, loss = 3.23 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 01:15:41.272353: step 119280, loss = 3.37 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 01:15:47.717262: step 119290, loss = 3.31 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 01:15:52.592846: step 119300, loss = 3.53 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 01:15:58.385595: step 119310, loss = 3.27 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 01:16:03.076570: step 119320, loss = 3.17 (279.2 examples/sec; 0.459 sec/batch)
2016-07-16 01:16:08.050883: step 119330, loss = 3.18 (226.9 examples/sec; 0.564 sec/batch)
2016-07-16 01:16:14.636925: step 119340, loss = 3.37 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 01:16:19.826063: step 119350, loss = 2.99 (248.2 examples/sec; 0.516 sec/batch)
2016-07-16 01:16:24.571703: step 119360, loss = 3.08 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 01:16:30.116813: step 119370, loss = 3.01 (190.7 examples/sec; 0.671 sec/batch)
2016-07-16 01:16:36.318075: step 119380, loss = 3.29 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 01:16:41.114524: step 119390, loss = 3.06 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 01:16:45.859866: step 119400, loss = 3.17 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 01:16:51.540648: step 119410, loss = 3.22 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 01:16:56.582677: step 119420, loss = 3.14 (187.3 examples/sec; 0.683 sec/batch)
2016-07-16 01:17:03.119892: step 119430, loss = 3.38 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 01:17:08.157161: step 119440, loss = 3.27 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 01:17:12.867728: step 119450, loss = 3.53 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 01:17:17.677279: step 119460, loss = 3.10 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 01:17:22.461718: step 119470, loss = 3.32 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 01:17:28.602972: step 119480, loss = 3.51 (205.6 examples/sec; 0.622 sec/batch)
2016-07-16 01:17:34.229604: step 119490, loss = 3.17 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 01:17:38.984745: step 119500, loss = 3.08 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 01:17:44.613565: step 119510, loss = 3.36 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 01:17:49.245375: step 119520, loss = 3.43 (289.8 examples/sec; 0.442 sec/batch)
2016-07-16 01:17:53.892815: step 119530, loss = 3.25 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 01:17:58.551055: step 119540, loss = 3.25 (282.9 examples/sec; 0.453 sec/batch)
2016-07-16 01:18:03.197370: step 119550, loss = 3.09 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 01:18:08.954230: step 119560, loss = 3.40 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 01:18:13.738375: step 119570, loss = 3.17 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 01:18:18.552687: step 119580, loss = 2.97 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 01:18:24.727991: step 119590, loss = 3.41 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 01:18:30.292592: step 119600, loss = 3.15 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 01:18:36.101542: step 119610, loss = 3.23 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 01:18:41.053106: step 119620, loss = 3.32 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 01:18:46.014229: step 119630, loss = 3.28 (251.6 examples/sec; 0.509 sec/batch)
2016-07-16 01:18:51.377427: step 119640, loss = 3.20 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 01:18:55.964616: step 119650, loss = 3.32 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 01:19:00.898692: step 119660, loss = 3.31 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 01:19:06.479759: step 119670, loss = 3.25 (255.8 examples/sec; 0.500 sec/batch)
2016-07-16 01:19:11.298311: step 119680, loss = 3.21 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 01:19:16.270317: step 119690, loss = 3.39 (224.9 examples/sec; 0.569 sec/batch)
2016-07-16 01:19:22.869664: step 119700, loss = 3.45 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 01:19:29.502170: step 119710, loss = 3.17 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 01:19:34.746359: step 119720, loss = 3.34 (284.4 examples/sec; 0.450 sec/batch)
2016-07-16 01:19:39.423920: step 119730, loss = 3.04 (252.5 examples/sec; 0.507 sec/batch)
2016-07-16 01:19:44.012364: step 119740, loss = 3.16 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 01:19:48.620440: step 119750, loss = 3.20 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 01:19:53.716448: step 119760, loss = 3.12 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 01:19:59.068790: step 119770, loss = 3.30 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 01:20:03.821566: step 119780, loss = 3.18 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 01:20:08.954930: step 119790, loss = 3.18 (187.5 examples/sec; 0.683 sec/batch)
2016-07-16 01:20:15.460841: step 119800, loss = 3.34 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 01:20:22.035886: step 119810, loss = 3.19 (202.4 examples/sec; 0.633 sec/batch)
2016-07-16 01:20:27.166863: step 119820, loss = 3.34 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 01:20:31.812981: step 119830, loss = 3.19 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 01:20:37.580060: step 119840, loss = 3.04 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 01:20:42.330516: step 119850, loss = 3.30 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 01:20:47.120334: step 119860, loss = 3.35 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 01:20:51.914139: step 119870, loss = 3.27 (251.7 examples/sec; 0.509 sec/batch)
2016-07-16 01:20:56.511786: step 119880, loss = 3.18 (283.8 examples/sec; 0.451 sec/batch)
2016-07-16 01:21:01.123919: step 119890, loss = 3.05 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 01:21:06.358161: step 119900, loss = 3.44 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 01:21:12.828025: step 119910, loss = 3.30 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:21:17.491210: step 119920, loss = 3.29 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 01:21:22.099261: step 119930, loss = 3.45 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 01:21:26.744089: step 119940, loss = 3.33 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 01:21:32.439594: step 119950, loss = 3.19 (246.0 examples/sec; 0.520 sec/batch)
2016-07-16 01:21:37.259248: step 119960, loss = 3.34 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 01:21:41.919286: step 119970, loss = 3.45 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 01:21:46.583632: step 119980, loss = 3.21 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 01:21:51.231143: step 119990, loss = 3.28 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 01:21:55.909315: step 120000, loss = 3.39 (276.2 examples/sec; 0.464 sec/batch)
2016-07-16 01:22:02.872204: step 120010, loss = 3.60 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 01:22:07.557127: step 120020, loss = 3.38 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 01:22:12.540503: step 120030, loss = 3.05 (210.0 examples/sec; 0.610 sec/batch)
2016-07-16 01:22:19.118358: step 120040, loss = 3.22 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 01:22:24.187774: step 120050, loss = 3.23 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 01:22:28.909111: step 120060, loss = 3.23 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 01:22:33.733401: step 120070, loss = 3.22 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 01:22:38.404725: step 120080, loss = 3.26 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 01:22:43.078821: step 120090, loss = 3.27 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 01:22:47.733659: step 120100, loss = 3.20 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 01:22:54.619471: step 120110, loss = 3.05 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 01:22:59.399963: step 120120, loss = 2.94 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 01:23:04.326233: step 120130, loss = 3.16 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 01:23:08.995946: step 120140, loss = 3.07 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 01:23:13.642538: step 120150, loss = 3.26 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 01:23:19.014241: step 120160, loss = 3.15 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 01:23:24.188159: step 120170, loss = 3.26 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 01:23:28.902523: step 120180, loss = 3.26 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 01:23:33.721023: step 120190, loss = 3.12 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 01:23:38.414076: step 120200, loss = 3.23 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 01:23:44.056614: step 120210, loss = 3.20 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 01:23:49.775814: step 120220, loss = 3.11 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 01:23:54.418352: step 120230, loss = 2.98 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 01:24:00.054921: step 120240, loss = 3.04 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 01:24:05.201910: step 120250, loss = 3.06 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 01:24:10.787470: step 120260, loss = 3.30 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 01:24:15.504276: step 120270, loss = 3.26 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 01:24:20.401146: step 120280, loss = 3.10 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 01:24:25.146403: step 120290, loss = 3.09 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 01:24:29.777214: step 120300, loss = 3.22 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 01:24:35.277967: step 120310, loss = 3.36 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 01:24:39.897115: step 120320, loss = 3.41 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 01:24:44.615705: step 120330, loss = 3.45 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 01:24:49.220241: step 120340, loss = 3.46 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 01:24:54.485877: step 120350, loss = 3.42 (200.1 examples/sec; 0.640 sec/batch)
2016-07-16 01:24:59.807901: step 120360, loss = 3.10 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 01:25:04.573747: step 120370, loss = 3.27 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 01:25:09.514693: step 120380, loss = 3.17 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 01:25:14.138540: step 120390, loss = 3.31 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 01:25:18.772699: step 120400, loss = 3.30 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 01:25:24.454732: step 120410, loss = 3.04 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 01:25:29.163062: step 120420, loss = 3.40 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 01:25:33.790994: step 120430, loss = 3.20 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 01:25:39.229174: step 120440, loss = 3.40 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 01:25:44.342037: step 120450, loss = 3.29 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 01:25:49.131596: step 120460, loss = 3.30 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 01:25:54.725535: step 120470, loss = 3.16 (188.4 examples/sec; 0.679 sec/batch)
2016-07-16 01:26:00.930693: step 120480, loss = 3.23 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 01:26:05.734490: step 120490, loss = 3.08 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 01:26:10.538043: step 120500, loss = 2.95 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 01:26:16.323995: step 120510, loss = 3.20 (252.4 examples/sec; 0.507 sec/batch)
2016-07-16 01:26:20.946394: step 120520, loss = 3.11 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 01:26:25.503117: step 120530, loss = 2.97 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 01:26:30.136083: step 120540, loss = 3.40 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 01:26:34.794412: step 120550, loss = 2.93 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 01:26:40.488075: step 120560, loss = 3.12 (202.2 examples/sec; 0.633 sec/batch)
2016-07-16 01:26:45.322111: step 120570, loss = 3.23 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 01:26:50.106732: step 120580, loss = 3.28 (247.3 examples/sec; 0.518 sec/batch)
2016-07-16 01:26:54.886505: step 120590, loss = 3.06 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 01:26:59.584288: step 120600, loss = 3.14 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 01:27:05.479819: step 120610, loss = 3.55 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 01:27:10.985189: step 120620, loss = 3.17 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 01:27:15.742690: step 120630, loss = 3.30 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 01:27:20.735448: step 120640, loss = 3.55 (220.5 examples/sec; 0.580 sec/batch)
2016-07-16 01:27:27.268133: step 120650, loss = 3.04 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 01:27:32.428330: step 120660, loss = 3.08 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 01:27:37.133471: step 120670, loss = 3.52 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 01:27:41.934218: step 120680, loss = 3.21 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:27:46.695436: step 120690, loss = 3.33 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 01:27:52.777650: step 120700, loss = 3.21 (196.4 examples/sec; 0.652 sec/batch)
2016-07-16 01:27:59.661354: step 120710, loss = 3.14 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 01:28:04.405513: step 120720, loss = 3.48 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 01:28:09.052400: step 120730, loss = 3.08 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 01:28:14.125946: step 120740, loss = 2.92 (205.6 examples/sec; 0.622 sec/batch)
2016-07-16 01:28:19.588203: step 120750, loss = 3.41 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 01:28:24.320505: step 120760, loss = 3.10 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 01:28:28.945913: step 120770, loss = 3.16 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 01:28:33.544492: step 120780, loss = 3.30 (283.4 examples/sec; 0.452 sec/batch)
2016-07-16 01:28:38.182686: step 120790, loss = 3.52 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 01:28:43.645228: step 120800, loss = 3.21 (202.0 examples/sec; 0.634 sec/batch)
2016-07-16 01:28:49.845671: step 120810, loss = 3.42 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 01:28:54.638154: step 120820, loss = 3.32 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 01:29:00.667709: step 120830, loss = 3.36 (195.1 examples/sec; 0.656 sec/batch)
2016-07-16 01:29:06.287394: step 120840, loss = 3.15 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 01:29:10.924249: step 120850, loss = 3.39 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 01:29:16.633043: step 120860, loss = 3.49 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 01:29:21.502597: step 120870, loss = 3.07 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 01:29:26.239146: step 120880, loss = 3.31 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 01:29:32.142247: step 120890, loss = 3.33 (191.0 examples/sec; 0.670 sec/batch)
2016-07-16 01:29:37.980562: step 120900, loss = 3.40 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 01:29:43.590533: step 120910, loss = 3.20 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 01:29:48.228173: step 120920, loss = 3.12 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 01:29:52.807558: step 120930, loss = 3.29 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 01:29:57.486042: step 120940, loss = 3.42 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 01:30:02.129238: step 120950, loss = 3.21 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 01:30:07.592960: step 120960, loss = 3.23 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 01:30:12.763273: step 120970, loss = 3.21 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 01:30:17.447856: step 120980, loss = 3.24 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 01:30:22.073270: step 120990, loss = 3.32 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 01:30:27.570275: step 121000, loss = 3.28 (202.2 examples/sec; 0.633 sec/batch)
2016-07-16 01:30:33.596491: step 121010, loss = 3.12 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 01:30:38.883140: step 121020, loss = 3.39 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 01:30:44.159548: step 121030, loss = 3.31 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 01:30:48.874433: step 121040, loss = 3.17 (251.6 examples/sec; 0.509 sec/batch)
2016-07-16 01:30:53.759205: step 121050, loss = 2.94 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 01:30:58.373736: step 121060, loss = 3.59 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 01:31:03.028987: step 121070, loss = 3.32 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 01:31:08.747777: step 121080, loss = 3.26 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 01:31:13.954068: step 121090, loss = 3.42 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 01:31:19.399163: step 121100, loss = 3.16 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 01:31:25.153295: step 121110, loss = 3.18 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 01:31:29.810561: step 121120, loss = 3.18 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 01:31:34.477940: step 121130, loss = 3.43 (284.4 examples/sec; 0.450 sec/batch)
2016-07-16 01:31:39.084541: step 121140, loss = 3.18 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 01:31:44.800696: step 121150, loss = 3.28 (218.4 examples/sec; 0.586 sec/batch)
2016-07-16 01:31:49.631645: step 121160, loss = 3.53 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 01:31:54.432738: step 121170, loss = 3.06 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 01:32:00.306031: step 121180, loss = 3.22 (186.5 examples/sec; 0.686 sec/batch)
2016-07-16 01:32:06.236967: step 121190, loss = 3.04 (244.5 examples/sec; 0.524 sec/batch)
2016-07-16 01:32:11.035169: step 121200, loss = 3.19 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 01:32:16.844217: step 121210, loss = 3.16 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 01:32:21.505321: step 121220, loss = 3.14 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 01:32:26.110584: step 121230, loss = 3.09 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:32:31.511425: step 121240, loss = 3.27 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 01:32:36.693246: step 121250, loss = 3.37 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 01:32:41.331154: step 121260, loss = 3.11 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 01:32:46.009591: step 121270, loss = 3.15 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 01:32:50.712269: step 121280, loss = 3.49 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 01:32:56.334209: step 121290, loss = 3.09 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 01:33:01.337905: step 121300, loss = 3.08 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 01:33:06.934466: step 121310, loss = 3.22 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 01:33:11.549246: step 121320, loss = 3.07 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:33:17.244926: step 121330, loss = 3.27 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 01:33:22.109669: step 121340, loss = 3.21 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 01:33:26.825670: step 121350, loss = 3.21 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 01:33:31.568614: step 121360, loss = 3.17 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 01:33:36.415329: step 121370, loss = 3.20 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 01:33:41.102893: step 121380, loss = 3.25 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 01:33:45.673778: step 121390, loss = 3.30 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 01:33:50.375671: step 121400, loss = 3.12 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 01:33:55.968485: step 121410, loss = 3.10 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 01:34:00.634666: step 121420, loss = 3.32 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 01:34:06.396833: step 121430, loss = 3.26 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 01:34:11.990000: step 121440, loss = 2.95 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 01:34:17.092427: step 121450, loss = 3.00 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 01:34:21.734283: step 121460, loss = 3.27 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 01:34:26.373428: step 121470, loss = 3.19 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 01:34:30.985178: step 121480, loss = 3.32 (283.5 examples/sec; 0.452 sec/batch)
2016-07-16 01:34:35.640115: step 121490, loss = 3.27 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 01:34:41.384484: step 121500, loss = 3.28 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 01:34:47.210746: step 121510, loss = 3.30 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 01:34:52.015190: step 121520, loss = 3.16 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 01:34:56.746521: step 121530, loss = 3.34 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 01:35:01.335179: step 121540, loss = 3.14 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 01:35:06.355523: step 121550, loss = 3.17 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 01:35:11.766148: step 121560, loss = 3.31 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 01:35:16.544161: step 121570, loss = 3.24 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 01:35:21.353816: step 121580, loss = 3.63 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 01:35:26.081369: step 121590, loss = 3.16 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 01:35:30.846247: step 121600, loss = 3.21 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 01:35:36.408695: step 121610, loss = 3.23 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 01:35:41.073543: step 121620, loss = 3.26 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 01:35:45.669487: step 121630, loss = 3.12 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 01:35:50.335918: step 121640, loss = 3.14 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 01:35:55.759958: step 121650, loss = 3.02 (209.6 examples/sec; 0.611 sec/batch)
2016-07-16 01:36:00.895511: step 121660, loss = 2.98 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 01:36:05.605274: step 121670, loss = 3.10 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 01:36:10.415273: step 121680, loss = 3.24 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 01:36:15.046744: step 121690, loss = 3.09 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 01:36:19.695319: step 121700, loss = 3.32 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 01:36:26.489378: step 121710, loss = 3.16 (244.9 examples/sec; 0.523 sec/batch)
2016-07-16 01:36:31.241522: step 121720, loss = 3.15 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 01:36:36.089981: step 121730, loss = 3.20 (253.7 examples/sec; 0.505 sec/batch)
2016-07-16 01:36:40.799170: step 121740, loss = 3.18 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 01:36:45.992084: step 121750, loss = 3.14 (187.3 examples/sec; 0.683 sec/batch)
2016-07-16 01:36:52.489892: step 121760, loss = 3.19 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 01:36:57.539864: step 121770, loss = 3.47 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 01:37:02.198048: step 121780, loss = 3.13 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 01:37:06.785713: step 121790, loss = 3.34 (286.7 examples/sec; 0.446 sec/batch)
2016-07-16 01:37:11.425065: step 121800, loss = 3.03 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 01:37:17.068109: step 121810, loss = 3.02 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 01:37:22.042228: step 121820, loss = 3.10 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 01:37:27.586639: step 121830, loss = 3.32 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 01:37:33.326247: step 121840, loss = 3.41 (241.3 examples/sec; 0.530 sec/batch)
2016-07-16 01:37:38.226245: step 121850, loss = 3.09 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 01:37:42.992471: step 121860, loss = 3.18 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 01:37:47.729206: step 121870, loss = 3.28 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 01:37:52.363400: step 121880, loss = 3.20 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 01:37:56.958664: step 121890, loss = 2.92 (271.5 examples/sec; 0.472 sec/batch)
2016-07-16 01:38:01.543942: step 121900, loss = 3.21 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 01:38:07.938549: step 121910, loss = 3.35 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 01:38:12.859654: step 121920, loss = 3.15 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 01:38:17.729815: step 121930, loss = 3.04 (197.9 examples/sec; 0.647 sec/batch)
2016-07-16 01:38:23.325676: step 121940, loss = 3.34 (257.0 examples/sec; 0.498 sec/batch)
2016-07-16 01:38:28.073985: step 121950, loss = 3.49 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 01:38:32.733022: step 121960, loss = 3.11 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 01:38:37.457324: step 121970, loss = 3.50 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 01:38:42.067299: step 121980, loss = 3.07 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 01:38:47.315408: step 121990, loss = 3.34 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 01:38:52.595558: step 122000, loss = 3.23 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 01:38:59.604761: step 122010, loss = 3.36 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 01:39:04.378984: step 122020, loss = 3.31 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 01:39:09.384561: step 122030, loss = 3.15 (218.8 examples/sec; 0.585 sec/batch)
2016-07-16 01:39:15.978305: step 122040, loss = 3.30 (199.1 examples/sec; 0.643 sec/batch)
2016-07-16 01:39:21.130110: step 122050, loss = 3.25 (250.2 examples/sec; 0.512 sec/batch)
2016-07-16 01:39:25.863395: step 122060, loss = 3.39 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 01:39:30.697331: step 122070, loss = 3.33 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 01:39:35.342734: step 122080, loss = 3.08 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 01:39:40.030652: step 122090, loss = 3.60 (253.1 examples/sec; 0.506 sec/batch)
2016-07-16 01:39:44.638039: step 122100, loss = 3.21 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 01:39:50.726580: step 122110, loss = 3.24 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 01:39:56.155074: step 122120, loss = 3.32 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 01:40:00.853484: step 122130, loss = 3.30 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 01:40:05.745103: step 122140, loss = 3.24 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 01:40:10.429818: step 122150, loss = 3.36 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 01:40:15.093852: step 122160, loss = 3.25 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 01:40:19.687573: step 122170, loss = 2.89 (292.7 examples/sec; 0.437 sec/batch)
2016-07-16 01:40:24.443030: step 122180, loss = 3.39 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 01:40:30.182645: step 122190, loss = 3.17 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 01:40:34.986886: step 122200, loss = 3.31 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 01:40:40.744524: step 122210, loss = 2.97 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 01:40:47.182387: step 122220, loss = 3.59 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 01:40:52.305621: step 122230, loss = 3.20 (284.4 examples/sec; 0.450 sec/batch)
2016-07-16 01:40:56.978789: step 122240, loss = 3.29 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 01:41:02.745365: step 122250, loss = 3.39 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 01:41:07.560508: step 122260, loss = 3.13 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 01:41:12.383692: step 122270, loss = 3.22 (260.4 examples/sec; 0.491 sec/batch)
2016-07-16 01:41:17.094987: step 122280, loss = 3.17 (249.9 examples/sec; 0.512 sec/batch)
2016-07-16 01:41:21.713336: step 122290, loss = 3.12 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 01:41:26.817111: step 122300, loss = 3.13 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 01:41:33.532049: step 122310, loss = 3.21 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 01:41:38.304699: step 122320, loss = 3.53 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 01:41:44.029232: step 122330, loss = 3.18 (187.5 examples/sec; 0.683 sec/batch)
2016-07-16 01:41:49.050232: step 122340, loss = 3.32 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 01:41:54.012536: step 122350, loss = 3.07 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 01:41:59.626341: step 122360, loss = 3.10 (251.1 examples/sec; 0.510 sec/batch)
2016-07-16 01:42:04.408753: step 122370, loss = 3.23 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 01:42:09.344734: step 122380, loss = 3.13 (250.4 examples/sec; 0.511 sec/batch)
2016-07-16 01:42:15.240734: step 122390, loss = 3.09 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 01:42:19.836112: step 122400, loss = 3.14 (284.7 examples/sec; 0.450 sec/batch)
2016-07-16 01:42:25.510413: step 122410, loss = 3.23 (241.5 examples/sec; 0.530 sec/batch)
2016-07-16 01:42:31.253735: step 122420, loss = 3.25 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 01:42:36.062857: step 122430, loss = 3.12 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 01:42:40.897919: step 122440, loss = 3.35 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 01:42:45.591654: step 122450, loss = 3.12 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 01:42:50.255028: step 122460, loss = 3.02 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 01:42:54.937048: step 122470, loss = 3.40 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 01:42:59.550325: step 122480, loss = 3.16 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 01:43:04.165709: step 122490, loss = 3.39 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 01:43:08.802745: step 122500, loss = 3.38 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 01:43:14.959536: step 122510, loss = 3.10 (200.6 examples/sec; 0.638 sec/batch)
2016-07-16 01:43:20.345711: step 122520, loss = 3.08 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 01:43:25.096146: step 122530, loss = 3.11 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 01:43:30.267470: step 122540, loss = 3.24 (186.7 examples/sec; 0.685 sec/batch)
2016-07-16 01:43:36.737711: step 122550, loss = 3.36 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 01:43:41.765355: step 122560, loss = 3.17 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 01:43:46.526230: step 122570, loss = 3.27 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 01:43:51.346285: step 122580, loss = 3.31 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 01:43:55.961979: step 122590, loss = 3.02 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 01:44:00.605097: step 122600, loss = 3.20 (283.4 examples/sec; 0.452 sec/batch)
2016-07-16 01:44:06.227107: step 122610, loss = 3.05 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 01:44:10.863404: step 122620, loss = 3.15 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 01:44:15.515837: step 122630, loss = 3.17 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 01:44:20.160256: step 122640, loss = 3.22 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 01:44:25.880014: step 122650, loss = 3.32 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 01:44:30.705860: step 122660, loss = 3.16 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 01:44:35.458943: step 122670, loss = 3.29 (248.5 examples/sec; 0.515 sec/batch)
2016-07-16 01:44:40.182255: step 122680, loss = 3.10 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 01:44:45.036224: step 122690, loss = 3.07 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 01:44:49.716804: step 122700, loss = 3.23 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 01:44:55.201162: step 122710, loss = 3.27 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 01:44:59.876671: step 122720, loss = 3.31 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 01:45:05.561436: step 122730, loss = 3.26 (223.9 examples/sec; 0.572 sec/batch)
2016-07-16 01:45:10.695925: step 122740, loss = 3.63 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 01:45:16.194809: step 122750, loss = 3.20 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 01:45:20.922302: step 122760, loss = 3.42 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 01:45:25.546079: step 122770, loss = 3.22 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 01:45:30.471332: step 122780, loss = 3.40 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 01:45:35.983300: step 122790, loss = 3.53 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 01:45:40.710325: step 122800, loss = 3.03 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 01:45:46.246656: step 122810, loss = 3.10 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 01:45:50.895298: step 122820, loss = 2.95 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 01:45:56.344589: step 122830, loss = 3.10 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 01:46:01.440640: step 122840, loss = 3.19 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 01:46:06.149254: step 122850, loss = 2.97 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 01:46:11.580020: step 122860, loss = 3.35 (186.7 examples/sec; 0.685 sec/batch)
2016-07-16 01:46:17.865200: step 122870, loss = 3.27 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 01:46:22.434816: step 122880, loss = 3.11 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 01:46:27.696716: step 122890, loss = 3.18 (199.8 examples/sec; 0.641 sec/batch)
2016-07-16 01:46:32.949010: step 122900, loss = 3.33 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 01:46:39.813824: step 122910, loss = 3.21 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 01:46:44.564295: step 122920, loss = 3.17 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 01:46:49.198631: step 122930, loss = 3.53 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 01:46:53.860823: step 122940, loss = 3.07 (284.3 examples/sec; 0.450 sec/batch)
2016-07-16 01:46:58.456820: step 122950, loss = 3.31 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 01:47:03.695948: step 122960, loss = 3.13 (208.4 examples/sec; 0.614 sec/batch)
2016-07-16 01:47:08.986490: step 122970, loss = 3.17 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 01:47:13.676518: step 122980, loss = 3.24 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 01:47:18.964685: step 122990, loss = 3.26 (185.5 examples/sec; 0.690 sec/batch)
2016-07-16 01:47:24.409390: step 123000, loss = 3.08 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 01:47:29.910018: step 123010, loss = 3.31 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 01:47:34.498875: step 123020, loss = 3.16 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 01:47:39.746541: step 123030, loss = 3.22 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 01:47:45.039607: step 123040, loss = 3.25 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 01:47:49.784062: step 123050, loss = 2.94 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 01:47:54.679848: step 123060, loss = 3.15 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 01:47:59.350688: step 123070, loss = 3.02 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 01:48:03.978275: step 123080, loss = 3.13 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 01:48:09.702138: step 123090, loss = 3.09 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 01:48:14.914972: step 123100, loss = 3.09 (207.3 examples/sec; 0.617 sec/batch)
2016-07-16 01:48:21.654820: step 123110, loss = 3.52 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 01:48:26.331672: step 123120, loss = 3.23 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 01:48:30.995782: step 123130, loss = 3.36 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 01:48:36.347687: step 123140, loss = 3.03 (199.2 examples/sec; 0.643 sec/batch)
2016-07-16 01:48:41.524668: step 123150, loss = 3.24 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 01:48:46.187078: step 123160, loss = 3.21 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 01:48:50.756389: step 123170, loss = 3.09 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 01:48:55.354076: step 123180, loss = 3.27 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 01:49:00.016998: step 123190, loss = 3.28 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 01:49:05.721196: step 123200, loss = 3.39 (210.1 examples/sec; 0.609 sec/batch)
2016-07-16 01:49:11.600383: step 123210, loss = 3.19 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 01:49:16.231553: step 123220, loss = 3.27 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 01:49:20.880955: step 123230, loss = 3.28 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:49:26.674681: step 123240, loss = 3.06 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 01:49:31.444506: step 123250, loss = 3.36 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 01:49:36.332813: step 123260, loss = 3.24 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 01:49:41.067038: step 123270, loss = 3.31 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 01:49:45.681574: step 123280, loss = 3.19 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 01:49:50.365514: step 123290, loss = 3.12 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 01:49:54.938385: step 123300, loss = 3.31 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 01:50:00.531953: step 123310, loss = 3.13 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 01:50:06.284648: step 123320, loss = 3.23 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 01:50:11.647310: step 123330, loss = 2.99 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 01:50:16.997539: step 123340, loss = 3.29 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 01:50:21.714975: step 123350, loss = 3.19 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 01:50:26.643418: step 123360, loss = 3.27 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 01:50:31.416136: step 123370, loss = 3.28 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 01:50:36.167924: step 123380, loss = 3.31 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 01:50:40.890582: step 123390, loss = 3.05 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 01:50:45.575032: step 123400, loss = 2.96 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 01:50:51.087309: step 123410, loss = 3.27 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 01:50:56.490093: step 123420, loss = 3.16 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 01:51:01.675332: step 123430, loss = 3.23 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 01:51:06.361361: step 123440, loss = 3.21 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 01:51:11.218871: step 123450, loss = 3.09 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 01:51:15.953232: step 123460, loss = 3.18 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 01:51:20.570148: step 123470, loss = 3.29 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 01:51:26.316076: step 123480, loss = 2.81 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 01:51:31.132906: step 123490, loss = 2.99 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 01:51:35.912784: step 123500, loss = 3.21 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 01:51:41.679215: step 123510, loss = 2.99 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 01:51:46.315194: step 123520, loss = 2.94 (252.8 examples/sec; 0.506 sec/batch)
2016-07-16 01:51:51.457392: step 123530, loss = 3.16 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 01:51:56.899792: step 123540, loss = 3.02 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 01:52:01.609092: step 123550, loss = 3.05 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 01:52:06.247876: step 123560, loss = 3.07 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 01:52:11.326029: step 123570, loss = 3.24 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 01:52:16.823673: step 123580, loss = 3.04 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 01:52:21.555978: step 123590, loss = 3.31 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 01:52:26.431765: step 123600, loss = 3.22 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 01:52:32.056854: step 123610, loss = 3.24 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 01:52:36.719591: step 123620, loss = 3.01 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 01:52:41.354985: step 123630, loss = 3.27 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 01:52:46.008127: step 123640, loss = 3.20 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 01:52:50.623215: step 123650, loss = 3.31 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 01:52:55.261191: step 123660, loss = 3.38 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 01:53:00.703589: step 123670, loss = 3.12 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 01:53:05.846974: step 123680, loss = 3.10 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 01:53:10.544357: step 123690, loss = 2.85 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 01:53:15.326073: step 123700, loss = 3.12 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 01:53:21.072670: step 123710, loss = 3.26 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 01:53:27.419969: step 123720, loss = 3.19 (208.4 examples/sec; 0.614 sec/batch)
2016-07-16 01:53:32.847707: step 123730, loss = 3.16 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 01:53:37.604087: step 123740, loss = 3.31 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 01:53:42.481528: step 123750, loss = 3.34 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 01:53:47.107347: step 123760, loss = 3.26 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 01:53:51.746724: step 123770, loss = 3.02 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 01:53:57.307648: step 123780, loss = 3.04 (208.1 examples/sec; 0.615 sec/batch)
2016-07-16 01:54:02.351833: step 123790, loss = 3.17 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 01:54:07.050220: step 123800, loss = 3.25 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 01:54:14.160616: step 123810, loss = 3.40 (196.0 examples/sec; 0.653 sec/batch)
2016-07-16 01:54:19.880649: step 123820, loss = 3.18 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 01:54:24.673570: step 123830, loss = 3.30 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 01:54:29.506429: step 123840, loss = 2.98 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 01:54:34.197129: step 123850, loss = 3.16 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 01:54:38.840101: step 123860, loss = 3.20 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 01:54:44.028082: step 123870, loss = 3.26 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 01:54:49.301604: step 123880, loss = 3.05 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 01:54:53.955262: step 123890, loss = 3.23 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 01:54:58.788649: step 123900, loss = 3.08 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 01:55:04.557487: step 123910, loss = 3.21 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 01:55:10.764203: step 123920, loss = 3.46 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 01:55:16.206886: step 123930, loss = 3.27 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 01:55:20.853918: step 123940, loss = 3.44 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 01:55:25.554490: step 123950, loss = 3.49 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 01:55:31.179433: step 123960, loss = 2.97 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 01:55:35.812225: step 123970, loss = 2.95 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 01:55:40.517809: step 123980, loss = 3.02 (275.6 examples/sec; 0.465 sec/batch)
2016-07-16 01:55:45.103946: step 123990, loss = 3.02 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 01:55:49.774307: step 124000, loss = 3.19 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 01:55:56.728461: step 124010, loss = 2.90 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 01:56:01.470098: step 124020, loss = 3.18 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 01:56:06.087794: step 124030, loss = 3.10 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 01:56:10.761246: step 124040, loss = 3.05 (284.3 examples/sec; 0.450 sec/batch)
2016-07-16 01:56:15.411556: step 124050, loss = 3.14 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 01:56:20.795147: step 124060, loss = 3.34 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 01:56:25.970775: step 124070, loss = 3.08 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 01:56:30.657152: step 124080, loss = 3.10 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 01:56:36.194878: step 124090, loss = 3.28 (185.5 examples/sec; 0.690 sec/batch)
2016-07-16 01:56:42.491397: step 124100, loss = 3.04 (257.8 examples/sec; 0.496 sec/batch)
2016-07-16 01:56:48.268928: step 124110, loss = 3.24 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 01:56:52.890246: step 124120, loss = 3.03 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 01:56:57.624722: step 124130, loss = 3.21 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 01:57:03.399394: step 124140, loss = 3.20 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 01:57:08.228390: step 124150, loss = 3.38 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 01:57:13.073665: step 124160, loss = 3.21 (239.8 examples/sec; 0.534 sec/batch)
2016-07-16 01:57:17.826602: step 124170, loss = 3.12 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 01:57:22.709526: step 124180, loss = 3.23 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 01:57:27.366894: step 124190, loss = 3.31 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 01:57:32.061888: step 124200, loss = 3.07 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 01:57:37.670461: step 124210, loss = 2.95 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 01:57:42.497009: step 124220, loss = 3.14 (214.9 examples/sec; 0.596 sec/batch)
2016-07-16 01:57:48.158446: step 124230, loss = 3.11 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 01:57:53.870784: step 124240, loss = 3.14 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 01:57:58.796002: step 124250, loss = 3.09 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 01:58:03.437150: step 124260, loss = 3.19 (279.8 examples/sec; 0.458 sec/batch)
2016-07-16 01:58:08.068197: step 124270, loss = 3.30 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 01:58:12.697908: step 124280, loss = 3.13 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 01:58:17.381192: step 124290, loss = 3.12 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 01:58:23.157244: step 124300, loss = 3.22 (256.1 examples/sec; 0.500 sec/batch)
2016-07-16 01:58:29.900018: step 124310, loss = 3.27 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 01:58:34.803095: step 124320, loss = 3.16 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 01:58:39.514649: step 124330, loss = 3.08 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 01:58:44.194294: step 124340, loss = 3.26 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 01:58:48.892345: step 124350, loss = 3.25 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 01:58:53.537867: step 124360, loss = 3.28 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 01:58:58.154844: step 124370, loss = 3.30 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 01:59:03.059195: step 124380, loss = 3.16 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 01:59:08.676432: step 124390, loss = 3.41 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 01:59:13.488003: step 124400, loss = 3.15 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 01:59:19.806752: step 124410, loss = 3.41 (186.0 examples/sec; 0.688 sec/batch)
2016-07-16 01:59:26.237455: step 124420, loss = 3.27 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 01:59:31.108431: step 124430, loss = 3.34 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 01:59:35.869901: step 124440, loss = 3.27 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 01:59:40.649969: step 124450, loss = 3.12 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 01:59:45.499468: step 124460, loss = 3.24 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 01:59:50.280861: step 124470, loss = 3.30 (264.7 examples/sec; 0.483 sec/batch)
2016-07-16 01:59:55.154051: step 124480, loss = 3.03 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 01:59:59.964095: step 124490, loss = 2.96 (257.8 examples/sec; 0.496 sec/batch)
2016-07-16 02:00:05.906876: step 124500, loss = 3.01 (183.4 examples/sec; 0.698 sec/batch)
2016-07-16 02:00:12.879272: step 124510, loss = 3.26 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 02:00:17.524046: step 124520, loss = 3.10 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 02:00:23.313853: step 124530, loss = 3.25 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 02:00:28.162923: step 124540, loss = 3.32 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 02:00:33.033155: step 124550, loss = 3.17 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 02:00:37.796773: step 124560, loss = 3.15 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 02:00:42.647089: step 124570, loss = 3.22 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 02:00:47.419306: step 124580, loss = 3.10 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 02:00:52.209568: step 124590, loss = 2.95 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 02:00:56.859154: step 124600, loss = 2.94 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 02:01:02.485958: step 124610, loss = 3.05 (246.3 examples/sec; 0.520 sec/batch)
2016-07-16 02:01:08.225316: step 124620, loss = 3.05 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 02:01:12.964384: step 124630, loss = 3.14 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 02:01:17.863861: step 124640, loss = 2.87 (241.8 examples/sec; 0.529 sec/batch)
2016-07-16 02:01:22.570531: step 124650, loss = 3.01 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 02:01:27.995900: step 124660, loss = 3.19 (188.7 examples/sec; 0.678 sec/batch)
2016-07-16 02:01:33.334166: step 124670, loss = 3.00 (282.9 examples/sec; 0.452 sec/batch)
2016-07-16 02:01:37.968858: step 124680, loss = 3.07 (282.9 examples/sec; 0.452 sec/batch)
2016-07-16 02:01:42.638779: step 124690, loss = 3.17 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 02:01:47.314974: step 124700, loss = 3.13 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 02:01:52.945554: step 124710, loss = 3.16 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 02:01:57.640371: step 124720, loss = 3.18 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 02:02:03.396484: step 124730, loss = 3.15 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 02:02:08.215911: step 124740, loss = 2.96 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 02:02:12.976066: step 124750, loss = 3.27 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 02:02:17.721485: step 124760, loss = 3.25 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 02:02:22.362552: step 124770, loss = 3.23 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 02:02:27.287282: step 124780, loss = 3.09 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 02:02:32.833419: step 124790, loss = 3.40 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 02:02:37.585907: step 124800, loss = 3.14 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 02:02:43.214295: step 124810, loss = 3.29 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 02:02:47.872118: step 124820, loss = 3.17 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 02:02:52.565368: step 124830, loss = 3.03 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 02:02:57.240893: step 124840, loss = 3.16 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 02:03:02.956351: step 124850, loss = 2.88 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 02:03:07.815169: step 124860, loss = 3.13 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 02:03:12.468756: step 124870, loss = 3.12 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 02:03:17.060393: step 124880, loss = 3.02 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 02:03:21.744233: step 124890, loss = 3.28 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 02:03:26.382344: step 124900, loss = 3.46 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 02:03:32.599947: step 124910, loss = 3.20 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 02:03:37.888163: step 124920, loss = 3.09 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 02:03:42.592489: step 124930, loss = 2.93 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 02:03:47.896135: step 124940, loss = 3.01 (189.5 examples/sec; 0.676 sec/batch)
2016-07-16 02:03:54.350956: step 124950, loss = 3.23 (201.8 examples/sec; 0.634 sec/batch)
2016-07-16 02:03:59.020513: step 124960, loss = 3.18 (249.5 examples/sec; 0.513 sec/batch)
2016-07-16 02:04:03.643618: step 124970, loss = 3.17 (284.0 examples/sec; 0.451 sec/batch)
2016-07-16 02:04:08.299799: step 124980, loss = 2.97 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 02:04:13.831695: step 124990, loss = 3.11 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 02:04:18.846247: step 125000, loss = 3.11 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 02:04:24.579468: step 125010, loss = 3.13 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 02:04:29.386655: step 125020, loss = 2.91 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 02:04:34.030746: step 125030, loss = 3.28 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 02:04:38.835458: step 125040, loss = 3.13 (216.8 examples/sec; 0.591 sec/batch)
2016-07-16 02:04:44.518056: step 125050, loss = 3.24 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 02:04:50.305632: step 125060, loss = 3.09 (200.6 examples/sec; 0.638 sec/batch)
2016-07-16 02:04:55.169938: step 125070, loss = 3.09 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 02:05:00.069219: step 125080, loss = 3.09 (247.9 examples/sec; 0.516 sec/batch)
2016-07-16 02:05:05.893375: step 125090, loss = 2.99 (188.6 examples/sec; 0.679 sec/batch)
2016-07-16 02:05:11.805808: step 125100, loss = 2.89 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 02:05:17.637970: step 125110, loss = 3.26 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 02:05:22.249315: step 125120, loss = 3.05 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 02:05:26.930272: step 125130, loss = 2.94 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 02:05:31.566276: step 125140, loss = 2.90 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 02:05:36.273873: step 125150, loss = 2.99 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 02:05:40.883416: step 125160, loss = 3.36 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 02:05:45.515277: step 125170, loss = 3.05 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 02:05:50.175174: step 125180, loss = 3.10 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 02:05:55.937969: step 125190, loss = 2.95 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 02:06:00.732625: step 125200, loss = 3.16 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 02:06:06.603129: step 125210, loss = 3.19 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 02:06:11.296551: step 125220, loss = 3.06 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 02:06:15.954002: step 125230, loss = 2.97 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:06:21.396657: step 125240, loss = 2.97 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 02:06:26.585742: step 125250, loss = 3.23 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 02:06:32.330989: step 125260, loss = 3.10 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 02:06:37.125206: step 125270, loss = 3.23 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 02:06:41.772342: step 125280, loss = 3.15 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 02:06:46.502401: step 125290, loss = 3.17 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 02:06:51.167552: step 125300, loss = 3.34 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 02:06:56.767847: step 125310, loss = 3.37 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 02:07:01.463532: step 125320, loss = 3.13 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 02:07:07.220718: step 125330, loss = 3.03 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 02:07:12.080630: step 125340, loss = 3.16 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 02:07:16.855036: step 125350, loss = 3.19 (250.5 examples/sec; 0.511 sec/batch)
2016-07-16 02:07:22.863966: step 125360, loss = 3.18 (197.3 examples/sec; 0.649 sec/batch)
2016-07-16 02:07:28.615663: step 125370, loss = 2.93 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 02:07:33.377470: step 125380, loss = 2.99 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 02:07:38.349995: step 125390, loss = 2.88 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 02:07:44.832844: step 125400, loss = 3.18 (208.6 examples/sec; 0.614 sec/batch)
2016-07-16 02:07:51.045028: step 125410, loss = 2.97 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 02:07:55.717416: step 125420, loss = 3.12 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 02:08:00.371130: step 125430, loss = 3.34 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 02:08:05.007162: step 125440, loss = 3.13 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 02:08:10.730625: step 125450, loss = 3.27 (212.5 examples/sec; 0.602 sec/batch)
2016-07-16 02:08:15.601792: step 125460, loss = 3.06 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 02:08:20.376774: step 125470, loss = 3.07 (253.4 examples/sec; 0.505 sec/batch)
2016-07-16 02:08:26.370456: step 125480, loss = 3.05 (189.4 examples/sec; 0.676 sec/batch)
2016-07-16 02:08:32.271110: step 125490, loss = 3.08 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 02:08:37.134948: step 125500, loss = 2.97 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 02:08:42.746809: step 125510, loss = 3.08 (283.8 examples/sec; 0.451 sec/batch)
2016-07-16 02:08:47.451340: step 125520, loss = 3.30 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:08:52.070727: step 125530, loss = 3.16 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 02:08:57.486728: step 125540, loss = 3.06 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 02:09:02.686360: step 125550, loss = 3.35 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 02:09:07.383197: step 125560, loss = 3.16 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 02:09:12.271960: step 125570, loss = 3.49 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 02:09:16.923543: step 125580, loss = 3.37 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 02:09:21.662971: step 125590, loss = 3.25 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 02:09:26.269837: step 125600, loss = 3.11 (284.6 examples/sec; 0.450 sec/batch)
2016-07-16 02:09:31.898003: step 125610, loss = 3.21 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 02:09:36.503536: step 125620, loss = 3.19 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 02:09:41.157886: step 125630, loss = 3.23 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 02:09:46.861171: step 125640, loss = 3.05 (222.7 examples/sec; 0.575 sec/batch)
2016-07-16 02:09:51.753479: step 125650, loss = 2.90 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 02:09:56.520284: step 125660, loss = 2.98 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 02:10:01.339904: step 125670, loss = 3.00 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 02:10:06.156876: step 125680, loss = 3.18 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 02:10:10.885999: step 125690, loss = 3.33 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 02:10:15.502091: step 125700, loss = 3.27 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:10:21.985654: step 125710, loss = 3.17 (201.8 examples/sec; 0.634 sec/batch)
2016-07-16 02:10:26.993810: step 125720, loss = 3.03 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 02:10:31.739819: step 125730, loss = 3.22 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 02:10:36.573934: step 125740, loss = 3.05 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 02:10:41.372135: step 125750, loss = 2.97 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 02:10:47.369923: step 125760, loss = 2.89 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 02:10:52.036886: step 125770, loss = 3.12 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 02:10:57.422701: step 125780, loss = 3.07 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 02:11:02.592055: step 125790, loss = 3.18 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 02:11:07.335406: step 125800, loss = 3.01 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 02:11:14.326284: step 125810, loss = 3.05 (189.5 examples/sec; 0.676 sec/batch)
2016-07-16 02:11:20.133232: step 125820, loss = 3.06 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 02:11:24.732345: step 125830, loss = 3.10 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 02:11:29.430700: step 125840, loss = 3.02 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 02:11:35.128034: step 125850, loss = 3.07 (247.8 examples/sec; 0.516 sec/batch)
2016-07-16 02:11:40.362801: step 125860, loss = 3.08 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 02:11:45.789106: step 125870, loss = 3.18 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 02:11:50.518961: step 125880, loss = 2.85 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 02:11:55.578555: step 125890, loss = 3.27 (189.6 examples/sec; 0.675 sec/batch)
2016-07-16 02:12:02.140904: step 125900, loss = 3.23 (199.8 examples/sec; 0.641 sec/batch)
2016-07-16 02:12:08.146893: step 125910, loss = 3.30 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 02:12:12.913237: step 125920, loss = 3.35 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 02:12:18.953257: step 125930, loss = 3.04 (188.1 examples/sec; 0.681 sec/batch)
2016-07-16 02:12:24.534801: step 125940, loss = 3.09 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 02:12:29.143789: step 125950, loss = 3.45 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 02:12:33.822449: step 125960, loss = 3.20 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 02:12:38.360792: step 125970, loss = 3.03 (286.6 examples/sec; 0.447 sec/batch)
2016-07-16 02:12:43.011211: step 125980, loss = 3.33 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 02:12:47.662789: step 125990, loss = 3.13 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 02:12:52.313546: step 126000, loss = 3.15 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 02:12:59.003650: step 126010, loss = 2.98 (208.4 examples/sec; 0.614 sec/batch)
2016-07-16 02:13:03.891857: step 126020, loss = 3.17 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 02:13:08.529301: step 126030, loss = 3.25 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 02:13:13.111977: step 126040, loss = 3.05 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 02:13:17.837508: step 126050, loss = 3.25 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:13:23.615951: step 126060, loss = 3.36 (251.7 examples/sec; 0.508 sec/batch)
2016-07-16 02:13:28.429116: step 126070, loss = 3.27 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 02:13:33.264593: step 126080, loss = 3.25 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 02:13:37.980535: step 126090, loss = 3.27 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 02:13:42.591702: step 126100, loss = 2.91 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 02:13:48.217874: step 126110, loss = 3.11 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 02:13:52.853956: step 126120, loss = 3.27 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 02:13:58.437964: step 126130, loss = 3.26 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 02:14:03.402607: step 126140, loss = 2.98 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 02:14:08.106355: step 126150, loss = 3.10 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 02:14:13.760124: step 126160, loss = 3.13 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 02:14:19.836575: step 126170, loss = 3.22 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 02:14:24.678614: step 126180, loss = 3.17 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 02:14:29.445385: step 126190, loss = 3.10 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 02:14:34.155814: step 126200, loss = 3.05 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 02:14:39.731458: step 126210, loss = 3.42 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 02:14:44.863858: step 126220, loss = 3.13 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 02:14:50.221188: step 126230, loss = 3.03 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 02:14:56.007857: step 126240, loss = 3.11 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 02:15:00.816151: step 126250, loss = 2.91 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 02:15:05.620439: step 126260, loss = 2.90 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 02:15:10.372092: step 126270, loss = 3.49 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 02:15:14.969575: step 126280, loss = 3.03 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 02:15:19.840641: step 126290, loss = 3.18 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 02:15:25.440196: step 126300, loss = 3.08 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 02:15:31.195053: step 126310, loss = 2.98 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 02:15:36.087857: step 126320, loss = 3.23 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 02:15:40.744522: step 126330, loss = 3.17 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 02:15:45.370607: step 126340, loss = 3.09 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:15:51.124644: step 126350, loss = 2.98 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 02:15:55.944261: step 126360, loss = 3.15 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 02:16:00.721985: step 126370, loss = 3.29 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 02:16:05.511157: step 126380, loss = 3.05 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 02:16:10.092107: step 126390, loss = 3.10 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 02:16:14.738947: step 126400, loss = 3.00 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 02:16:20.391009: step 126410, loss = 3.33 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 02:16:25.018772: step 126420, loss = 3.06 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 02:16:29.663894: step 126430, loss = 3.24 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 02:16:34.335851: step 126440, loss = 3.04 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 02:16:40.109056: step 126450, loss = 3.05 (244.9 examples/sec; 0.523 sec/batch)
2016-07-16 02:16:44.962263: step 126460, loss = 3.23 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 02:16:49.778677: step 126470, loss = 3.43 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 02:16:55.986440: step 126480, loss = 2.96 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 02:17:01.568455: step 126490, loss = 2.97 (254.2 examples/sec; 0.503 sec/batch)
2016-07-16 02:17:06.308607: step 126500, loss = 3.24 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 02:17:12.160009: step 126510, loss = 3.21 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 02:17:16.907821: step 126520, loss = 3.11 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 02:17:21.573180: step 126530, loss = 3.03 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 02:17:26.254826: step 126540, loss = 3.19 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 02:17:30.871651: step 126550, loss = 2.89 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 02:17:36.653085: step 126560, loss = 3.03 (256.2 examples/sec; 0.500 sec/batch)
2016-07-16 02:17:41.427689: step 126570, loss = 3.12 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 02:17:46.255936: step 126580, loss = 3.11 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 02:17:52.578322: step 126590, loss = 3.23 (207.3 examples/sec; 0.618 sec/batch)
2016-07-16 02:17:57.890918: step 126600, loss = 3.33 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 02:18:03.477733: step 126610, loss = 3.28 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 02:18:08.093443: step 126620, loss = 3.05 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 02:18:13.354194: step 126630, loss = 3.23 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 02:18:18.692664: step 126640, loss = 3.08 (255.7 examples/sec; 0.500 sec/batch)
2016-07-16 02:18:23.424252: step 126650, loss = 3.03 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 02:18:28.774422: step 126660, loss = 2.97 (190.3 examples/sec; 0.672 sec/batch)
2016-07-16 02:18:35.181032: step 126670, loss = 3.19 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 02:18:39.792535: step 126680, loss = 3.13 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:18:44.448561: step 126690, loss = 3.29 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 02:18:49.107878: step 126700, loss = 3.06 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:18:54.709024: step 126710, loss = 3.33 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 02:18:59.311763: step 126720, loss = 3.15 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 02:19:04.272552: step 126730, loss = 3.45 (197.9 examples/sec; 0.647 sec/batch)
2016-07-16 02:19:09.812800: step 126740, loss = 3.32 (254.2 examples/sec; 0.503 sec/batch)
2016-07-16 02:19:15.594315: step 126750, loss = 3.27 (253.9 examples/sec; 0.504 sec/batch)
2016-07-16 02:19:20.436731: step 126760, loss = 3.17 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 02:19:25.211276: step 126770, loss = 3.20 (251.9 examples/sec; 0.508 sec/batch)
2016-07-16 02:19:29.964602: step 126780, loss = 2.94 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 02:19:34.863658: step 126790, loss = 2.90 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 02:19:39.535074: step 126800, loss = 3.16 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 02:19:45.027539: step 126810, loss = 2.92 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 02:19:49.717021: step 126820, loss = 2.96 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 02:19:54.352801: step 126830, loss = 2.93 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 02:19:59.026633: step 126840, loss = 3.16 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 02:20:04.843325: step 126850, loss = 3.19 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 02:20:09.618154: step 126860, loss = 3.32 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 02:20:14.391904: step 126870, loss = 3.21 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 02:20:20.711044: step 126880, loss = 2.96 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 02:20:25.999878: step 126890, loss = 3.13 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 02:20:30.677876: step 126900, loss = 3.01 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 02:20:37.519936: step 126910, loss = 2.90 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 02:20:42.302531: step 126920, loss = 2.87 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 02:20:46.933188: step 126930, loss = 3.09 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 02:20:51.607835: step 126940, loss = 2.99 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:20:56.244163: step 126950, loss = 3.28 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 02:21:01.503527: step 126960, loss = 2.80 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 02:21:06.773732: step 126970, loss = 2.82 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 02:21:11.499399: step 126980, loss = 3.00 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 02:21:16.387971: step 126990, loss = 3.35 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 02:21:21.178595: step 127000, loss = 3.19 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 02:21:26.937946: step 127010, loss = 2.95 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 02:21:31.801687: step 127020, loss = 3.04 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 02:21:36.561885: step 127030, loss = 3.20 (245.3 examples/sec; 0.522 sec/batch)
2016-07-16 02:21:42.305411: step 127040, loss = 2.73 (183.0 examples/sec; 0.699 sec/batch)
2016-07-16 02:21:48.366354: step 127050, loss = 3.29 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 02:21:53.209923: step 127060, loss = 3.09 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 02:21:57.978827: step 127070, loss = 3.08 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 02:22:02.732558: step 127080, loss = 3.33 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 02:22:07.385761: step 127090, loss = 3.13 (285.7 examples/sec; 0.448 sec/batch)
2016-07-16 02:22:12.371124: step 127100, loss = 2.96 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 02:22:19.124886: step 127110, loss = 3.41 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 02:22:24.897254: step 127120, loss = 3.07 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 02:22:29.647696: step 127130, loss = 2.99 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 02:22:34.479308: step 127140, loss = 3.31 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 02:22:40.790468: step 127150, loss = 3.23 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 02:22:46.078161: step 127160, loss = 3.10 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 02:22:50.718770: step 127170, loss = 2.98 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 02:22:56.451832: step 127180, loss = 3.36 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 02:23:01.311481: step 127190, loss = 3.22 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 02:23:06.016934: step 127200, loss = 3.07 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 02:23:11.528872: step 127210, loss = 3.20 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 02:23:16.234452: step 127220, loss = 3.22 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 02:23:20.874878: step 127230, loss = 3.28 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 02:23:26.285716: step 127240, loss = 3.31 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 02:23:31.426829: step 127250, loss = 3.09 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 02:23:36.149033: step 127260, loss = 2.81 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 02:23:41.011386: step 127270, loss = 3.00 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 02:23:45.787639: step 127280, loss = 3.09 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 02:23:50.468023: step 127290, loss = 2.91 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 02:23:55.044680: step 127300, loss = 3.17 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 02:24:01.110907: step 127310, loss = 3.08 (199.2 examples/sec; 0.643 sec/batch)
2016-07-16 02:24:06.560664: step 127320, loss = 3.10 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 02:24:12.290115: step 127330, loss = 3.01 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 02:24:16.942198: step 127340, loss = 3.39 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 02:24:22.417067: step 127350, loss = 3.09 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 02:24:27.536274: step 127360, loss = 3.11 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 02:24:33.319684: step 127370, loss = 3.02 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 02:24:38.232119: step 127380, loss = 3.16 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 02:24:43.123089: step 127390, loss = 3.18 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 02:24:47.828410: step 127400, loss = 3.13 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 02:24:53.444927: step 127410, loss = 2.96 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 02:24:58.110006: step 127420, loss = 3.01 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 02:25:02.702179: step 127430, loss = 3.29 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 02:25:07.365820: step 127440, loss = 3.20 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 02:25:11.989027: step 127450, loss = 3.01 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 02:25:16.604592: step 127460, loss = 3.16 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 02:25:21.343350: step 127470, loss = 3.10 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 02:25:25.930799: step 127480, loss = 3.18 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 02:25:31.669163: step 127490, loss = 3.14 (212.2 examples/sec; 0.603 sec/batch)
2016-07-16 02:25:36.518254: step 127500, loss = 2.90 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 02:25:42.232586: step 127510, loss = 3.20 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 02:25:46.989509: step 127520, loss = 3.09 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 02:25:51.925075: step 127530, loss = 3.23 (210.8 examples/sec; 0.607 sec/batch)
2016-07-16 02:25:58.448211: step 127540, loss = 3.21 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 02:26:03.573077: step 127550, loss = 3.13 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 02:26:08.271224: step 127560, loss = 3.04 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 02:26:13.099187: step 127570, loss = 3.06 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 02:26:17.755628: step 127580, loss = 3.21 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 02:26:22.455488: step 127590, loss = 3.19 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 02:26:28.198232: step 127600, loss = 3.20 (251.8 examples/sec; 0.508 sec/batch)
2016-07-16 02:26:34.862295: step 127610, loss = 3.09 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 02:26:39.873984: step 127620, loss = 3.06 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 02:26:44.590868: step 127630, loss = 3.42 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 02:26:49.222444: step 127640, loss = 2.87 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 02:26:53.911365: step 127650, loss = 2.93 (254.8 examples/sec; 0.502 sec/batch)
2016-07-16 02:26:58.572265: step 127660, loss = 3.29 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 02:27:03.198465: step 127670, loss = 2.93 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 02:27:07.894354: step 127680, loss = 3.22 (247.1 examples/sec; 0.518 sec/batch)
2016-07-16 02:27:13.567159: step 127690, loss = 3.11 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 02:27:18.327688: step 127700, loss = 3.04 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 02:27:24.126718: step 127710, loss = 2.80 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 02:27:28.823338: step 127720, loss = 2.98 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 02:27:33.636868: step 127730, loss = 3.03 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 02:27:38.464016: step 127740, loss = 3.07 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 02:27:43.185465: step 127750, loss = 3.17 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 02:27:47.762602: step 127760, loss = 3.34 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 02:27:52.433327: step 127770, loss = 2.69 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 02:27:57.056640: step 127780, loss = 3.30 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 02:28:01.707879: step 127790, loss = 3.04 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 02:28:06.386707: step 127800, loss = 3.11 (276.2 examples/sec; 0.464 sec/batch)
2016-07-16 02:28:12.121658: step 127810, loss = 3.01 (215.4 examples/sec; 0.594 sec/batch)
2016-07-16 02:28:17.828139: step 127820, loss = 3.02 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 02:28:23.527990: step 127830, loss = 3.11 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 02:28:28.645393: step 127840, loss = 2.98 (208.8 examples/sec; 0.613 sec/batch)
2016-07-16 02:28:34.249014: step 127850, loss = 3.29 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 02:28:38.971777: step 127860, loss = 2.97 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 02:28:43.810377: step 127870, loss = 3.18 (252.4 examples/sec; 0.507 sec/batch)
2016-07-16 02:28:49.726053: step 127880, loss = 3.01 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 02:28:54.398656: step 127890, loss = 3.29 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 02:29:00.103950: step 127900, loss = 3.17 (253.2 examples/sec; 0.506 sec/batch)
2016-07-16 02:29:05.935216: step 127910, loss = 3.33 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 02:29:10.822965: step 127920, loss = 3.22 (253.2 examples/sec; 0.505 sec/batch)
2016-07-16 02:29:17.141818: step 127930, loss = 3.05 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 02:29:22.372602: step 127940, loss = 2.98 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 02:29:26.943537: step 127950, loss = 3.00 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 02:29:31.601118: step 127960, loss = 3.18 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 02:29:37.406206: step 127970, loss = 3.15 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 02:29:42.208281: step 127980, loss = 3.14 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 02:29:47.086425: step 127990, loss = 3.08 (251.1 examples/sec; 0.510 sec/batch)
2016-07-16 02:29:51.824642: step 128000, loss = 3.00 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 02:29:57.732521: step 128010, loss = 3.18 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 02:30:02.379305: step 128020, loss = 3.26 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 02:30:07.108699: step 128030, loss = 3.08 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 02:30:12.850541: step 128040, loss = 3.13 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 02:30:17.634672: step 128050, loss = 3.23 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 02:30:22.434693: step 128060, loss = 2.90 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 02:30:28.446931: step 128070, loss = 2.97 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 02:30:33.057475: step 128080, loss = 3.22 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 02:30:38.387643: step 128090, loss = 3.13 (209.0 examples/sec; 0.612 sec/batch)
2016-07-16 02:30:43.587743: step 128100, loss = 3.28 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 02:30:50.504475: step 128110, loss = 3.38 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 02:30:55.256232: step 128120, loss = 2.81 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 02:30:59.865693: step 128130, loss = 2.83 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 02:31:04.774475: step 128140, loss = 2.93 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 02:31:10.331272: step 128150, loss = 3.29 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 02:31:15.114508: step 128160, loss = 3.22 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 02:31:19.997683: step 128170, loss = 3.07 (248.3 examples/sec; 0.515 sec/batch)
2016-07-16 02:31:26.621401: step 128180, loss = 2.97 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 02:31:31.574098: step 128190, loss = 3.30 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 02:31:36.268597: step 128200, loss = 3.23 (285.6 examples/sec; 0.448 sec/batch)
2016-07-16 02:31:41.837033: step 128210, loss = 3.20 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 02:31:47.490126: step 128220, loss = 3.06 (200.2 examples/sec; 0.639 sec/batch)
2016-07-16 02:31:52.489274: step 128230, loss = 3.13 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 02:31:57.203614: step 128240, loss = 3.30 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 02:32:02.868087: step 128250, loss = 3.09 (191.8 examples/sec; 0.667 sec/batch)
2016-07-16 02:32:08.950758: step 128260, loss = 2.96 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 02:32:13.738996: step 128270, loss = 3.19 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 02:32:18.568373: step 128280, loss = 3.30 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 02:32:23.380403: step 128290, loss = 2.76 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 02:32:28.000606: step 128300, loss = 3.06 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 02:32:33.618836: step 128310, loss = 3.06 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 02:32:38.279540: step 128320, loss = 3.17 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 02:32:43.983344: step 128330, loss = 2.93 (200.8 examples/sec; 0.638 sec/batch)
2016-07-16 02:32:49.264533: step 128340, loss = 3.18 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 02:32:54.737416: step 128350, loss = 3.22 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 02:33:00.537631: step 128360, loss = 2.99 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 02:33:05.358664: step 128370, loss = 3.22 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 02:33:10.131445: step 128380, loss = 3.22 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 02:33:14.933008: step 128390, loss = 2.96 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 02:33:19.585804: step 128400, loss = 2.96 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 02:33:25.607054: step 128410, loss = 3.12 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 02:33:31.023798: step 128420, loss = 2.76 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 02:33:35.708323: step 128430, loss = 3.10 (269.2 examples/sec; 0.476 sec/batch)
2016-07-16 02:33:40.754496: step 128440, loss = 3.07 (185.0 examples/sec; 0.692 sec/batch)
2016-07-16 02:33:47.263768: step 128450, loss = 2.98 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 02:33:52.292973: step 128460, loss = 3.00 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 02:33:57.037256: step 128470, loss = 2.79 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 02:34:01.861449: step 128480, loss = 2.95 (279.8 examples/sec; 0.458 sec/batch)
2016-07-16 02:34:06.682097: step 128490, loss = 3.13 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 02:34:11.445875: step 128500, loss = 3.27 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 02:34:17.275671: step 128510, loss = 3.05 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 02:34:22.065515: step 128520, loss = 3.10 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 02:34:26.807525: step 128530, loss = 3.01 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 02:34:31.636373: step 128540, loss = 3.08 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 02:34:38.063620: step 128550, loss = 3.18 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 02:34:43.391797: step 128560, loss = 3.08 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 02:34:48.060172: step 128570, loss = 3.04 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 02:34:52.892591: step 128580, loss = 2.95 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 02:34:57.674325: step 128590, loss = 3.27 (254.8 examples/sec; 0.502 sec/batch)
2016-07-16 02:35:02.503816: step 128600, loss = 2.69 (268.6 examples/sec; 0.476 sec/batch)
2016-07-16 02:35:08.178710: step 128610, loss = 2.90 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 02:35:12.890211: step 128620, loss = 3.14 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 02:35:17.512218: step 128630, loss = 3.08 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 02:35:22.943583: step 128640, loss = 2.97 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 02:35:28.110667: step 128650, loss = 3.07 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 02:35:33.869640: step 128660, loss = 2.82 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 02:35:39.541857: step 128670, loss = 2.95 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 02:35:44.534335: step 128680, loss = 3.02 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 02:35:49.159833: step 128690, loss = 3.04 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 02:35:53.828351: step 128700, loss = 3.19 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 02:36:00.441282: step 128710, loss = 3.10 (246.1 examples/sec; 0.520 sec/batch)
2016-07-16 02:36:05.624391: step 128720, loss = 2.90 (207.8 examples/sec; 0.616 sec/batch)
2016-07-16 02:36:11.094097: step 128730, loss = 2.95 (245.8 examples/sec; 0.521 sec/batch)
2016-07-16 02:36:15.803343: step 128740, loss = 3.16 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 02:36:20.664554: step 128750, loss = 3.01 (275.0 examples/sec; 0.466 sec/batch)
2016-07-16 02:36:25.421616: step 128760, loss = 3.17 (244.0 examples/sec; 0.525 sec/batch)
2016-07-16 02:36:31.097937: step 128770, loss = 2.93 (186.0 examples/sec; 0.688 sec/batch)
2016-07-16 02:36:37.191358: step 128780, loss = 3.22 (254.2 examples/sec; 0.503 sec/batch)
2016-07-16 02:36:41.969245: step 128790, loss = 3.21 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 02:36:46.805568: step 128800, loss = 3.12 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 02:36:52.599534: step 128810, loss = 3.06 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 02:36:57.229973: step 128820, loss = 3.17 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 02:37:01.932870: step 128830, loss = 2.92 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 02:37:06.571759: step 128840, loss = 3.05 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 02:37:12.262637: step 128850, loss = 3.16 (208.5 examples/sec; 0.614 sec/batch)
2016-07-16 02:37:17.113750: step 128860, loss = 2.96 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 02:37:21.896911: step 128870, loss = 3.00 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 02:37:26.701565: step 128880, loss = 3.19 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 02:37:31.366826: step 128890, loss = 3.02 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 02:37:36.094651: step 128900, loss = 3.15 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 02:37:41.662352: step 128910, loss = 3.14 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 02:37:47.115129: step 128920, loss = 3.31 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 02:37:52.247080: step 128930, loss = 2.80 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 02:37:56.978277: step 128940, loss = 3.36 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 02:38:01.840628: step 128950, loss = 2.99 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 02:38:06.516513: step 128960, loss = 2.98 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 02:38:11.242696: step 128970, loss = 3.07 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 02:38:15.884921: step 128980, loss = 3.10 (284.0 examples/sec; 0.451 sec/batch)
2016-07-16 02:38:20.577064: step 128990, loss = 3.00 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 02:38:25.140898: step 129000, loss = 3.06 (287.2 examples/sec; 0.446 sec/batch)
2016-07-16 02:38:30.681242: step 129010, loss = 3.26 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 02:38:35.334263: step 129020, loss = 3.02 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 02:38:40.062137: step 129030, loss = 3.24 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 02:38:45.857783: step 129040, loss = 3.00 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 02:38:51.422606: step 129050, loss = 3.22 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 02:38:56.449851: step 129060, loss = 3.21 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 02:39:01.128229: step 129070, loss = 2.88 (284.3 examples/sec; 0.450 sec/batch)
2016-07-16 02:39:05.762595: step 129080, loss = 3.03 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 02:39:10.448111: step 129090, loss = 3.24 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 02:39:15.066116: step 129100, loss = 2.97 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 02:39:21.753277: step 129110, loss = 3.03 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 02:39:26.541487: step 129120, loss = 3.12 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 02:39:31.402121: step 129130, loss = 3.11 (246.1 examples/sec; 0.520 sec/batch)
2016-07-16 02:39:37.702099: step 129140, loss = 3.08 (200.9 examples/sec; 0.637 sec/batch)
2016-07-16 02:39:43.014200: step 129150, loss = 2.97 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 02:39:47.729122: step 129160, loss = 3.10 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 02:39:52.400847: step 129170, loss = 3.16 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 02:39:57.062710: step 129180, loss = 3.14 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 02:40:01.649425: step 129190, loss = 3.05 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 02:40:06.311735: step 129200, loss = 3.16 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 02:40:11.921784: step 129210, loss = 3.16 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 02:40:16.752644: step 129220, loss = 2.89 (217.6 examples/sec; 0.588 sec/batch)
2016-07-16 02:40:22.398633: step 129230, loss = 3.00 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 02:40:27.169954: step 129240, loss = 3.05 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 02:40:32.036323: step 129250, loss = 3.11 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 02:40:36.808684: step 129260, loss = 3.19 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 02:40:41.460562: step 129270, loss = 2.98 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 02:40:46.748037: step 129280, loss = 3.01 (202.2 examples/sec; 0.633 sec/batch)
2016-07-16 02:40:51.971337: step 129290, loss = 2.90 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 02:40:57.706875: step 129300, loss = 3.05 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 02:41:03.530681: step 129310, loss = 3.31 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 02:41:08.389288: step 129320, loss = 3.22 (253.9 examples/sec; 0.504 sec/batch)
2016-07-16 02:41:13.131091: step 129330, loss = 3.08 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 02:41:18.691108: step 129340, loss = 2.94 (189.5 examples/sec; 0.675 sec/batch)
2016-07-16 02:41:24.928500: step 129350, loss = 3.25 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 02:41:29.811240: step 129360, loss = 3.02 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 02:41:34.594412: step 129370, loss = 3.20 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 02:41:39.367258: step 129380, loss = 3.21 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 02:41:43.988453: step 129390, loss = 3.11 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 02:41:48.797230: step 129400, loss = 3.11 (218.5 examples/sec; 0.586 sec/batch)
2016-07-16 02:41:55.726735: step 129410, loss = 3.18 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 02:42:00.509145: step 129420, loss = 2.97 (248.5 examples/sec; 0.515 sec/batch)
2016-07-16 02:42:05.365019: step 129430, loss = 3.28 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 02:42:10.173911: step 129440, loss = 3.11 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 02:42:14.911990: step 129450, loss = 2.86 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 02:42:19.561992: step 129460, loss = 2.98 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 02:42:24.213351: step 129470, loss = 3.09 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 02:42:30.008969: step 129480, loss = 2.93 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 02:42:34.782292: step 129490, loss = 3.08 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 02:42:39.627298: step 129500, loss = 3.21 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 02:42:45.314505: step 129510, loss = 3.30 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 02:42:49.951901: step 129520, loss = 3.03 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 02:42:54.716982: step 129530, loss = 3.14 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 02:42:59.376021: step 129540, loss = 3.18 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 02:43:04.079878: step 129550, loss = 3.08 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 02:43:08.928099: step 129560, loss = 2.94 (210.5 examples/sec; 0.608 sec/batch)
2016-07-16 02:43:14.603349: step 129570, loss = 3.09 (262.0 examples/sec; 0.488 sec/batch)
2016-07-16 02:43:20.339500: step 129580, loss = 3.18 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 02:43:25.205310: step 129590, loss = 3.08 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 02:43:29.870431: step 129600, loss = 3.08 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 02:43:35.513199: step 129610, loss = 2.92 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 02:43:40.155021: step 129620, loss = 3.01 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 02:43:44.846135: step 129630, loss = 2.73 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 02:43:49.493740: step 129640, loss = 2.97 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 02:43:54.797292: step 129650, loss = 3.07 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 02:44:00.079316: step 129660, loss = 3.13 (262.0 examples/sec; 0.488 sec/batch)
2016-07-16 02:44:04.789168: step 129670, loss = 3.31 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 02:44:09.627055: step 129680, loss = 3.16 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 02:44:14.265247: step 129690, loss = 3.18 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 02:44:18.914226: step 129700, loss = 2.91 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 02:44:25.636843: step 129710, loss = 2.95 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 02:44:30.447306: step 129720, loss = 3.02 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 02:44:35.218724: step 129730, loss = 3.02 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 02:44:41.419473: step 129740, loss = 2.75 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 02:44:47.013697: step 129750, loss = 3.17 (244.5 examples/sec; 0.523 sec/batch)
2016-07-16 02:44:51.745344: step 129760, loss = 2.89 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 02:44:56.567118: step 129770, loss = 3.17 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 02:45:01.283344: step 129780, loss = 2.98 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 02:45:06.088844: step 129790, loss = 2.94 (278.0 examples/sec; 0.461 sec/batch)
2016-07-16 02:45:10.715192: step 129800, loss = 2.85 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 02:45:16.242951: step 129810, loss = 2.85 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 02:45:21.030277: step 129820, loss = 2.77 (215.1 examples/sec; 0.595 sec/batch)
2016-07-16 02:45:26.623425: step 129830, loss = 2.92 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 02:45:31.358866: step 129840, loss = 2.95 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 02:45:37.083830: step 129850, loss = 3.07 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 02:45:41.882385: step 129860, loss = 3.03 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 02:45:46.662671: step 129870, loss = 2.84 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 02:45:51.379733: step 129880, loss = 3.04 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 02:45:55.987876: step 129890, loss = 3.05 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 02:46:00.679156: step 129900, loss = 2.94 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 02:46:07.720472: step 129910, loss = 2.95 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 02:46:13.516835: step 129920, loss = 3.01 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 02:46:18.353157: step 129930, loss = 3.04 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 02:46:23.156906: step 129940, loss = 3.25 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 02:46:27.884658: step 129950, loss = 3.11 (279.2 examples/sec; 0.459 sec/batch)
2016-07-16 02:46:32.723429: step 129960, loss = 3.15 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 02:46:37.423172: step 129970, loss = 2.85 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 02:46:42.130316: step 129980, loss = 3.19 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 02:46:47.559899: step 129990, loss = 3.13 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 02:46:52.755932: step 130000, loss = 3.01 (251.0 examples/sec; 0.510 sec/batch)
2016-07-16 02:46:59.770375: step 130010, loss = 3.20 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 02:47:04.579192: step 130020, loss = 2.88 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 02:47:09.154816: step 130030, loss = 3.00 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 02:47:14.232298: step 130040, loss = 3.09 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 02:47:19.711353: step 130050, loss = 2.93 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 02:47:24.425943: step 130060, loss = 3.14 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 02:47:29.317439: step 130070, loss = 3.08 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 02:47:33.948201: step 130080, loss = 2.91 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 02:47:38.531137: step 130090, loss = 3.20 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 02:47:43.148143: step 130100, loss = 2.89 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 02:47:49.890761: step 130110, loss = 2.98 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 02:47:55.278487: step 130120, loss = 2.99 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 02:48:00.477038: step 130130, loss = 2.89 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 02:48:06.265429: step 130140, loss = 2.83 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 02:48:11.084431: step 130150, loss = 3.02 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 02:48:15.982675: step 130160, loss = 3.11 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 02:48:22.324148: step 130170, loss = 3.23 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 02:48:27.717295: step 130180, loss = 2.91 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 02:48:32.417802: step 130190, loss = 2.94 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 02:48:37.232497: step 130200, loss = 3.05 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 02:48:42.941787: step 130210, loss = 3.23 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 02:48:47.687181: step 130220, loss = 3.26 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 02:48:52.280532: step 130230, loss = 2.95 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 02:48:56.900170: step 130240, loss = 2.98 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 02:49:01.570126: step 130250, loss = 3.13 (284.0 examples/sec; 0.451 sec/batch)
2016-07-16 02:49:06.219169: step 130260, loss = 3.01 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 02:49:11.618085: step 130270, loss = 2.98 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 02:49:16.771852: step 130280, loss = 3.16 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 02:49:21.510549: step 130290, loss = 3.33 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 02:49:26.985540: step 130300, loss = 3.12 (189.8 examples/sec; 0.674 sec/batch)
2016-07-16 02:49:33.494659: step 130310, loss = 2.86 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 02:49:38.207638: step 130320, loss = 2.99 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 02:49:42.853478: step 130330, loss = 3.20 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 02:49:47.508315: step 130340, loss = 3.21 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 02:49:52.154789: step 130350, loss = 2.97 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 02:49:57.903977: step 130360, loss = 3.19 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 02:50:02.789151: step 130370, loss = 2.99 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 02:50:07.627193: step 130380, loss = 3.11 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 02:50:13.860123: step 130390, loss = 3.33 (207.7 examples/sec; 0.616 sec/batch)
2016-07-16 02:50:19.391283: step 130400, loss = 3.14 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 02:50:26.156172: step 130410, loss = 3.07 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 02:50:30.965840: step 130420, loss = 3.03 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 02:50:35.632695: step 130430, loss = 2.94 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 02:50:40.339894: step 130440, loss = 3.07 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 02:50:44.963007: step 130450, loss = 2.83 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 02:50:49.912314: step 130460, loss = 3.00 (205.3 examples/sec; 0.624 sec/batch)
2016-07-16 02:50:55.457910: step 130470, loss = 3.19 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 02:51:00.198877: step 130480, loss = 3.29 (269.2 examples/sec; 0.476 sec/batch)
2016-07-16 02:51:05.076783: step 130490, loss = 3.08 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 02:51:09.735355: step 130500, loss = 3.07 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 02:51:15.319259: step 130510, loss = 3.16 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 02:51:19.948434: step 130520, loss = 2.89 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 02:51:24.561199: step 130530, loss = 3.13 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 02:51:29.403636: step 130540, loss = 3.24 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 02:51:35.102487: step 130550, loss = 3.01 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 02:51:39.825907: step 130560, loss = 3.13 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 02:51:44.495911: step 130570, loss = 2.95 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 02:51:49.265294: step 130580, loss = 2.79 (230.0 examples/sec; 0.557 sec/batch)
2016-07-16 02:51:54.965238: step 130590, loss = 3.05 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 02:51:59.691183: step 130600, loss = 2.95 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 02:52:05.245916: step 130610, loss = 2.81 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 02:52:09.885850: step 130620, loss = 3.10 (286.4 examples/sec; 0.447 sec/batch)
2016-07-16 02:52:14.511863: step 130630, loss = 3.04 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 02:52:19.980034: step 130640, loss = 2.96 (210.1 examples/sec; 0.609 sec/batch)
2016-07-16 02:52:25.099656: step 130650, loss = 3.05 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 02:52:29.762794: step 130660, loss = 3.28 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 02:52:35.316362: step 130670, loss = 2.81 (188.8 examples/sec; 0.678 sec/batch)
2016-07-16 02:52:41.551472: step 130680, loss = 3.02 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 02:52:46.323893: step 130690, loss = 3.20 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 02:52:51.054647: step 130700, loss = 3.05 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 02:52:56.651075: step 130710, loss = 3.14 (267.5 examples/sec; 0.478 sec/batch)
2016-07-16 02:53:02.370566: step 130720, loss = 2.96 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 02:53:07.210304: step 130730, loss = 3.16 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 02:53:12.023220: step 130740, loss = 3.18 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 02:53:16.733580: step 130750, loss = 3.14 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 02:53:21.600512: step 130760, loss = 3.08 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 02:53:26.350458: step 130770, loss = 3.12 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 02:53:30.990936: step 130780, loss = 3.06 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 02:53:36.569244: step 130790, loss = 2.98 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 02:53:41.586832: step 130800, loss = 3.20 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 02:53:47.235744: step 130810, loss = 3.05 (283.7 examples/sec; 0.451 sec/batch)
2016-07-16 02:53:51.921665: step 130820, loss = 3.11 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 02:53:57.658923: step 130830, loss = 3.16 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 02:54:02.452881: step 130840, loss = 3.06 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 02:54:07.214083: step 130850, loss = 2.89 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 02:54:13.254483: step 130860, loss = 2.82 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 02:54:17.861247: step 130870, loss = 3.16 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 02:54:23.045772: step 130880, loss = 2.99 (201.0 examples/sec; 0.637 sec/batch)
2016-07-16 02:54:28.381858: step 130890, loss = 3.07 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 02:54:34.185389: step 130900, loss = 3.07 (244.9 examples/sec; 0.523 sec/batch)
2016-07-16 02:54:40.937616: step 130910, loss = 3.21 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 02:54:45.764926: step 130920, loss = 2.87 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 02:54:50.409736: step 130930, loss = 3.06 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 02:54:55.094348: step 130940, loss = 2.83 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 02:54:59.721665: step 130950, loss = 2.86 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 02:55:05.434993: step 130960, loss = 2.97 (249.2 examples/sec; 0.514 sec/batch)
2016-07-16 02:55:10.283719: step 130970, loss = 3.00 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 02:55:14.911481: step 130980, loss = 2.98 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 02:55:19.580827: step 130990, loss = 2.75 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 02:55:24.192322: step 131000, loss = 2.95 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 02:55:29.754834: step 131010, loss = 3.06 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 02:55:34.837198: step 131020, loss = 2.96 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 02:55:40.222166: step 131030, loss = 2.93 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 02:55:44.925432: step 131040, loss = 3.18 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 02:55:49.810563: step 131050, loss = 3.01 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 02:55:54.489746: step 131060, loss = 3.00 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 02:55:59.214963: step 131070, loss = 2.86 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 02:56:03.870045: step 131080, loss = 3.09 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 02:56:08.603527: step 131090, loss = 2.85 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 02:56:14.382767: step 131100, loss = 3.04 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 02:56:21.212980: step 131110, loss = 3.21 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 02:56:26.100343: step 131120, loss = 3.07 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 02:56:30.858456: step 131130, loss = 2.75 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 02:56:36.768061: step 131140, loss = 3.14 (187.6 examples/sec; 0.682 sec/batch)
2016-07-16 02:56:42.564355: step 131150, loss = 2.96 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 02:56:47.192665: step 131160, loss = 2.97 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 02:56:52.835069: step 131170, loss = 3.19 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 02:56:57.794609: step 131180, loss = 3.26 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 02:57:02.447585: step 131190, loss = 3.02 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 02:57:07.103644: step 131200, loss = 2.96 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 02:57:13.784720: step 131210, loss = 2.87 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 02:57:18.615777: step 131220, loss = 3.03 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 02:57:23.425607: step 131230, loss = 3.04 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 02:57:28.158984: step 131240, loss = 2.91 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 02:57:33.062221: step 131250, loss = 3.20 (238.2 examples/sec; 0.537 sec/batch)
2016-07-16 02:57:37.747162: step 131260, loss = 3.02 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 02:57:42.573269: step 131270, loss = 2.89 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 02:57:47.237524: step 131280, loss = 2.92 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 02:57:51.930448: step 131290, loss = 3.09 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 02:57:57.613443: step 131300, loss = 3.17 (243.8 examples/sec; 0.525 sec/batch)
2016-07-16 02:58:03.475899: step 131310, loss = 3.00 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 02:58:08.329203: step 131320, loss = 3.04 (239.5 examples/sec; 0.535 sec/batch)
2016-07-16 02:58:13.044828: step 131330, loss = 3.22 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 02:58:17.925207: step 131340, loss = 2.99 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 02:58:22.656305: step 131350, loss = 3.03 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 02:58:27.333962: step 131360, loss = 3.07 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 02:58:32.011169: step 131370, loss = 3.26 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 02:58:36.669467: step 131380, loss = 3.04 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 02:58:42.412837: step 131390, loss = 2.98 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 02:58:47.187800: step 131400, loss = 2.93 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 02:58:52.976452: step 131410, loss = 3.07 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 02:58:57.757398: step 131420, loss = 2.97 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 02:59:02.386421: step 131430, loss = 2.93 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 02:59:07.733832: step 131440, loss = 3.02 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 02:59:12.985438: step 131450, loss = 3.37 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 02:59:17.715825: step 131460, loss = 3.07 (275.0 examples/sec; 0.466 sec/batch)
2016-07-16 02:59:22.358795: step 131470, loss = 3.18 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 02:59:27.685062: step 131480, loss = 2.68 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 02:59:32.925624: step 131490, loss = 3.00 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 02:59:38.660845: step 131500, loss = 2.95 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 02:59:45.476677: step 131510, loss = 2.90 (209.5 examples/sec; 0.611 sec/batch)
2016-07-16 02:59:50.336303: step 131520, loss = 3.07 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 02:59:55.123364: step 131530, loss = 2.96 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 02:59:59.914319: step 131540, loss = 2.92 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 03:00:04.570489: step 131550, loss = 3.42 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 03:00:09.316134: step 131560, loss = 3.12 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 03:00:13.946134: step 131570, loss = 2.97 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 03:00:19.033915: step 131580, loss = 3.21 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 03:00:24.441880: step 131590, loss = 2.95 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 03:00:29.158040: step 131600, loss = 2.86 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 03:00:35.759303: step 131610, loss = 2.82 (189.6 examples/sec; 0.675 sec/batch)
2016-07-16 03:00:41.991146: step 131620, loss = 2.96 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 03:00:46.809901: step 131630, loss = 3.13 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 03:00:51.605732: step 131640, loss = 3.38 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 03:00:56.359795: step 131650, loss = 3.14 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 03:01:00.983241: step 131660, loss = 3.06 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 03:01:05.681580: step 131670, loss = 2.96 (245.1 examples/sec; 0.522 sec/batch)
2016-07-16 03:01:11.257744: step 131680, loss = 3.01 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 03:01:15.856126: step 131690, loss = 2.82 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 03:01:20.542515: step 131700, loss = 2.95 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 03:01:26.188340: step 131710, loss = 2.93 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 03:01:31.333510: step 131720, loss = 3.22 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 03:01:36.734955: step 131730, loss = 3.02 (241.5 examples/sec; 0.530 sec/batch)
2016-07-16 03:01:42.587955: step 131740, loss = 2.83 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 03:01:47.367524: step 131750, loss = 2.98 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 03:01:52.201921: step 131760, loss = 3.09 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 03:01:56.990771: step 131770, loss = 2.84 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 03:02:01.665958: step 131780, loss = 2.83 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 03:02:06.350380: step 131790, loss = 2.96 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 03:02:10.987438: step 131800, loss = 2.96 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 03:02:17.621793: step 131810, loss = 3.16 (210.2 examples/sec; 0.609 sec/batch)
2016-07-16 03:02:22.462755: step 131820, loss = 3.28 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 03:02:27.222144: step 131830, loss = 2.90 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 03:02:32.041633: step 131840, loss = 3.06 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 03:02:36.685795: step 131850, loss = 3.17 (286.4 examples/sec; 0.447 sec/batch)
2016-07-16 03:02:41.389207: step 131860, loss = 3.02 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 03:02:47.187333: step 131870, loss = 3.12 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 03:02:52.771641: step 131880, loss = 2.70 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 03:02:57.829939: step 131890, loss = 3.23 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 03:03:02.483609: step 131900, loss = 2.97 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 03:03:08.097236: step 131910, loss = 3.22 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 03:03:12.778492: step 131920, loss = 3.08 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 03:03:17.437668: step 131930, loss = 2.98 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 03:03:22.095720: step 131940, loss = 3.08 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 03:03:27.262480: step 131950, loss = 3.01 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 03:03:32.673347: step 131960, loss = 3.09 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 03:03:38.418996: step 131970, loss = 3.05 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 03:03:43.858940: step 131980, loss = 3.17 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 03:03:49.182996: step 131990, loss = 3.31 (255.7 examples/sec; 0.501 sec/batch)
2016-07-16 03:03:53.881480: step 132000, loss = 3.00 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 03:04:00.662633: step 132010, loss = 3.01 (188.4 examples/sec; 0.679 sec/batch)
2016-07-16 03:04:06.708930: step 132020, loss = 2.83 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 03:04:11.506871: step 132030, loss = 3.07 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 03:04:16.299025: step 132040, loss = 2.97 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 03:04:21.040504: step 132050, loss = 3.06 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 03:04:25.706919: step 132060, loss = 3.11 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 03:04:30.696306: step 132070, loss = 3.04 (207.4 examples/sec; 0.617 sec/batch)
2016-07-16 03:04:36.214226: step 132080, loss = 2.99 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 03:04:40.896578: step 132090, loss = 3.02 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 03:04:45.789743: step 132100, loss = 2.88 (267.5 examples/sec; 0.478 sec/batch)
2016-07-16 03:04:51.364636: step 132110, loss = 2.85 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 03:04:56.017165: step 132120, loss = 2.92 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 03:05:01.721558: step 132130, loss = 2.96 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 03:05:06.608112: step 132140, loss = 2.94 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 03:05:11.338471: step 132150, loss = 2.77 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 03:05:16.018254: step 132160, loss = 2.99 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 03:05:20.644809: step 132170, loss = 2.91 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 03:05:25.293271: step 132180, loss = 3.12 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 03:05:29.979008: step 132190, loss = 2.86 (239.0 examples/sec; 0.536 sec/batch)
2016-07-16 03:05:35.649046: step 132200, loss = 3.01 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 03:05:41.519615: step 132210, loss = 3.13 (269.2 examples/sec; 0.476 sec/batch)
2016-07-16 03:05:46.697946: step 132220, loss = 3.09 (190.3 examples/sec; 0.673 sec/batch)
2016-07-16 03:05:53.177392: step 132230, loss = 2.78 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 03:05:58.166212: step 132240, loss = 2.97 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 03:06:02.880428: step 132250, loss = 2.88 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 03:06:08.516703: step 132260, loss = 3.02 (188.6 examples/sec; 0.679 sec/batch)
2016-07-16 03:06:14.628855: step 132270, loss = 2.99 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 03:06:19.476130: step 132280, loss = 3.02 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 03:06:24.264947: step 132290, loss = 2.94 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 03:06:29.056127: step 132300, loss = 3.08 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 03:06:34.641034: step 132310, loss = 2.92 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 03:06:39.301455: step 132320, loss = 2.97 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 03:06:43.974433: step 132330, loss = 3.12 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 03:06:48.649651: step 132340, loss = 3.25 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 03:06:53.276119: step 132350, loss = 2.96 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 03:06:58.064327: step 132360, loss = 2.99 (230.7 examples/sec; 0.555 sec/batch)
2016-07-16 03:07:03.769765: step 132370, loss = 2.96 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 03:07:08.504576: step 132380, loss = 3.05 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 03:07:13.378876: step 132390, loss = 3.16 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 03:07:19.860105: step 132400, loss = 3.00 (208.5 examples/sec; 0.614 sec/batch)
2016-07-16 03:07:26.340177: step 132410, loss = 3.11 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 03:07:31.109052: step 132420, loss = 3.08 (284.4 examples/sec; 0.450 sec/batch)
2016-07-16 03:07:35.778306: step 132430, loss = 3.08 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 03:07:40.517829: step 132440, loss = 2.96 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 03:07:45.288656: step 132450, loss = 3.14 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 03:07:49.920102: step 132460, loss = 3.02 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 03:07:54.855305: step 132470, loss = 3.02 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 03:08:00.373234: step 132480, loss = 3.13 (257.0 examples/sec; 0.498 sec/batch)
2016-07-16 03:08:05.128164: step 132490, loss = 2.94 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 03:08:10.118454: step 132500, loss = 2.78 (228.8 examples/sec; 0.559 sec/batch)
2016-07-16 03:08:18.219718: step 132510, loss = 2.98 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 03:08:22.845678: step 132520, loss = 3.05 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 03:08:28.118106: step 132530, loss = 3.17 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 03:08:33.416936: step 132540, loss = 2.98 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 03:08:39.194941: step 132550, loss = 2.96 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 03:08:44.660474: step 132560, loss = 2.87 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 03:08:49.851603: step 132570, loss = 2.95 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 03:08:54.566777: step 132580, loss = 2.95 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 03:08:59.376519: step 132590, loss = 2.97 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 03:09:04.169804: step 132600, loss = 3.23 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 03:09:09.992146: step 132610, loss = 3.41 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 03:09:14.813832: step 132620, loss = 3.02 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 03:09:19.584067: step 132630, loss = 3.03 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 03:09:24.403914: step 132640, loss = 2.75 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 03:09:29.047264: step 132650, loss = 2.92 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 03:09:33.700985: step 132660, loss = 2.90 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 03:09:39.503089: step 132670, loss = 3.31 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 03:09:45.068940: step 132680, loss = 3.06 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 03:09:50.113044: step 132690, loss = 3.07 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 03:09:54.821131: step 132700, loss = 2.93 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 03:10:00.346084: step 132710, loss = 3.28 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 03:10:05.018288: step 132720, loss = 3.12 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 03:10:09.668731: step 132730, loss = 3.06 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 03:10:15.415557: step 132740, loss = 2.98 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 03:10:20.209522: step 132750, loss = 2.85 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 03:10:25.034496: step 132760, loss = 2.88 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 03:10:31.022564: step 132770, loss = 3.25 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 03:10:35.656867: step 132780, loss = 2.96 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 03:10:41.173121: step 132790, loss = 3.06 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 03:10:46.306432: step 132800, loss = 2.99 (225.0 examples/sec; 0.569 sec/batch)
2016-07-16 03:10:53.268746: step 132810, loss = 3.04 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 03:10:57.951510: step 132820, loss = 3.11 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 03:11:02.578223: step 132830, loss = 3.10 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 03:11:07.322577: step 132840, loss = 3.06 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 03:11:11.942490: step 132850, loss = 2.81 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 03:11:17.566605: step 132860, loss = 2.79 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 03:11:22.506143: step 132870, loss = 3.11 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 03:11:27.240756: step 132880, loss = 2.86 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 03:11:32.947054: step 132890, loss = 2.90 (186.4 examples/sec; 0.687 sec/batch)
2016-07-16 03:11:38.898529: step 132900, loss = 2.97 (283.7 examples/sec; 0.451 sec/batch)
2016-07-16 03:11:44.469245: step 132910, loss = 2.91 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 03:11:49.129894: step 132920, loss = 3.02 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 03:11:53.772407: step 132930, loss = 2.91 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 03:11:59.561500: step 132940, loss = 2.80 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 03:12:04.312696: step 132950, loss = 2.92 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 03:12:09.105064: step 132960, loss = 2.88 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 03:12:15.596304: step 132970, loss = 3.02 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 03:12:21.065866: step 132980, loss = 3.00 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 03:12:25.798924: step 132990, loss = 2.87 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 03:12:30.674384: step 133000, loss = 2.93 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 03:12:36.285480: step 133010, loss = 3.29 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 03:12:40.966857: step 133020, loss = 2.95 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 03:12:45.606101: step 133030, loss = 2.97 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 03:12:50.313471: step 133040, loss = 2.75 (246.5 examples/sec; 0.519 sec/batch)
2016-07-16 03:12:56.063621: step 133050, loss = 3.13 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 03:13:00.861207: step 133060, loss = 3.03 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 03:13:05.703974: step 133070, loss = 3.06 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 03:13:12.136790: step 133080, loss = 3.09 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 03:13:17.492695: step 133090, loss = 3.02 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 03:13:22.208558: step 133100, loss = 3.01 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 03:13:28.042528: step 133110, loss = 2.87 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 03:13:32.823484: step 133120, loss = 3.06 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 03:13:37.589108: step 133130, loss = 3.09 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 03:13:42.196587: step 133140, loss = 3.22 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 03:13:46.891084: step 133150, loss = 3.19 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 03:13:51.528742: step 133160, loss = 3.24 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 03:13:56.880768: step 133170, loss = 3.15 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 03:14:02.017867: step 133180, loss = 2.88 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 03:14:06.701302: step 133190, loss = 2.86 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 03:14:11.497648: step 133200, loss = 3.07 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 03:14:17.307681: step 133210, loss = 3.02 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 03:14:22.066611: step 133220, loss = 2.96 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 03:14:26.742251: step 133230, loss = 3.08 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 03:14:31.946613: step 133240, loss = 2.86 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 03:14:37.360578: step 133250, loss = 3.08 (255.7 examples/sec; 0.501 sec/batch)
2016-07-16 03:14:42.088491: step 133260, loss = 2.98 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 03:14:46.686375: step 133270, loss = 2.74 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 03:14:51.729237: step 133280, loss = 3.07 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 03:14:57.199162: step 133290, loss = 2.79 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 03:15:01.907963: step 133300, loss = 2.97 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 03:15:07.736482: step 133310, loss = 2.90 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 03:15:12.483563: step 133320, loss = 3.03 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 03:15:18.573013: step 133330, loss = 2.84 (200.2 examples/sec; 0.640 sec/batch)
2016-07-16 03:15:24.239654: step 133340, loss = 3.14 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 03:15:29.012931: step 133350, loss = 2.80 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 03:15:33.881307: step 133360, loss = 2.89 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 03:15:38.550126: step 133370, loss = 2.69 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 03:15:43.151059: step 133380, loss = 2.85 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 03:15:47.789673: step 133390, loss = 2.84 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 03:15:52.420465: step 133400, loss = 2.85 (275.0 examples/sec; 0.466 sec/batch)
2016-07-16 03:15:58.014069: step 133410, loss = 2.89 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 03:16:03.823021: step 133420, loss = 3.17 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 03:16:08.661322: step 133430, loss = 2.96 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 03:16:13.487970: step 133440, loss = 2.94 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 03:16:19.882405: step 133450, loss = 2.88 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 03:16:25.373892: step 133460, loss = 3.16 (252.5 examples/sec; 0.507 sec/batch)
2016-07-16 03:16:31.170111: step 133470, loss = 3.04 (255.4 examples/sec; 0.501 sec/batch)
2016-07-16 03:16:36.060456: step 133480, loss = 3.11 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 03:16:40.867110: step 133490, loss = 3.07 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 03:16:47.041762: step 133500, loss = 3.05 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 03:16:53.912786: step 133510, loss = 3.15 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 03:16:58.594360: step 133520, loss = 2.94 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 03:17:03.994730: step 133530, loss = 2.98 (190.1 examples/sec; 0.673 sec/batch)
2016-07-16 03:17:10.442962: step 133540, loss = 2.96 (212.1 examples/sec; 0.603 sec/batch)
2016-07-16 03:17:15.297131: step 133550, loss = 2.90 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 03:17:20.071171: step 133560, loss = 2.98 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 03:17:24.928478: step 133570, loss = 3.31 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 03:17:29.579826: step 133580, loss = 2.84 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 03:17:34.289911: step 133590, loss = 3.16 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 03:17:38.898844: step 133600, loss = 3.14 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 03:17:44.478885: step 133610, loss = 3.15 (283.7 examples/sec; 0.451 sec/batch)
2016-07-16 03:17:49.095754: step 133620, loss = 3.11 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 03:17:53.721624: step 133630, loss = 2.82 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 03:17:58.329885: step 133640, loss = 2.89 (286.2 examples/sec; 0.447 sec/batch)
2016-07-16 03:18:02.981726: step 133650, loss = 2.97 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 03:18:08.128492: step 133660, loss = 2.94 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 03:18:13.509253: step 133670, loss = 2.81 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 03:18:19.190712: step 133680, loss = 2.66 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 03:18:23.823087: step 133690, loss = 3.06 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 03:18:29.204329: step 133700, loss = 2.89 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 03:18:35.298321: step 133710, loss = 2.94 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 03:18:39.955671: step 133720, loss = 2.82 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 03:18:44.681581: step 133730, loss = 2.81 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 03:18:50.357396: step 133740, loss = 2.99 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 03:18:55.231917: step 133750, loss = 2.96 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 03:18:59.927226: step 133760, loss = 3.19 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 03:19:04.748478: step 133770, loss = 2.90 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 03:19:09.563493: step 133780, loss = 2.93 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 03:19:14.285393: step 133790, loss = 3.24 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 03:19:19.145393: step 133800, loss = 2.88 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 03:19:24.915273: step 133810, loss = 2.88 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 03:19:29.702778: step 133820, loss = 3.00 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 03:19:34.608298: step 133830, loss = 3.11 (239.9 examples/sec; 0.534 sec/batch)
2016-07-16 03:19:41.192757: step 133840, loss = 3.07 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 03:19:46.139601: step 133850, loss = 3.07 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 03:19:50.802799: step 133860, loss = 2.80 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 03:19:55.437083: step 133870, loss = 3.10 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 03:20:00.607975: step 133880, loss = 2.87 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 03:20:05.899120: step 133890, loss = 2.89 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 03:20:10.584300: step 133900, loss = 3.02 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 03:20:16.420226: step 133910, loss = 2.87 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 03:20:21.225863: step 133920, loss = 2.93 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 03:20:27.438143: step 133930, loss = 3.10 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 03:20:33.016974: step 133940, loss = 2.99 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 03:20:37.785279: step 133950, loss = 3.08 (252.6 examples/sec; 0.507 sec/batch)
2016-07-16 03:20:42.762521: step 133960, loss = 2.90 (234.3 examples/sec; 0.546 sec/batch)
2016-07-16 03:20:49.280423: step 133970, loss = 3.01 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 03:20:54.503695: step 133980, loss = 2.99 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 03:20:59.199778: step 133990, loss = 2.92 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 03:21:04.043212: step 134000, loss = 3.14 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 03:21:09.608160: step 134010, loss = 2.81 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 03:21:14.228677: step 134020, loss = 2.87 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 03:21:18.917692: step 134030, loss = 3.17 (283.5 examples/sec; 0.452 sec/batch)
2016-07-16 03:21:23.547894: step 134040, loss = 2.83 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 03:21:28.301729: step 134050, loss = 2.80 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 03:21:32.953967: step 134060, loss = 3.04 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 03:21:38.632648: step 134070, loss = 2.77 (210.2 examples/sec; 0.609 sec/batch)
2016-07-16 03:21:43.520924: step 134080, loss = 3.05 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 03:21:48.278588: step 134090, loss = 2.91 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 03:21:53.058484: step 134100, loss = 3.01 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 03:21:58.654182: step 134110, loss = 2.85 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 03:22:03.338733: step 134120, loss = 2.90 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 03:22:07.936410: step 134130, loss = 2.98 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 03:22:13.325288: step 134140, loss = 3.00 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 03:22:18.490429: step 134150, loss = 2.86 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 03:22:23.182049: step 134160, loss = 3.03 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 03:22:28.694223: step 134170, loss = 3.15 (183.8 examples/sec; 0.696 sec/batch)
2016-07-16 03:22:34.985479: step 134180, loss = 3.02 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 03:22:39.667410: step 134190, loss = 2.79 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 03:22:44.323032: step 134200, loss = 2.75 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 03:22:49.996589: step 134210, loss = 3.13 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 03:22:55.717944: step 134220, loss = 3.09 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 03:23:00.543247: step 134230, loss = 3.06 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 03:23:05.413491: step 134240, loss = 3.16 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 03:23:11.410333: step 134250, loss = 2.96 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 03:23:16.056645: step 134260, loss = 2.88 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 03:23:21.457820: step 134270, loss = 2.98 (203.7 examples/sec; 0.629 sec/batch)
2016-07-16 03:23:26.644730: step 134280, loss = 3.25 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 03:23:31.379524: step 134290, loss = 2.87 (246.2 examples/sec; 0.520 sec/batch)
2016-07-16 03:23:36.194986: step 134300, loss = 3.00 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 03:23:41.731709: step 134310, loss = 3.19 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 03:23:46.377660: step 134320, loss = 3.19 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 03:23:52.137533: step 134330, loss = 3.16 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 03:23:56.927984: step 134340, loss = 2.72 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 03:24:01.745523: step 134350, loss = 2.86 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 03:24:07.718391: step 134360, loss = 2.88 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 03:24:12.322276: step 134370, loss = 3.03 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 03:24:16.937083: step 134380, loss = 2.93 (285.8 examples/sec; 0.448 sec/batch)
2016-07-16 03:24:21.646821: step 134390, loss = 2.93 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 03:24:26.280069: step 134400, loss = 2.98 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 03:24:31.941330: step 134410, loss = 2.96 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 03:24:36.551754: step 134420, loss = 2.93 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 03:24:42.116797: step 134430, loss = 2.77 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 03:24:47.155030: step 134440, loss = 2.99 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 03:24:51.874120: step 134450, loss = 2.81 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 03:24:57.567549: step 134460, loss = 3.10 (188.2 examples/sec; 0.680 sec/batch)
2016-07-16 03:25:03.586512: step 134470, loss = 2.93 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 03:25:08.192513: step 134480, loss = 2.98 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 03:25:13.557137: step 134490, loss = 3.06 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 03:25:18.706349: step 134500, loss = 2.96 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 03:25:24.452022: step 134510, loss = 2.82 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 03:25:29.201469: step 134520, loss = 2.84 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 03:25:33.995094: step 134530, loss = 2.76 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 03:25:38.706114: step 134540, loss = 2.76 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 03:25:43.309794: step 134550, loss = 2.77 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 03:25:48.445617: step 134560, loss = 2.79 (200.4 examples/sec; 0.639 sec/batch)
2016-07-16 03:25:53.876583: step 134570, loss = 3.04 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 03:25:59.623846: step 134580, loss = 2.94 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 03:26:04.988060: step 134590, loss = 2.76 (207.4 examples/sec; 0.617 sec/batch)
2016-07-16 03:26:10.304558: step 134600, loss = 2.76 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 03:26:17.161904: step 134610, loss = 2.76 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 03:26:21.979353: step 134620, loss = 3.09 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 03:26:26.832037: step 134630, loss = 2.95 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 03:26:31.604120: step 134640, loss = 2.94 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 03:26:37.149614: step 134650, loss = 3.06 (187.1 examples/sec; 0.684 sec/batch)
2016-07-16 03:26:43.418352: step 134660, loss = 3.08 (255.7 examples/sec; 0.501 sec/batch)
2016-07-16 03:26:48.256794: step 134670, loss = 2.83 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 03:26:53.063088: step 134680, loss = 2.97 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 03:26:59.156923: step 134690, loss = 2.76 (196.8 examples/sec; 0.650 sec/batch)
2016-07-16 03:27:04.887417: step 134700, loss = 3.05 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 03:27:10.745374: step 134710, loss = 2.91 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 03:27:15.340209: step 134720, loss = 2.86 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 03:27:20.488717: step 134730, loss = 3.17 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 03:27:25.900574: step 134740, loss = 2.90 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 03:27:30.591587: step 134750, loss = 3.05 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 03:27:35.442268: step 134760, loss = 3.05 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 03:27:40.097702: step 134770, loss = 3.19 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 03:27:44.816173: step 134780, loss = 2.91 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 03:27:49.484477: step 134790, loss = 2.94 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 03:27:54.082569: step 134800, loss = 2.86 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 03:28:00.032865: step 134810, loss = 2.96 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 03:28:05.550705: step 134820, loss = 2.82 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 03:28:10.286550: step 134830, loss = 2.77 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 03:28:15.190073: step 134840, loss = 2.74 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 03:28:19.984197: step 134850, loss = 2.97 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 03:28:24.794320: step 134860, loss = 2.72 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 03:28:29.588396: step 134870, loss = 2.90 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 03:28:34.325300: step 134880, loss = 3.09 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 03:28:39.207580: step 134890, loss = 2.97 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 03:28:43.994665: step 134900, loss = 2.81 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 03:28:49.804557: step 134910, loss = 2.94 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 03:28:54.463852: step 134920, loss = 3.19 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 03:28:59.065423: step 134930, loss = 2.90 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 03:29:03.763646: step 134940, loss = 3.01 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 03:29:08.333709: step 134950, loss = 3.26 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 03:29:13.725934: step 134960, loss = 2.85 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 03:29:18.876692: step 134970, loss = 2.74 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 03:29:23.634873: step 134980, loss = 3.29 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 03:29:29.113675: step 134990, loss = 3.08 (187.6 examples/sec; 0.682 sec/batch)
2016-07-16 03:29:35.411234: step 135000, loss = 2.89 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 03:29:40.963562: step 135010, loss = 2.99 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 03:29:45.613119: step 135020, loss = 2.66 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 03:29:51.335881: step 135030, loss = 3.09 (245.8 examples/sec; 0.521 sec/batch)
2016-07-16 03:29:56.168819: step 135040, loss = 2.81 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 03:30:00.885796: step 135050, loss = 2.75 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 03:30:05.603096: step 135060, loss = 3.13 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 03:30:11.308374: step 135070, loss = 2.79 (237.2 examples/sec; 0.540 sec/batch)
2016-07-16 03:30:16.168595: step 135080, loss = 2.82 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 03:30:20.867065: step 135090, loss = 2.92 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 03:30:25.508026: step 135100, loss = 3.15 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 03:30:31.093450: step 135110, loss = 2.98 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 03:30:35.994248: step 135120, loss = 3.04 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 03:30:41.596438: step 135130, loss = 3.01 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 03:30:46.371673: step 135140, loss = 2.90 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 03:30:51.211293: step 135150, loss = 2.93 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 03:30:55.898307: step 135160, loss = 3.11 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 03:31:00.722233: step 135170, loss = 3.10 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 03:31:05.368417: step 135180, loss = 2.85 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 03:31:10.050204: step 135190, loss = 3.09 (268.6 examples/sec; 0.476 sec/batch)
2016-07-16 03:31:14.654107: step 135200, loss = 3.28 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 03:31:20.772025: step 135210, loss = 2.90 (205.3 examples/sec; 0.624 sec/batch)
2016-07-16 03:31:26.125335: step 135220, loss = 2.87 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 03:31:30.919868: step 135230, loss = 2.83 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 03:31:35.596798: step 135240, loss = 3.04 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 03:31:40.289789: step 135250, loss = 2.94 (287.0 examples/sec; 0.446 sec/batch)
2016-07-16 03:31:44.965956: step 135260, loss = 2.94 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 03:31:50.496128: step 135270, loss = 3.12 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 03:31:55.489150: step 135280, loss = 2.89 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 03:32:00.222586: step 135290, loss = 3.05 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 03:32:05.077938: step 135300, loss = 3.12 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 03:32:10.722690: step 135310, loss = 2.79 (286.1 examples/sec; 0.447 sec/batch)
2016-07-16 03:32:15.554779: step 135320, loss = 2.92 (216.2 examples/sec; 0.592 sec/batch)
2016-07-16 03:32:21.218144: step 135330, loss = 3.09 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 03:32:25.994097: step 135340, loss = 2.99 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 03:32:30.817162: step 135350, loss = 2.92 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 03:32:35.532370: step 135360, loss = 2.76 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 03:32:40.114737: step 135370, loss = 3.20 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 03:32:45.336927: step 135380, loss = 3.06 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 03:32:50.652026: step 135390, loss = 3.01 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 03:32:55.341728: step 135400, loss = 3.16 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 03:33:01.228704: step 135410, loss = 3.01 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 03:33:06.021267: step 135420, loss = 2.86 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 03:33:10.747272: step 135430, loss = 2.92 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 03:33:15.586529: step 135440, loss = 3.11 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 03:33:20.374230: step 135450, loss = 2.93 (234.7 examples/sec; 0.545 sec/batch)
2016-07-16 03:33:25.195485: step 135460, loss = 2.94 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 03:33:29.851605: step 135470, loss = 3.03 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 03:33:34.522236: step 135480, loss = 2.77 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 03:33:39.130977: step 135490, loss = 2.93 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 03:33:43.777552: step 135500, loss = 2.79 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 03:33:50.160232: step 135510, loss = 2.85 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 03:33:55.318380: step 135520, loss = 2.94 (253.2 examples/sec; 0.506 sec/batch)
2016-07-16 03:34:01.055567: step 135530, loss = 2.88 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 03:34:05.826135: step 135540, loss = 2.93 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 03:34:10.511534: step 135550, loss = 2.94 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 03:34:15.156252: step 135560, loss = 3.02 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 03:34:19.758378: step 135570, loss = 2.85 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 03:34:24.884747: step 135580, loss = 2.95 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 03:34:30.306046: step 135590, loss = 3.14 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 03:34:35.011212: step 135600, loss = 2.91 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 03:34:41.603317: step 135610, loss = 2.82 (188.4 examples/sec; 0.679 sec/batch)
2016-07-16 03:34:47.805741: step 135620, loss = 3.01 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 03:34:52.624461: step 135630, loss = 2.89 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 03:34:57.378409: step 135640, loss = 2.89 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 03:35:02.278839: step 135650, loss = 2.96 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 03:35:06.935477: step 135660, loss = 2.88 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 03:35:11.580434: step 135670, loss = 2.97 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 03:35:16.617387: step 135680, loss = 2.90 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 03:35:21.948574: step 135690, loss = 2.95 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 03:35:26.644229: step 135700, loss = 3.20 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 03:35:32.252118: step 135710, loss = 2.86 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 03:35:37.019617: step 135720, loss = 2.81 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 03:35:41.702310: step 135730, loss = 2.89 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 03:35:47.421017: step 135740, loss = 2.94 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 03:35:52.264683: step 135750, loss = 2.79 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 03:35:57.047468: step 135760, loss = 2.78 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 03:36:01.833194: step 135770, loss = 2.88 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 03:36:06.455294: step 135780, loss = 2.80 (284.0 examples/sec; 0.451 sec/batch)
2016-07-16 03:36:11.095512: step 135790, loss = 3.17 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 03:36:16.868997: step 135800, loss = 2.92 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 03:36:22.659524: step 135810, loss = 2.90 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 03:36:27.494709: step 135820, loss = 3.03 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 03:36:32.169265: step 135830, loss = 2.82 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 03:36:37.740665: step 135840, loss = 2.97 (186.4 examples/sec; 0.687 sec/batch)
2016-07-16 03:36:43.964891: step 135850, loss = 2.87 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 03:36:48.808283: step 135860, loss = 3.04 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 03:36:53.583830: step 135870, loss = 2.94 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 03:36:58.318077: step 135880, loss = 2.95 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 03:37:03.206730: step 135890, loss = 3.28 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 03:37:07.927711: step 135900, loss = 2.87 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 03:37:13.726260: step 135910, loss = 3.01 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 03:37:18.581021: step 135920, loss = 2.93 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 03:37:24.957119: step 135930, loss = 3.10 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 03:37:30.398300: step 135940, loss = 2.90 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 03:37:35.113121: step 135950, loss = 2.89 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 03:37:39.980383: step 135960, loss = 2.84 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 03:37:44.684618: step 135970, loss = 2.72 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 03:37:49.362286: step 135980, loss = 2.83 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 03:37:54.938353: step 135990, loss = 2.98 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 03:37:59.914301: step 136000, loss = 2.94 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 03:38:05.573510: step 136010, loss = 3.08 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 03:38:10.166074: step 136020, loss = 2.81 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 03:38:14.792001: step 136030, loss = 2.84 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 03:38:20.559594: step 136040, loss = 2.95 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 03:38:25.371694: step 136050, loss = 3.01 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 03:38:30.127878: step 136060, loss = 2.86 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 03:38:34.878756: step 136070, loss = 2.87 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 03:38:39.551098: step 136080, loss = 3.16 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 03:38:44.489670: step 136090, loss = 2.87 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 03:38:50.056526: step 136100, loss = 3.07 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 03:38:56.817259: step 136110, loss = 3.22 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 03:39:01.596826: step 136120, loss = 2.74 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 03:39:06.189932: step 136130, loss = 2.76 (283.8 examples/sec; 0.451 sec/batch)
2016-07-16 03:39:10.876960: step 136140, loss = 3.31 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 03:39:15.535288: step 136150, loss = 2.97 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 03:39:20.442730: step 136160, loss = 2.94 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 03:39:26.015523: step 136170, loss = 2.87 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 03:39:30.827148: step 136180, loss = 3.00 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 03:39:35.763716: step 136190, loss = 3.01 (227.4 examples/sec; 0.563 sec/batch)
2016-07-16 03:39:42.359875: step 136200, loss = 2.62 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 03:39:48.441674: step 136210, loss = 2.76 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 03:39:53.070260: step 136220, loss = 3.10 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 03:39:58.495438: step 136230, loss = 2.80 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 03:40:03.610041: step 136240, loss = 2.72 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 03:40:09.393462: step 136250, loss = 3.06 (252.5 examples/sec; 0.507 sec/batch)
2016-07-16 03:40:14.214268: step 136260, loss = 3.25 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 03:40:19.002620: step 136270, loss = 2.80 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 03:40:23.732424: step 136280, loss = 3.03 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 03:40:28.670232: step 136290, loss = 2.73 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 03:40:33.366423: step 136300, loss = 3.02 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 03:40:38.940466: step 136310, loss = 2.88 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 03:40:44.714451: step 136320, loss = 3.04 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 03:40:49.483404: step 136330, loss = 3.14 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 03:40:54.258768: step 136340, loss = 2.81 (253.8 examples/sec; 0.504 sec/batch)
2016-07-16 03:40:59.012927: step 136350, loss = 2.93 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 03:41:03.877751: step 136360, loss = 3.17 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 03:41:08.608123: step 136370, loss = 3.18 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 03:41:13.425472: step 136380, loss = 2.92 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 03:41:18.242820: step 136390, loss = 2.74 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 03:41:24.412444: step 136400, loss = 3.28 (200.9 examples/sec; 0.637 sec/batch)
2016-07-16 03:41:31.191909: step 136410, loss = 3.11 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 03:41:35.881007: step 136420, loss = 2.91 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 03:41:40.767502: step 136430, loss = 2.97 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 03:41:45.375820: step 136440, loss = 2.88 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 03:41:50.059669: step 136450, loss = 3.01 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 03:41:55.814999: step 136460, loss = 3.16 (225.0 examples/sec; 0.569 sec/batch)
2016-07-16 03:42:00.683696: step 136470, loss = 2.97 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 03:42:05.441496: step 136480, loss = 2.78 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 03:42:11.371250: step 136490, loss = 2.99 (188.6 examples/sec; 0.679 sec/batch)
2016-07-16 03:42:17.238668: step 136500, loss = 3.01 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 03:42:23.039410: step 136510, loss = 2.84 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 03:42:28.018145: step 136520, loss = 2.60 (215.5 examples/sec; 0.594 sec/batch)
2016-07-16 03:42:34.563391: step 136530, loss = 3.30 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 03:42:39.697461: step 136540, loss = 3.05 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 03:42:44.431900: step 136550, loss = 3.01 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 03:42:49.287574: step 136560, loss = 3.16 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 03:42:53.916309: step 136570, loss = 2.85 (286.9 examples/sec; 0.446 sec/batch)
2016-07-16 03:42:58.574444: step 136580, loss = 2.96 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 03:43:04.475313: step 136590, loss = 2.86 (255.7 examples/sec; 0.501 sec/batch)
2016-07-16 03:43:09.297129: step 136600, loss = 2.94 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 03:43:14.916902: step 136610, loss = 2.90 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 03:43:19.649044: step 136620, loss = 2.89 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 03:43:24.243426: step 136630, loss = 3.13 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 03:43:29.398921: step 136640, loss = 2.86 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 03:43:34.793807: step 136650, loss = 2.97 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 03:43:39.474578: step 136660, loss = 2.76 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 03:43:44.503356: step 136670, loss = 2.84 (190.5 examples/sec; 0.672 sec/batch)
2016-07-16 03:43:51.001238: step 136680, loss = 2.81 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 03:43:56.064114: step 136690, loss = 2.73 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 03:44:00.808258: step 136700, loss = 2.82 (246.9 examples/sec; 0.518 sec/batch)
2016-07-16 03:44:06.587243: step 136710, loss = 2.92 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 03:44:11.457041: step 136720, loss = 3.09 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 03:44:16.134737: step 136730, loss = 2.84 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 03:44:20.760236: step 136740, loss = 2.88 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 03:44:26.043361: step 136750, loss = 3.12 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 03:44:31.308982: step 136760, loss = 2.84 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 03:44:36.061840: step 136770, loss = 2.74 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 03:44:41.451342: step 136780, loss = 3.22 (180.2 examples/sec; 0.710 sec/batch)
2016-07-16 03:44:47.882308: step 136790, loss = 2.65 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 03:44:52.702442: step 136800, loss = 3.00 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 03:44:58.460758: step 136810, loss = 2.89 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 03:45:03.189251: step 136820, loss = 2.84 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 03:45:08.145266: step 136830, loss = 2.83 (221.7 examples/sec; 0.577 sec/batch)
2016-07-16 03:45:14.674846: step 136840, loss = 2.86 (206.6 examples/sec; 0.619 sec/batch)
2016-07-16 03:45:19.847261: step 136850, loss = 3.00 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 03:45:24.546139: step 136860, loss = 3.03 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 03:45:30.004629: step 136870, loss = 2.74 (187.2 examples/sec; 0.684 sec/batch)
2016-07-16 03:45:36.233895: step 136880, loss = 2.85 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 03:45:40.869981: step 136890, loss = 2.93 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 03:45:46.160410: step 136900, loss = 2.82 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 03:45:52.310310: step 136910, loss = 2.70 (284.2 examples/sec; 0.450 sec/batch)
2016-07-16 03:45:56.951202: step 136920, loss = 2.66 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 03:46:01.601864: step 136930, loss = 2.98 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 03:46:06.235142: step 136940, loss = 2.87 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 03:46:11.975700: step 136950, loss = 3.06 (202.0 examples/sec; 0.634 sec/batch)
2016-07-16 03:46:16.810958: step 136960, loss = 3.05 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 03:46:21.541444: step 136970, loss = 2.88 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 03:46:27.413178: step 136980, loss = 3.05 (188.9 examples/sec; 0.677 sec/batch)
2016-07-16 03:46:32.198211: step 136990, loss = 2.73 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 03:46:36.833499: step 137000, loss = 2.77 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 03:46:42.365702: step 137010, loss = 2.69 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 03:46:46.988652: step 137020, loss = 2.90 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 03:46:51.592379: step 137030, loss = 2.99 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 03:46:56.410797: step 137040, loss = 2.94 (208.9 examples/sec; 0.613 sec/batch)
2016-07-16 03:47:02.131244: step 137050, loss = 3.19 (252.7 examples/sec; 0.506 sec/batch)
2016-07-16 03:47:06.959427: step 137060, loss = 2.84 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 03:47:11.544292: step 137070, loss = 2.72 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 03:47:16.310573: step 137080, loss = 2.84 (225.6 examples/sec; 0.567 sec/batch)
2016-07-16 03:47:21.975173: step 137090, loss = 2.99 (271.5 examples/sec; 0.472 sec/batch)
2016-07-16 03:47:26.779307: step 137100, loss = 3.13 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 03:47:32.931040: step 137110, loss = 2.84 (186.9 examples/sec; 0.685 sec/batch)
2016-07-16 03:47:39.440393: step 137120, loss = 2.76 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 03:47:44.487897: step 137130, loss = 2.65 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 03:47:49.218604: step 137140, loss = 2.70 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 03:47:54.957951: step 137150, loss = 2.92 (193.3 examples/sec; 0.662 sec/batch)
2016-07-16 03:48:00.977306: step 137160, loss = 2.86 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 03:48:05.743463: step 137170, loss = 2.93 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 03:48:10.473514: step 137180, loss = 3.00 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 03:48:15.121551: step 137190, loss = 2.81 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 03:48:20.836516: step 137200, loss = 2.99 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 03:48:26.620968: step 137210, loss = 2.83 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 03:48:31.487608: step 137220, loss = 2.83 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 03:48:36.171531: step 137230, loss = 2.89 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 03:48:40.809191: step 137240, loss = 3.20 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 03:48:46.015566: step 137250, loss = 2.85 (207.7 examples/sec; 0.616 sec/batch)
2016-07-16 03:48:51.347084: step 137260, loss = 2.96 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 03:48:56.040139: step 137270, loss = 2.67 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 03:49:00.666315: step 137280, loss = 2.87 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 03:49:05.366464: step 137290, loss = 2.96 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 03:49:10.041247: step 137300, loss = 3.17 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 03:49:15.647541: step 137310, loss = 3.04 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 03:49:20.446996: step 137320, loss = 2.83 (217.1 examples/sec; 0.590 sec/batch)
2016-07-16 03:49:26.107495: step 137330, loss = 2.77 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 03:49:30.886424: step 137340, loss = 2.95 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 03:49:35.717674: step 137350, loss = 2.86 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 03:49:40.409009: step 137360, loss = 2.57 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 03:49:45.056405: step 137370, loss = 2.90 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 03:49:50.288302: step 137380, loss = 2.86 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 03:49:55.600527: step 137390, loss = 2.74 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 03:50:00.361741: step 137400, loss = 2.92 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 03:50:07.213782: step 137410, loss = 2.93 (190.1 examples/sec; 0.673 sec/batch)
2016-07-16 03:50:13.203144: step 137420, loss = 3.14 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 03:50:18.022368: step 137430, loss = 2.83 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 03:50:22.885237: step 137440, loss = 3.07 (249.0 examples/sec; 0.514 sec/batch)
2016-07-16 03:50:28.888171: step 137450, loss = 2.78 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 03:50:33.577018: step 137460, loss = 2.95 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 03:50:38.999191: step 137470, loss = 2.85 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 03:50:44.160343: step 137480, loss = 2.79 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 03:50:48.879978: step 137490, loss = 2.79 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 03:50:54.421399: step 137500, loss = 2.89 (191.3 examples/sec; 0.669 sec/batch)
2016-07-16 03:51:00.858742: step 137510, loss = 2.94 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 03:51:05.507948: step 137520, loss = 3.01 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 03:51:10.139563: step 137530, loss = 3.02 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 03:51:15.723799: step 137540, loss = 2.97 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 03:51:20.731910: step 137550, loss = 2.82 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 03:51:25.453328: step 137560, loss = 3.18 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 03:51:30.073439: step 137570, loss = 2.79 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 03:51:35.628673: step 137580, loss = 3.05 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 03:51:40.681845: step 137590, loss = 2.93 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 03:51:45.555534: step 137600, loss = 2.92 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 03:51:51.476338: step 137610, loss = 2.86 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 03:51:56.345091: step 137620, loss = 2.98 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 03:52:01.026782: step 137630, loss = 2.83 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 03:52:05.681384: step 137640, loss = 2.61 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 03:52:11.116913: step 137650, loss = 2.94 (198.6 examples/sec; 0.645 sec/batch)
2016-07-16 03:52:16.247349: step 137660, loss = 2.69 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 03:52:20.981472: step 137670, loss = 2.86 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 03:52:25.792920: step 137680, loss = 2.89 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 03:52:30.401647: step 137690, loss = 3.07 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 03:52:35.061896: step 137700, loss = 2.80 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 03:52:41.766227: step 137710, loss = 2.91 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 03:52:46.545660: step 137720, loss = 2.98 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 03:52:51.351589: step 137730, loss = 2.79 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 03:52:57.350110: step 137740, loss = 2.80 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 03:53:01.996507: step 137750, loss = 2.81 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 03:53:06.668543: step 137760, loss = 2.74 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 03:53:12.410202: step 137770, loss = 2.78 (220.3 examples/sec; 0.581 sec/batch)
2016-07-16 03:53:17.599020: step 137780, loss = 2.98 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 03:53:23.060634: step 137790, loss = 2.57 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 03:53:28.830265: step 137800, loss = 2.69 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 03:53:34.677376: step 137810, loss = 2.93 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 03:53:39.425682: step 137820, loss = 2.68 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 03:53:45.759204: step 137830, loss = 2.76 (208.1 examples/sec; 0.615 sec/batch)
2016-07-16 03:53:51.039617: step 137840, loss = 2.84 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 03:53:55.692219: step 137850, loss = 2.77 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 03:54:01.462233: step 137860, loss = 2.81 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 03:54:06.903455: step 137870, loss = 2.76 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 03:54:12.092539: step 137880, loss = 2.92 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 03:54:17.877549: step 137890, loss = 2.62 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 03:54:22.689267: step 137900, loss = 2.86 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 03:54:28.350283: step 137910, loss = 3.11 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 03:54:33.198807: step 137920, loss = 2.83 (210.1 examples/sec; 0.609 sec/batch)
2016-07-16 03:54:38.644628: step 137930, loss = 2.67 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 03:54:43.266858: step 137940, loss = 3.01 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 03:54:47.908964: step 137950, loss = 2.85 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 03:54:52.693711: step 137960, loss = 3.05 (220.4 examples/sec; 0.581 sec/batch)
2016-07-16 03:54:58.398050: step 137970, loss = 3.00 (255.7 examples/sec; 0.501 sec/batch)
2016-07-16 03:55:03.151789: step 137980, loss = 2.97 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 03:55:08.010980: step 137990, loss = 2.75 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 03:55:12.717852: step 138000, loss = 2.70 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 03:55:18.301344: step 138010, loss = 2.83 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 03:55:23.896250: step 138020, loss = 2.92 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 03:55:29.087942: step 138030, loss = 2.84 (208.0 examples/sec; 0.615 sec/batch)
2016-07-16 03:55:34.693508: step 138040, loss = 2.88 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 03:55:39.525770: step 138050, loss = 2.89 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 03:55:44.374081: step 138060, loss = 2.79 (252.6 examples/sec; 0.507 sec/batch)
2016-07-16 03:55:49.061533: step 138070, loss = 2.80 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 03:55:54.565255: step 138080, loss = 2.87 (190.2 examples/sec; 0.673 sec/batch)
2016-07-16 03:56:00.843709: step 138090, loss = 2.73 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 03:56:05.742765: step 138100, loss = 2.87 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 03:56:11.554663: step 138110, loss = 2.92 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 03:56:17.890475: step 138120, loss = 2.99 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 03:56:23.349639: step 138130, loss = 2.83 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 03:56:28.101558: step 138140, loss = 2.95 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 03:56:32.700946: step 138150, loss = 2.89 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 03:56:37.351762: step 138160, loss = 2.67 (286.7 examples/sec; 0.446 sec/batch)
2016-07-16 03:56:41.955249: step 138170, loss = 2.89 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 03:56:46.643253: step 138180, loss = 2.84 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 03:56:51.383660: step 138190, loss = 2.80 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 03:56:56.007881: step 138200, loss = 2.86 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 03:57:02.138149: step 138210, loss = 3.13 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 03:57:07.561947: step 138220, loss = 2.85 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 03:57:13.328802: step 138230, loss = 2.95 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 03:57:18.134067: step 138240, loss = 2.86 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 03:57:22.927398: step 138250, loss = 3.07 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 03:57:27.674981: step 138260, loss = 3.11 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 03:57:32.320875: step 138270, loss = 2.74 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 03:57:37.158217: step 138280, loss = 2.66 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 03:57:42.831129: step 138290, loss = 2.84 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 03:57:48.620645: step 138300, loss = 2.75 (213.6 examples/sec; 0.599 sec/batch)
2016-07-16 03:57:54.378519: step 138310, loss = 2.94 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 03:57:59.156422: step 138320, loss = 2.76 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 03:58:03.871625: step 138330, loss = 2.89 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 03:58:08.865577: step 138340, loss = 2.90 (216.5 examples/sec; 0.591 sec/batch)
2016-07-16 03:58:15.399186: step 138350, loss = 2.77 (204.3 examples/sec; 0.626 sec/batch)
2016-07-16 03:58:20.541037: step 138360, loss = 2.89 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 03:58:25.245318: step 138370, loss = 2.92 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 03:58:30.679525: step 138380, loss = 2.89 (193.1 examples/sec; 0.663 sec/batch)
2016-07-16 03:58:36.926212: step 138390, loss = 2.58 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 03:58:41.534629: step 138400, loss = 2.85 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 03:58:47.135941: step 138410, loss = 2.84 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 03:58:51.751301: step 138420, loss = 2.96 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 03:58:57.510128: step 138430, loss = 2.85 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 03:59:02.318607: step 138440, loss = 2.82 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 03:59:07.072990: step 138450, loss = 2.84 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 03:59:11.792589: step 138460, loss = 3.06 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 03:59:16.674219: step 138470, loss = 2.66 (247.8 examples/sec; 0.517 sec/batch)
2016-07-16 03:59:23.223451: step 138480, loss = 2.92 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 03:59:28.377270: step 138490, loss = 2.95 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 03:59:34.123232: step 138500, loss = 2.87 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 03:59:39.977581: step 138510, loss = 2.94 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 03:59:44.843126: step 138520, loss = 2.64 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 03:59:49.486034: step 138530, loss = 2.88 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 03:59:54.094733: step 138540, loss = 2.75 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 03:59:59.586950: step 138550, loss = 2.77 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 04:00:04.675105: step 138560, loss = 2.75 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 04:00:10.426137: step 138570, loss = 2.79 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 04:00:16.051514: step 138580, loss = 2.94 (201.6 examples/sec; 0.635 sec/batch)
2016-07-16 04:00:21.079942: step 138590, loss = 2.79 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 04:00:25.850650: step 138600, loss = 2.77 (253.2 examples/sec; 0.506 sec/batch)
2016-07-16 04:00:31.647261: step 138610, loss = 2.80 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 04:00:36.363480: step 138620, loss = 2.92 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 04:00:41.098359: step 138630, loss = 2.87 (214.3 examples/sec; 0.597 sec/batch)
2016-07-16 04:00:46.737063: step 138640, loss = 2.76 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 04:00:51.538767: step 138650, loss = 2.92 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 04:00:56.366481: step 138660, loss = 2.86 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 04:01:01.075684: step 138670, loss = 3.01 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 04:01:05.923631: step 138680, loss = 2.96 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 04:01:10.652048: step 138690, loss = 2.86 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 04:01:15.405894: step 138700, loss = 2.98 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 04:01:21.309926: step 138710, loss = 2.74 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 04:01:25.990652: step 138720, loss = 3.01 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 04:01:30.630811: step 138730, loss = 2.88 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 04:01:36.230743: step 138740, loss = 3.01 (201.4 examples/sec; 0.635 sec/batch)
2016-07-16 04:01:41.212778: step 138750, loss = 2.93 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 04:01:45.970564: step 138760, loss = 2.81 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 04:01:50.741137: step 138770, loss = 2.96 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 04:01:55.407445: step 138780, loss = 2.96 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 04:02:00.098520: step 138790, loss = 2.86 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 04:02:05.848495: step 138800, loss = 2.65 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 04:02:11.759002: step 138810, loss = 2.90 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 04:02:16.362700: step 138820, loss = 2.80 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 04:02:21.017443: step 138830, loss = 2.81 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 04:02:26.137458: step 138840, loss = 2.93 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 04:02:31.561550: step 138850, loss = 2.94 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 04:02:36.282464: step 138860, loss = 2.82 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 04:02:41.004472: step 138870, loss = 2.74 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 04:02:46.074625: step 138880, loss = 3.01 (204.3 examples/sec; 0.626 sec/batch)
2016-07-16 04:02:51.537217: step 138890, loss = 2.88 (263.1 examples/sec; 0.486 sec/batch)
2016-07-16 04:02:57.340125: step 138900, loss = 2.61 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 04:03:03.146221: step 138910, loss = 2.87 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 04:03:07.988995: step 138920, loss = 2.78 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 04:03:12.753326: step 138930, loss = 2.80 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 04:03:17.399424: step 138940, loss = 2.94 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 04:03:22.528757: step 138950, loss = 2.54 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 04:03:27.885532: step 138960, loss = 2.62 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 04:03:33.645217: step 138970, loss = 2.81 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 04:03:38.497608: step 138980, loss = 2.81 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 04:03:43.145123: step 138990, loss = 2.77 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 04:03:47.849088: step 139000, loss = 2.81 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 04:03:53.461141: step 139010, loss = 2.76 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 04:03:58.113485: step 139020, loss = 2.74 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 04:04:02.809755: step 139030, loss = 2.79 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 04:04:07.573073: step 139040, loss = 2.86 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 04:04:12.285944: step 139050, loss = 2.63 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 04:04:18.099678: step 139060, loss = 2.91 (244.1 examples/sec; 0.524 sec/batch)
2016-07-16 04:04:22.880161: step 139070, loss = 2.92 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 04:04:27.558937: step 139080, loss = 2.78 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 04:04:32.245451: step 139090, loss = 2.85 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 04:04:38.024608: step 139100, loss = 3.05 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 04:04:43.782095: step 139110, loss = 2.77 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 04:04:48.675339: step 139120, loss = 2.98 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 04:04:53.430972: step 139130, loss = 3.01 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 04:04:58.299690: step 139140, loss = 2.82 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 04:05:03.097458: step 139150, loss = 2.79 (249.6 examples/sec; 0.513 sec/batch)
2016-07-16 04:05:07.854974: step 139160, loss = 2.63 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 04:05:12.525282: step 139170, loss = 2.83 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 04:05:17.221831: step 139180, loss = 2.85 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 04:05:21.854711: step 139190, loss = 2.84 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 04:05:27.146461: step 139200, loss = 2.85 (201.2 examples/sec; 0.636 sec/batch)
2016-07-16 04:05:33.569116: step 139210, loss = 2.96 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 04:05:38.242014: step 139220, loss = 2.89 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 04:05:42.869015: step 139230, loss = 2.90 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 04:05:47.555812: step 139240, loss = 2.71 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 04:05:52.228710: step 139250, loss = 2.65 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 04:05:58.014202: step 139260, loss = 2.90 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 04:06:02.840571: step 139270, loss = 2.94 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 04:06:07.691680: step 139280, loss = 2.89 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 04:06:12.443184: step 139290, loss = 2.74 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 04:06:17.108004: step 139300, loss = 2.79 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 04:06:22.791737: step 139310, loss = 2.75 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 04:06:27.458290: step 139320, loss = 2.80 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 04:06:33.230773: step 139330, loss = 2.90 (220.5 examples/sec; 0.581 sec/batch)
2016-07-16 04:06:38.057455: step 139340, loss = 2.89 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 04:06:42.717259: step 139350, loss = 2.71 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 04:06:47.416574: step 139360, loss = 2.93 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 04:06:53.193313: step 139370, loss = 2.65 (207.8 examples/sec; 0.616 sec/batch)
2016-07-16 04:06:58.390611: step 139380, loss = 2.83 (200.5 examples/sec; 0.638 sec/batch)
2016-07-16 04:07:03.892097: step 139390, loss = 2.87 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 04:07:08.613352: step 139400, loss = 2.53 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 04:07:15.136731: step 139410, loss = 3.04 (187.3 examples/sec; 0.684 sec/batch)
2016-07-16 04:07:21.399970: step 139420, loss = 3.19 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 04:07:26.030483: step 139430, loss = 2.75 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 04:07:31.289565: step 139440, loss = 2.66 (201.8 examples/sec; 0.634 sec/batch)
2016-07-16 04:07:36.615308: step 139450, loss = 2.71 (253.0 examples/sec; 0.506 sec/batch)
2016-07-16 04:07:41.401416: step 139460, loss = 2.73 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 04:07:46.247363: step 139470, loss = 2.95 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 04:07:50.919224: step 139480, loss = 2.73 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 04:07:55.607214: step 139490, loss = 2.66 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 04:08:00.232137: step 139500, loss = 2.68 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 04:08:06.009959: step 139510, loss = 2.73 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 04:08:11.644167: step 139520, loss = 2.77 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 04:08:16.360001: step 139530, loss = 3.02 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 04:08:21.337457: step 139540, loss = 2.78 (229.1 examples/sec; 0.559 sec/batch)
2016-07-16 04:08:27.902603: step 139550, loss = 2.78 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 04:08:33.070835: step 139560, loss = 2.77 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 04:08:37.863467: step 139570, loss = 2.69 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 04:08:42.700855: step 139580, loss = 2.74 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 04:08:47.339687: step 139590, loss = 2.95 (283.5 examples/sec; 0.451 sec/batch)
2016-07-16 04:08:52.036520: step 139600, loss = 2.94 (266.9 examples/sec; 0.479 sec/batch)
2016-07-16 04:08:57.683293: step 139610, loss = 2.75 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 04:09:02.871850: step 139620, loss = 2.96 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 04:09:08.304087: step 139630, loss = 2.81 (254.2 examples/sec; 0.504 sec/batch)
2016-07-16 04:09:13.043941: step 139640, loss = 2.78 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 04:09:17.698119: step 139650, loss = 2.86 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 04:09:22.832151: step 139660, loss = 2.96 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 04:09:28.230497: step 139670, loss = 2.79 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 04:09:33.021412: step 139680, loss = 2.78 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 04:09:37.943065: step 139690, loss = 2.71 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 04:09:42.656101: step 139700, loss = 2.64 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 04:09:49.943017: step 139710, loss = 2.82 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 04:09:55.534930: step 139720, loss = 2.81 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 04:10:00.259760: step 139730, loss = 2.79 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 04:10:04.881082: step 139740, loss = 2.94 (289.9 examples/sec; 0.441 sec/batch)
2016-07-16 04:10:09.692267: step 139750, loss = 2.79 (209.5 examples/sec; 0.611 sec/batch)
2016-07-16 04:10:15.399066: step 139760, loss = 2.88 (241.3 examples/sec; 0.530 sec/batch)
2016-07-16 04:10:20.163677: step 139770, loss = 2.86 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 04:10:24.752909: step 139780, loss = 2.86 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 04:10:29.495573: step 139790, loss = 2.84 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 04:10:34.495744: step 139800, loss = 2.76 (201.2 examples/sec; 0.636 sec/batch)
2016-07-16 04:10:41.308458: step 139810, loss = 2.84 (242.2 examples/sec; 0.529 sec/batch)
2016-07-16 04:10:45.974524: step 139820, loss = 2.93 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 04:10:50.597295: step 139830, loss = 3.08 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 04:10:55.245805: step 139840, loss = 3.01 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 04:10:59.891223: step 139850, loss = 2.87 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 04:11:04.600695: step 139860, loss = 3.14 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 04:11:10.347525: step 139870, loss = 2.96 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 04:11:15.160258: step 139880, loss = 3.09 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 04:11:19.957600: step 139890, loss = 3.13 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 04:11:24.666923: step 139900, loss = 2.79 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 04:11:30.520429: step 139910, loss = 2.79 (263.1 examples/sec; 0.486 sec/batch)
2016-07-16 04:11:35.284707: step 139920, loss = 2.89 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 04:11:40.055715: step 139930, loss = 3.01 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 04:11:44.887721: step 139940, loss = 2.99 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 04:11:51.366501: step 139950, loss = 2.84 (201.7 examples/sec; 0.634 sec/batch)
2016-07-16 04:11:56.679299: step 139960, loss = 2.76 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 04:12:01.376307: step 139970, loss = 2.83 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 04:12:06.256723: step 139980, loss = 2.73 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 04:12:11.005725: step 139990, loss = 2.80 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 04:12:15.805461: step 140000, loss = 2.97 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 04:12:21.446243: step 140010, loss = 3.01 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 04:12:26.446306: step 140020, loss = 2.86 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 04:12:31.972264: step 140030, loss = 2.74 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 04:12:36.757079: step 140040, loss = 3.01 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 04:12:41.409478: step 140050, loss = 2.74 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 04:12:46.402161: step 140060, loss = 2.65 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 04:12:51.917873: step 140070, loss = 2.62 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 04:12:56.665384: step 140080, loss = 2.80 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 04:13:01.519889: step 140090, loss = 2.81 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 04:13:06.171724: step 140100, loss = 2.90 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 04:13:11.738802: step 140110, loss = 2.57 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 04:13:17.475700: step 140120, loss = 2.71 (208.0 examples/sec; 0.616 sec/batch)
2016-07-16 04:13:22.715070: step 140130, loss = 2.91 (200.8 examples/sec; 0.637 sec/batch)
2016-07-16 04:13:28.185812: step 140140, loss = 2.66 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 04:13:32.876892: step 140150, loss = 2.69 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 04:13:37.550690: step 140160, loss = 2.78 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 04:13:42.226599: step 140170, loss = 2.71 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 04:13:46.875469: step 140180, loss = 2.91 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 04:13:51.501661: step 140190, loss = 2.58 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 04:13:56.122105: step 140200, loss = 3.01 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 04:14:01.807103: step 140210, loss = 2.67 (212.9 examples/sec; 0.601 sec/batch)
2016-07-16 04:14:07.502648: step 140220, loss = 2.91 (261.0 examples/sec; 0.491 sec/batch)
2016-07-16 04:14:13.256620: step 140230, loss = 2.98 (198.7 examples/sec; 0.644 sec/batch)
2016-07-16 04:14:18.181760: step 140240, loss = 2.90 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 04:14:22.919908: step 140250, loss = 2.79 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 04:14:27.772138: step 140260, loss = 3.00 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 04:14:32.611880: step 140270, loss = 2.83 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 04:14:37.347427: step 140280, loss = 2.71 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 04:14:41.997074: step 140290, loss = 3.02 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 04:14:46.692585: step 140300, loss = 2.95 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 04:14:52.287541: step 140310, loss = 2.84 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 04:14:58.028662: step 140320, loss = 2.62 (254.8 examples/sec; 0.502 sec/batch)
2016-07-16 04:15:02.968080: step 140330, loss = 2.84 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 04:15:07.738245: step 140340, loss = 2.91 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 04:15:13.857075: step 140350, loss = 2.74 (196.5 examples/sec; 0.651 sec/batch)
2016-07-16 04:15:19.556346: step 140360, loss = 2.99 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 04:15:24.345256: step 140370, loss = 3.05 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 04:15:29.233756: step 140380, loss = 3.16 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 04:15:35.784372: step 140390, loss = 2.96 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 04:15:41.068962: step 140400, loss = 2.97 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 04:15:48.036452: step 140410, loss = 2.67 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 04:15:52.804217: step 140420, loss = 2.93 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 04:15:57.762929: step 140430, loss = 2.94 (231.5 examples/sec; 0.553 sec/batch)
2016-07-16 04:16:04.334766: step 140440, loss = 2.88 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 04:16:09.581198: step 140450, loss = 2.67 (245.7 examples/sec; 0.521 sec/batch)
2016-07-16 04:16:14.229822: step 140460, loss = 2.73 (281.6 examples/sec; 0.454 sec/batch)
2016-07-16 04:16:18.824263: step 140470, loss = 2.80 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 04:16:24.275121: step 140480, loss = 2.97 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 04:16:29.437930: step 140490, loss = 2.90 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 04:16:34.196289: step 140500, loss = 2.84 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 04:16:40.039997: step 140510, loss = 2.76 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 04:16:44.868496: step 140520, loss = 2.90 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 04:16:49.575817: step 140530, loss = 2.72 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 04:16:54.546343: step 140540, loss = 2.75 (245.5 examples/sec; 0.521 sec/batch)
2016-07-16 04:16:59.311799: step 140550, loss = 2.91 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 04:17:05.286034: step 140560, loss = 2.60 (191.4 examples/sec; 0.669 sec/batch)
2016-07-16 04:17:11.152918: step 140570, loss = 2.58 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 04:17:15.976684: step 140580, loss = 3.17 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 04:17:20.822466: step 140590, loss = 2.79 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 04:17:27.246935: step 140600, loss = 2.95 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 04:17:33.848273: step 140610, loss = 2.67 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 04:17:38.534567: step 140620, loss = 2.75 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 04:17:43.210842: step 140630, loss = 3.03 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 04:17:47.862961: step 140640, loss = 2.80 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 04:17:52.518945: step 140650, loss = 2.74 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 04:17:58.251488: step 140660, loss = 2.75 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 04:18:03.544299: step 140670, loss = 2.82 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 04:18:08.932138: step 140680, loss = 2.95 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 04:18:13.633868: step 140690, loss = 2.89 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 04:18:18.540262: step 140700, loss = 2.85 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 04:18:24.307649: step 140710, loss = 2.61 (251.2 examples/sec; 0.509 sec/batch)
2016-07-16 04:18:28.984928: step 140720, loss = 2.88 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 04:18:33.817868: step 140730, loss = 2.76 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 04:18:38.453845: step 140740, loss = 2.82 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 04:18:43.074761: step 140750, loss = 2.86 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 04:18:48.500230: step 140760, loss = 2.84 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 04:18:53.767128: step 140770, loss = 3.06 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 04:18:59.480224: step 140780, loss = 3.16 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 04:19:05.060562: step 140790, loss = 2.73 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 04:19:10.080316: step 140800, loss = 2.76 (253.0 examples/sec; 0.506 sec/batch)
2016-07-16 04:19:15.741579: step 140810, loss = 2.98 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 04:19:20.527173: step 140820, loss = 2.93 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 04:19:25.325345: step 140830, loss = 2.83 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 04:19:30.032571: step 140840, loss = 2.72 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 04:19:35.434648: step 140850, loss = 2.67 (188.4 examples/sec; 0.679 sec/batch)
2016-07-16 04:19:41.845256: step 140860, loss = 2.65 (217.1 examples/sec; 0.590 sec/batch)
2016-07-16 04:19:46.681669: step 140870, loss = 2.89 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 04:19:51.492794: step 140880, loss = 2.71 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 04:19:56.262826: step 140890, loss = 2.68 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 04:20:00.891366: step 140900, loss = 2.92 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 04:20:06.564873: step 140910, loss = 2.67 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 04:20:11.186020: step 140920, loss = 2.89 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 04:20:16.634868: step 140930, loss = 2.69 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 04:20:21.861812: step 140940, loss = 2.80 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 04:20:27.600614: step 140950, loss = 2.74 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 04:20:32.398535: step 140960, loss = 2.78 (275.0 examples/sec; 0.466 sec/batch)
2016-07-16 04:20:37.240217: step 140970, loss = 2.76 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 04:20:41.943551: step 140980, loss = 2.90 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 04:20:47.272613: step 140990, loss = 2.85 (193.3 examples/sec; 0.662 sec/batch)
2016-07-16 04:20:53.760329: step 141000, loss = 3.14 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 04:20:59.669165: step 141010, loss = 2.92 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 04:21:04.460001: step 141020, loss = 3.11 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 04:21:09.218980: step 141030, loss = 2.83 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 04:21:13.826797: step 141040, loss = 3.24 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 04:21:18.786366: step 141050, loss = 3.14 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 04:21:24.405695: step 141060, loss = 2.96 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 04:21:30.182488: step 141070, loss = 2.79 (247.7 examples/sec; 0.517 sec/batch)
2016-07-16 04:21:35.393037: step 141080, loss = 2.98 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 04:21:40.862489: step 141090, loss = 2.77 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 04:21:45.568779: step 141100, loss = 2.73 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 04:21:51.375871: step 141110, loss = 2.84 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 04:21:56.165872: step 141120, loss = 2.62 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 04:22:00.930166: step 141130, loss = 2.72 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 04:22:05.575020: step 141140, loss = 2.74 (284.8 examples/sec; 0.450 sec/batch)
2016-07-16 04:22:10.495198: step 141150, loss = 2.92 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 04:22:16.106798: step 141160, loss = 3.03 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 04:22:21.891589: step 141170, loss = 3.02 (202.2 examples/sec; 0.633 sec/batch)
2016-07-16 04:22:26.793783: step 141180, loss = 2.83 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 04:22:31.590057: step 141190, loss = 2.81 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 04:22:36.377502: step 141200, loss = 2.88 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 04:22:42.065042: step 141210, loss = 2.71 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 04:22:46.731226: step 141220, loss = 2.59 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 04:22:51.348065: step 141230, loss = 2.61 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 04:22:56.006011: step 141240, loss = 2.87 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 04:23:00.634651: step 141250, loss = 2.76 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 04:23:05.252103: step 141260, loss = 2.90 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 04:23:11.037523: step 141270, loss = 2.70 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 04:23:16.515916: step 141280, loss = 2.60 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 04:23:21.704227: step 141290, loss = 2.70 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 04:23:27.533202: step 141300, loss = 3.06 (254.3 examples/sec; 0.503 sec/batch)
2016-07-16 04:23:33.295781: step 141310, loss = 2.76 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 04:23:37.925491: step 141320, loss = 2.73 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 04:23:42.785779: step 141330, loss = 2.78 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 04:23:48.372977: step 141340, loss = 2.88 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 04:23:53.084448: step 141350, loss = 2.51 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 04:23:57.979940: step 141360, loss = 2.90 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 04:24:02.658554: step 141370, loss = 2.90 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 04:24:07.527004: step 141380, loss = 2.73 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 04:24:12.221331: step 141390, loss = 3.07 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 04:24:16.864924: step 141400, loss = 2.87 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 04:24:23.595863: step 141410, loss = 2.76 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 04:24:29.249199: step 141420, loss = 3.11 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 04:24:34.249755: step 141430, loss = 2.82 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 04:24:39.009776: step 141440, loss = 2.77 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 04:24:43.787142: step 141450, loss = 3.00 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 04:24:48.620759: step 141460, loss = 2.90 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 04:24:54.868998: step 141470, loss = 2.84 (199.7 examples/sec; 0.641 sec/batch)
2016-07-16 04:25:00.495964: step 141480, loss = 2.93 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 04:25:05.246351: step 141490, loss = 3.01 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 04:25:10.154068: step 141500, loss = 2.93 (244.7 examples/sec; 0.523 sec/batch)
2016-07-16 04:25:18.332463: step 141510, loss = 2.76 (252.7 examples/sec; 0.507 sec/batch)
2016-07-16 04:25:23.589660: step 141520, loss = 2.98 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 04:25:29.070843: step 141530, loss = 3.05 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 04:25:33.803041: step 141540, loss = 2.96 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 04:25:38.728384: step 141550, loss = 2.93 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 04:25:43.439847: step 141560, loss = 2.89 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 04:25:48.067722: step 141570, loss = 2.66 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 04:25:52.704475: step 141580, loss = 2.85 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 04:25:57.410312: step 141590, loss = 2.91 (250.9 examples/sec; 0.510 sec/batch)
2016-07-16 04:26:03.236925: step 141600, loss = 2.61 (253.7 examples/sec; 0.505 sec/batch)
2016-07-16 04:26:10.051597: step 141610, loss = 2.75 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 04:26:14.949106: step 141620, loss = 2.84 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 04:26:19.754878: step 141630, loss = 2.87 (245.0 examples/sec; 0.522 sec/batch)
2016-07-16 04:26:24.562077: step 141640, loss = 2.87 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 04:26:29.325735: step 141650, loss = 2.91 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 04:26:35.757666: step 141660, loss = 2.91 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 04:26:41.183584: step 141670, loss = 2.71 (256.8 examples/sec; 0.499 sec/batch)
2016-07-16 04:26:46.967877: step 141680, loss = 2.68 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 04:26:51.794523: step 141690, loss = 2.73 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 04:26:56.590543: step 141700, loss = 2.89 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 04:27:02.317488: step 141710, loss = 2.79 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 04:27:07.634726: step 141720, loss = 2.66 (190.5 examples/sec; 0.672 sec/batch)
2016-07-16 04:27:14.103994: step 141730, loss = 2.63 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 04:27:18.968855: step 141740, loss = 3.16 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 04:27:23.731792: step 141750, loss = 2.88 (257.0 examples/sec; 0.498 sec/batch)
2016-07-16 04:27:28.523840: step 141760, loss = 2.86 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 04:27:33.344646: step 141770, loss = 2.99 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 04:27:38.080499: step 141780, loss = 2.82 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 04:27:42.736630: step 141790, loss = 2.77 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 04:27:47.361160: step 141800, loss = 2.86 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 04:27:53.975419: step 141810, loss = 2.73 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 04:27:58.932460: step 141820, loss = 2.72 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 04:28:03.736685: step 141830, loss = 2.84 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 04:28:08.522705: step 141840, loss = 3.01 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 04:28:13.352132: step 141850, loss = 2.83 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 04:28:18.074629: step 141860, loss = 2.80 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 04:28:23.265180: step 141870, loss = 2.73 (189.5 examples/sec; 0.676 sec/batch)
2016-07-16 04:28:29.704454: step 141880, loss = 2.77 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 04:28:34.868267: step 141890, loss = 2.94 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 04:28:40.476733: step 141900, loss = 2.83 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 04:28:46.292182: step 141910, loss = 2.78 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 04:28:51.163661: step 141920, loss = 2.90 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 04:28:55.936258: step 141930, loss = 2.95 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 04:29:00.698618: step 141940, loss = 3.26 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 04:29:05.394289: step 141950, loss = 2.70 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 04:29:10.082562: step 141960, loss = 2.69 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 04:29:15.859170: step 141970, loss = 2.92 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 04:29:21.444947: step 141980, loss = 2.76 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 04:29:26.553186: step 141990, loss = 2.88 (230.1 examples/sec; 0.556 sec/batch)
2016-07-16 04:29:32.327692: step 142000, loss = 3.00 (254.1 examples/sec; 0.504 sec/batch)
2016-07-16 04:29:39.134028: step 142010, loss = 2.85 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 04:29:43.980203: step 142020, loss = 2.66 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 04:29:48.623069: step 142030, loss = 2.78 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 04:29:53.288241: step 142040, loss = 2.73 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 04:29:57.935553: step 142050, loss = 2.76 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 04:30:02.718716: step 142060, loss = 2.95 (212.6 examples/sec; 0.602 sec/batch)
2016-07-16 04:30:08.400208: step 142070, loss = 3.01 (253.5 examples/sec; 0.505 sec/batch)
2016-07-16 04:30:14.159906: step 142080, loss = 2.59 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 04:30:19.031780: step 142090, loss = 2.92 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 04:30:23.711848: step 142100, loss = 2.83 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 04:30:29.410943: step 142110, loss = 2.73 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 04:30:35.113358: step 142120, loss = 2.75 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 04:30:40.522443: step 142130, loss = 2.86 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 04:30:45.817180: step 142140, loss = 3.01 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 04:30:50.538213: step 142150, loss = 2.80 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 04:30:55.841781: step 142160, loss = 3.04 (190.3 examples/sec; 0.673 sec/batch)
2016-07-16 04:31:01.336614: step 142170, loss = 2.86 (283.4 examples/sec; 0.452 sec/batch)
2016-07-16 04:31:06.024392: step 142180, loss = 3.02 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 04:31:10.830107: step 142190, loss = 2.86 (213.6 examples/sec; 0.599 sec/batch)
2016-07-16 04:31:16.554948: step 142200, loss = 2.98 (249.9 examples/sec; 0.512 sec/batch)
2016-07-16 04:31:22.337616: step 142210, loss = 3.16 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 04:31:27.007890: step 142220, loss = 3.04 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 04:31:31.656285: step 142230, loss = 2.90 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 04:31:36.358144: step 142240, loss = 2.99 (246.7 examples/sec; 0.519 sec/batch)
2016-07-16 04:31:41.032667: step 142250, loss = 2.70 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 04:31:45.713719: step 142260, loss = 2.66 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 04:31:51.504666: step 142270, loss = 2.92 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 04:31:56.899518: step 142280, loss = 3.03 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 04:32:02.133110: step 142290, loss = 2.86 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 04:32:07.878958: step 142300, loss = 2.85 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 04:32:13.642579: step 142310, loss = 2.81 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 04:32:18.487686: step 142320, loss = 2.96 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 04:32:23.177100: step 142330, loss = 3.03 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 04:32:27.987768: step 142340, loss = 2.80 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 04:32:32.777594: step 142350, loss = 2.77 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 04:32:37.617732: step 142360, loss = 2.99 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 04:32:42.429666: step 142370, loss = 2.70 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 04:32:47.105715: step 142380, loss = 2.89 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 04:32:52.564603: step 142390, loss = 2.98 (187.9 examples/sec; 0.681 sec/batch)
2016-07-16 04:32:58.836940: step 142400, loss = 2.80 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 04:33:04.670896: step 142410, loss = 2.73 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 04:33:09.515856: step 142420, loss = 2.83 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 04:33:14.228734: step 142430, loss = 2.65 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 04:33:19.164804: step 142440, loss = 2.61 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 04:33:23.857633: step 142450, loss = 2.75 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 04:33:28.474704: step 142460, loss = 2.99 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 04:33:34.098738: step 142470, loss = 2.85 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 04:33:39.077339: step 142480, loss = 2.86 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 04:33:43.753117: step 142490, loss = 2.77 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 04:33:48.401883: step 142500, loss = 2.96 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 04:33:54.067655: step 142510, loss = 2.54 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 04:33:58.744938: step 142520, loss = 3.00 (245.2 examples/sec; 0.522 sec/batch)
2016-07-16 04:34:04.470422: step 142530, loss = 2.93 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 04:34:09.280393: step 142540, loss = 2.84 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 04:34:14.135619: step 142550, loss = 3.04 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 04:34:20.604995: step 142560, loss = 2.90 (206.6 examples/sec; 0.619 sec/batch)
2016-07-16 04:34:25.720001: step 142570, loss = 2.79 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 04:34:30.395733: step 142580, loss = 2.97 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 04:34:36.192398: step 142590, loss = 2.82 (246.6 examples/sec; 0.519 sec/batch)
2016-07-16 04:34:41.793673: step 142600, loss = 2.61 (205.0 examples/sec; 0.625 sec/batch)
2016-07-16 04:34:47.921703: step 142610, loss = 2.98 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 04:34:52.664621: step 142620, loss = 2.87 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 04:34:58.691719: step 142630, loss = 2.81 (194.3 examples/sec; 0.659 sec/batch)
2016-07-16 04:35:03.326334: step 142640, loss = 2.76 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 04:35:08.536362: step 142650, loss = 3.05 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 04:35:13.827574: step 142660, loss = 2.87 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 04:35:19.653923: step 142670, loss = 2.81 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 04:35:24.432428: step 142680, loss = 2.96 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 04:35:29.257334: step 142690, loss = 2.89 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 04:35:34.022509: step 142700, loss = 2.81 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 04:35:39.865152: step 142710, loss = 3.00 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 04:35:44.659399: step 142720, loss = 2.84 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 04:35:50.752930: step 142730, loss = 2.61 (193.2 examples/sec; 0.662 sec/batch)
2016-07-16 04:35:56.499655: step 142740, loss = 2.81 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 04:36:01.299270: step 142750, loss = 2.95 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 04:36:06.161380: step 142760, loss = 2.69 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 04:36:10.839263: step 142770, loss = 2.90 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 04:36:15.664181: step 142780, loss = 2.70 (282.9 examples/sec; 0.452 sec/batch)
2016-07-16 04:36:20.403394: step 142790, loss = 2.67 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 04:36:26.363486: step 142800, loss = 2.88 (188.6 examples/sec; 0.679 sec/batch)
2016-07-16 04:36:33.481226: step 142810, loss = 2.97 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 04:36:38.268833: step 142820, loss = 2.81 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 04:36:43.159300: step 142830, loss = 2.71 (267.5 examples/sec; 0.478 sec/batch)
2016-07-16 04:36:47.931495: step 142840, loss = 2.84 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 04:36:53.664026: step 142850, loss = 2.95 (183.9 examples/sec; 0.696 sec/batch)
2016-07-16 04:36:59.740355: step 142860, loss = 3.05 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 04:37:04.580047: step 142870, loss = 2.89 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 04:37:09.423598: step 142880, loss = 3.10 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 04:37:15.622998: step 142890, loss = 2.85 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 04:37:21.249908: step 142900, loss = 3.07 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 04:37:26.921223: step 142910, loss = 2.93 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 04:37:31.748090: step 142920, loss = 2.73 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 04:37:36.499501: step 142930, loss = 2.87 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 04:37:41.304243: step 142940, loss = 2.79 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 04:37:45.987229: step 142950, loss = 2.79 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 04:37:50.712908: step 142960, loss = 2.97 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 04:37:55.336022: step 142970, loss = 2.73 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 04:38:00.052261: step 142980, loss = 2.67 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 04:38:04.693313: step 142990, loss = 2.79 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 04:38:10.299279: step 143000, loss = 2.84 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 04:38:16.825407: step 143010, loss = 2.84 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 04:38:21.931974: step 143020, loss = 2.85 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 04:38:26.632264: step 143030, loss = 2.74 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 04:38:31.276616: step 143040, loss = 2.85 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 04:38:36.471647: step 143050, loss = 2.96 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 04:38:41.904993: step 143060, loss = 2.86 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 04:38:47.637486: step 143070, loss = 2.97 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 04:38:52.261107: step 143080, loss = 2.69 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 04:38:57.735601: step 143090, loss = 3.02 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 04:39:02.880567: step 143100, loss = 2.86 (247.3 examples/sec; 0.518 sec/batch)
2016-07-16 04:39:08.627921: step 143110, loss = 2.66 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 04:39:13.441258: step 143120, loss = 2.70 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 04:39:18.265383: step 143130, loss = 2.96 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 04:39:22.978142: step 143140, loss = 2.68 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 04:39:28.427633: step 143150, loss = 2.91 (180.8 examples/sec; 0.708 sec/batch)
2016-07-16 04:39:34.835083: step 143160, loss = 2.79 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 04:39:39.676915: step 143170, loss = 2.82 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 04:39:44.416711: step 143180, loss = 2.90 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 04:39:49.196517: step 143190, loss = 2.90 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 04:39:53.895695: step 143200, loss = 2.73 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 04:39:59.727688: step 143210, loss = 2.70 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 04:40:05.411213: step 143220, loss = 3.00 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 04:40:10.159327: step 143230, loss = 3.01 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 04:40:15.144858: step 143240, loss = 2.51 (221.5 examples/sec; 0.578 sec/batch)
2016-07-16 04:40:21.706033: step 143250, loss = 2.98 (208.3 examples/sec; 0.614 sec/batch)
2016-07-16 04:40:26.884218: step 143260, loss = 2.77 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 04:40:31.582574: step 143270, loss = 2.81 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 04:40:36.441523: step 143280, loss = 2.82 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 04:40:41.041540: step 143290, loss = 2.84 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 04:40:45.726747: step 143300, loss = 2.73 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 04:40:52.465296: step 143310, loss = 2.78 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 04:40:57.247306: step 143320, loss = 2.98 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 04:41:02.093140: step 143330, loss = 2.75 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 04:41:08.478282: step 143340, loss = 3.07 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 04:41:13.947872: step 143350, loss = 2.74 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 04:41:18.691014: step 143360, loss = 3.04 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 04:41:23.877869: step 143370, loss = 2.87 (187.2 examples/sec; 0.684 sec/batch)
2016-07-16 04:41:30.395905: step 143380, loss = 2.70 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 04:41:35.389328: step 143390, loss = 2.83 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 04:41:40.100527: step 143400, loss = 2.93 (263.1 examples/sec; 0.486 sec/batch)
2016-07-16 04:41:45.953673: step 143410, loss = 2.82 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 04:41:50.601984: step 143420, loss = 2.74 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 04:41:55.436500: step 143430, loss = 2.85 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 04:42:01.082184: step 143440, loss = 2.88 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 04:42:06.779966: step 143450, loss = 2.84 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 04:42:11.700364: step 143460, loss = 2.68 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 04:42:16.455415: step 143470, loss = 2.85 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 04:42:21.257335: step 143480, loss = 2.84 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 04:42:26.147707: step 143490, loss = 2.96 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 04:42:30.893818: step 143500, loss = 2.79 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 04:42:36.713911: step 143510, loss = 2.79 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 04:42:41.528663: step 143520, loss = 3.03 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 04:42:46.285971: step 143530, loss = 2.72 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 04:42:51.070532: step 143540, loss = 2.80 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 04:42:56.020984: step 143550, loss = 2.96 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 04:43:01.568276: step 143560, loss = 2.81 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 04:43:06.312490: step 143570, loss = 2.97 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 04:43:11.149180: step 143580, loss = 2.67 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 04:43:15.849267: step 143590, loss = 2.76 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 04:43:20.510421: step 143600, loss = 2.84 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 04:43:27.207820: step 143610, loss = 2.70 (220.9 examples/sec; 0.579 sec/batch)
2016-07-16 04:43:32.460549: step 143620, loss = 3.02 (202.1 examples/sec; 0.633 sec/batch)
2016-07-16 04:43:37.888315: step 143630, loss = 2.92 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 04:43:42.630667: step 143640, loss = 2.94 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 04:43:47.483655: step 143650, loss = 2.68 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 04:43:52.220342: step 143660, loss = 2.98 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 04:43:57.053561: step 143670, loss = 2.94 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 04:44:01.899571: step 143680, loss = 2.85 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 04:44:06.639834: step 143690, loss = 2.80 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 04:44:11.540995: step 143700, loss = 2.85 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 04:44:17.238617: step 143710, loss = 2.95 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 04:44:22.023578: step 143720, loss = 2.94 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 04:44:26.861335: step 143730, loss = 2.68 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 04:44:31.591615: step 143740, loss = 2.95 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 04:44:36.448700: step 143750, loss = 2.98 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 04:44:41.331593: step 143760, loss = 2.59 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 04:44:47.469921: step 143770, loss = 2.89 (193.1 examples/sec; 0.663 sec/batch)
2016-07-16 04:44:53.217643: step 143780, loss = 2.90 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 04:44:58.931796: step 143790, loss = 2.75 (207.3 examples/sec; 0.617 sec/batch)
2016-07-16 04:45:04.063105: step 143800, loss = 2.89 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 04:45:10.877069: step 143810, loss = 2.94 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 04:45:15.593303: step 143820, loss = 3.01 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 04:45:20.256480: step 143830, loss = 2.83 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 04:45:24.971245: step 143840, loss = 3.05 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 04:45:29.638308: step 143850, loss = 3.03 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 04:45:34.240552: step 143860, loss = 2.73 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 04:45:38.954238: step 143870, loss = 2.68 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 04:45:44.755491: step 143880, loss = 2.78 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 04:45:50.304294: step 143890, loss = 3.06 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 04:45:55.344426: step 143900, loss = 2.92 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 04:46:00.873588: step 143910, loss = 2.89 (286.1 examples/sec; 0.447 sec/batch)
2016-07-16 04:46:05.523048: step 143920, loss = 2.76 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 04:46:11.233074: step 143930, loss = 2.74 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 04:46:16.111112: step 143940, loss = 2.65 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 04:46:20.899713: step 143950, loss = 2.70 (252.9 examples/sec; 0.506 sec/batch)
2016-07-16 04:46:27.149726: step 143960, loss = 2.89 (184.1 examples/sec; 0.695 sec/batch)
2016-07-16 04:46:31.778455: step 143970, loss = 2.95 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 04:46:36.474903: step 143980, loss = 3.03 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 04:46:41.157967: step 143990, loss = 3.04 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 04:46:45.809088: step 144000, loss = 2.88 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 04:46:51.513580: step 144010, loss = 2.78 (232.7 examples/sec; 0.550 sec/batch)
2016-07-16 04:46:57.269344: step 144020, loss = 2.88 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 04:47:02.033019: step 144030, loss = 2.87 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 04:47:06.896558: step 144040, loss = 2.88 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 04:47:11.585640: step 144050, loss = 2.58 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 04:47:16.194812: step 144060, loss = 3.01 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 04:47:21.527640: step 144070, loss = 2.49 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 04:47:26.839824: step 144080, loss = 2.94 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 04:47:31.542090: step 144090, loss = 2.90 (258.3 examples/sec; 0.495 sec/batch)
2016-07-16 04:47:36.887351: step 144100, loss = 2.64 (192.1 examples/sec; 0.666 sec/batch)
2016-07-16 04:47:43.571078: step 144110, loss = 2.85 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 04:47:48.588306: step 144120, loss = 2.93 (208.4 examples/sec; 0.614 sec/batch)
2016-07-16 04:47:54.057299: step 144130, loss = 2.65 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 04:47:58.814665: step 144140, loss = 2.66 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 04:48:03.427976: step 144150, loss = 2.84 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 04:48:08.148332: step 144160, loss = 2.81 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 04:48:12.816002: step 144170, loss = 2.84 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 04:48:18.268761: step 144180, loss = 2.75 (200.0 examples/sec; 0.640 sec/batch)
2016-07-16 04:48:23.431312: step 144190, loss = 2.91 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 04:48:28.149567: step 144200, loss = 2.70 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 04:48:33.668100: step 144210, loss = 2.81 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 04:48:39.403790: step 144220, loss = 2.86 (208.3 examples/sec; 0.614 sec/batch)
2016-07-16 04:48:44.257330: step 144230, loss = 2.96 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 04:48:49.023360: step 144240, loss = 2.73 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 04:48:53.805535: step 144250, loss = 2.91 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 04:48:58.495248: step 144260, loss = 2.81 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 04:49:03.152800: step 144270, loss = 2.95 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 04:49:08.924427: step 144280, loss = 2.66 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 04:49:13.731447: step 144290, loss = 3.03 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 04:49:18.581028: step 144300, loss = 3.05 (246.7 examples/sec; 0.519 sec/batch)
2016-07-16 04:49:24.265972: step 144310, loss = 2.87 (284.7 examples/sec; 0.450 sec/batch)
2016-07-16 04:49:28.849357: step 144320, loss = 2.82 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 04:49:34.304574: step 144330, loss = 2.83 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 04:49:39.497276: step 144340, loss = 2.84 (241.6 examples/sec; 0.530 sec/batch)
2016-07-16 04:49:45.269790: step 144350, loss = 2.96 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 04:49:50.042999: step 144360, loss = 2.77 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 04:49:54.900724: step 144370, loss = 2.91 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 04:49:59.602843: step 144380, loss = 2.79 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 04:50:04.428478: step 144390, loss = 2.64 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 04:50:09.230815: step 144400, loss = 2.94 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 04:50:15.005486: step 144410, loss = 3.05 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 04:50:19.917127: step 144420, loss = 2.72 (275.6 examples/sec; 0.465 sec/batch)
2016-07-16 04:50:24.662540: step 144430, loss = 2.78 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 04:50:30.495657: step 144440, loss = 2.86 (190.7 examples/sec; 0.671 sec/batch)
2016-07-16 04:50:36.538316: step 144450, loss = 2.73 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 04:50:41.298699: step 144460, loss = 2.91 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 04:50:46.120175: step 144470, loss = 2.81 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 04:50:50.890978: step 144480, loss = 2.87 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 04:50:55.505111: step 144490, loss = 2.85 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 04:51:00.488399: step 144500, loss = 2.76 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 04:51:07.194313: step 144510, loss = 2.78 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 04:51:13.015402: step 144520, loss = 3.03 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 04:51:17.800778: step 144530, loss = 2.91 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 04:51:22.608637: step 144540, loss = 2.74 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 04:51:28.913200: step 144550, loss = 2.80 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 04:51:34.187234: step 144560, loss = 2.95 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 04:51:38.888059: step 144570, loss = 2.80 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 04:51:43.551385: step 144580, loss = 2.72 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 04:51:48.541764: step 144590, loss = 2.91 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 04:51:54.097805: step 144600, loss = 2.98 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 04:51:59.839169: step 144610, loss = 2.81 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 04:52:04.454794: step 144620, loss = 2.72 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 04:52:09.740555: step 144630, loss = 2.77 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 04:52:15.048261: step 144640, loss = 2.96 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 04:52:19.730769: step 144650, loss = 2.98 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 04:52:24.600740: step 144660, loss = 2.78 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 04:52:29.342216: step 144670, loss = 3.03 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 04:52:33.993938: step 144680, loss = 2.89 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 04:52:38.661019: step 144690, loss = 2.73 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 04:52:44.377080: step 144700, loss = 2.97 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 04:52:50.148818: step 144710, loss = 2.76 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 04:52:54.733162: step 144720, loss = 2.90 (286.1 examples/sec; 0.447 sec/batch)
2016-07-16 04:52:59.442537: step 144730, loss = 2.91 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 04:53:05.220334: step 144740, loss = 2.80 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 04:53:09.974696: step 144750, loss = 2.80 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 04:53:14.604319: step 144760, loss = 2.79 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 04:53:19.246383: step 144770, loss = 2.78 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 04:53:23.978394: step 144780, loss = 2.98 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 04:53:28.592499: step 144790, loss = 3.00 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 04:53:33.893616: step 144800, loss = 2.88 (199.7 examples/sec; 0.641 sec/batch)
2016-07-16 04:53:40.123671: step 144810, loss = 2.65 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 04:53:45.432688: step 144820, loss = 3.00 (176.3 examples/sec; 0.726 sec/batch)
2016-07-16 04:53:51.359496: step 144830, loss = 2.69 (234.0 examples/sec; 0.547 sec/batch)
2016-07-16 04:53:57.765426: step 144840, loss = 2.85 (228.3 examples/sec; 0.561 sec/batch)
2016-07-16 04:54:03.872214: step 144850, loss = 3.01 (228.3 examples/sec; 0.561 sec/batch)
2016-07-16 04:54:08.720054: step 144860, loss = 2.77 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 04:54:13.435200: step 144870, loss = 2.77 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 04:54:18.092865: step 144880, loss = 2.76 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 04:54:22.746059: step 144890, loss = 2.80 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 04:54:27.456165: step 144900, loss = 2.83 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 04:54:34.424544: step 144910, loss = 2.89 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 04:54:40.247141: step 144920, loss = 2.63 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 04:54:45.056679: step 144930, loss = 2.79 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 04:54:49.842674: step 144940, loss = 2.90 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 04:54:55.973312: step 144950, loss = 2.73 (198.4 examples/sec; 0.645 sec/batch)
2016-07-16 04:55:01.924993: step 144960, loss = 2.83 (233.0 examples/sec; 0.549 sec/batch)
2016-07-16 04:55:06.761552: step 144970, loss = 2.90 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 04:55:11.650449: step 144980, loss = 2.63 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 04:55:16.336819: step 144990, loss = 2.77 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 04:55:21.170966: step 145000, loss = 3.13 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 04:55:26.982655: step 145010, loss = 2.97 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 04:55:32.919451: step 145020, loss = 2.82 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 04:55:37.614904: step 145030, loss = 2.72 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 04:55:43.212011: step 145040, loss = 2.87 (202.1 examples/sec; 0.633 sec/batch)
2016-07-16 04:55:48.370425: step 145050, loss = 2.92 (202.7 examples/sec; 0.632 sec/batch)
2016-07-16 04:55:53.983285: step 145060, loss = 2.83 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 04:55:59.757441: step 145070, loss = 2.92 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 04:56:04.618122: step 145080, loss = 2.77 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 04:56:09.373168: step 145090, loss = 2.94 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 04:56:14.017547: step 145100, loss = 2.79 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 04:56:19.582709: step 145110, loss = 2.71 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 04:56:25.401148: step 145120, loss = 2.90 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 04:56:30.146710: step 145130, loss = 2.87 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 04:56:34.935365: step 145140, loss = 3.12 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 04:56:41.303180: step 145150, loss = 2.94 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 04:56:46.743620: step 145160, loss = 2.88 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 04:56:51.483087: step 145170, loss = 2.84 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 04:56:56.318782: step 145180, loss = 2.92 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 04:57:01.013445: step 145190, loss = 2.69 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 04:57:05.665849: step 145200, loss = 2.92 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 04:57:11.290970: step 145210, loss = 2.57 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 04:57:15.891993: step 145220, loss = 2.92 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 04:57:20.872716: step 145230, loss = 2.60 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 04:57:26.381945: step 145240, loss = 2.95 (256.8 examples/sec; 0.498 sec/batch)
2016-07-16 04:57:31.113131: step 145250, loss = 2.86 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 04:57:35.985068: step 145260, loss = 2.70 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 04:57:40.655225: step 145270, loss = 2.65 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 04:57:45.329739: step 145280, loss = 2.88 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 04:57:50.003165: step 145290, loss = 3.01 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 04:57:54.690942: step 145300, loss = 2.55 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 04:58:01.444958: step 145310, loss = 2.79 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 04:58:07.029687: step 145320, loss = 2.91 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 04:58:12.078791: step 145330, loss = 2.77 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 04:58:16.784642: step 145340, loss = 2.67 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 04:58:21.609211: step 145350, loss = 2.89 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 04:58:26.435174: step 145360, loss = 2.80 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 04:58:32.629275: step 145370, loss = 2.80 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 04:58:38.091701: step 145380, loss = 2.72 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 04:58:42.736903: step 145390, loss = 2.89 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 04:58:48.501213: step 145400, loss = 2.75 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 04:58:55.092786: step 145410, loss = 3.06 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 04:59:00.133972: step 145420, loss = 2.96 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 04:59:04.832392: step 145430, loss = 2.93 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 04:59:09.448918: step 145440, loss = 2.56 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 04:59:14.076786: step 145450, loss = 2.90 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 04:59:18.744877: step 145460, loss = 2.90 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 04:59:23.440684: step 145470, loss = 2.77 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 04:59:28.249790: step 145480, loss = 3.17 (221.3 examples/sec; 0.578 sec/batch)
2016-07-16 04:59:33.941948: step 145490, loss = 3.00 (265.8 examples/sec; 0.481 sec/batch)
2016-07-16 04:59:38.779508: step 145500, loss = 2.78 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 04:59:44.406002: step 145510, loss = 2.88 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 04:59:49.570954: step 145520, loss = 2.85 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 04:59:54.959897: step 145530, loss = 2.89 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 04:59:59.743475: step 145540, loss = 2.94 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 05:00:04.386638: step 145550, loss = 2.72 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 05:00:09.556376: step 145560, loss = 2.96 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 05:00:14.975939: step 145570, loss = 2.88 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 05:00:19.718956: step 145580, loss = 2.65 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 05:00:24.542212: step 145590, loss = 2.85 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 05:00:29.202099: step 145600, loss = 2.99 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 05:00:34.810842: step 145610, loss = 2.58 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 05:00:40.588429: step 145620, loss = 2.86 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 05:00:45.943922: step 145630, loss = 2.79 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 05:00:51.312896: step 145640, loss = 3.01 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 05:00:56.023884: step 145650, loss = 2.87 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 05:01:00.890195: step 145660, loss = 2.87 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 05:01:05.697161: step 145670, loss = 2.81 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 05:01:10.466650: step 145680, loss = 2.99 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 05:01:15.277235: step 145690, loss = 2.81 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 05:01:20.069609: step 145700, loss = 2.88 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 05:01:25.885592: step 145710, loss = 2.70 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 05:01:30.668267: step 145720, loss = 2.93 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 05:01:35.324115: step 145730, loss = 2.82 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 05:01:40.174292: step 145740, loss = 2.65 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 05:01:44.862589: step 145750, loss = 2.92 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 05:01:49.480595: step 145760, loss = 3.13 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 05:01:54.127869: step 145770, loss = 2.84 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 05:01:58.836821: step 145780, loss = 3.17 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 05:02:04.601899: step 145790, loss = 2.65 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 05:02:09.549569: step 145800, loss = 2.70 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 05:02:15.233922: step 145810, loss = 2.89 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 05:02:19.818139: step 145820, loss = 2.81 (284.4 examples/sec; 0.450 sec/batch)
2016-07-16 05:02:24.476361: step 145830, loss = 3.02 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 05:02:29.060781: step 145840, loss = 2.82 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 05:02:34.312115: step 145850, loss = 2.73 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 05:02:39.608880: step 145860, loss = 3.07 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 05:02:44.395422: step 145870, loss = 2.88 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 05:02:49.811548: step 145880, loss = 2.81 (190.8 examples/sec; 0.671 sec/batch)
2016-07-16 05:02:56.211861: step 145890, loss = 2.99 (212.2 examples/sec; 0.603 sec/batch)
2016-07-16 05:03:01.069508: step 145900, loss = 2.65 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 05:03:06.896053: step 145910, loss = 2.81 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 05:03:13.158442: step 145920, loss = 2.54 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 05:03:18.593550: step 145930, loss = 2.74 (291.4 examples/sec; 0.439 sec/batch)
2016-07-16 05:03:23.263892: step 145940, loss = 3.01 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 05:03:28.932292: step 145950, loss = 3.09 (283.8 examples/sec; 0.451 sec/batch)
2016-07-16 05:03:33.521581: step 145960, loss = 2.92 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 05:03:38.887914: step 145970, loss = 2.81 (200.3 examples/sec; 0.639 sec/batch)
2016-07-16 05:03:44.097974: step 145980, loss = 2.77 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 05:03:48.876142: step 145990, loss = 2.97 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 05:03:54.499902: step 146000, loss = 2.80 (188.8 examples/sec; 0.678 sec/batch)
2016-07-16 05:04:00.913037: step 146010, loss = 3.03 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 05:04:06.083515: step 146020, loss = 2.84 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 05:04:11.381322: step 146030, loss = 3.00 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 05:04:17.197690: step 146040, loss = 2.80 (251.5 examples/sec; 0.509 sec/batch)
2016-07-16 05:04:22.011787: step 146050, loss = 2.72 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 05:04:26.828192: step 146060, loss = 2.76 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 05:04:31.556532: step 146070, loss = 2.73 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 05:04:36.376309: step 146080, loss = 2.76 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 05:04:41.060553: step 146090, loss = 2.80 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 05:04:45.682393: step 146100, loss = 2.83 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 05:04:51.268762: step 146110, loss = 2.77 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 05:04:55.948365: step 146120, loss = 2.89 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 05:05:01.781298: step 146130, loss = 3.01 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 05:05:06.602451: step 146140, loss = 2.64 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 05:05:11.462216: step 146150, loss = 2.80 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 05:05:17.788593: step 146160, loss = 3.07 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 05:05:23.226075: step 146170, loss = 2.85 (254.8 examples/sec; 0.502 sec/batch)
2016-07-16 05:05:27.980579: step 146180, loss = 2.82 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 05:05:32.843419: step 146190, loss = 2.65 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 05:05:37.519216: step 146200, loss = 3.16 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 05:05:43.129857: step 146210, loss = 3.01 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 05:05:48.907194: step 146220, loss = 2.85 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 05:05:53.731252: step 146230, loss = 2.92 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 05:05:58.401924: step 146240, loss = 2.93 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 05:06:03.059324: step 146250, loss = 2.90 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 05:06:08.790089: step 146260, loss = 2.75 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 05:06:13.653831: step 146270, loss = 3.05 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 05:06:18.450532: step 146280, loss = 2.82 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 05:06:23.232758: step 146290, loss = 2.90 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 05:06:27.842332: step 146300, loss = 3.17 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 05:06:33.817345: step 146310, loss = 2.86 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 05:06:39.236347: step 146320, loss = 2.86 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 05:06:43.999458: step 146330, loss = 2.65 (250.2 examples/sec; 0.512 sec/batch)
2016-07-16 05:06:49.092004: step 146340, loss = 2.98 (188.0 examples/sec; 0.681 sec/batch)
2016-07-16 05:06:54.756271: step 146350, loss = 2.77 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 05:06:59.439660: step 146360, loss = 2.93 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 05:07:05.143418: step 146370, loss = 2.88 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 05:07:10.541865: step 146380, loss = 2.89 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 05:07:15.817568: step 146390, loss = 2.89 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 05:07:20.484990: step 146400, loss = 2.79 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 05:07:26.324793: step 146410, loss = 2.82 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 05:07:30.979839: step 146420, loss = 2.69 (284.3 examples/sec; 0.450 sec/batch)
2016-07-16 05:07:35.657377: step 146430, loss = 2.74 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 05:07:41.669643: step 146440, loss = 2.87 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 05:07:46.416804: step 146450, loss = 2.81 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 05:07:51.082035: step 146460, loss = 2.54 (281.6 examples/sec; 0.454 sec/batch)
2016-07-16 05:07:55.735729: step 146470, loss = 2.76 (284.2 examples/sec; 0.450 sec/batch)
2016-07-16 05:08:00.395205: step 146480, loss = 2.84 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 05:08:05.313893: step 146490, loss = 2.96 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 05:08:10.878765: step 146500, loss = 2.72 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 05:08:16.670134: step 146510, loss = 2.89 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 05:08:21.566832: step 146520, loss = 2.77 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 05:08:26.222757: step 146530, loss = 2.80 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 05:08:30.923980: step 146540, loss = 2.67 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 05:08:35.610248: step 146550, loss = 2.86 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 05:08:40.256338: step 146560, loss = 2.60 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 05:08:46.050117: step 146570, loss = 2.80 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 05:08:50.921688: step 146580, loss = 2.72 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 05:08:55.572607: step 146590, loss = 2.95 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 05:09:00.309742: step 146600, loss = 2.83 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 05:09:07.119433: step 146610, loss = 2.94 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 05:09:11.785799: step 146620, loss = 2.88 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 05:09:16.452738: step 146630, loss = 2.82 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 05:09:21.429496: step 146640, loss = 3.05 (202.7 examples/sec; 0.632 sec/batch)
2016-07-16 05:09:26.996910: step 146650, loss = 3.07 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 05:09:31.740453: step 146660, loss = 2.78 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 05:09:36.627479: step 146670, loss = 2.79 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 05:09:41.330992: step 146680, loss = 2.91 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 05:09:46.009469: step 146690, loss = 2.75 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 05:09:50.735562: step 146700, loss = 2.88 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 05:09:56.358237: step 146710, loss = 2.77 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 05:10:00.949202: step 146720, loss = 2.85 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 05:10:06.064858: step 146730, loss = 2.86 (202.2 examples/sec; 0.633 sec/batch)
2016-07-16 05:10:11.517274: step 146740, loss = 2.69 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 05:10:16.226579: step 146750, loss = 2.78 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 05:10:21.070038: step 146760, loss = 3.01 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 05:10:25.753294: step 146770, loss = 2.86 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 05:10:30.408570: step 146780, loss = 2.82 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 05:10:35.947189: step 146790, loss = 2.80 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 05:10:41.019122: step 146800, loss = 2.82 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 05:10:46.844560: step 146810, loss = 2.83 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 05:10:51.621942: step 146820, loss = 2.82 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 05:10:56.502995: step 146830, loss = 2.89 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 05:11:01.266995: step 146840, loss = 3.06 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 05:11:05.926228: step 146850, loss = 2.79 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 05:11:11.288061: step 146860, loss = 2.82 (197.7 examples/sec; 0.648 sec/batch)
2016-07-16 05:11:16.459112: step 146870, loss = 2.64 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 05:11:21.201215: step 146880, loss = 2.94 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 05:11:26.069604: step 146890, loss = 2.88 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 05:11:30.794460: step 146900, loss = 2.85 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 05:11:36.374821: step 146910, loss = 2.86 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 05:11:40.977173: step 146920, loss = 3.20 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 05:11:45.627993: step 146930, loss = 2.90 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 05:11:50.351175: step 146940, loss = 2.94 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 05:11:54.934539: step 146950, loss = 2.57 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 05:12:00.652999: step 146960, loss = 2.84 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 05:12:05.483603: step 146970, loss = 2.81 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 05:12:10.302630: step 146980, loss = 3.15 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 05:12:15.109105: step 146990, loss = 2.88 (246.9 examples/sec; 0.518 sec/batch)
2016-07-16 05:12:19.742581: step 147000, loss = 2.85 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 05:12:25.702356: step 147010, loss = 2.78 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 05:12:31.251959: step 147020, loss = 3.08 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 05:12:36.019977: step 147030, loss = 2.95 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 05:12:40.713705: step 147040, loss = 2.85 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 05:12:45.346265: step 147050, loss = 3.07 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 05:12:50.546888: step 147060, loss = 2.67 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 05:12:55.857609: step 147070, loss = 2.69 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 05:13:01.610022: step 147080, loss = 3.04 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 05:13:06.426626: step 147090, loss = 2.87 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 05:13:11.253200: step 147100, loss = 2.91 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 05:13:17.026944: step 147110, loss = 3.17 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 05:13:21.655345: step 147120, loss = 2.96 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 05:13:26.866125: step 147130, loss = 2.83 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 05:13:32.148009: step 147140, loss = 2.78 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 05:13:36.816668: step 147150, loss = 2.82 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 05:13:41.656988: step 147160, loss = 2.70 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 05:13:46.386949: step 147170, loss = 2.62 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 05:13:51.161040: step 147180, loss = 2.98 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 05:13:55.976999: step 147190, loss = 2.74 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 05:14:00.681556: step 147200, loss = 2.77 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 05:14:06.241377: step 147210, loss = 2.79 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 05:14:10.961964: step 147220, loss = 2.88 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 05:14:15.644302: step 147230, loss = 2.90 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 05:14:20.325762: step 147240, loss = 2.63 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 05:14:25.054150: step 147250, loss = 2.77 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 05:14:29.715800: step 147260, loss = 2.83 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 05:14:34.402859: step 147270, loss = 2.71 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 05:14:39.068009: step 147280, loss = 2.87 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 05:14:44.805774: step 147290, loss = 2.84 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 05:14:49.634740: step 147300, loss = 2.95 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 05:14:55.384511: step 147310, loss = 2.80 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 05:15:00.196410: step 147320, loss = 3.00 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 05:15:04.884567: step 147330, loss = 2.82 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 05:15:09.667474: step 147340, loss = 2.86 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 05:15:14.372339: step 147350, loss = 2.61 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 05:15:19.022985: step 147360, loss = 2.98 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 05:15:23.675283: step 147370, loss = 2.90 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 05:15:29.390920: step 147380, loss = 2.84 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 05:15:34.259625: step 147390, loss = 2.93 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 05:15:39.036879: step 147400, loss = 2.75 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 05:15:44.820967: step 147410, loss = 2.91 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 05:15:49.688206: step 147420, loss = 2.78 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 05:15:54.436938: step 147430, loss = 3.02 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 05:16:00.323299: step 147440, loss = 2.86 (180.9 examples/sec; 0.708 sec/batch)
2016-07-16 05:16:06.112747: step 147450, loss = 3.01 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 05:16:10.743642: step 147460, loss = 3.09 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 05:16:16.365883: step 147470, loss = 2.98 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 05:16:21.470458: step 147480, loss = 3.06 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 05:16:26.206858: step 147490, loss = 2.69 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 05:16:30.862889: step 147500, loss = 2.67 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 05:16:36.487413: step 147510, loss = 2.76 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 05:16:41.194875: step 147520, loss = 2.86 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 05:16:45.810533: step 147530, loss = 2.86 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 05:16:51.107083: step 147540, loss = 2.88 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 05:16:56.376280: step 147550, loss = 2.92 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 05:17:01.073170: step 147560, loss = 2.90 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 05:17:06.336724: step 147570, loss = 2.66 (187.4 examples/sec; 0.683 sec/batch)
2016-07-16 05:17:11.738496: step 147580, loss = 2.69 (285.1 examples/sec; 0.449 sec/batch)
2016-07-16 05:17:16.441443: step 147590, loss = 2.88 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 05:17:22.163290: step 147600, loss = 2.65 (262.6 examples/sec; 0.488 sec/batch)
2016-07-16 05:17:28.981546: step 147610, loss = 2.89 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 05:17:33.842236: step 147620, loss = 2.62 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 05:17:38.602023: step 147630, loss = 2.81 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 05:17:43.374790: step 147640, loss = 2.83 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 05:17:48.057393: step 147650, loss = 3.01 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 05:17:52.689543: step 147660, loss = 2.70 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 05:17:58.473131: step 147670, loss = 2.75 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 05:18:03.249838: step 147680, loss = 2.69 (284.7 examples/sec; 0.450 sec/batch)
2016-07-16 05:18:08.070502: step 147690, loss = 2.75 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 05:18:12.789562: step 147700, loss = 2.73 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 05:18:18.684573: step 147710, loss = 2.79 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 05:18:23.469958: step 147720, loss = 2.66 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 05:18:28.193046: step 147730, loss = 2.88 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 05:18:33.061536: step 147740, loss = 2.74 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 05:18:37.801327: step 147750, loss = 2.80 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 05:18:42.672583: step 147760, loss = 2.81 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 05:18:47.456518: step 147770, loss = 2.98 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 05:18:52.200935: step 147780, loss = 2.97 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 05:18:57.109692: step 147790, loss = 2.92 (234.9 examples/sec; 0.545 sec/batch)
2016-07-16 05:19:03.707765: step 147800, loss = 2.84 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 05:19:09.831201: step 147810, loss = 3.06 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 05:19:15.102286: step 147820, loss = 2.78 (207.3 examples/sec; 0.617 sec/batch)
2016-07-16 05:19:20.416587: step 147830, loss = 2.85 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 05:19:25.106089: step 147840, loss = 2.86 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 05:19:29.725347: step 147850, loss = 2.78 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 05:19:34.369776: step 147860, loss = 2.85 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 05:19:39.038627: step 147870, loss = 2.63 (263.6 examples/sec; 0.485 sec/batch)
2016-07-16 05:19:43.661636: step 147880, loss = 3.08 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 05:19:48.315256: step 147890, loss = 2.68 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 05:19:54.054573: step 147900, loss = 3.02 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 05:19:59.892317: step 147910, loss = 2.77 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 05:20:04.698574: step 147920, loss = 2.96 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 05:20:09.411442: step 147930, loss = 2.96 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 05:20:14.260062: step 147940, loss = 2.82 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 05:20:19.050134: step 147950, loss = 2.90 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 05:20:23.839811: step 147960, loss = 2.70 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 05:20:28.693924: step 147970, loss = 2.64 (255.8 examples/sec; 0.500 sec/batch)
2016-07-16 05:20:33.387948: step 147980, loss = 2.69 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 05:20:38.229464: step 147990, loss = 2.56 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 05:20:42.885220: step 148000, loss = 2.86 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 05:20:48.505692: step 148010, loss = 2.84 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 05:20:53.179352: step 148020, loss = 2.78 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 05:20:58.145215: step 148030, loss = 2.72 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 05:21:03.703607: step 148040, loss = 2.87 (251.1 examples/sec; 0.510 sec/batch)
2016-07-16 05:21:08.449366: step 148050, loss = 2.91 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 05:21:13.295169: step 148060, loss = 2.80 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 05:21:17.982526: step 148070, loss = 3.05 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 05:21:22.720770: step 148080, loss = 2.62 (245.9 examples/sec; 0.521 sec/batch)
2016-07-16 05:21:27.340877: step 148090, loss = 2.83 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 05:21:32.015757: step 148100, loss = 3.05 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 05:21:38.765589: step 148110, loss = 2.63 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 05:21:44.414718: step 148120, loss = 2.69 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 05:21:49.200572: step 148130, loss = 2.86 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 05:21:54.167765: step 148140, loss = 3.10 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 05:21:59.747416: step 148150, loss = 2.90 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 05:22:05.494160: step 148160, loss = 2.84 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 05:22:10.344087: step 148170, loss = 2.71 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 05:22:15.034401: step 148180, loss = 2.86 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 05:22:19.682103: step 148190, loss = 2.80 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 05:22:24.400098: step 148200, loss = 2.71 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 05:22:30.004364: step 148210, loss = 2.88 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 05:22:34.635612: step 148220, loss = 2.68 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 05:22:40.082876: step 148230, loss = 2.70 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 05:22:45.311231: step 148240, loss = 3.02 (246.2 examples/sec; 0.520 sec/batch)
2016-07-16 05:22:50.917298: step 148250, loss = 2.78 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 05:22:55.600153: step 148260, loss = 2.93 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 05:23:00.221264: step 148270, loss = 2.86 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 05:23:04.840543: step 148280, loss = 2.77 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 05:23:09.708950: step 148290, loss = 2.90 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 05:23:15.387918: step 148300, loss = 2.77 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 05:23:22.204875: step 148310, loss = 2.78 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 05:23:27.010418: step 148320, loss = 2.62 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 05:23:31.839034: step 148330, loss = 2.68 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 05:23:37.845808: step 148340, loss = 2.95 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 05:23:42.524068: step 148350, loss = 2.67 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 05:23:47.232217: step 148360, loss = 2.80 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 05:23:51.869061: step 148370, loss = 2.72 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 05:23:56.530756: step 148380, loss = 2.73 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 05:24:02.252078: step 148390, loss = 2.85 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 05:24:07.085081: step 148400, loss = 2.79 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 05:24:12.899397: step 148410, loss = 2.68 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 05:24:17.615395: step 148420, loss = 2.78 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 05:24:22.263855: step 148430, loss = 2.67 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 05:24:27.577771: step 148440, loss = 2.83 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 05:24:32.812739: step 148450, loss = 2.81 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 05:24:37.560369: step 148460, loss = 2.77 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 05:24:42.919836: step 148470, loss = 2.80 (186.8 examples/sec; 0.685 sec/batch)
2016-07-16 05:24:49.361298: step 148480, loss = 2.95 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 05:24:54.207076: step 148490, loss = 2.92 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 05:24:58.976040: step 148500, loss = 2.77 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 05:25:04.721726: step 148510, loss = 2.59 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 05:25:09.389052: step 148520, loss = 3.04 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 05:25:14.089348: step 148530, loss = 2.80 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 05:25:18.718766: step 148540, loss = 2.97 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 05:25:24.098246: step 148550, loss = 3.00 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 05:25:29.300219: step 148560, loss = 2.82 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 05:25:34.017157: step 148570, loss = 2.79 (255.4 examples/sec; 0.501 sec/batch)
2016-07-16 05:25:38.880834: step 148580, loss = 2.90 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 05:25:43.537513: step 148590, loss = 2.92 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 05:25:48.242993: step 148600, loss = 2.62 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 05:25:53.763821: step 148610, loss = 3.16 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 05:25:58.401844: step 148620, loss = 3.04 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 05:26:03.662597: step 148630, loss = 3.00 (207.4 examples/sec; 0.617 sec/batch)
2016-07-16 05:26:08.883364: step 148640, loss = 2.99 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 05:26:13.636811: step 148650, loss = 2.73 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 05:26:19.002991: step 148660, loss = 2.71 (187.8 examples/sec; 0.682 sec/batch)
2016-07-16 05:26:25.470958: step 148670, loss = 2.88 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 05:26:30.752723: step 148680, loss = 2.79 (197.0 examples/sec; 0.650 sec/batch)
2016-07-16 05:26:36.061699: step 148690, loss = 2.84 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 05:26:40.734254: step 148700, loss = 2.84 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 05:26:46.361383: step 148710, loss = 2.67 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 05:26:51.646436: step 148720, loss = 2.82 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 05:26:56.906965: step 148730, loss = 2.80 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 05:27:01.643190: step 148740, loss = 2.61 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 05:27:06.531545: step 148750, loss = 2.97 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 05:27:11.213794: step 148760, loss = 2.71 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 05:27:15.847679: step 148770, loss = 2.89 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 05:27:20.475586: step 148780, loss = 2.72 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 05:27:25.124889: step 148790, loss = 2.91 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 05:27:30.883301: step 148800, loss = 2.94 (251.4 examples/sec; 0.509 sec/batch)
2016-07-16 05:27:36.662637: step 148810, loss = 2.91 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 05:27:41.583585: step 148820, loss = 2.65 (231.9 examples/sec; 0.552 sec/batch)
2016-07-16 05:27:47.433962: step 148830, loss = 2.68 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 05:27:52.121623: step 148840, loss = 2.55 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 05:27:57.763578: step 148850, loss = 2.70 (241.4 examples/sec; 0.530 sec/batch)
2016-07-16 05:28:02.669015: step 148860, loss = 2.73 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 05:28:07.445434: step 148870, loss = 2.74 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 05:28:12.230063: step 148880, loss = 2.97 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 05:28:17.096419: step 148890, loss = 2.69 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 05:28:21.828784: step 148900, loss = 2.60 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 05:28:27.401300: step 148910, loss = 2.89 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 05:28:32.962039: step 148920, loss = 2.65 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 05:28:37.999449: step 148930, loss = 2.94 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 05:28:42.752413: step 148940, loss = 2.86 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 05:28:47.530051: step 148950, loss = 2.90 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 05:28:52.331636: step 148960, loss = 2.97 (251.1 examples/sec; 0.510 sec/batch)
2016-07-16 05:28:57.051889: step 148970, loss = 2.87 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 05:29:01.908384: step 148980, loss = 2.63 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 05:29:06.599236: step 148990, loss = 2.84 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 05:29:11.419172: step 149000, loss = 2.66 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 05:29:17.245688: step 149010, loss = 2.76 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 05:29:23.737725: step 149020, loss = 2.92 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 05:29:29.079989: step 149030, loss = 2.78 (239.1 examples/sec; 0.535 sec/batch)
2016-07-16 05:29:33.800577: step 149040, loss = 3.00 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 05:29:39.189799: step 149050, loss = 2.69 (187.1 examples/sec; 0.684 sec/batch)
2016-07-16 05:29:45.621251: step 149060, loss = 2.96 (208.4 examples/sec; 0.614 sec/batch)
2016-07-16 05:29:50.523965: step 149070, loss = 3.05 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 05:29:55.275670: step 149080, loss = 3.07 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 05:30:00.068396: step 149090, loss = 2.89 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 05:30:04.715638: step 149100, loss = 2.79 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 05:30:10.577116: step 149110, loss = 2.78 (201.7 examples/sec; 0.635 sec/batch)
2016-07-16 05:30:16.137201: step 149120, loss = 2.76 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 05:30:20.870518: step 149130, loss = 2.67 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 05:30:25.886126: step 149140, loss = 2.70 (219.7 examples/sec; 0.583 sec/batch)
2016-07-16 05:30:32.478115: step 149150, loss = 2.50 (201.8 examples/sec; 0.634 sec/batch)
2016-07-16 05:30:37.630204: step 149160, loss = 2.90 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 05:30:42.348704: step 149170, loss = 2.69 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 05:30:47.180721: step 149180, loss = 2.83 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 05:30:51.837929: step 149190, loss = 2.87 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 05:30:56.522880: step 149200, loss = 2.94 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 05:31:03.231543: step 149210, loss = 2.81 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 05:31:07.996645: step 149220, loss = 2.79 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 05:31:12.839479: step 149230, loss = 2.96 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 05:31:17.633670: step 149240, loss = 2.78 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 05:31:22.247779: step 149250, loss = 2.81 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 05:31:26.883652: step 149260, loss = 3.08 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 05:31:31.574353: step 149270, loss = 2.79 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 05:31:37.199916: step 149280, loss = 2.63 (203.0 examples/sec; 0.630 sec/batch)
2016-07-16 05:31:42.426876: step 149290, loss = 3.03 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 05:31:48.016050: step 149300, loss = 2.89 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 05:31:53.853944: step 149310, loss = 2.94 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 05:31:58.791980: step 149320, loss = 2.86 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 05:32:03.485652: step 149330, loss = 2.64 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 05:32:08.145985: step 149340, loss = 2.93 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 05:32:12.782435: step 149350, loss = 2.78 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 05:32:17.435114: step 149360, loss = 2.66 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 05:32:23.159905: step 149370, loss = 2.88 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 05:32:27.959419: step 149380, loss = 2.76 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 05:32:32.752443: step 149390, loss = 2.70 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 05:32:37.485185: step 149400, loss = 2.82 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 05:32:43.969830: step 149410, loss = 2.86 (184.8 examples/sec; 0.693 sec/batch)
2016-07-16 05:32:50.165913: step 149420, loss = 3.04 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 05:32:54.805556: step 149430, loss = 2.87 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 05:32:59.487032: step 149440, loss = 2.81 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 05:33:04.104394: step 149450, loss = 2.83 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 05:33:09.846945: step 149460, loss = 2.76 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 05:33:14.729661: step 149470, loss = 2.87 (255.4 examples/sec; 0.501 sec/batch)
2016-07-16 05:33:19.516112: step 149480, loss = 3.00 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 05:33:25.409331: step 149490, loss = 2.76 (187.0 examples/sec; 0.684 sec/batch)
2016-07-16 05:33:31.274660: step 149500, loss = 2.93 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 05:33:37.081744: step 149510, loss = 2.68 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 05:33:42.067191: step 149520, loss = 2.62 (229.1 examples/sec; 0.559 sec/batch)
2016-07-16 05:33:48.626192: step 149530, loss = 2.83 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 05:33:53.801808: step 149540, loss = 2.58 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 05:33:58.529267: step 149550, loss = 2.97 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 05:34:04.095550: step 149560, loss = 2.80 (183.6 examples/sec; 0.697 sec/batch)
2016-07-16 05:34:10.369860: step 149570, loss = 2.72 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 05:34:15.203690: step 149580, loss = 2.83 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 05:34:20.000271: step 149590, loss = 2.96 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 05:34:24.780675: step 149600, loss = 2.87 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 05:34:30.381304: step 149610, loss = 2.92 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 05:34:35.512941: step 149620, loss = 2.76 (198.3 examples/sec; 0.646 sec/batch)
2016-07-16 05:34:41.004938: step 149630, loss = 2.80 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 05:34:46.743236: step 149640, loss = 3.03 (262.6 examples/sec; 0.488 sec/batch)
2016-07-16 05:34:51.556801: step 149650, loss = 2.83 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 05:34:56.355729: step 149660, loss = 2.70 (254.5 examples/sec; 0.503 sec/batch)
2016-07-16 05:35:01.127715: step 149670, loss = 2.74 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 05:35:06.017539: step 149680, loss = 3.03 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 05:35:10.715300: step 149690, loss = 2.86 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 05:35:15.339044: step 149700, loss = 2.94 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 05:35:22.022063: step 149710, loss = 2.89 (224.1 examples/sec; 0.571 sec/batch)
2016-07-16 05:35:27.191284: step 149720, loss = 2.82 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 05:35:32.680737: step 149730, loss = 2.77 (257.8 examples/sec; 0.497 sec/batch)
2016-07-16 05:35:37.402370: step 149740, loss = 2.78 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 05:35:42.055218: step 149750, loss = 3.08 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 05:35:46.714780: step 149760, loss = 2.77 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 05:35:51.341264: step 149770, loss = 2.75 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 05:35:56.017928: step 149780, loss = 2.89 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 05:36:00.671644: step 149790, loss = 2.72 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 05:36:05.326746: step 149800, loss = 2.86 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 05:36:12.222029: step 149810, loss = 2.59 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 05:36:18.046739: step 149820, loss = 2.74 (212.0 examples/sec; 0.604 sec/batch)
2016-07-16 05:36:23.235787: step 149830, loss = 2.94 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 05:36:28.563361: step 149840, loss = 2.75 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 05:36:33.196778: step 149850, loss = 2.92 (286.8 examples/sec; 0.446 sec/batch)
2016-07-16 05:36:38.964598: step 149860, loss = 2.83 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 05:36:43.768065: step 149870, loss = 2.88 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 05:36:48.574100: step 149880, loss = 2.60 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 05:36:53.332411: step 149890, loss = 2.81 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 05:36:57.988885: step 149900, loss = 3.17 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 05:37:04.258014: step 149910, loss = 2.75 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 05:37:09.527826: step 149920, loss = 2.69 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 05:37:14.246351: step 149930, loss = 2.90 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 05:37:19.621891: step 149940, loss = 2.75 (188.0 examples/sec; 0.681 sec/batch)
2016-07-16 05:37:26.040321: step 149950, loss = 2.90 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 05:37:30.869017: step 149960, loss = 2.97 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 05:37:35.657271: step 149970, loss = 2.79 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 05:37:41.531145: step 149980, loss = 2.75 (186.3 examples/sec; 0.687 sec/batch)
2016-07-16 05:37:46.283870: step 149990, loss = 2.88 (279.8 examples/sec; 0.458 sec/batch)
2016-07-16 05:37:51.410673: step 150000, loss = 2.76 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 05:37:58.031224: step 150010, loss = 2.73 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 05:38:02.741294: step 150020, loss = 2.64 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 05:38:07.563677: step 150030, loss = 2.67 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 05:38:12.382504: step 150040, loss = 2.90 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 05:38:17.139684: step 150050, loss = 2.72 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 05:38:21.990404: step 150060, loss = 2.68 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 05:38:26.795830: step 150070, loss = 2.64 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 05:38:31.701497: step 150080, loss = 2.81 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 05:38:36.547088: step 150090, loss = 2.81 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 05:38:41.359634: step 150100, loss = 2.98 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 05:38:46.939471: step 150110, loss = 2.99 (279.2 examples/sec; 0.459 sec/batch)
2016-07-16 05:38:51.575146: step 150120, loss = 2.95 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 05:38:56.262745: step 150130, loss = 2.99 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 05:39:00.943523: step 150140, loss = 2.69 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 05:39:05.640954: step 150150, loss = 2.85 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 05:39:10.449352: step 150160, loss = 2.59 (213.1 examples/sec; 0.601 sec/batch)
2016-07-16 05:39:16.137233: step 150170, loss = 2.89 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 05:39:20.923560: step 150180, loss = 2.88 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 05:39:25.735055: step 150190, loss = 2.89 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 05:39:32.247679: step 150200, loss = 2.98 (200.1 examples/sec; 0.640 sec/batch)
2016-07-16 05:39:38.958396: step 150210, loss = 2.87 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 05:39:44.576017: step 150220, loss = 2.90 (251.5 examples/sec; 0.509 sec/batch)
2016-07-16 05:39:49.302338: step 150230, loss = 2.90 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 05:39:54.205212: step 150240, loss = 2.96 (245.6 examples/sec; 0.521 sec/batch)
2016-07-16 05:40:00.781697: step 150250, loss = 2.77 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 05:40:05.698541: step 150260, loss = 2.90 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 05:40:10.416949: step 150270, loss = 2.67 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 05:40:15.018087: step 150280, loss = 2.57 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 05:40:19.637220: step 150290, loss = 2.60 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 05:40:25.107989: step 150300, loss = 3.01 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 05:40:31.091232: step 150310, loss = 2.65 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 05:40:36.321512: step 150320, loss = 2.89 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 05:40:41.653649: step 150330, loss = 2.60 (256.1 examples/sec; 0.500 sec/batch)
2016-07-16 05:40:46.361655: step 150340, loss = 2.71 (252.9 examples/sec; 0.506 sec/batch)
2016-07-16 05:40:51.754849: step 150350, loss = 2.93 (186.3 examples/sec; 0.687 sec/batch)
2016-07-16 05:40:58.214790: step 150360, loss = 2.98 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 05:41:03.050673: step 150370, loss = 2.62 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 05:41:07.792045: step 150380, loss = 2.71 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 05:41:12.570384: step 150390, loss = 3.10 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 05:41:17.395840: step 150400, loss = 2.68 (250.9 examples/sec; 0.510 sec/batch)
2016-07-16 05:41:23.092928: step 150410, loss = 2.90 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 05:41:27.745206: step 150420, loss = 3.10 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 05:41:32.373854: step 150430, loss = 2.72 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 05:41:37.069256: step 150440, loss = 2.64 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 05:41:42.799839: step 150450, loss = 3.06 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 05:41:47.463304: step 150460, loss = 2.91 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 05:41:52.819429: step 150470, loss = 2.85 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 05:41:58.003946: step 150480, loss = 2.80 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 05:42:02.777718: step 150490, loss = 2.87 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 05:42:07.410013: step 150500, loss = 2.78 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 05:42:13.039957: step 150510, loss = 2.66 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 05:42:17.720382: step 150520, loss = 2.66 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 05:42:22.343349: step 150530, loss = 2.96 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 05:42:27.062615: step 150540, loss = 2.70 (285.5 examples/sec; 0.448 sec/batch)
2016-07-16 05:42:31.631794: step 150550, loss = 2.71 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 05:42:37.022746: step 150560, loss = 2.81 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 05:42:42.211228: step 150570, loss = 3.05 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 05:42:46.932206: step 150580, loss = 2.71 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 05:42:51.788501: step 150590, loss = 2.85 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 05:42:56.434998: step 150600, loss = 2.69 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 05:43:02.056038: step 150610, loss = 2.77 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 05:43:07.878702: step 150620, loss = 2.84 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 05:43:12.666975: step 150630, loss = 2.92 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 05:43:17.462537: step 150640, loss = 2.99 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 05:43:22.231732: step 150650, loss = 2.72 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 05:43:26.843552: step 150660, loss = 2.91 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 05:43:31.987579: step 150670, loss = 2.99 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 05:43:37.408070: step 150680, loss = 2.95 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 05:43:43.125171: step 150690, loss = 2.96 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 05:43:48.513563: step 150700, loss = 2.76 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 05:43:55.194150: step 150710, loss = 2.91 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 05:44:00.813098: step 150720, loss = 2.79 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 05:44:05.536072: step 150730, loss = 2.66 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 05:44:10.173983: step 150740, loss = 2.88 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 05:44:15.046517: step 150750, loss = 2.84 (207.3 examples/sec; 0.617 sec/batch)
2016-07-16 05:44:20.678935: step 150760, loss = 2.89 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 05:44:25.406493: step 150770, loss = 2.98 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 05:44:30.016937: step 150780, loss = 2.99 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 05:44:34.712919: step 150790, loss = 2.68 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 05:44:39.419983: step 150800, loss = 2.73 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 05:44:45.072144: step 150810, loss = 2.90 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 05:44:49.780299: step 150820, loss = 2.69 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 05:44:55.512498: step 150830, loss = 2.64 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 05:45:00.282808: step 150840, loss = 3.02 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 05:45:05.090635: step 150850, loss = 2.84 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 05:45:09.858106: step 150860, loss = 2.87 (276.2 examples/sec; 0.464 sec/batch)
2016-07-16 05:45:14.507410: step 150870, loss = 2.95 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 05:45:19.529734: step 150880, loss = 2.78 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 05:45:25.035844: step 150890, loss = 2.77 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 05:45:30.843387: step 150900, loss = 2.65 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 05:45:36.490740: step 150910, loss = 3.09 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 05:45:41.137534: step 150920, loss = 2.79 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 05:45:45.760305: step 150930, loss = 2.74 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 05:45:50.446751: step 150940, loss = 3.01 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 05:45:56.200041: step 150950, loss = 2.75 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 05:46:01.853808: step 150960, loss = 2.89 (204.3 examples/sec; 0.627 sec/batch)
2016-07-16 05:46:06.909464: step 150970, loss = 2.58 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 05:46:11.644201: step 150980, loss = 2.55 (257.8 examples/sec; 0.497 sec/batch)
2016-07-16 05:46:16.515472: step 150990, loss = 2.75 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 05:46:21.315097: step 151000, loss = 2.79 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 05:46:27.075744: step 151010, loss = 2.78 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 05:46:31.917071: step 151020, loss = 2.78 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 05:46:36.649869: step 151030, loss = 2.92 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 05:46:41.401140: step 151040, loss = 2.74 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 05:46:46.286281: step 151050, loss = 2.85 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 05:46:51.030670: step 151060, loss = 2.94 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 05:46:55.850782: step 151070, loss = 2.77 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 05:47:00.620537: step 151080, loss = 2.70 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 05:47:06.686726: step 151090, loss = 3.10 (191.3 examples/sec; 0.669 sec/batch)
2016-07-16 05:47:12.353198: step 151100, loss = 2.67 (252.7 examples/sec; 0.507 sec/batch)
2016-07-16 05:47:17.999694: step 151110, loss = 2.86 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 05:47:22.638964: step 151120, loss = 2.88 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 05:47:27.235080: step 151130, loss = 2.74 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 05:47:31.857615: step 151140, loss = 2.79 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 05:47:36.530823: step 151150, loss = 2.88 (254.0 examples/sec; 0.504 sec/batch)
2016-07-16 05:47:42.092629: step 151160, loss = 2.81 (203.3 examples/sec; 0.629 sec/batch)
2016-07-16 05:47:47.049743: step 151170, loss = 2.87 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 05:47:51.813246: step 151180, loss = 2.83 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 05:47:56.617182: step 151190, loss = 2.98 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 05:48:01.292343: step 151200, loss = 2.96 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 05:48:06.818909: step 151210, loss = 2.85 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 05:48:11.879907: step 151220, loss = 2.75 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 05:48:17.350566: step 151230, loss = 2.66 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 05:48:22.135942: step 151240, loss = 2.89 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 05:48:27.002053: step 151250, loss = 2.78 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 05:48:31.718860: step 151260, loss = 3.01 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 05:48:36.496206: step 151270, loss = 2.85 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 05:48:41.325577: step 151280, loss = 2.74 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 05:48:46.055603: step 151290, loss = 2.93 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 05:48:50.697881: step 151300, loss = 2.74 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 05:48:56.956908: step 151310, loss = 2.86 (200.5 examples/sec; 0.638 sec/batch)
2016-07-16 05:49:02.247960: step 151320, loss = 2.66 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 05:49:08.004147: step 151330, loss = 2.89 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 05:49:12.775940: step 151340, loss = 2.86 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 05:49:17.417172: step 151350, loss = 2.91 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 05:49:22.156315: step 151360, loss = 3.11 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 05:49:26.800702: step 151370, loss = 2.82 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 05:49:31.747915: step 151380, loss = 2.89 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 05:49:37.299729: step 151390, loss = 2.75 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 05:49:42.040383: step 151400, loss = 2.85 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 05:49:47.837102: step 151410, loss = 2.85 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 05:49:52.475219: step 151420, loss = 2.83 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 05:49:57.156242: step 151430, loss = 2.70 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 05:50:01.788023: step 151440, loss = 2.58 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 05:50:06.441111: step 151450, loss = 2.77 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 05:50:12.258365: step 151460, loss = 2.88 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 05:50:17.877954: step 151470, loss = 2.76 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 05:50:23.064798: step 151480, loss = 2.86 (216.0 examples/sec; 0.593 sec/batch)
2016-07-16 05:50:28.794658: step 151490, loss = 2.95 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 05:50:33.594746: step 151500, loss = 2.79 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 05:50:39.160819: step 151510, loss = 2.74 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 05:50:43.806052: step 151520, loss = 2.88 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 05:50:48.470447: step 151530, loss = 2.71 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 05:50:53.139514: step 151540, loss = 2.85 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 05:50:58.843793: step 151550, loss = 3.06 (208.1 examples/sec; 0.615 sec/batch)
2016-07-16 05:51:03.683641: step 151560, loss = 2.67 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 05:51:08.416366: step 151570, loss = 3.00 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 05:51:13.269192: step 151580, loss = 2.80 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 05:51:17.949063: step 151590, loss = 2.82 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 05:51:22.659394: step 151600, loss = 2.91 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 05:51:28.177526: step 151610, loss = 3.05 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 05:51:33.629680: step 151620, loss = 2.84 (207.3 examples/sec; 0.617 sec/batch)
2016-07-16 05:51:38.851666: step 151630, loss = 2.92 (250.6 examples/sec; 0.511 sec/batch)
2016-07-16 05:51:44.581593: step 151640, loss = 2.84 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 05:51:49.383951: step 151650, loss = 2.84 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 05:51:54.230376: step 151660, loss = 2.90 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 05:51:58.929350: step 151670, loss = 2.62 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 05:52:03.563473: step 151680, loss = 2.89 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 05:52:08.220795: step 151690, loss = 2.73 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 05:52:13.710201: step 151700, loss = 2.74 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 05:52:20.000839: step 151710, loss = 2.72 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 05:52:24.622345: step 151720, loss = 2.83 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 05:52:29.237790: step 151730, loss = 3.10 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 05:52:34.956219: step 151740, loss = 2.78 (217.9 examples/sec; 0.588 sec/batch)
2016-07-16 05:52:39.796150: step 151750, loss = 2.84 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 05:52:44.534305: step 151760, loss = 2.62 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 05:52:49.296346: step 151770, loss = 2.95 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 05:52:54.102208: step 151780, loss = 2.80 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 05:53:00.486679: step 151790, loss = 2.73 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 05:53:05.886815: step 151800, loss = 2.84 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 05:53:11.652078: step 151810, loss = 2.64 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 05:53:16.280858: step 151820, loss = 2.86 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 05:53:21.672183: step 151830, loss = 3.01 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 05:53:26.858861: step 151840, loss = 2.89 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 05:53:31.592163: step 151850, loss = 2.87 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 05:53:37.220164: step 151860, loss = 2.66 (185.5 examples/sec; 0.690 sec/batch)
2016-07-16 05:53:43.493122: step 151870, loss = 3.19 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 05:53:48.360691: step 151880, loss = 2.85 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 05:53:53.132307: step 151890, loss = 3.00 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 05:53:59.220463: step 151900, loss = 3.11 (199.2 examples/sec; 0.642 sec/batch)
2016-07-16 05:54:06.143095: step 151910, loss = 2.78 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 05:54:11.944582: step 151920, loss = 2.82 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 05:54:16.786243: step 151930, loss = 2.95 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 05:54:21.628076: step 151940, loss = 2.82 (250.1 examples/sec; 0.512 sec/batch)
2016-07-16 05:54:26.354361: step 151950, loss = 2.67 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 05:54:31.020392: step 151960, loss = 2.82 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 05:54:35.671625: step 151970, loss = 2.77 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 05:54:40.348376: step 151980, loss = 3.01 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 05:54:44.993407: step 151990, loss = 2.99 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 05:54:49.587567: step 152000, loss = 3.01 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 05:54:55.300236: step 152010, loss = 2.84 (220.5 examples/sec; 0.580 sec/batch)
2016-07-16 05:55:00.981218: step 152020, loss = 2.95 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 05:55:05.785657: step 152030, loss = 3.01 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 05:55:10.660803: step 152040, loss = 2.80 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 05:55:15.370170: step 152050, loss = 2.84 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 05:55:19.996340: step 152060, loss = 2.75 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 05:55:24.623419: step 152070, loss = 2.83 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 05:55:29.237996: step 152080, loss = 2.76 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 05:55:33.945615: step 152090, loss = 2.90 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 05:55:38.570578: step 152100, loss = 2.57 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 05:55:44.244770: step 152110, loss = 2.80 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 05:55:48.873297: step 152120, loss = 2.86 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 05:55:54.427552: step 152130, loss = 2.67 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 05:55:59.437629: step 152140, loss = 2.79 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 05:56:04.173184: step 152150, loss = 2.77 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 05:56:09.021253: step 152160, loss = 2.77 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 05:56:13.658636: step 152170, loss = 2.71 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 05:56:18.366703: step 152180, loss = 2.73 (275.6 examples/sec; 0.465 sec/batch)
2016-07-16 05:56:24.139826: step 152190, loss = 2.98 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 05:56:28.944091: step 152200, loss = 2.62 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 05:56:34.549056: step 152210, loss = 2.79 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 05:56:39.199452: step 152220, loss = 3.07 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 05:56:43.878668: step 152230, loss = 2.66 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 05:56:48.527218: step 152240, loss = 2.71 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 05:56:53.915554: step 152250, loss = 3.10 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 05:56:59.104870: step 152260, loss = 2.67 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 05:57:03.871525: step 152270, loss = 2.99 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 05:57:09.452709: step 152280, loss = 2.91 (187.5 examples/sec; 0.683 sec/batch)
2016-07-16 05:57:15.669764: step 152290, loss = 2.83 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 05:57:20.487036: step 152300, loss = 2.78 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 05:57:26.278237: step 152310, loss = 2.85 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 05:57:31.037210: step 152320, loss = 2.83 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 05:57:35.628827: step 152330, loss = 2.94 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 05:57:40.298793: step 152340, loss = 2.67 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 05:57:44.964448: step 152350, loss = 2.75 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 05:57:50.546844: step 152360, loss = 2.82 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 05:57:55.552827: step 152370, loss = 2.77 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 05:58:00.316571: step 152380, loss = 2.75 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 05:58:06.115246: step 152390, loss = 2.74 (189.8 examples/sec; 0.675 sec/batch)
2016-07-16 05:58:12.155025: step 152400, loss = 2.49 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 05:58:17.999572: step 152410, loss = 2.88 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 05:58:22.821695: step 152420, loss = 2.96 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 05:58:29.311079: step 152430, loss = 2.97 (202.1 examples/sec; 0.633 sec/batch)
2016-07-16 05:58:34.662755: step 152440, loss = 2.86 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 05:58:39.374547: step 152450, loss = 3.00 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 05:58:44.262828: step 152460, loss = 2.73 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 05:58:48.928369: step 152470, loss = 2.72 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 05:58:53.598220: step 152480, loss = 2.87 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 05:58:58.224138: step 152490, loss = 2.78 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 05:59:02.942982: step 152500, loss = 2.82 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 05:59:09.862454: step 152510, loss = 2.93 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 05:59:14.605521: step 152520, loss = 3.18 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 05:59:19.256601: step 152530, loss = 2.80 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 05:59:23.988608: step 152540, loss = 2.76 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 05:59:28.636468: step 152550, loss = 2.77 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 05:59:33.309347: step 152560, loss = 3.03 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 05:59:37.970633: step 152570, loss = 3.19 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 05:59:42.663175: step 152580, loss = 2.72 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 05:59:47.295720: step 152590, loss = 2.60 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 05:59:51.944543: step 152600, loss = 2.92 (287.8 examples/sec; 0.445 sec/batch)
2016-07-16 05:59:57.620830: step 152610, loss = 2.63 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 06:00:03.338074: step 152620, loss = 2.88 (222.8 examples/sec; 0.574 sec/batch)
2016-07-16 06:00:08.540939: step 152630, loss = 2.75 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 06:00:13.992184: step 152640, loss = 2.89 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 06:00:19.781002: step 152650, loss = 2.96 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 06:00:24.587871: step 152660, loss = 2.84 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 06:00:29.386900: step 152670, loss = 2.89 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 06:00:34.247916: step 152680, loss = 3.00 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 06:00:38.862669: step 152690, loss = 2.90 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 06:00:43.683285: step 152700, loss = 2.57 (212.3 examples/sec; 0.603 sec/batch)
2016-07-16 06:00:50.613336: step 152710, loss = 3.18 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 06:00:55.307799: step 152720, loss = 2.79 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 06:01:00.166064: step 152730, loss = 2.96 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 06:01:04.937280: step 152740, loss = 2.70 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 06:01:09.761096: step 152750, loss = 2.94 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 06:01:14.447355: step 152760, loss = 2.94 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 06:01:19.165724: step 152770, loss = 3.07 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 06:01:23.824186: step 152780, loss = 2.66 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 06:01:28.925137: step 152790, loss = 2.86 (206.3 examples/sec; 0.620 sec/batch)
2016-07-16 06:01:34.349530: step 152800, loss = 2.90 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 06:01:40.058970: step 152810, loss = 2.89 (256.2 examples/sec; 0.500 sec/batch)
2016-07-16 06:01:45.625015: step 152820, loss = 2.95 (187.0 examples/sec; 0.685 sec/batch)
2016-07-16 06:01:51.896495: step 152830, loss = 2.61 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 06:01:56.736030: step 152840, loss = 2.77 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 06:02:01.554227: step 152850, loss = 2.75 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 06:02:07.665242: step 152860, loss = 2.73 (198.9 examples/sec; 0.644 sec/batch)
2016-07-16 06:02:13.382608: step 152870, loss = 2.84 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 06:02:18.139076: step 152880, loss = 2.72 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 06:02:22.997677: step 152890, loss = 2.86 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 06:02:27.658657: step 152900, loss = 2.79 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 06:02:33.532624: step 152910, loss = 3.03 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 06:02:38.363093: step 152920, loss = 2.77 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 06:02:43.088187: step 152930, loss = 2.91 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 06:02:47.964995: step 152940, loss = 2.83 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 06:02:52.691036: step 152950, loss = 2.67 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 06:02:57.567674: step 152960, loss = 2.84 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 06:03:02.417613: step 152970, loss = 2.84 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 06:03:07.157985: step 152980, loss = 2.94 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 06:03:11.995347: step 152990, loss = 2.84 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 06:03:16.718602: step 153000, loss = 2.73 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 06:03:22.607487: step 153010, loss = 2.96 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 06:03:27.514339: step 153020, loss = 2.76 (257.8 examples/sec; 0.497 sec/batch)
2016-07-16 06:03:32.210211: step 153030, loss = 3.02 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 06:03:36.866355: step 153040, loss = 2.76 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 06:03:41.541501: step 153050, loss = 3.01 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 06:03:46.236510: step 153060, loss = 2.85 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 06:03:51.964848: step 153070, loss = 2.83 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 06:03:56.746970: step 153080, loss = 2.86 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 06:04:01.565655: step 153090, loss = 2.82 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 06:04:07.577554: step 153100, loss = 2.89 (201.7 examples/sec; 0.635 sec/batch)
2016-07-16 06:04:13.161837: step 153110, loss = 2.63 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 06:04:18.684275: step 153120, loss = 2.89 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 06:04:23.719226: step 153130, loss = 2.74 (275.6 examples/sec; 0.465 sec/batch)
2016-07-16 06:04:28.456683: step 153140, loss = 2.91 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 06:04:33.242399: step 153150, loss = 2.94 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 06:04:38.033290: step 153160, loss = 3.02 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 06:04:44.214722: step 153170, loss = 2.84 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 06:04:49.680162: step 153180, loss = 2.68 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 06:04:54.350087: step 153190, loss = 2.98 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 06:04:59.009294: step 153200, loss = 2.68 (284.8 examples/sec; 0.449 sec/batch)
2016-07-16 06:05:05.110207: step 153210, loss = 2.84 (205.0 examples/sec; 0.625 sec/batch)
2016-07-16 06:05:10.496106: step 153220, loss = 2.84 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 06:05:15.185666: step 153230, loss = 2.74 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 06:05:20.053418: step 153240, loss = 2.81 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 06:05:24.773240: step 153250, loss = 3.01 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 06:05:29.418195: step 153260, loss = 2.45 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 06:05:35.061502: step 153270, loss = 2.60 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 06:05:40.004682: step 153280, loss = 2.74 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 06:05:44.756895: step 153290, loss = 2.95 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 06:05:50.499481: step 153300, loss = 2.71 (187.2 examples/sec; 0.684 sec/batch)
2016-07-16 06:05:56.668529: step 153310, loss = 2.79 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 06:06:01.346940: step 153320, loss = 2.78 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 06:06:05.984868: step 153330, loss = 2.86 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 06:06:10.630674: step 153340, loss = 2.80 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 06:06:15.333385: step 153350, loss = 2.86 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 06:06:21.115846: step 153360, loss = 2.56 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 06:06:25.922443: step 153370, loss = 2.95 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 06:06:30.597122: step 153380, loss = 2.82 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 06:06:35.294603: step 153390, loss = 2.83 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 06:06:39.987415: step 153400, loss = 2.71 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 06:06:45.577840: step 153410, loss = 2.97 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 06:06:51.214067: step 153420, loss = 2.73 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 06:06:56.165859: step 153430, loss = 2.87 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 06:07:00.815054: step 153440, loss = 2.87 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 06:07:05.467752: step 153450, loss = 2.95 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 06:07:11.014039: step 153460, loss = 2.71 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 06:07:16.051655: step 153470, loss = 2.64 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 06:07:20.846768: step 153480, loss = 2.80 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 06:07:25.633134: step 153490, loss = 2.92 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 06:07:30.284121: step 153500, loss = 2.74 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 06:07:35.958475: step 153510, loss = 2.85 (224.3 examples/sec; 0.571 sec/batch)
2016-07-16 06:07:41.853011: step 153520, loss = 2.90 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 06:07:47.645031: step 153530, loss = 2.64 (198.3 examples/sec; 0.645 sec/batch)
2016-07-16 06:07:52.566096: step 153540, loss = 2.79 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 06:07:57.344235: step 153550, loss = 2.70 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 06:08:02.192364: step 153560, loss = 2.58 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 06:08:06.810601: step 153570, loss = 2.79 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 06:08:11.526005: step 153580, loss = 2.84 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 06:08:17.331171: step 153590, loss = 2.71 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 06:08:22.094975: step 153600, loss = 2.66 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 06:08:27.797687: step 153610, loss = 2.94 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 06:08:32.466495: step 153620, loss = 2.77 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 06:08:37.086400: step 153630, loss = 2.72 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 06:08:42.515705: step 153640, loss = 2.85 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 06:08:47.722498: step 153650, loss = 2.75 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 06:08:53.381099: step 153660, loss = 2.74 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 06:08:58.017772: step 153670, loss = 2.80 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 06:09:03.798086: step 153680, loss = 2.73 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 06:09:08.668190: step 153690, loss = 2.71 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 06:09:13.392592: step 153700, loss = 2.90 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 06:09:19.182615: step 153710, loss = 2.86 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 06:09:23.820883: step 153720, loss = 3.03 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 06:09:28.552617: step 153730, loss = 3.18 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 06:09:33.190334: step 153740, loss = 2.74 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 06:09:38.630925: step 153750, loss = 2.59 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 06:09:43.763541: step 153760, loss = 2.82 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 06:09:48.497909: step 153770, loss = 2.66 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 06:09:53.304424: step 153780, loss = 2.84 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 06:09:58.035687: step 153790, loss = 2.75 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 06:10:02.724970: step 153800, loss = 2.68 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 06:10:09.465693: step 153810, loss = 2.83 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 06:10:14.244209: step 153820, loss = 2.77 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 06:10:18.946741: step 153830, loss = 2.85 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 06:10:23.672872: step 153840, loss = 2.94 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 06:10:28.311764: step 153850, loss = 2.85 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 06:10:33.425125: step 153860, loss = 2.61 (201.0 examples/sec; 0.637 sec/batch)
2016-07-16 06:10:38.839405: step 153870, loss = 2.77 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 06:10:43.562515: step 153880, loss = 2.70 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 06:10:48.385399: step 153890, loss = 2.76 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 06:10:53.122613: step 153900, loss = 3.02 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 06:10:58.933360: step 153910, loss = 2.80 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 06:11:03.783620: step 153920, loss = 2.78 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 06:11:08.520807: step 153930, loss = 2.95 (251.2 examples/sec; 0.510 sec/batch)
2016-07-16 06:11:14.085299: step 153940, loss = 2.66 (188.1 examples/sec; 0.681 sec/batch)
2016-07-16 06:11:20.341198: step 153950, loss = 2.80 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 06:11:25.206463: step 153960, loss = 3.01 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 06:11:29.990429: step 153970, loss = 2.73 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 06:11:35.978044: step 153980, loss = 2.72 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 06:11:40.581425: step 153990, loss = 2.92 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 06:11:45.231955: step 154000, loss = 2.65 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 06:11:50.876284: step 154010, loss = 2.65 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 06:11:55.522966: step 154020, loss = 2.75 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 06:12:00.225554: step 154030, loss = 2.76 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 06:12:04.824040: step 154040, loss = 2.69 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 06:12:09.561144: step 154050, loss = 2.89 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 06:12:14.167461: step 154060, loss = 2.80 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 06:12:18.789147: step 154070, loss = 2.67 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 06:12:23.461624: step 154080, loss = 2.83 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 06:12:28.163339: step 154090, loss = 2.74 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 06:12:32.770610: step 154100, loss = 2.79 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 06:12:38.285826: step 154110, loss = 2.66 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 06:12:44.012525: step 154120, loss = 2.89 (219.2 examples/sec; 0.584 sec/batch)
2016-07-16 06:12:49.232606: step 154130, loss = 2.82 (201.2 examples/sec; 0.636 sec/batch)
2016-07-16 06:12:54.740096: step 154140, loss = 2.97 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 06:12:59.478874: step 154150, loss = 2.88 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 06:13:04.344264: step 154160, loss = 2.77 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 06:13:09.118595: step 154170, loss = 2.89 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 06:13:13.923593: step 154180, loss = 2.85 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 06:13:18.586892: step 154190, loss = 3.00 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 06:13:23.284379: step 154200, loss = 3.04 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 06:13:28.861215: step 154210, loss = 2.92 (276.2 examples/sec; 0.464 sec/batch)
2016-07-16 06:13:34.147041: step 154220, loss = 2.81 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 06:13:39.491067: step 154230, loss = 2.74 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 06:13:44.226546: step 154240, loss = 2.91 (253.1 examples/sec; 0.506 sec/batch)
2016-07-16 06:13:49.063352: step 154250, loss = 2.85 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 06:13:53.742032: step 154260, loss = 2.85 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 06:13:58.378924: step 154270, loss = 2.64 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 06:14:04.113786: step 154280, loss = 2.67 (222.5 examples/sec; 0.575 sec/batch)
2016-07-16 06:14:09.347643: step 154290, loss = 2.70 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 06:14:14.786785: step 154300, loss = 2.88 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 06:14:20.575809: step 154310, loss = 2.91 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 06:14:25.435106: step 154320, loss = 2.88 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 06:14:30.242447: step 154330, loss = 2.83 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 06:14:36.374077: step 154340, loss = 2.71 (196.0 examples/sec; 0.653 sec/batch)
2016-07-16 06:14:42.053827: step 154350, loss = 2.93 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 06:14:46.840133: step 154360, loss = 2.70 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 06:14:51.732451: step 154370, loss = 2.51 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 06:14:58.228514: step 154380, loss = 2.59 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 06:15:03.542794: step 154390, loss = 2.76 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 06:15:08.287075: step 154400, loss = 2.89 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 06:15:13.816332: step 154410, loss = 2.90 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 06:15:19.375108: step 154420, loss = 2.53 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 06:15:24.186906: step 154430, loss = 2.57 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 06:15:28.860364: step 154440, loss = 2.80 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 06:15:33.481192: step 154450, loss = 2.80 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 06:15:38.901207: step 154460, loss = 2.73 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 06:15:44.123061: step 154470, loss = 2.70 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 06:15:48.811571: step 154480, loss = 2.64 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 06:15:53.521207: step 154490, loss = 2.92 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 06:15:58.993894: step 154500, loss = 2.63 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 06:16:05.043707: step 154510, loss = 2.90 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 06:16:09.739763: step 154520, loss = 2.51 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 06:16:14.346552: step 154530, loss = 2.92 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 06:16:18.986717: step 154540, loss = 2.92 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 06:16:24.701869: step 154550, loss = 2.83 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 06:16:30.034465: step 154560, loss = 2.72 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 06:16:35.374577: step 154570, loss = 3.05 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 06:16:41.284333: step 154580, loss = 2.78 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 06:16:46.126594: step 154590, loss = 2.78 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 06:16:50.787486: step 154600, loss = 3.03 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 06:16:56.458056: step 154610, loss = 2.82 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 06:17:01.526335: step 154620, loss = 2.78 (204.3 examples/sec; 0.627 sec/batch)
2016-07-16 06:17:06.851101: step 154630, loss = 2.62 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 06:17:11.451208: step 154640, loss = 2.74 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 06:17:16.103071: step 154650, loss = 2.94 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 06:17:20.763551: step 154660, loss = 2.81 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 06:17:25.885905: step 154670, loss = 2.86 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 06:17:31.313962: step 154680, loss = 3.14 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 06:17:36.052189: step 154690, loss = 2.93 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 06:17:40.688033: step 154700, loss = 2.87 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 06:17:46.308217: step 154710, loss = 2.78 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 06:17:51.046431: step 154720, loss = 2.71 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 06:17:55.690796: step 154730, loss = 2.79 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 06:18:00.318499: step 154740, loss = 2.94 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 06:18:04.988116: step 154750, loss = 2.90 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 06:18:09.591004: step 154760, loss = 2.71 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 06:18:14.985024: step 154770, loss = 2.72 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 06:18:20.197995: step 154780, loss = 2.64 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 06:18:24.926124: step 154790, loss = 2.81 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 06:18:29.758451: step 154800, loss = 2.75 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 06:18:35.658362: step 154810, loss = 2.88 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 06:18:40.395230: step 154820, loss = 2.75 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 06:18:45.289489: step 154830, loss = 2.81 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 06:18:49.934076: step 154840, loss = 2.70 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 06:18:54.607085: step 154850, loss = 2.75 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 06:19:00.342466: step 154860, loss = 2.84 (211.9 examples/sec; 0.604 sec/batch)
2016-07-16 06:19:05.205748: step 154870, loss = 2.48 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 06:19:10.022009: step 154880, loss = 2.70 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 06:19:16.015507: step 154890, loss = 3.05 (190.0 examples/sec; 0.674 sec/batch)
2016-07-16 06:19:20.758833: step 154900, loss = 2.65 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 06:19:26.370986: step 154910, loss = 2.61 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 06:19:31.016232: step 154920, loss = 2.69 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 06:19:35.754747: step 154930, loss = 2.67 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 06:19:40.351519: step 154940, loss = 2.88 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 06:19:45.323589: step 154950, loss = 2.71 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 06:19:50.828721: step 154960, loss = 2.60 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 06:19:56.593875: step 154970, loss = 2.64 (250.2 examples/sec; 0.512 sec/batch)
2016-07-16 06:20:01.896504: step 154980, loss = 2.83 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 06:20:07.361946: step 154990, loss = 3.00 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 06:20:13.057397: step 155000, loss = 2.67 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 06:20:18.612798: step 155010, loss = 2.71 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 06:20:23.271424: step 155020, loss = 2.74 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 06:20:29.010014: step 155030, loss = 2.81 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 06:20:33.806597: step 155040, loss = 2.68 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 06:20:38.499181: step 155050, loss = 2.77 (279.2 examples/sec; 0.459 sec/batch)
2016-07-16 06:20:43.130042: step 155060, loss = 2.82 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 06:20:48.881570: step 155070, loss = 2.83 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 06:20:53.735247: step 155080, loss = 2.96 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 06:20:58.455392: step 155090, loss = 3.12 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 06:21:03.085352: step 155100, loss = 2.73 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 06:21:08.750144: step 155110, loss = 2.83 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 06:21:13.364546: step 155120, loss = 2.80 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 06:21:18.067046: step 155130, loss = 2.96 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 06:21:22.698013: step 155140, loss = 2.71 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 06:21:28.403821: step 155150, loss = 2.74 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 06:21:33.264192: step 155160, loss = 2.62 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 06:21:38.055663: step 155170, loss = 2.64 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 06:21:42.839242: step 155180, loss = 2.84 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 06:21:47.659369: step 155190, loss = 2.67 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 06:21:52.419603: step 155200, loss = 2.84 (249.2 examples/sec; 0.514 sec/batch)
2016-07-16 06:21:59.268089: step 155210, loss = 2.97 (188.2 examples/sec; 0.680 sec/batch)
2016-07-16 06:22:05.388083: step 155220, loss = 2.73 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 06:22:10.207703: step 155230, loss = 2.81 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 06:22:14.829587: step 155240, loss = 2.84 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 06:22:19.487921: step 155250, loss = 2.84 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 06:22:25.233093: step 155260, loss = 2.78 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 06:22:30.610060: step 155270, loss = 2.63 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 06:22:35.891740: step 155280, loss = 2.64 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 06:22:40.613833: step 155290, loss = 2.82 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 06:22:45.953207: step 155300, loss = 2.79 (185.9 examples/sec; 0.688 sec/batch)
2016-07-16 06:22:52.598636: step 155310, loss = 2.99 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 06:22:57.283982: step 155320, loss = 2.78 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 06:23:01.930224: step 155330, loss = 3.13 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 06:23:07.361579: step 155340, loss = 2.83 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 06:23:12.512596: step 155350, loss = 3.15 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 06:23:18.216252: step 155360, loss = 3.01 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 06:23:23.841319: step 155370, loss = 2.71 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 06:23:28.880214: step 155380, loss = 2.81 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 06:23:33.673166: step 155390, loss = 2.90 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 06:23:38.528044: step 155400, loss = 2.80 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 06:23:44.120645: step 155410, loss = 2.93 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 06:23:48.717565: step 155420, loss = 2.73 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 06:23:53.374240: step 155430, loss = 2.68 (285.2 examples/sec; 0.449 sec/batch)
2016-07-16 06:23:58.029014: step 155440, loss = 2.68 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 06:24:03.512480: step 155450, loss = 2.76 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 06:24:08.634322: step 155460, loss = 2.80 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 06:24:13.322574: step 155470, loss = 2.87 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 06:24:17.948447: step 155480, loss = 2.92 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 06:24:23.375983: step 155490, loss = 2.75 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 06:24:28.534133: step 155500, loss = 2.85 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 06:24:34.194559: step 155510, loss = 2.98 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 06:24:38.800612: step 155520, loss = 2.79 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 06:24:43.465187: step 155530, loss = 2.48 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 06:24:49.194398: step 155540, loss = 2.64 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 06:24:54.579607: step 155550, loss = 2.82 (199.8 examples/sec; 0.641 sec/batch)
2016-07-16 06:24:59.693254: step 155560, loss = 2.85 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 06:25:04.372340: step 155570, loss = 2.80 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 06:25:09.050884: step 155580, loss = 2.77 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 06:25:14.132553: step 155590, loss = 2.64 (198.0 examples/sec; 0.647 sec/batch)
2016-07-16 06:25:19.580496: step 155600, loss = 2.70 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 06:25:25.301011: step 155610, loss = 2.59 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 06:25:29.952721: step 155620, loss = 2.66 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 06:25:34.594721: step 155630, loss = 2.75 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 06:25:39.239801: step 155640, loss = 2.66 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 06:25:43.890282: step 155650, loss = 2.94 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 06:25:49.679977: step 155660, loss = 2.57 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 06:25:54.485477: step 155670, loss = 2.92 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 06:25:59.268419: step 155680, loss = 2.82 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 06:26:04.059632: step 155690, loss = 2.66 (250.5 examples/sec; 0.511 sec/batch)
2016-07-16 06:26:08.698348: step 155700, loss = 3.09 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 06:26:14.347517: step 155710, loss = 2.80 (282.9 examples/sec; 0.452 sec/batch)
2016-07-16 06:26:19.009879: step 155720, loss = 2.77 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 06:26:24.749059: step 155730, loss = 3.02 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 06:26:29.933972: step 155740, loss = 2.72 (199.6 examples/sec; 0.641 sec/batch)
2016-07-16 06:26:35.410242: step 155750, loss = 2.68 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 06:26:41.215184: step 155760, loss = 3.08 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 06:26:46.113543: step 155770, loss = 2.90 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 06:26:50.906924: step 155780, loss = 2.80 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 06:26:55.674084: step 155790, loss = 2.89 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 06:27:00.286915: step 155800, loss = 2.92 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 06:27:05.988290: step 155810, loss = 2.86 (283.7 examples/sec; 0.451 sec/batch)
2016-07-16 06:27:10.642435: step 155820, loss = 2.71 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 06:27:16.264918: step 155830, loss = 2.89 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 06:27:21.431625: step 155840, loss = 3.11 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 06:27:26.917748: step 155850, loss = 2.99 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 06:27:31.569784: step 155860, loss = 2.91 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 06:27:36.280524: step 155870, loss = 2.92 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 06:27:41.033437: step 155880, loss = 2.63 (227.4 examples/sec; 0.563 sec/batch)
2016-07-16 06:27:46.735591: step 155890, loss = 2.71 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 06:27:51.503742: step 155900, loss = 2.75 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 06:27:57.282549: step 155910, loss = 2.80 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 06:28:01.923273: step 155920, loss = 2.65 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 06:28:06.656611: step 155930, loss = 2.71 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 06:28:12.944057: step 155940, loss = 2.80 (201.0 examples/sec; 0.637 sec/batch)
2016-07-16 06:28:17.833225: step 155950, loss = 2.89 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 06:28:22.512424: step 155960, loss = 2.74 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 06:28:27.184726: step 155970, loss = 2.90 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 06:28:31.835101: step 155980, loss = 2.76 (284.6 examples/sec; 0.450 sec/batch)
2016-07-16 06:28:36.482623: step 155990, loss = 2.68 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 06:28:42.252942: step 156000, loss = 2.83 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 06:28:48.025524: step 156010, loss = 2.86 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 06:28:52.846749: step 156020, loss = 2.81 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 06:28:57.549230: step 156030, loss = 2.63 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 06:29:03.110769: step 156040, loss = 2.57 (190.3 examples/sec; 0.673 sec/batch)
2016-07-16 06:29:09.337050: step 156050, loss = 3.12 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 06:29:14.143530: step 156060, loss = 2.75 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 06:29:18.952169: step 156070, loss = 2.81 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 06:29:23.801147: step 156080, loss = 2.78 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 06:29:28.424163: step 156090, loss = 2.71 (285.6 examples/sec; 0.448 sec/batch)
2016-07-16 06:29:33.125300: step 156100, loss = 2.56 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 06:29:38.766152: step 156110, loss = 2.81 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 06:29:43.445693: step 156120, loss = 2.72 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 06:29:48.111679: step 156130, loss = 2.73 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 06:29:53.840847: step 156140, loss = 2.86 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 06:29:58.669644: step 156150, loss = 2.86 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 06:30:03.476194: step 156160, loss = 2.88 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 06:30:09.745236: step 156170, loss = 2.85 (198.1 examples/sec; 0.646 sec/batch)
2016-07-16 06:30:15.321629: step 156180, loss = 2.79 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 06:30:20.053134: step 156190, loss = 2.80 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 06:30:24.908266: step 156200, loss = 2.96 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 06:30:33.086174: step 156210, loss = 2.87 (216.4 examples/sec; 0.591 sec/batch)
2016-07-16 06:30:37.901259: step 156220, loss = 2.83 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 06:30:42.569108: step 156230, loss = 2.87 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 06:30:47.242651: step 156240, loss = 2.98 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 06:30:51.905912: step 156250, loss = 3.21 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 06:30:56.544853: step 156260, loss = 2.62 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 06:31:01.569404: step 156270, loss = 2.58 (187.4 examples/sec; 0.683 sec/batch)
2016-07-16 06:31:07.408917: step 156280, loss = 3.00 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 06:31:12.141391: step 156290, loss = 2.74 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 06:31:17.023643: step 156300, loss = 2.94 (242.4 examples/sec; 0.528 sec/batch)
2016-07-16 06:31:22.668705: step 156310, loss = 2.73 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 06:31:27.318011: step 156320, loss = 2.66 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 06:31:33.084767: step 156330, loss = 2.65 (221.9 examples/sec; 0.577 sec/batch)
2016-07-16 06:31:37.933281: step 156340, loss = 2.77 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 06:31:42.703585: step 156350, loss = 2.76 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 06:31:48.646208: step 156360, loss = 2.71 (188.0 examples/sec; 0.681 sec/batch)
2016-07-16 06:31:54.500175: step 156370, loss = 2.87 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 06:31:59.250676: step 156380, loss = 2.89 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 06:32:04.107315: step 156390, loss = 2.59 (253.9 examples/sec; 0.504 sec/batch)
2016-07-16 06:32:08.906705: step 156400, loss = 2.94 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 06:32:14.530476: step 156410, loss = 2.81 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 06:32:19.173296: step 156420, loss = 2.72 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 06:32:23.810524: step 156430, loss = 2.68 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 06:32:28.491522: step 156440, loss = 2.71 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 06:32:33.262694: step 156450, loss = 2.88 (233.0 examples/sec; 0.549 sec/batch)
2016-07-16 06:32:39.029275: step 156460, loss = 2.82 (251.7 examples/sec; 0.508 sec/batch)
2016-07-16 06:32:43.773577: step 156470, loss = 3.00 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 06:32:48.639212: step 156480, loss = 2.94 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 06:32:53.348536: step 156490, loss = 3.01 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 06:32:57.959342: step 156500, loss = 2.86 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 06:33:04.534202: step 156510, loss = 3.26 (202.1 examples/sec; 0.633 sec/batch)
2016-07-16 06:33:09.487445: step 156520, loss = 3.12 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 06:33:14.219415: step 156530, loss = 2.86 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 06:33:19.049549: step 156540, loss = 2.89 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 06:33:23.679875: step 156550, loss = 2.92 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 06:33:28.409554: step 156560, loss = 2.78 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 06:33:34.212130: step 156570, loss = 2.69 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 06:33:39.037744: step 156580, loss = 2.84 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 06:33:43.704930: step 156590, loss = 3.07 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 06:33:48.408646: step 156600, loss = 2.82 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 06:33:55.127412: step 156610, loss = 2.65 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 06:33:59.842548: step 156620, loss = 2.84 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 06:34:04.728887: step 156630, loss = 3.05 (253.6 examples/sec; 0.505 sec/batch)
2016-07-16 06:34:09.455660: step 156640, loss = 2.97 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 06:34:14.091010: step 156650, loss = 2.82 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 06:34:19.373077: step 156660, loss = 2.82 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 06:34:24.658709: step 156670, loss = 2.84 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 06:34:30.376001: step 156680, loss = 2.71 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 06:34:35.043462: step 156690, loss = 2.94 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 06:34:39.727866: step 156700, loss = 3.00 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 06:34:45.418315: step 156710, loss = 2.83 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 06:34:50.461350: step 156720, loss = 2.72 (208.3 examples/sec; 0.615 sec/batch)
2016-07-16 06:34:55.759966: step 156730, loss = 2.90 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 06:35:00.428998: step 156740, loss = 2.63 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 06:35:06.209347: step 156750, loss = 2.56 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 06:35:11.050300: step 156760, loss = 2.77 (275.0 examples/sec; 0.466 sec/batch)
2016-07-16 06:35:15.868685: step 156770, loss = 3.09 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 06:35:21.859352: step 156780, loss = 2.97 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 06:35:26.513346: step 156790, loss = 2.87 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 06:35:31.871897: step 156800, loss = 2.74 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 06:35:37.939624: step 156810, loss = 2.89 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 06:35:42.562509: step 156820, loss = 2.50 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 06:35:48.030417: step 156830, loss = 2.90 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 06:35:53.169078: step 156840, loss = 2.91 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 06:35:58.959099: step 156850, loss = 2.64 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 06:36:04.597109: step 156860, loss = 3.07 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 06:36:09.581552: step 156870, loss = 2.85 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 06:36:14.277897: step 156880, loss = 2.81 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 06:36:19.141017: step 156890, loss = 2.84 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 06:36:23.766357: step 156900, loss = 2.70 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 06:36:29.350377: step 156910, loss = 2.79 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 06:36:34.035993: step 156920, loss = 2.98 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 06:36:38.699008: step 156930, loss = 2.83 (281.6 examples/sec; 0.454 sec/batch)
2016-07-16 06:36:43.319231: step 156940, loss = 2.76 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 06:36:48.988429: step 156950, loss = 2.93 (204.6 examples/sec; 0.625 sec/batch)
2016-07-16 06:36:54.115165: step 156960, loss = 3.06 (207.3 examples/sec; 0.617 sec/batch)
2016-07-16 06:36:59.738512: step 156970, loss = 2.91 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 06:37:04.527845: step 156980, loss = 2.86 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 06:37:09.347143: step 156990, loss = 2.97 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 06:37:14.024369: step 157000, loss = 2.82 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 06:37:19.632738: step 157010, loss = 2.92 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 06:37:25.362769: step 157020, loss = 2.93 (220.7 examples/sec; 0.580 sec/batch)
2016-07-16 06:37:30.242963: step 157030, loss = 2.91 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 06:37:34.923446: step 157040, loss = 2.59 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 06:37:39.582025: step 157050, loss = 2.88 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 06:37:45.258116: step 157060, loss = 2.90 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 06:37:50.130189: step 157070, loss = 2.60 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 06:37:54.876151: step 157080, loss = 2.87 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 06:37:59.700471: step 157090, loss = 2.64 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 06:38:04.326468: step 157100, loss = 2.68 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 06:38:10.341145: step 157110, loss = 2.81 (200.7 examples/sec; 0.638 sec/batch)
2016-07-16 06:38:15.842244: step 157120, loss = 2.86 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 06:38:21.557665: step 157130, loss = 2.94 (241.5 examples/sec; 0.530 sec/batch)
2016-07-16 06:38:26.773144: step 157140, loss = 2.69 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 06:38:32.218875: step 157150, loss = 2.70 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 06:38:36.924141: step 157160, loss = 2.69 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 06:38:41.766623: step 157170, loss = 2.95 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 06:38:46.516641: step 157180, loss = 2.76 (252.3 examples/sec; 0.507 sec/batch)
2016-07-16 06:38:51.345324: step 157190, loss = 2.91 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 06:38:56.097009: step 157200, loss = 2.82 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 06:39:01.755052: step 157210, loss = 3.15 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 06:39:06.405771: step 157220, loss = 2.78 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 06:39:11.064368: step 157230, loss = 2.78 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 06:39:15.715007: step 157240, loss = 2.81 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 06:39:21.447947: step 157250, loss = 2.72 (210.7 examples/sec; 0.607 sec/batch)
2016-07-16 06:39:26.615637: step 157260, loss = 2.83 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 06:39:32.094885: step 157270, loss = 2.77 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 06:39:36.848025: step 157280, loss = 2.72 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 06:39:41.463829: step 157290, loss = 2.76 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 06:39:46.443709: step 157300, loss = 2.75 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 06:39:53.002578: step 157310, loss = 2.89 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 06:39:57.790721: step 157320, loss = 2.76 (216.5 examples/sec; 0.591 sec/batch)
2016-07-16 06:40:03.347071: step 157330, loss = 2.80 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 06:40:07.975650: step 157340, loss = 2.63 (284.4 examples/sec; 0.450 sec/batch)
2016-07-16 06:40:12.677307: step 157350, loss = 2.77 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 06:40:18.459777: step 157360, loss = 2.99 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 06:40:23.252966: step 157370, loss = 2.70 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 06:40:28.029377: step 157380, loss = 2.93 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 06:40:32.763713: step 157390, loss = 2.69 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 06:40:37.358855: step 157400, loss = 3.07 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 06:40:42.948436: step 157410, loss = 3.15 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 06:40:47.593710: step 157420, loss = 2.67 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 06:40:52.237977: step 157430, loss = 2.65 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 06:40:57.987582: step 157440, loss = 2.95 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 06:41:02.806780: step 157450, loss = 2.72 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 06:41:07.562177: step 157460, loss = 2.86 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 06:41:12.357658: step 157470, loss = 2.95 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 06:41:17.171822: step 157480, loss = 2.77 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 06:41:23.094325: step 157490, loss = 2.84 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 06:41:27.726122: step 157500, loss = 2.89 (286.3 examples/sec; 0.447 sec/batch)
2016-07-16 06:41:33.311813: step 157510, loss = 2.80 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 06:41:39.104776: step 157520, loss = 2.81 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 06:41:43.909652: step 157530, loss = 2.92 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 06:41:48.725459: step 157540, loss = 3.01 (250.6 examples/sec; 0.511 sec/batch)
2016-07-16 06:41:55.096900: step 157550, loss = 2.53 (205.0 examples/sec; 0.625 sec/batch)
2016-07-16 06:42:00.536187: step 157560, loss = 2.96 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 06:42:05.282468: step 157570, loss = 2.86 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 06:42:09.890571: step 157580, loss = 2.84 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 06:42:14.476728: step 157590, loss = 2.89 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 06:42:19.758800: step 157600, loss = 2.74 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 06:42:25.993124: step 157610, loss = 2.94 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 06:42:31.061261: step 157620, loss = 2.86 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 06:42:36.501294: step 157630, loss = 2.97 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 06:42:41.255153: step 157640, loss = 2.77 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 06:42:46.452595: step 157650, loss = 3.05 (188.3 examples/sec; 0.680 sec/batch)
2016-07-16 06:42:52.961237: step 157660, loss = 2.89 (197.9 examples/sec; 0.647 sec/batch)
2016-07-16 06:42:57.948984: step 157670, loss = 2.83 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 06:43:02.694022: step 157680, loss = 2.85 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 06:43:07.505274: step 157690, loss = 2.73 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 06:43:12.298610: step 157700, loss = 2.72 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 06:43:18.167068: step 157710, loss = 2.96 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 06:43:22.973489: step 157720, loss = 2.68 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 06:43:27.780424: step 157730, loss = 2.66 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 06:43:33.855061: step 157740, loss = 2.82 (197.4 examples/sec; 0.649 sec/batch)
2016-07-16 06:43:39.577321: step 157750, loss = 2.95 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 06:43:44.349435: step 157760, loss = 2.81 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 06:43:49.192715: step 157770, loss = 2.71 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 06:43:53.849589: step 157780, loss = 2.75 (284.0 examples/sec; 0.451 sec/batch)
2016-07-16 06:43:58.434381: step 157790, loss = 2.82 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 06:44:03.098690: step 157800, loss = 2.80 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 06:44:09.712438: step 157810, loss = 2.80 (221.6 examples/sec; 0.578 sec/batch)
2016-07-16 06:44:14.321641: step 157820, loss = 2.69 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 06:44:18.973500: step 157830, loss = 2.88 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 06:44:24.336380: step 157840, loss = 2.69 (207.3 examples/sec; 0.618 sec/batch)
2016-07-16 06:44:29.489773: step 157850, loss = 2.87 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 06:44:34.170816: step 157860, loss = 3.00 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 06:44:39.027844: step 157870, loss = 2.78 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 06:44:43.872636: step 157880, loss = 2.95 (262.0 examples/sec; 0.488 sec/batch)
2016-07-16 06:44:49.991128: step 157890, loss = 2.69 (198.5 examples/sec; 0.645 sec/batch)
2016-07-16 06:44:55.701045: step 157900, loss = 2.64 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 06:45:01.512557: step 157910, loss = 2.53 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 06:45:06.132899: step 157920, loss = 2.73 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 06:45:11.184045: step 157930, loss = 2.99 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 06:45:16.682692: step 157940, loss = 2.85 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 06:45:21.369196: step 157950, loss = 2.92 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 06:45:26.271213: step 157960, loss = 2.70 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 06:45:30.944076: step 157970, loss = 2.90 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 06:45:35.632521: step 157980, loss = 2.89 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 06:45:41.210796: step 157990, loss = 2.70 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 06:45:46.168548: step 158000, loss = 2.71 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 06:45:51.840744: step 158010, loss = 2.81 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 06:45:56.520674: step 158020, loss = 2.92 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 06:46:01.231983: step 158030, loss = 2.94 (284.4 examples/sec; 0.450 sec/batch)
2016-07-16 06:46:05.963880: step 158040, loss = 2.77 (236.8 examples/sec; 0.541 sec/batch)
2016-07-16 06:46:11.699760: step 158050, loss = 2.89 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 06:46:16.404999: step 158060, loss = 2.92 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 06:46:21.241381: step 158070, loss = 2.68 (264.7 examples/sec; 0.483 sec/batch)
2016-07-16 06:46:27.688320: step 158080, loss = 2.79 (209.0 examples/sec; 0.612 sec/batch)
2016-07-16 06:46:32.996016: step 158090, loss = 2.74 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 06:46:37.740927: step 158100, loss = 2.66 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 06:46:43.601223: step 158110, loss = 2.71 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 06:46:48.543716: step 158120, loss = 2.97 (240.3 examples/sec; 0.533 sec/batch)
2016-07-16 06:46:54.539992: step 158130, loss = 2.90 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 06:46:59.169188: step 158140, loss = 2.82 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 06:47:04.604268: step 158150, loss = 2.94 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 06:47:09.757826: step 158160, loss = 2.85 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 06:47:14.492009: step 158170, loss = 2.67 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 06:47:19.292489: step 158180, loss = 2.61 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 06:47:24.088747: step 158190, loss = 2.84 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 06:47:28.829079: step 158200, loss = 2.95 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 06:47:34.646392: step 158210, loss = 2.71 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 06:47:39.431850: step 158220, loss = 2.87 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 06:47:44.201466: step 158230, loss = 2.67 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 06:47:48.807412: step 158240, loss = 2.89 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 06:47:53.403726: step 158250, loss = 2.67 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 06:47:58.066656: step 158260, loss = 2.72 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 06:48:02.647534: step 158270, loss = 2.85 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 06:48:07.246441: step 158280, loss = 2.99 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 06:48:12.659283: step 158290, loss = 2.94 (208.6 examples/sec; 0.614 sec/batch)
2016-07-16 06:48:17.843809: step 158300, loss = 3.01 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 06:48:24.829609: step 158310, loss = 2.82 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 06:48:29.549264: step 158320, loss = 2.86 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 06:48:34.732087: step 158330, loss = 2.92 (186.1 examples/sec; 0.688 sec/batch)
2016-07-16 06:48:41.241272: step 158340, loss = 2.85 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 06:48:46.280784: step 158350, loss = 2.96 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 06:48:51.033502: step 158360, loss = 2.81 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 06:48:55.805505: step 158370, loss = 2.99 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 06:49:00.565951: step 158380, loss = 2.47 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 06:49:06.744817: step 158390, loss = 2.78 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 06:49:12.327600: step 158400, loss = 2.73 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 06:49:18.008482: step 158410, loss = 2.79 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 06:49:23.294140: step 158420, loss = 2.80 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 06:49:29.762468: step 158430, loss = 2.87 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 06:49:34.394592: step 158440, loss = 2.61 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 06:49:39.403680: step 158450, loss = 2.71 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 06:49:44.844248: step 158460, loss = 2.74 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 06:49:49.597493: step 158470, loss = 2.79 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 06:49:54.474275: step 158480, loss = 2.84 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 06:49:59.126750: step 158490, loss = 2.79 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 06:50:03.789805: step 158500, loss = 2.80 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 06:50:10.502172: step 158510, loss = 2.83 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 06:50:15.300925: step 158520, loss = 2.91 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 06:50:20.150943: step 158530, loss = 2.75 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 06:50:24.871741: step 158540, loss = 2.94 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 06:50:29.541650: step 158550, loss = 2.64 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 06:50:34.399483: step 158560, loss = 2.96 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 06:50:40.115848: step 158570, loss = 2.82 (253.2 examples/sec; 0.506 sec/batch)
2016-07-16 06:50:44.884512: step 158580, loss = 2.95 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 06:50:49.510181: step 158590, loss = 2.55 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 06:50:54.280378: step 158600, loss = 2.88 (224.1 examples/sec; 0.571 sec/batch)
2016-07-16 06:51:01.176990: step 158610, loss = 2.66 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 06:51:06.949088: step 158620, loss = 2.88 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 06:51:11.726154: step 158630, loss = 2.85 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 06:51:16.487325: step 158640, loss = 2.74 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 06:51:21.244492: step 158650, loss = 2.88 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 06:51:26.132774: step 158660, loss = 2.91 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 06:51:30.776458: step 158670, loss = 2.72 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 06:51:35.412531: step 158680, loss = 2.81 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 06:51:40.784830: step 158690, loss = 2.62 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 06:51:45.960098: step 158700, loss = 2.58 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 06:51:51.682937: step 158710, loss = 2.50 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 06:51:56.471212: step 158720, loss = 2.99 (275.6 examples/sec; 0.465 sec/batch)
2016-07-16 06:52:01.057649: step 158730, loss = 2.72 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 06:52:05.746384: step 158740, loss = 2.87 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 06:52:11.556721: step 158750, loss = 2.83 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 06:52:16.326178: step 158760, loss = 2.81 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 06:52:21.202657: step 158770, loss = 2.77 (248.7 examples/sec; 0.515 sec/batch)
2016-07-16 06:52:25.918761: step 158780, loss = 2.92 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 06:52:30.524355: step 158790, loss = 2.84 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 06:52:35.213894: step 158800, loss = 2.86 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 06:52:40.797136: step 158810, loss = 3.03 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 06:52:45.438228: step 158820, loss = 2.75 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 06:52:50.092870: step 158830, loss = 2.99 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 06:52:54.772588: step 158840, loss = 2.86 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 06:52:59.389151: step 158850, loss = 2.72 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 06:53:04.044079: step 158860, loss = 2.76 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 06:53:09.682811: step 158870, loss = 2.89 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 06:53:14.630922: step 158880, loss = 2.82 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 06:53:19.349163: step 158890, loss = 2.70 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 06:53:25.168482: step 158900, loss = 2.88 (187.0 examples/sec; 0.685 sec/batch)
2016-07-16 06:53:32.470328: step 158910, loss = 2.87 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 06:53:37.188624: step 158920, loss = 2.83 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 06:53:42.088423: step 158930, loss = 2.65 (246.6 examples/sec; 0.519 sec/batch)
2016-07-16 06:53:46.828990: step 158940, loss = 2.72 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 06:53:51.632235: step 158950, loss = 2.80 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 06:53:56.383675: step 158960, loss = 2.87 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 06:54:02.519174: step 158970, loss = 2.80 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 06:54:08.215405: step 158980, loss = 2.92 (252.6 examples/sec; 0.507 sec/batch)
2016-07-16 06:54:12.979214: step 158990, loss = 2.76 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 06:54:17.850739: step 159000, loss = 2.69 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 06:54:24.857362: step 159010, loss = 2.79 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 06:54:29.559368: step 159020, loss = 2.97 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 06:54:34.142244: step 159030, loss = 2.78 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 06:54:39.037821: step 159040, loss = 3.01 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 06:54:44.628711: step 159050, loss = 2.60 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 06:54:49.397819: step 159060, loss = 2.77 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 06:54:54.266470: step 159070, loss = 2.72 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 06:54:58.935659: step 159080, loss = 2.87 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 06:55:03.576460: step 159090, loss = 2.88 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 06:55:09.020428: step 159100, loss = 2.95 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 06:55:15.064601: step 159110, loss = 2.85 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 06:55:19.671119: step 159120, loss = 2.67 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 06:55:24.319526: step 159130, loss = 2.98 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 06:55:29.021906: step 159140, loss = 2.87 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 06:55:34.796941: step 159150, loss = 2.86 (257.0 examples/sec; 0.498 sec/batch)
2016-07-16 06:55:39.615774: step 159160, loss = 2.85 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 06:55:44.406675: step 159170, loss = 2.82 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 06:55:50.508812: step 159180, loss = 2.85 (195.4 examples/sec; 0.655 sec/batch)
2016-07-16 06:55:56.247435: step 159190, loss = 2.65 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 06:56:01.007108: step 159200, loss = 2.63 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 06:56:06.915561: step 159210, loss = 2.91 (251.7 examples/sec; 0.509 sec/batch)
2016-07-16 06:56:11.550057: step 159220, loss = 2.95 (282.2 examples/sec; 0.454 sec/batch)
2016-07-16 06:56:16.167625: step 159230, loss = 2.58 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 06:56:20.845539: step 159240, loss = 2.79 (283.4 examples/sec; 0.452 sec/batch)
2016-07-16 06:56:25.489774: step 159250, loss = 2.79 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 06:56:31.251946: step 159260, loss = 2.69 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 06:56:36.032990: step 159270, loss = 2.96 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 06:56:40.713399: step 159280, loss = 2.85 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 06:56:45.351863: step 159290, loss = 2.71 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 06:56:50.055254: step 159300, loss = 2.72 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 06:56:57.066406: step 159310, loss = 2.86 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 06:57:01.826479: step 159320, loss = 2.95 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 06:57:06.726205: step 159330, loss = 2.70 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 06:57:11.518916: step 159340, loss = 2.63 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 06:57:17.342667: step 159350, loss = 2.81 (189.8 examples/sec; 0.674 sec/batch)
2016-07-16 06:57:23.381551: step 159360, loss = 2.94 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 06:57:28.833703: step 159370, loss = 2.86 (201.4 examples/sec; 0.635 sec/batch)
2016-07-16 06:57:34.011355: step 159380, loss = 3.01 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 06:57:38.695759: step 159390, loss = 2.79 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 06:57:43.359566: step 159400, loss = 2.78 (282.2 examples/sec; 0.454 sec/batch)
2016-07-16 06:57:48.950528: step 159410, loss = 2.56 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 06:57:53.609925: step 159420, loss = 2.69 (283.8 examples/sec; 0.451 sec/batch)
2016-07-16 06:57:58.440282: step 159430, loss = 2.82 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 06:58:04.082992: step 159440, loss = 2.99 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 06:58:08.700511: step 159450, loss = 2.99 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 06:58:14.373198: step 159460, loss = 3.09 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 06:58:19.259647: step 159470, loss = 2.90 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 06:58:23.985231: step 159480, loss = 2.65 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 06:58:28.749756: step 159490, loss = 2.77 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 06:58:33.390778: step 159500, loss = 2.66 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 06:58:39.180817: step 159510, loss = 2.81 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 06:58:44.760255: step 159520, loss = 2.64 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 06:58:49.533181: step 159530, loss = 2.94 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 06:58:54.383312: step 159540, loss = 2.45 (250.6 examples/sec; 0.511 sec/batch)
2016-07-16 06:58:59.096271: step 159550, loss = 2.88 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 06:59:03.720715: step 159560, loss = 2.73 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 06:59:08.344773: step 159570, loss = 2.57 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 06:59:13.970677: step 159580, loss = 2.59 (201.2 examples/sec; 0.636 sec/batch)
2016-07-16 06:59:18.990664: step 159590, loss = 2.77 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 06:59:23.737201: step 159600, loss = 2.99 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 06:59:29.511626: step 159610, loss = 2.69 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 06:59:34.416373: step 159620, loss = 2.79 (252.5 examples/sec; 0.507 sec/batch)
2016-07-16 06:59:39.176292: step 159630, loss = 2.83 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 06:59:44.828886: step 159640, loss = 2.76 (185.6 examples/sec; 0.690 sec/batch)
2016-07-16 06:59:51.034811: step 159650, loss = 2.76 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 06:59:56.324919: step 159660, loss = 2.87 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 07:00:01.679531: step 159670, loss = 2.69 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 07:00:07.489023: step 159680, loss = 2.71 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 07:00:12.256334: step 159690, loss = 2.56 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 07:00:17.066636: step 159700, loss = 2.71 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 07:00:22.796227: step 159710, loss = 2.70 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 07:00:27.386963: step 159720, loss = 2.98 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 07:00:32.592457: step 159730, loss = 2.84 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 07:00:37.861429: step 159740, loss = 2.89 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 07:00:42.545452: step 159750, loss = 2.60 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 07:00:47.145864: step 159760, loss = 2.94 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 07:00:51.851811: step 159770, loss = 3.00 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 07:00:56.525671: step 159780, loss = 2.71 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 07:01:01.176863: step 159790, loss = 3.00 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 07:01:05.832284: step 159800, loss = 2.88 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 07:01:12.732921: step 159810, loss = 2.80 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 07:01:17.579304: step 159820, loss = 2.99 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 07:01:22.385534: step 159830, loss = 2.72 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 07:01:27.086231: step 159840, loss = 2.84 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 07:01:31.856810: step 159850, loss = 2.79 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 07:01:36.566677: step 159860, loss = 2.60 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 07:01:41.343452: step 159870, loss = 2.99 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 07:01:46.191762: step 159880, loss = 2.72 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 07:01:52.700361: step 159890, loss = 2.74 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 07:01:58.024568: step 159900, loss = 2.94 (263.1 examples/sec; 0.486 sec/batch)
2016-07-16 07:02:04.750270: step 159910, loss = 2.78 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 07:02:09.527874: step 159920, loss = 2.82 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 07:02:14.142991: step 159930, loss = 2.68 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 07:02:18.958484: step 159940, loss = 2.84 (213.8 examples/sec; 0.599 sec/batch)
2016-07-16 07:02:24.662776: step 159950, loss = 2.87 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 07:02:29.500111: step 159960, loss = 2.86 (283.7 examples/sec; 0.451 sec/batch)
2016-07-16 07:02:34.129522: step 159970, loss = 2.66 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 07:02:38.944654: step 159980, loss = 2.76 (216.9 examples/sec; 0.590 sec/batch)
2016-07-16 07:02:44.662821: step 159990, loss = 2.91 (257.8 examples/sec; 0.496 sec/batch)
2016-07-16 07:02:50.372500: step 160000, loss = 2.77 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 07:02:56.254993: step 160010, loss = 2.74 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 07:03:01.114292: step 160020, loss = 2.74 (252.0 examples/sec; 0.508 sec/batch)
2016-07-16 07:03:05.869851: step 160030, loss = 2.85 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 07:03:10.704117: step 160040, loss = 2.76 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 07:03:15.408636: step 160050, loss = 2.71 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 07:03:20.064627: step 160060, loss = 2.79 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 07:03:25.508073: step 160070, loss = 2.94 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 07:03:30.383988: step 160080, loss = 2.75 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 07:03:35.054110: step 160090, loss = 2.92 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 07:03:39.705596: step 160100, loss = 2.65 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 07:03:45.293960: step 160110, loss = 2.87 (279.8 examples/sec; 0.458 sec/batch)
2016-07-16 07:03:49.938587: step 160120, loss = 2.91 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 07:03:55.707088: step 160130, loss = 2.72 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 07:04:01.174360: step 160140, loss = 2.90 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 07:04:06.364167: step 160150, loss = 2.65 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 07:04:12.187777: step 160160, loss = 2.75 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 07:04:16.950633: step 160170, loss = 2.91 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 07:04:21.868078: step 160180, loss = 2.88 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 07:04:26.582731: step 160190, loss = 2.67 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 07:04:31.214640: step 160200, loss = 3.00 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 07:04:37.662001: step 160210, loss = 2.77 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 07:04:42.831622: step 160220, loss = 2.81 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 07:04:47.521181: step 160230, loss = 2.74 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 07:04:52.120409: step 160240, loss = 2.73 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 07:04:57.545903: step 160250, loss = 2.93 (208.8 examples/sec; 0.613 sec/batch)
2016-07-16 07:05:02.673241: step 160260, loss = 3.02 (268.6 examples/sec; 0.476 sec/batch)
2016-07-16 07:05:07.358439: step 160270, loss = 2.71 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 07:05:12.866001: step 160280, loss = 2.87 (186.7 examples/sec; 0.685 sec/batch)
2016-07-16 07:05:18.098374: step 160290, loss = 2.72 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 07:05:22.757901: step 160300, loss = 2.92 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 07:05:29.766526: step 160310, loss = 2.90 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 07:05:35.494714: step 160320, loss = 2.90 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 07:05:40.841883: step 160330, loss = 2.81 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 07:05:46.205408: step 160340, loss = 2.83 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 07:05:50.921132: step 160350, loss = 2.71 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 07:05:56.281496: step 160360, loss = 2.70 (184.8 examples/sec; 0.692 sec/batch)
2016-07-16 07:06:02.740463: step 160370, loss = 2.73 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 07:06:07.374998: step 160380, loss = 2.88 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 07:06:12.000549: step 160390, loss = 2.76 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 07:06:16.649824: step 160400, loss = 2.78 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 07:06:22.258017: step 160410, loss = 2.96 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 07:06:26.899703: step 160420, loss = 2.70 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 07:06:31.583080: step 160430, loss = 2.84 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 07:06:36.224643: step 160440, loss = 2.93 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 07:06:41.626256: step 160450, loss = 2.65 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 07:06:46.779432: step 160460, loss = 2.81 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 07:06:51.473395: step 160470, loss = 2.95 (255.1 examples/sec; 0.502 sec/batch)
2016-07-16 07:06:57.111513: step 160480, loss = 2.76 (178.8 examples/sec; 0.716 sec/batch)
2016-07-16 07:07:03.399338: step 160490, loss = 2.74 (253.0 examples/sec; 0.506 sec/batch)
2016-07-16 07:07:08.229367: step 160500, loss = 2.63 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 07:07:13.876869: step 160510, loss = 2.84 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 07:07:18.577811: step 160520, loss = 2.99 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 07:07:24.371585: step 160530, loss = 2.88 (260.4 examples/sec; 0.491 sec/batch)
2016-07-16 07:07:29.176733: step 160540, loss = 2.85 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 07:07:33.990036: step 160550, loss = 2.80 (252.4 examples/sec; 0.507 sec/batch)
2016-07-16 07:07:38.804020: step 160560, loss = 2.88 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 07:07:43.439124: step 160570, loss = 2.84 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 07:07:48.137825: step 160580, loss = 3.00 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 07:07:52.784441: step 160590, loss = 2.80 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 07:07:58.358768: step 160600, loss = 2.83 (200.4 examples/sec; 0.639 sec/batch)
2016-07-16 07:08:04.812746: step 160610, loss = 3.07 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 07:08:10.140465: step 160620, loss = 2.97 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 07:08:14.842337: step 160630, loss = 2.93 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 07:08:19.512589: step 160640, loss = 2.66 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 07:08:24.230441: step 160650, loss = 2.80 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 07:08:28.901984: step 160660, loss = 2.65 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 07:08:33.551266: step 160670, loss = 2.84 (283.5 examples/sec; 0.452 sec/batch)
2016-07-16 07:08:38.206709: step 160680, loss = 2.86 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 07:08:42.902851: step 160690, loss = 2.89 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 07:08:47.889404: step 160700, loss = 2.93 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 07:08:54.716840: step 160710, loss = 2.82 (256.2 examples/sec; 0.500 sec/batch)
2016-07-16 07:08:59.471915: step 160720, loss = 2.79 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 07:09:04.325598: step 160730, loss = 2.67 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 07:09:09.121403: step 160740, loss = 2.54 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 07:09:13.828887: step 160750, loss = 2.78 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 07:09:18.419310: step 160760, loss = 2.98 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 07:09:23.046906: step 160770, loss = 2.46 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 07:09:27.672655: step 160780, loss = 2.74 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 07:09:32.305503: step 160790, loss = 2.80 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 07:09:36.983181: step 160800, loss = 2.76 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 07:09:42.629513: step 160810, loss = 2.60 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 07:09:47.327454: step 160820, loss = 2.92 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 07:09:51.917309: step 160830, loss = 2.81 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 07:09:57.250730: step 160840, loss = 2.78 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 07:10:02.523476: step 160850, loss = 2.56 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 07:10:08.319634: step 160860, loss = 2.89 (256.8 examples/sec; 0.499 sec/batch)
2016-07-16 07:10:13.160322: step 160870, loss = 2.72 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 07:10:17.986112: step 160880, loss = 2.75 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 07:10:22.757337: step 160890, loss = 2.65 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 07:10:27.949676: step 160900, loss = 2.82 (186.5 examples/sec; 0.686 sec/batch)
2016-07-16 07:10:35.838215: step 160910, loss = 2.67 (257.0 examples/sec; 0.498 sec/batch)
2016-07-16 07:10:40.685888: step 160920, loss = 2.93 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 07:10:45.472826: step 160930, loss = 2.83 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 07:10:50.245893: step 160940, loss = 2.71 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 07:10:55.132428: step 160950, loss = 3.00 (243.5 examples/sec; 0.526 sec/batch)
2016-07-16 07:11:01.015319: step 160960, loss = 3.02 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 07:11:05.688174: step 160970, loss = 2.71 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 07:11:11.397104: step 160980, loss = 2.82 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 07:11:16.259178: step 160990, loss = 2.78 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 07:11:21.062354: step 161000, loss = 2.74 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 07:11:28.405232: step 161010, loss = 2.86 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 07:11:33.861150: step 161020, loss = 2.59 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 07:11:38.616791: step 161030, loss = 2.86 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 07:11:43.558899: step 161040, loss = 2.70 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 07:11:48.313726: step 161050, loss = 2.76 (256.3 examples/sec; 0.500 sec/batch)
2016-07-16 07:11:54.058220: step 161060, loss = 2.86 (190.5 examples/sec; 0.672 sec/batch)
2016-07-16 07:12:00.148766: step 161070, loss = 2.92 (253.4 examples/sec; 0.505 sec/batch)
2016-07-16 07:12:04.977016: step 161080, loss = 2.90 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 07:12:09.788889: step 161090, loss = 2.79 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 07:12:14.516974: step 161100, loss = 2.68 (272.6 examples/sec; 0.469 sec/batch)
2016-07-16 07:12:20.424303: step 161110, loss = 2.96 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 07:12:25.221613: step 161120, loss = 2.77 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 07:12:30.043507: step 161130, loss = 2.87 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 07:12:34.686163: step 161140, loss = 2.76 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 07:12:39.479519: step 161150, loss = 3.08 (218.9 examples/sec; 0.585 sec/batch)
2016-07-16 07:12:45.199368: step 161160, loss = 2.77 (241.3 examples/sec; 0.530 sec/batch)
2016-07-16 07:12:50.947737: step 161170, loss = 2.83 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 07:12:55.812912: step 161180, loss = 2.82 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 07:13:00.476604: step 161190, loss = 2.87 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 07:13:05.071976: step 161200, loss = 2.76 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 07:13:10.632042: step 161210, loss = 2.66 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 07:13:15.268459: step 161220, loss = 2.95 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 07:13:19.972291: step 161230, loss = 3.12 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 07:13:24.699679: step 161240, loss = 2.74 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 07:13:29.375441: step 161250, loss = 3.02 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 07:13:33.988753: step 161260, loss = 2.63 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 07:13:39.735005: step 161270, loss = 2.56 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 07:13:44.559134: step 161280, loss = 2.93 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 07:13:49.367242: step 161290, loss = 3.06 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 07:13:54.113251: step 161300, loss = 2.77 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 07:13:59.980123: step 161310, loss = 2.70 (269.2 examples/sec; 0.476 sec/batch)
2016-07-16 07:14:04.709746: step 161320, loss = 2.74 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 07:14:09.527299: step 161330, loss = 3.06 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 07:14:14.333184: step 161340, loss = 2.81 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 07:14:19.074244: step 161350, loss = 2.76 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 07:14:23.723398: step 161360, loss = 2.97 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 07:14:28.825651: step 161370, loss = 2.71 (200.8 examples/sec; 0.638 sec/batch)
2016-07-16 07:14:34.240786: step 161380, loss = 2.63 (253.8 examples/sec; 0.504 sec/batch)
2016-07-16 07:14:39.030003: step 161390, loss = 3.10 (268.6 examples/sec; 0.476 sec/batch)
2016-07-16 07:14:44.154696: step 161400, loss = 2.85 (189.2 examples/sec; 0.677 sec/batch)
2016-07-16 07:14:51.041951: step 161410, loss = 3.01 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 07:14:55.930837: step 161420, loss = 3.00 (202.1 examples/sec; 0.633 sec/batch)
2016-07-16 07:15:01.567188: step 161430, loss = 2.77 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 07:15:06.306062: step 161440, loss = 2.88 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 07:15:11.181534: step 161450, loss = 2.83 (252.9 examples/sec; 0.506 sec/batch)
2016-07-16 07:15:15.871669: step 161460, loss = 2.85 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 07:15:21.333344: step 161470, loss = 2.72 (190.5 examples/sec; 0.672 sec/batch)
2016-07-16 07:15:27.613896: step 161480, loss = 2.53 (255.3 examples/sec; 0.501 sec/batch)
2016-07-16 07:15:32.453899: step 161490, loss = 2.87 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 07:15:37.242222: step 161500, loss = 2.77 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 07:15:43.003520: step 161510, loss = 2.83 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 07:15:47.848659: step 161520, loss = 2.61 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 07:15:52.519306: step 161530, loss = 2.81 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 07:15:57.196356: step 161540, loss = 2.70 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 07:16:01.914396: step 161550, loss = 2.73 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 07:16:06.626717: step 161560, loss = 2.73 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 07:16:12.381261: step 161570, loss = 2.81 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 07:16:17.166165: step 161580, loss = 2.79 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 07:16:21.922601: step 161590, loss = 2.79 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 07:16:26.758832: step 161600, loss = 2.77 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 07:16:32.645805: step 161610, loss = 2.57 (268.6 examples/sec; 0.476 sec/batch)
2016-07-16 07:16:37.389598: step 161620, loss = 2.73 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 07:16:43.398079: step 161630, loss = 2.94 (193.0 examples/sec; 0.663 sec/batch)
2016-07-16 07:16:49.152324: step 161640, loss = 2.77 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 07:16:53.893754: step 161650, loss = 2.79 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 07:16:58.553826: step 161660, loss = 2.58 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 07:17:03.259108: step 161670, loss = 3.05 (254.5 examples/sec; 0.503 sec/batch)
2016-07-16 07:17:07.905521: step 161680, loss = 2.72 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 07:17:12.554711: step 161690, loss = 2.83 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 07:17:17.155907: step 161700, loss = 2.91 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 07:17:22.717779: step 161710, loss = 2.84 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 07:17:28.444547: step 161720, loss = 2.87 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 07:17:33.268222: step 161730, loss = 2.79 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 07:17:38.064531: step 161740, loss = 2.93 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 07:17:42.869338: step 161750, loss = 2.80 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 07:17:47.482933: step 161760, loss = 2.90 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 07:17:52.168960: step 161770, loss = 3.11 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 07:17:56.797005: step 161780, loss = 2.93 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 07:18:02.230614: step 161790, loss = 2.89 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 07:18:07.376062: step 161800, loss = 2.72 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 07:18:13.036998: step 161810, loss = 2.84 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 07:18:17.809399: step 161820, loss = 2.71 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 07:18:22.473477: step 161830, loss = 2.93 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 07:18:27.150173: step 161840, loss = 2.75 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 07:18:32.915497: step 161850, loss = 2.85 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 07:18:37.716488: step 161860, loss = 2.86 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 07:18:42.541494: step 161870, loss = 2.80 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 07:18:48.878290: step 161880, loss = 2.68 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 07:18:54.139436: step 161890, loss = 2.83 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 07:18:58.855984: step 161900, loss = 2.70 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 07:19:04.453234: step 161910, loss = 2.81 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 07:19:09.779742: step 161920, loss = 2.80 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 07:19:15.041078: step 161930, loss = 2.60 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 07:19:19.724183: step 161940, loss = 2.66 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 07:19:24.382428: step 161950, loss = 2.80 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 07:19:29.512589: step 161960, loss = 2.90 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 07:19:34.827027: step 161970, loss = 2.77 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 07:19:39.532715: step 161980, loss = 2.85 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 07:19:44.409996: step 161990, loss = 2.71 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 07:19:49.168128: step 162000, loss = 2.83 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 07:19:54.945005: step 162010, loss = 2.66 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 07:19:59.609518: step 162020, loss = 2.86 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 07:20:04.309814: step 162030, loss = 2.66 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 07:20:08.934121: step 162040, loss = 2.86 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 07:20:14.391958: step 162050, loss = 2.70 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 07:20:19.601137: step 162060, loss = 2.85 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 07:20:24.278547: step 162070, loss = 2.83 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 07:20:28.866464: step 162080, loss = 2.79 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 07:20:33.507649: step 162090, loss = 2.98 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 07:20:38.195091: step 162100, loss = 2.90 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 07:20:43.866263: step 162110, loss = 3.04 (222.2 examples/sec; 0.576 sec/batch)
2016-07-16 07:20:49.576463: step 162120, loss = 2.96 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 07:20:54.309458: step 162130, loss = 2.78 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 07:20:59.170934: step 162140, loss = 2.78 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 07:21:03.845961: step 162150, loss = 2.83 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 07:21:08.652382: step 162160, loss = 2.78 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 07:21:13.325936: step 162170, loss = 2.97 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 07:21:18.026027: step 162180, loss = 2.81 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 07:21:22.619051: step 162190, loss = 3.18 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 07:21:27.270509: step 162200, loss = 2.72 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 07:21:34.223249: step 162210, loss = 2.73 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 07:21:38.929388: step 162220, loss = 2.71 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 07:21:43.887573: step 162230, loss = 2.89 (217.2 examples/sec; 0.589 sec/batch)
2016-07-16 07:21:50.435061: step 162240, loss = 2.84 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 07:21:55.591716: step 162250, loss = 2.83 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 07:22:00.352398: step 162260, loss = 2.76 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 07:22:05.186405: step 162270, loss = 2.71 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 07:22:09.842625: step 162280, loss = 2.72 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 07:22:14.497624: step 162290, loss = 2.67 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 07:22:19.164265: step 162300, loss = 2.52 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 07:22:26.174750: step 162310, loss = 3.00 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 07:22:31.964192: step 162320, loss = 2.80 (253.8 examples/sec; 0.504 sec/batch)
2016-07-16 07:22:36.806199: step 162330, loss = 2.85 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 07:22:41.585262: step 162340, loss = 2.70 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 07:22:46.386221: step 162350, loss = 2.88 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 07:22:51.242479: step 162360, loss = 2.81 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 07:22:55.953756: step 162370, loss = 2.78 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 07:23:01.314810: step 162380, loss = 3.08 (186.3 examples/sec; 0.687 sec/batch)
2016-07-16 07:23:07.735330: step 162390, loss = 2.66 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 07:23:12.617724: step 162400, loss = 2.95 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 07:23:18.376039: step 162410, loss = 2.91 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 07:23:23.154778: step 162420, loss = 2.62 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 07:23:27.805517: step 162430, loss = 2.74 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 07:23:32.483453: step 162440, loss = 2.74 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 07:23:37.121124: step 162450, loss = 3.17 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 07:23:42.539294: step 162460, loss = 2.96 (199.2 examples/sec; 0.643 sec/batch)
2016-07-16 07:23:47.684808: step 162470, loss = 2.77 (257.8 examples/sec; 0.496 sec/batch)
2016-07-16 07:23:52.439433: step 162480, loss = 2.76 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 07:23:58.004457: step 162490, loss = 2.69 (188.1 examples/sec; 0.680 sec/batch)
2016-07-16 07:24:04.276391: step 162500, loss = 2.77 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 07:24:10.937430: step 162510, loss = 2.71 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 07:24:16.046911: step 162520, loss = 2.98 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 07:24:20.844295: step 162530, loss = 2.71 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 07:24:25.647956: step 162540, loss = 2.61 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 07:24:30.245963: step 162550, loss = 3.10 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 07:24:34.862680: step 162560, loss = 2.89 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 07:24:39.506823: step 162570, loss = 2.85 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 07:24:45.371898: step 162580, loss = 2.78 (236.7 examples/sec; 0.541 sec/batch)
2016-07-16 07:24:50.216337: step 162590, loss = 2.71 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 07:24:54.817551: step 162600, loss = 2.96 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 07:25:00.736717: step 162610, loss = 2.75 (206.3 examples/sec; 0.620 sec/batch)
2016-07-16 07:25:06.307047: step 162620, loss = 2.74 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 07:25:11.065671: step 162630, loss = 2.71 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 07:25:15.691072: step 162640, loss = 2.60 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 07:25:20.287102: step 162650, loss = 2.91 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 07:25:24.936917: step 162660, loss = 2.72 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 07:25:29.532780: step 162670, loss = 2.66 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 07:25:34.197338: step 162680, loss = 2.82 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 07:25:38.821308: step 162690, loss = 2.57 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 07:25:43.451193: step 162700, loss = 2.87 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 07:25:49.520650: step 162710, loss = 2.61 (202.4 examples/sec; 0.632 sec/batch)
2016-07-16 07:25:54.927153: step 162720, loss = 2.72 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 07:26:00.614939: step 162730, loss = 2.75 (285.6 examples/sec; 0.448 sec/batch)
2016-07-16 07:26:05.227733: step 162740, loss = 2.70 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 07:26:10.602888: step 162750, loss = 2.85 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 07:26:15.703430: step 162760, loss = 2.73 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 07:26:20.384846: step 162770, loss = 2.82 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 07:26:25.206296: step 162780, loss = 2.71 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 07:26:30.036393: step 162790, loss = 2.86 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 07:26:34.784519: step 162800, loss = 2.82 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 07:26:40.358279: step 162810, loss = 2.70 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 07:26:45.045989: step 162820, loss = 2.50 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 07:26:49.670819: step 162830, loss = 2.94 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 07:26:54.292560: step 162840, loss = 2.75 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 07:27:00.108508: step 162850, loss = 2.75 (226.0 examples/sec; 0.566 sec/batch)
2016-07-16 07:27:04.976637: step 162860, loss = 2.84 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 07:27:09.618036: step 162870, loss = 2.76 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 07:27:14.237187: step 162880, loss = 2.72 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 07:27:18.867114: step 162890, loss = 2.84 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 07:27:23.527065: step 162900, loss = 2.87 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 07:27:30.446463: step 162910, loss = 2.84 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 07:27:35.153818: step 162920, loss = 2.97 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 07:27:40.044233: step 162930, loss = 3.09 (234.5 examples/sec; 0.546 sec/batch)
2016-07-16 07:27:45.921798: step 162940, loss = 2.95 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 07:27:50.591086: step 162950, loss = 3.01 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 07:27:56.265294: step 162960, loss = 2.69 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 07:28:00.886146: step 162970, loss = 2.70 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 07:28:06.117965: step 162980, loss = 2.82 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 07:28:11.443362: step 162990, loss = 2.57 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 07:28:16.109900: step 163000, loss = 2.83 (268.6 examples/sec; 0.476 sec/batch)
2016-07-16 07:28:21.869845: step 163010, loss = 2.81 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 07:28:26.709047: step 163020, loss = 2.85 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 07:28:31.476961: step 163030, loss = 2.83 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 07:28:36.082143: step 163040, loss = 2.64 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 07:28:41.027350: step 163050, loss = 2.71 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 07:28:46.530040: step 163060, loss = 2.73 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 07:28:52.272790: step 163070, loss = 2.66 (245.1 examples/sec; 0.522 sec/batch)
2016-07-16 07:28:57.079272: step 163080, loss = 2.98 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 07:29:01.726915: step 163090, loss = 2.77 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 07:29:06.412590: step 163100, loss = 2.86 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 07:29:11.963704: step 163110, loss = 2.74 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 07:29:16.602798: step 163120, loss = 2.95 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 07:29:21.249037: step 163130, loss = 2.60 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 07:29:25.863303: step 163140, loss = 2.75 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 07:29:31.408260: step 163150, loss = 2.61 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 07:29:36.401977: step 163160, loss = 2.80 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 07:29:41.146396: step 163170, loss = 2.66 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 07:29:46.823471: step 163180, loss = 2.88 (187.1 examples/sec; 0.684 sec/batch)
2016-07-16 07:29:52.836458: step 163190, loss = 2.89 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 07:29:57.526338: step 163200, loss = 2.67 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 07:30:03.154935: step 163210, loss = 2.70 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 07:30:07.816986: step 163220, loss = 2.75 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 07:30:13.594117: step 163230, loss = 2.80 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 07:30:18.388974: step 163240, loss = 2.77 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 07:30:23.228160: step 163250, loss = 2.89 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 07:30:27.957058: step 163260, loss = 2.94 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 07:30:32.568169: step 163270, loss = 2.79 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 07:30:37.627889: step 163280, loss = 2.80 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 07:30:43.157899: step 163290, loss = 2.68 (249.1 examples/sec; 0.514 sec/batch)
2016-07-16 07:30:48.957355: step 163300, loss = 2.84 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 07:30:54.738048: step 163310, loss = 2.82 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 07:30:59.590022: step 163320, loss = 2.80 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 07:31:04.314670: step 163330, loss = 2.91 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 07:31:09.203144: step 163340, loss = 2.69 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 07:31:13.869650: step 163350, loss = 3.00 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 07:31:18.492970: step 163360, loss = 2.73 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 07:31:23.122085: step 163370, loss = 3.02 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 07:31:27.804711: step 163380, loss = 3.02 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 07:31:32.470055: step 163390, loss = 3.01 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 07:31:37.062086: step 163400, loss = 2.86 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 07:31:43.613268: step 163410, loss = 2.86 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 07:31:48.604368: step 163420, loss = 2.90 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 07:31:53.339840: step 163430, loss = 2.71 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 07:31:58.246310: step 163440, loss = 2.67 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 07:32:03.064821: step 163450, loss = 2.73 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 07:32:07.745064: step 163460, loss = 2.65 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 07:32:12.600372: step 163470, loss = 2.84 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 07:32:17.397989: step 163480, loss = 2.85 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 07:32:22.173876: step 163490, loss = 2.89 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 07:32:26.800139: step 163500, loss = 2.92 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 07:32:32.622109: step 163510, loss = 2.88 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 07:32:38.330662: step 163520, loss = 2.87 (248.4 examples/sec; 0.515 sec/batch)
2016-07-16 07:32:43.105684: step 163530, loss = 2.70 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 07:32:47.743802: step 163540, loss = 3.06 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 07:32:52.561324: step 163550, loss = 2.65 (219.8 examples/sec; 0.582 sec/batch)
2016-07-16 07:32:58.280890: step 163560, loss = 2.82 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 07:33:03.992742: step 163570, loss = 2.97 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 07:33:09.077216: step 163580, loss = 2.85 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 07:33:14.672684: step 163590, loss = 2.60 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 07:33:19.401785: step 163600, loss = 2.94 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 07:33:24.942127: step 163610, loss = 3.07 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 07:33:30.164347: step 163620, loss = 2.92 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 07:33:35.527853: step 163630, loss = 2.76 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 07:33:40.275573: step 163640, loss = 2.65 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 07:33:45.108050: step 163650, loss = 2.78 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 07:33:49.899306: step 163660, loss = 2.73 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 07:33:54.671283: step 163670, loss = 2.74 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 07:33:59.324857: step 163680, loss = 2.77 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 07:34:03.952329: step 163690, loss = 2.78 (278.0 examples/sec; 0.461 sec/batch)
2016-07-16 07:34:09.676514: step 163700, loss = 2.73 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 07:34:15.536160: step 163710, loss = 2.99 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 07:34:20.363605: step 163720, loss = 2.74 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 07:34:25.065425: step 163730, loss = 2.86 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 07:34:29.950302: step 163740, loss = 2.90 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 07:34:34.744946: step 163750, loss = 2.82 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 07:34:39.546713: step 163760, loss = 2.76 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 07:34:44.370587: step 163770, loss = 2.78 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 07:34:49.104357: step 163780, loss = 2.81 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 07:34:53.944168: step 163790, loss = 3.01 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 07:34:58.740090: step 163800, loss = 2.83 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 07:35:04.507340: step 163810, loss = 2.95 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 07:35:09.296443: step 163820, loss = 2.67 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 07:35:14.079125: step 163830, loss = 2.68 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 07:35:20.040328: step 163840, loss = 2.83 (186.5 examples/sec; 0.686 sec/batch)
2016-07-16 07:35:25.887705: step 163850, loss = 2.68 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 07:35:30.691726: step 163860, loss = 2.93 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 07:35:35.519793: step 163870, loss = 2.82 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 07:35:41.871886: step 163880, loss = 2.82 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 07:35:47.301750: step 163890, loss = 2.81 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 07:35:52.026472: step 163900, loss = 2.72 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 07:35:57.838048: step 163910, loss = 2.73 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 07:36:02.463747: step 163920, loss = 2.87 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 07:36:07.020510: step 163930, loss = 2.75 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 07:36:11.659421: step 163940, loss = 2.81 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 07:36:17.412302: step 163950, loss = 3.01 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 07:36:22.239328: step 163960, loss = 2.90 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 07:36:27.019074: step 163970, loss = 2.73 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 07:36:31.725946: step 163980, loss = 2.75 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 07:36:36.683291: step 163990, loss = 2.98 (220.0 examples/sec; 0.582 sec/batch)
2016-07-16 07:36:43.240518: step 164000, loss = 2.82 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 07:36:49.234690: step 164010, loss = 2.91 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 07:36:53.841073: step 164020, loss = 2.91 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 07:36:58.546712: step 164030, loss = 2.79 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 07:37:03.113744: step 164040, loss = 2.79 (286.9 examples/sec; 0.446 sec/batch)
2016-07-16 07:37:07.755050: step 164050, loss = 2.87 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 07:37:12.396733: step 164060, loss = 2.95 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 07:37:17.002542: step 164070, loss = 2.57 (283.7 examples/sec; 0.451 sec/batch)
2016-07-16 07:37:21.714537: step 164080, loss = 2.83 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 07:37:26.371261: step 164090, loss = 2.71 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 07:37:31.917162: step 164100, loss = 2.71 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 07:37:38.503949: step 164110, loss = 2.91 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 07:37:43.834128: step 164120, loss = 2.70 (256.8 examples/sec; 0.498 sec/batch)
2016-07-16 07:37:48.541394: step 164130, loss = 2.55 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 07:37:53.378749: step 164140, loss = 2.79 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 07:37:58.131085: step 164150, loss = 2.79 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 07:38:02.865445: step 164160, loss = 2.86 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 07:38:07.544603: step 164170, loss = 2.80 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 07:38:12.264590: step 164180, loss = 2.88 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 07:38:16.850905: step 164190, loss = 2.54 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 07:38:21.447699: step 164200, loss = 2.81 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 07:38:28.049517: step 164210, loss = 2.52 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 07:38:33.050474: step 164220, loss = 2.88 (248.8 examples/sec; 0.515 sec/batch)
2016-07-16 07:38:37.767952: step 164230, loss = 2.63 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 07:38:42.405624: step 164240, loss = 2.84 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 07:38:48.002601: step 164250, loss = 2.74 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 07:38:53.175257: step 164260, loss = 2.84 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 07:38:58.655734: step 164270, loss = 2.89 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 07:39:03.375138: step 164280, loss = 2.79 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 07:39:08.020172: step 164290, loss = 2.85 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 07:39:12.633318: step 164300, loss = 2.70 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 07:39:18.223313: step 164310, loss = 2.50 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 07:39:22.885444: step 164320, loss = 2.67 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 07:39:28.602727: step 164330, loss = 2.66 (201.6 examples/sec; 0.635 sec/batch)
2016-07-16 07:39:33.427914: step 164340, loss = 2.68 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 07:39:38.230214: step 164350, loss = 2.99 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 07:39:43.042551: step 164360, loss = 2.62 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 07:39:47.687130: step 164370, loss = 2.56 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 07:39:52.341069: step 164380, loss = 2.63 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 07:39:58.124068: step 164390, loss = 2.86 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 07:40:02.879673: step 164400, loss = 2.81 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 07:40:08.729406: step 164410, loss = 2.99 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 07:40:13.411326: step 164420, loss = 3.02 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 07:40:18.048420: step 164430, loss = 2.84 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 07:40:23.474876: step 164440, loss = 2.84 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 07:40:28.607922: step 164450, loss = 2.67 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 07:40:33.298427: step 164460, loss = 2.85 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 07:40:38.067400: step 164470, loss = 2.62 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 07:40:42.852618: step 164480, loss = 2.84 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 07:40:47.563756: step 164490, loss = 2.76 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 07:40:52.198576: step 164500, loss = 2.89 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 07:40:58.250897: step 164510, loss = 2.91 (209.6 examples/sec; 0.611 sec/batch)
2016-07-16 07:41:03.480686: step 164520, loss = 2.63 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 07:41:08.208595: step 164530, loss = 3.00 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 07:41:13.907823: step 164540, loss = 2.96 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 07:41:19.394969: step 164550, loss = 3.05 (208.3 examples/sec; 0.615 sec/batch)
2016-07-16 07:41:24.536997: step 164560, loss = 2.83 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 07:41:29.325008: step 164570, loss = 2.79 (257.0 examples/sec; 0.498 sec/batch)
2016-07-16 07:41:34.212573: step 164580, loss = 2.66 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 07:41:38.868236: step 164590, loss = 2.58 (279.2 examples/sec; 0.459 sec/batch)
2016-07-16 07:41:43.496471: step 164600, loss = 2.66 (276.8 examples/sec; 0.463 sec/batch)
2016-07-16 07:41:50.186682: step 164610, loss = 2.61 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 07:41:54.946818: step 164620, loss = 2.51 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 07:41:59.765409: step 164630, loss = 2.78 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 07:42:05.772299: step 164640, loss = 2.67 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 07:42:10.405503: step 164650, loss = 2.70 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 07:42:15.859823: step 164660, loss = 2.82 (207.8 examples/sec; 0.616 sec/batch)
2016-07-16 07:42:20.937286: step 164670, loss = 2.95 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 07:42:25.560244: step 164680, loss = 2.57 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 07:42:30.170631: step 164690, loss = 2.67 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 07:42:35.562134: step 164700, loss = 3.04 (207.4 examples/sec; 0.617 sec/batch)
2016-07-16 07:42:41.624273: step 164710, loss = 2.73 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 07:42:46.882680: step 164720, loss = 2.81 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 07:42:52.184513: step 164730, loss = 3.09 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 07:42:56.899632: step 164740, loss = 2.78 (249.6 examples/sec; 0.513 sec/batch)
2016-07-16 07:43:01.548649: step 164750, loss = 2.70 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 07:43:06.128161: step 164760, loss = 3.03 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 07:43:10.815953: step 164770, loss = 2.85 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 07:43:15.406959: step 164780, loss = 2.75 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 07:43:20.092445: step 164790, loss = 2.85 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 07:43:24.769684: step 164800, loss = 2.90 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 07:43:31.066435: step 164810, loss = 2.84 (202.2 examples/sec; 0.633 sec/batch)
2016-07-16 07:43:36.345636: step 164820, loss = 2.76 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 07:43:41.033818: step 164830, loss = 2.63 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 07:43:45.668354: step 164840, loss = 2.87 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 07:43:50.914570: step 164850, loss = 2.79 (205.6 examples/sec; 0.622 sec/batch)
2016-07-16 07:43:56.171308: step 164860, loss = 2.71 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 07:44:00.915391: step 164870, loss = 2.73 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 07:44:06.256456: step 164880, loss = 2.99 (185.7 examples/sec; 0.689 sec/batch)
2016-07-16 07:44:12.748109: step 164890, loss = 2.61 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 07:44:17.360591: step 164900, loss = 2.98 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 07:44:23.711559: step 164910, loss = 2.74 (202.4 examples/sec; 0.633 sec/batch)
2016-07-16 07:44:28.857150: step 164920, loss = 2.83 (251.4 examples/sec; 0.509 sec/batch)
2016-07-16 07:44:34.570516: step 164930, loss = 2.79 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 07:44:40.195320: step 164940, loss = 2.92 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 07:44:45.206978: step 164950, loss = 3.05 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 07:44:50.033088: step 164960, loss = 2.73 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 07:44:54.848621: step 164970, loss = 2.92 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 07:44:59.507528: step 164980, loss = 3.01 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 07:45:04.141170: step 164990, loss = 2.93 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 07:45:09.936920: step 165000, loss = 2.92 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 07:45:15.772965: step 165010, loss = 2.65 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 07:45:20.385913: step 165020, loss = 2.89 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 07:45:25.126926: step 165030, loss = 2.73 (237.0 examples/sec; 0.540 sec/batch)
2016-07-16 07:45:30.890800: step 165040, loss = 2.74 (244.1 examples/sec; 0.524 sec/batch)
2016-07-16 07:45:35.601246: step 165050, loss = 2.82 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 07:45:40.419240: step 165060, loss = 2.87 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 07:45:46.850941: step 165070, loss = 2.85 (207.3 examples/sec; 0.617 sec/batch)
2016-07-16 07:45:52.011819: step 165080, loss = 2.90 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 07:45:56.709721: step 165090, loss = 2.92 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 07:46:02.550125: step 165100, loss = 2.59 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 07:46:08.377574: step 165110, loss = 3.00 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 07:46:13.004839: step 165120, loss = 2.87 (286.7 examples/sec; 0.447 sec/batch)
2016-07-16 07:46:17.736768: step 165130, loss = 2.61 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 07:46:22.366437: step 165140, loss = 2.67 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 07:46:27.041757: step 165150, loss = 3.05 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 07:46:31.762024: step 165160, loss = 2.83 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 07:46:36.395860: step 165170, loss = 2.81 (284.6 examples/sec; 0.450 sec/batch)
2016-07-16 07:46:41.072224: step 165180, loss = 2.87 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 07:46:45.724954: step 165190, loss = 2.71 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 07:46:50.345075: step 165200, loss = 2.64 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 07:46:57.044751: step 165210, loss = 2.72 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 07:47:01.846404: step 165220, loss = 3.06 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 07:47:06.563041: step 165230, loss = 2.71 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 07:47:12.393102: step 165240, loss = 2.66 (186.9 examples/sec; 0.685 sec/batch)
2016-07-16 07:47:18.309399: step 165250, loss = 2.87 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 07:47:23.149456: step 165260, loss = 2.70 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 07:47:27.958141: step 165270, loss = 2.82 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 07:47:32.692654: step 165280, loss = 2.72 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 07:47:37.546556: step 165290, loss = 2.65 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 07:47:42.233824: step 165300, loss = 2.70 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 07:47:47.804319: step 165310, loss = 2.75 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 07:47:53.546251: step 165320, loss = 2.88 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 07:47:58.389833: step 165330, loss = 2.81 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 07:48:03.252918: step 165340, loss = 2.85 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 07:48:09.359787: step 165350, loss = 2.54 (196.8 examples/sec; 0.650 sec/batch)
2016-07-16 07:48:15.085380: step 165360, loss = 2.79 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 07:48:19.823504: step 165370, loss = 3.09 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 07:48:24.632692: step 165380, loss = 2.76 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 07:48:29.358301: step 165390, loss = 2.84 (252.8 examples/sec; 0.506 sec/batch)
2016-07-16 07:48:34.201120: step 165400, loss = 3.12 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 07:48:40.072432: step 165410, loss = 2.75 (253.7 examples/sec; 0.505 sec/batch)
2016-07-16 07:48:44.784472: step 165420, loss = 2.90 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 07:48:49.626144: step 165430, loss = 2.85 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 07:48:54.307965: step 165440, loss = 2.80 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 07:48:58.893745: step 165450, loss = 2.81 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 07:49:04.488761: step 165460, loss = 2.79 (202.0 examples/sec; 0.634 sec/batch)
2016-07-16 07:49:09.529181: step 165470, loss = 2.76 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 07:49:14.258129: step 165480, loss = 2.91 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 07:49:19.042554: step 165490, loss = 2.80 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 07:49:23.704408: step 165500, loss = 2.95 (283.4 examples/sec; 0.452 sec/batch)
2016-07-16 07:49:29.360571: step 165510, loss = 2.81 (236.0 examples/sec; 0.542 sec/batch)
2016-07-16 07:49:35.082580: step 165520, loss = 2.73 (258.3 examples/sec; 0.495 sec/batch)
2016-07-16 07:49:39.880441: step 165530, loss = 2.61 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 07:49:44.744924: step 165540, loss = 2.65 (242.9 examples/sec; 0.527 sec/batch)
2016-07-16 07:49:51.196192: step 165550, loss = 2.75 (208.3 examples/sec; 0.615 sec/batch)
2016-07-16 07:49:56.305534: step 165560, loss = 2.84 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 07:50:00.976288: step 165570, loss = 2.98 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 07:50:05.591515: step 165580, loss = 2.85 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 07:50:10.690015: step 165590, loss = 2.65 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 07:50:16.112562: step 165600, loss = 2.84 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 07:50:21.891056: step 165610, loss = 2.63 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 07:50:27.452445: step 165620, loss = 2.71 (191.5 examples/sec; 0.668 sec/batch)
2016-07-16 07:50:33.734869: step 165630, loss = 2.59 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 07:50:38.589174: step 165640, loss = 2.82 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 07:50:43.276118: step 165650, loss = 2.91 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 07:50:47.887890: step 165660, loss = 2.87 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 07:50:52.563621: step 165670, loss = 2.92 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 07:50:57.145297: step 165680, loss = 2.79 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 07:51:02.031642: step 165690, loss = 2.93 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 07:51:07.648138: step 165700, loss = 2.66 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 07:51:14.485298: step 165710, loss = 2.97 (262.6 examples/sec; 0.488 sec/batch)
2016-07-16 07:51:20.003532: step 165720, loss = 2.80 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 07:51:25.184130: step 165730, loss = 2.74 (255.1 examples/sec; 0.502 sec/batch)
2016-07-16 07:51:29.918680: step 165740, loss = 2.62 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 07:51:34.553519: step 165750, loss = 2.44 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 07:51:39.209047: step 165760, loss = 2.66 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 07:51:43.856990: step 165770, loss = 2.91 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 07:51:48.530629: step 165780, loss = 2.61 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 07:51:54.319854: step 165790, loss = 2.83 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 07:51:59.129136: step 165800, loss = 2.88 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 07:52:04.948782: step 165810, loss = 2.64 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 07:52:11.412371: step 165820, loss = 2.87 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 07:52:16.768296: step 165830, loss = 2.94 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 07:52:21.460584: step 165840, loss = 2.59 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 07:52:26.328649: step 165850, loss = 2.62 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 07:52:30.996615: step 165860, loss = 3.11 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 07:52:35.613910: step 165870, loss = 2.75 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 07:52:41.333836: step 165880, loss = 2.91 (209.9 examples/sec; 0.610 sec/batch)
2016-07-16 07:52:46.202540: step 165890, loss = 2.64 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 07:52:50.982393: step 165900, loss = 2.66 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 07:52:58.038218: step 165910, loss = 2.69 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 07:53:02.660577: step 165920, loss = 3.03 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 07:53:07.303991: step 165930, loss = 2.91 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 07:53:11.949416: step 165940, loss = 2.80 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 07:53:16.618646: step 165950, loss = 2.77 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 07:53:21.241227: step 165960, loss = 2.83 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 07:53:25.900740: step 165970, loss = 2.80 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 07:53:31.054041: step 165980, loss = 2.66 (200.8 examples/sec; 0.637 sec/batch)
2016-07-16 07:53:36.493245: step 165990, loss = 2.91 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 07:53:42.311153: step 166000, loss = 2.62 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 07:53:49.118688: step 166010, loss = 2.64 (200.7 examples/sec; 0.638 sec/batch)
2016-07-16 07:53:53.787895: step 166020, loss = 2.68 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 07:53:58.918344: step 166030, loss = 2.81 (200.8 examples/sec; 0.638 sec/batch)
2016-07-16 07:54:04.370617: step 166040, loss = 2.98 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 07:54:09.108618: step 166050, loss = 2.67 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 07:54:13.988397: step 166060, loss = 2.83 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 07:54:18.677396: step 166070, loss = 2.71 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 07:54:23.290659: step 166080, loss = 2.93 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 07:54:28.892389: step 166090, loss = 2.69 (204.3 examples/sec; 0.627 sec/batch)
2016-07-16 07:54:33.638751: step 166100, loss = 2.65 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 07:54:39.887191: step 166110, loss = 2.81 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 07:54:45.081408: step 166120, loss = 2.68 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 07:54:50.856837: step 166130, loss = 2.82 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 07:54:55.630507: step 166140, loss = 2.97 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 07:55:00.391785: step 166150, loss = 2.97 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 07:55:05.123021: step 166160, loss = 2.89 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 07:55:09.961924: step 166170, loss = 2.75 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 07:55:14.658061: step 166180, loss = 3.14 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 07:55:20.395961: step 166190, loss = 2.88 (185.9 examples/sec; 0.688 sec/batch)
2016-07-16 07:55:26.454736: step 166200, loss = 3.04 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 07:55:32.294936: step 166210, loss = 2.86 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 07:55:37.111438: step 166220, loss = 2.77 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 07:55:43.643451: step 166230, loss = 2.84 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 07:55:48.912440: step 166240, loss = 3.00 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 07:55:53.625586: step 166250, loss = 2.83 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 07:55:58.509547: step 166260, loss = 2.59 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 07:56:03.205039: step 166270, loss = 2.73 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 07:56:07.810372: step 166280, loss = 2.78 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 07:56:13.518915: step 166290, loss = 2.79 (205.6 examples/sec; 0.622 sec/batch)
2016-07-16 07:56:18.352013: step 166300, loss = 2.89 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 07:56:24.121244: step 166310, loss = 2.96 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 07:56:28.837752: step 166320, loss = 2.87 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 07:56:33.755788: step 166330, loss = 2.84 (219.8 examples/sec; 0.582 sec/batch)
2016-07-16 07:56:40.283637: step 166340, loss = 2.92 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 07:56:45.474279: step 166350, loss = 2.71 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 07:56:50.151514: step 166360, loss = 2.64 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 07:56:54.956447: step 166370, loss = 2.63 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 07:56:59.605292: step 166380, loss = 2.75 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 07:57:04.274469: step 166390, loss = 2.69 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 07:57:08.954516: step 166400, loss = 2.96 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 07:57:15.846453: step 166410, loss = 2.98 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 07:57:20.587001: step 166420, loss = 2.68 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 07:57:25.398351: step 166430, loss = 2.80 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 07:57:30.091341: step 166440, loss = 2.66 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 07:57:34.749455: step 166450, loss = 2.58 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 07:57:40.053994: step 166460, loss = 2.67 (207.3 examples/sec; 0.618 sec/batch)
2016-07-16 07:57:45.310373: step 166470, loss = 2.69 (255.7 examples/sec; 0.501 sec/batch)
2016-07-16 07:57:49.994378: step 166480, loss = 2.69 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 07:57:54.668082: step 166490, loss = 2.59 (251.4 examples/sec; 0.509 sec/batch)
2016-07-16 07:57:59.366041: step 166500, loss = 2.87 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 07:58:04.965174: step 166510, loss = 2.71 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 07:58:10.746387: step 166520, loss = 2.86 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 07:58:16.202952: step 166530, loss = 2.99 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 07:58:21.184945: step 166540, loss = 2.66 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 07:58:25.999475: step 166550, loss = 2.78 (214.3 examples/sec; 0.597 sec/batch)
2016-07-16 07:58:31.592183: step 166560, loss = 3.15 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 07:58:36.245720: step 166570, loss = 2.58 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 07:58:41.914596: step 166580, loss = 2.94 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 07:58:46.548969: step 166590, loss = 2.92 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 07:58:51.819218: step 166600, loss = 2.73 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 07:58:58.116732: step 166610, loss = 2.74 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 07:59:03.248474: step 166620, loss = 2.73 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 07:59:08.653620: step 166630, loss = 2.79 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 07:59:13.392124: step 166640, loss = 2.85 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 07:59:18.566375: step 166650, loss = 2.75 (183.9 examples/sec; 0.696 sec/batch)
2016-07-16 07:59:25.099860: step 166660, loss = 2.90 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 07:59:30.279981: step 166670, loss = 2.93 (208.3 examples/sec; 0.614 sec/batch)
2016-07-16 07:59:35.878800: step 166680, loss = 2.84 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 07:59:40.686847: step 166690, loss = 2.88 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 07:59:45.311175: step 166700, loss = 2.76 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 07:59:50.927852: step 166710, loss = 2.91 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 07:59:55.597630: step 166720, loss = 2.74 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 08:00:00.239947: step 166730, loss = 2.98 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 08:00:04.843340: step 166740, loss = 2.83 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 08:00:09.527825: step 166750, loss = 2.78 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 08:00:14.125104: step 166760, loss = 2.88 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 08:00:19.199797: step 166770, loss = 2.68 (197.3 examples/sec; 0.649 sec/batch)
2016-07-16 08:00:24.693922: step 166780, loss = 2.73 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 08:00:29.400954: step 166790, loss = 3.05 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 08:00:34.241927: step 166800, loss = 2.77 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 08:00:39.955912: step 166810, loss = 2.88 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 08:00:44.590040: step 166820, loss = 2.79 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 08:00:49.240597: step 166830, loss = 2.99 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 08:00:54.056682: step 166840, loss = 2.80 (217.6 examples/sec; 0.588 sec/batch)
2016-07-16 08:00:59.752572: step 166850, loss = 2.84 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 08:01:04.562687: step 166860, loss = 2.77 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 08:01:09.390848: step 166870, loss = 2.80 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 08:01:14.070896: step 166880, loss = 3.14 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 08:01:18.722961: step 166890, loss = 2.75 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 08:01:24.003182: step 166900, loss = 2.65 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 08:01:30.443547: step 166910, loss = 2.53 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 08:01:35.087484: step 166920, loss = 2.66 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 08:01:39.746645: step 166930, loss = 2.78 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 08:01:44.469079: step 166940, loss = 2.77 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 08:01:49.129547: step 166950, loss = 2.87 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 08:01:54.863694: step 166960, loss = 2.97 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 08:02:00.312800: step 166970, loss = 2.81 (196.6 examples/sec; 0.651 sec/batch)
2016-07-16 08:02:05.494484: step 166980, loss = 2.86 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 08:02:11.267096: step 166990, loss = 2.88 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 08:02:16.099721: step 167000, loss = 2.92 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 08:02:21.869624: step 167010, loss = 2.76 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 08:02:26.541274: step 167020, loss = 3.07 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 08:02:31.183788: step 167030, loss = 2.67 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 08:02:36.588218: step 167040, loss = 2.98 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 08:02:41.758830: step 167050, loss = 2.77 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 08:02:46.440651: step 167060, loss = 2.77 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 08:02:51.900722: step 167070, loss = 2.87 (187.2 examples/sec; 0.684 sec/batch)
2016-07-16 08:02:58.180474: step 167080, loss = 3.02 (251.0 examples/sec; 0.510 sec/batch)
2016-07-16 08:03:03.004616: step 167090, loss = 2.74 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 08:03:07.833411: step 167100, loss = 2.77 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 08:03:15.220117: step 167110, loss = 2.71 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 08:03:20.526543: step 167120, loss = 2.85 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 08:03:25.184137: step 167130, loss = 2.81 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 08:03:29.768062: step 167140, loss = 2.52 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 08:03:34.440645: step 167150, loss = 2.86 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 08:03:39.559982: step 167160, loss = 2.56 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 08:03:44.964318: step 167170, loss = 2.69 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 08:03:50.694246: step 167180, loss = 2.67 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 08:03:56.081440: step 167190, loss = 2.82 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 08:04:01.387977: step 167200, loss = 2.72 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 08:04:08.192727: step 167210, loss = 2.72 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 08:04:12.932524: step 167220, loss = 2.84 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 08:04:17.560812: step 167230, loss = 2.64 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 08:04:22.208886: step 167240, loss = 2.86 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 08:04:26.765191: step 167250, loss = 2.77 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 08:04:31.386148: step 167260, loss = 2.68 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 08:04:36.816880: step 167270, loss = 2.84 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 08:04:41.677581: step 167280, loss = 2.96 (283.5 examples/sec; 0.452 sec/batch)
2016-07-16 08:04:46.531468: step 167290, loss = 2.90 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 08:04:52.139031: step 167300, loss = 2.78 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 08:04:57.932417: step 167310, loss = 2.91 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 08:05:02.834626: step 167320, loss = 2.88 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 08:05:07.595882: step 167330, loss = 2.69 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 08:05:13.443174: step 167340, loss = 3.09 (182.8 examples/sec; 0.700 sec/batch)
2016-07-16 08:05:19.249046: step 167350, loss = 2.79 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 08:05:23.892111: step 167360, loss = 2.98 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 08:05:28.566449: step 167370, loss = 2.93 (264.7 examples/sec; 0.483 sec/batch)
2016-07-16 08:05:33.259829: step 167380, loss = 2.91 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 08:05:39.046149: step 167390, loss = 2.89 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 08:05:44.529949: step 167400, loss = 2.75 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 08:05:50.612616: step 167410, loss = 2.84 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 08:05:55.264987: step 167420, loss = 2.65 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 08:05:59.958256: step 167430, loss = 2.82 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 08:06:05.555178: step 167440, loss = 2.74 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 08:06:10.704052: step 167450, loss = 2.95 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 08:06:16.321258: step 167460, loss = 2.86 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 08:06:21.110644: step 167470, loss = 2.90 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 08:06:25.984505: step 167480, loss = 2.71 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 08:06:30.703728: step 167490, loss = 2.68 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 08:06:35.536465: step 167500, loss = 2.97 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 08:06:41.192187: step 167510, loss = 2.70 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 08:06:45.858811: step 167520, loss = 2.79 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 08:06:50.501167: step 167530, loss = 2.70 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 08:06:55.528960: step 167540, loss = 2.74 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 08:07:00.956850: step 167550, loss = 2.78 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 08:07:05.728445: step 167560, loss = 2.87 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 08:07:10.643063: step 167570, loss = 2.46 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 08:07:15.321563: step 167580, loss = 2.65 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 08:07:19.997944: step 167590, loss = 2.66 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 08:07:25.592639: step 167600, loss = 2.90 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 08:07:32.145507: step 167610, loss = 2.68 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 08:07:37.380446: step 167620, loss = 2.68 (251.8 examples/sec; 0.508 sec/batch)
2016-07-16 08:07:42.073439: step 167630, loss = 2.69 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 08:07:46.688337: step 167640, loss = 2.62 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 08:07:51.313231: step 167650, loss = 2.65 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 08:07:56.028387: step 167660, loss = 2.89 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 08:08:00.674492: step 167670, loss = 2.56 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 08:08:05.283172: step 167680, loss = 2.82 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 08:08:09.878233: step 167690, loss = 2.89 (287.2 examples/sec; 0.446 sec/batch)
2016-07-16 08:08:14.548511: step 167700, loss = 2.89 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 08:08:20.049091: step 167710, loss = 2.67 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 08:08:25.652495: step 167720, loss = 3.09 (208.0 examples/sec; 0.615 sec/batch)
2016-07-16 08:08:30.617909: step 167730, loss = 2.60 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 08:08:35.335284: step 167740, loss = 2.94 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 08:08:40.196901: step 167750, loss = 3.06 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 08:08:44.807659: step 167760, loss = 2.76 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 08:08:49.473800: step 167770, loss = 2.72 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 08:08:54.112173: step 167780, loss = 2.68 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 08:08:58.775911: step 167790, loss = 2.81 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 08:09:03.488276: step 167800, loss = 2.99 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 08:09:09.043048: step 167810, loss = 2.72 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 08:09:14.737250: step 167820, loss = 2.85 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 08:09:19.524633: step 167830, loss = 2.82 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 08:09:24.254768: step 167840, loss = 2.73 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 08:09:30.251927: step 167850, loss = 2.87 (191.0 examples/sec; 0.670 sec/batch)
2016-07-16 08:09:35.970347: step 167860, loss = 2.78 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 08:09:40.780752: step 167870, loss = 2.48 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 08:09:45.431174: step 167880, loss = 2.74 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 08:09:50.118338: step 167890, loss = 2.94 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 08:09:54.720169: step 167900, loss = 2.72 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 08:10:01.163362: step 167910, loss = 2.81 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 08:10:06.322515: step 167920, loss = 2.94 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 08:10:11.003667: step 167930, loss = 2.66 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 08:10:15.656351: step 167940, loss = 2.63 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 08:10:21.071802: step 167950, loss = 2.87 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 08:10:26.266708: step 167960, loss = 2.78 (252.6 examples/sec; 0.507 sec/batch)
2016-07-16 08:10:31.009467: step 167970, loss = 2.78 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 08:10:35.860622: step 167980, loss = 2.74 (282.9 examples/sec; 0.452 sec/batch)
2016-07-16 08:10:40.488203: step 167990, loss = 2.75 (287.7 examples/sec; 0.445 sec/batch)
2016-07-16 08:10:45.163943: step 168000, loss = 2.74 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 08:10:50.815852: step 168010, loss = 2.73 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 08:10:55.972103: step 168020, loss = 2.77 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 08:11:01.367251: step 168030, loss = 2.91 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 08:11:07.103819: step 168040, loss = 3.12 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 08:11:11.760972: step 168050, loss = 2.76 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 08:11:17.209589: step 168060, loss = 2.65 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 08:11:22.321143: step 168070, loss = 2.84 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 08:11:28.097266: step 168080, loss = 2.63 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 08:11:32.863323: step 168090, loss = 2.74 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 08:11:37.558826: step 168100, loss = 2.64 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 08:11:43.108198: step 168110, loss = 2.87 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 08:11:47.737214: step 168120, loss = 2.94 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 08:11:52.407908: step 168130, loss = 2.74 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 08:11:56.991692: step 168140, loss = 2.88 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 08:12:01.699674: step 168150, loss = 2.90 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 08:12:06.314468: step 168160, loss = 2.85 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 08:12:11.177797: step 168170, loss = 2.49 (201.4 examples/sec; 0.636 sec/batch)
2016-07-16 08:12:16.767229: step 168180, loss = 2.74 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 08:12:21.574589: step 168190, loss = 2.75 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 08:12:26.492021: step 168200, loss = 2.66 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 08:12:32.135703: step 168210, loss = 2.68 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 08:12:36.752256: step 168220, loss = 2.80 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 08:12:41.431556: step 168230, loss = 2.83 (284.6 examples/sec; 0.450 sec/batch)
2016-07-16 08:12:46.161079: step 168240, loss = 3.00 (252.0 examples/sec; 0.508 sec/batch)
2016-07-16 08:12:50.781748: step 168250, loss = 2.63 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 08:12:55.408155: step 168260, loss = 2.88 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 08:13:00.710708: step 168270, loss = 2.95 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 08:13:05.979750: step 168280, loss = 2.68 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 08:13:10.720714: step 168290, loss = 2.75 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 08:13:16.092702: step 168300, loss = 2.90 (190.0 examples/sec; 0.674 sec/batch)
2016-07-16 08:13:23.697554: step 168310, loss = 2.81 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 08:13:28.333381: step 168320, loss = 2.78 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 08:13:33.004787: step 168330, loss = 2.74 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 08:13:37.698381: step 168340, loss = 2.79 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 08:13:43.462586: step 168350, loss = 2.64 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 08:13:48.253410: step 168360, loss = 2.83 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 08:13:53.030959: step 168370, loss = 2.71 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 08:13:59.169729: step 168380, loss = 2.52 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 08:14:04.737463: step 168390, loss = 2.88 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 08:14:09.467639: step 168400, loss = 2.71 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 08:14:15.037671: step 168410, loss = 2.77 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 08:14:19.666825: step 168420, loss = 2.66 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 08:14:24.326640: step 168430, loss = 2.78 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 08:14:29.855752: step 168440, loss = 2.83 (207.6 examples/sec; 0.616 sec/batch)
2016-07-16 08:14:34.857549: step 168450, loss = 2.79 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 08:14:39.569715: step 168460, loss = 2.75 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 08:14:44.341693: step 168470, loss = 2.74 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 08:14:49.092929: step 168480, loss = 2.90 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 08:14:53.871168: step 168490, loss = 2.67 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 08:14:58.558807: step 168500, loss = 2.99 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 08:15:04.170000: step 168510, loss = 2.65 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 08:15:08.884324: step 168520, loss = 2.98 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 08:15:14.610170: step 168530, loss = 2.84 (222.0 examples/sec; 0.577 sec/batch)
2016-07-16 08:15:19.821986: step 168540, loss = 2.87 (200.9 examples/sec; 0.637 sec/batch)
2016-07-16 08:15:25.273908: step 168550, loss = 2.91 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 08:15:30.053462: step 168560, loss = 2.73 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 08:15:34.907710: step 168570, loss = 2.80 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 08:15:39.699157: step 168580, loss = 2.70 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 08:15:44.481907: step 168590, loss = 2.62 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 08:15:49.254605: step 168600, loss = 2.65 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 08:15:54.932820: step 168610, loss = 2.88 (287.3 examples/sec; 0.446 sec/batch)
2016-07-16 08:15:59.592568: step 168620, loss = 2.81 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 08:16:04.879573: step 168630, loss = 2.98 (202.4 examples/sec; 0.632 sec/batch)
2016-07-16 08:16:10.127427: step 168640, loss = 2.85 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 08:16:15.891902: step 168650, loss = 2.74 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 08:16:20.637468: step 168660, loss = 2.92 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 08:16:25.278823: step 168670, loss = 2.65 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 08:16:29.854991: step 168680, loss = 2.55 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 08:16:34.530130: step 168690, loss = 2.56 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 08:16:39.156990: step 168700, loss = 2.83 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 08:16:44.781875: step 168710, loss = 2.77 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 08:16:49.391343: step 168720, loss = 2.93 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 08:16:55.121663: step 168730, loss = 2.44 (253.6 examples/sec; 0.505 sec/batch)
2016-07-16 08:16:59.968150: step 168740, loss = 2.52 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 08:17:04.773423: step 168750, loss = 2.66 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 08:17:09.554166: step 168760, loss = 2.91 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 08:17:14.383961: step 168770, loss = 2.65 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 08:17:19.068273: step 168780, loss = 2.61 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 08:17:23.706844: step 168790, loss = 2.74 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 08:17:28.377884: step 168800, loss = 2.90 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 08:17:34.042868: step 168810, loss = 2.75 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 08:17:38.692875: step 168820, loss = 2.85 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 08:17:43.604323: step 168830, loss = 2.55 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 08:17:49.197691: step 168840, loss = 2.80 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 08:17:54.976628: step 168850, loss = 2.86 (240.2 examples/sec; 0.533 sec/batch)
2016-07-16 08:17:59.832516: step 168860, loss = 2.79 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 08:18:04.486062: step 168870, loss = 2.73 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 08:18:09.177362: step 168880, loss = 2.80 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 08:18:13.819479: step 168890, loss = 2.81 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 08:18:18.529531: step 168900, loss = 2.56 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 08:18:25.350297: step 168910, loss = 2.88 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 08:18:30.054760: step 168920, loss = 2.85 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 08:18:35.800993: step 168930, loss = 2.70 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 08:18:40.610290: step 168940, loss = 2.66 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 08:18:45.460720: step 168950, loss = 2.75 (256.2 examples/sec; 0.500 sec/batch)
2016-07-16 08:18:51.663001: step 168960, loss = 2.83 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 08:18:57.260846: step 168970, loss = 2.70 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 08:19:02.043136: step 168980, loss = 2.90 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 08:19:07.050412: step 168990, loss = 3.08 (217.9 examples/sec; 0.587 sec/batch)
2016-07-16 08:19:13.568815: step 169000, loss = 2.86 (211.2 examples/sec; 0.606 sec/batch)
2016-07-16 08:19:19.689640: step 169010, loss = 2.77 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 08:19:24.887109: step 169020, loss = 2.71 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 08:19:30.205281: step 169030, loss = 2.63 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 08:19:34.859516: step 169040, loss = 2.80 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 08:19:39.491925: step 169050, loss = 2.91 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 08:19:44.141228: step 169060, loss = 2.83 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 08:19:48.784997: step 169070, loss = 2.64 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 08:19:54.379834: step 169080, loss = 2.57 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 08:19:59.369635: step 169090, loss = 2.66 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 08:20:03.988238: step 169100, loss = 2.77 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 08:20:09.627469: step 169110, loss = 2.88 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 08:20:15.351701: step 169120, loss = 2.78 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 08:20:19.982763: step 169130, loss = 2.77 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 08:20:24.682278: step 169140, loss = 2.73 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 08:20:29.342851: step 169150, loss = 2.66 (253.0 examples/sec; 0.506 sec/batch)
2016-07-16 08:20:33.997698: step 169160, loss = 2.76 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 08:20:38.611037: step 169170, loss = 2.85 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 08:20:44.389458: step 169180, loss = 2.61 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 08:20:49.163646: step 169190, loss = 2.56 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 08:20:54.010167: step 169200, loss = 2.90 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 08:20:59.667745: step 169210, loss = 2.68 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 08:21:04.311185: step 169220, loss = 2.58 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 08:21:09.029481: step 169230, loss = 2.73 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 08:21:13.709487: step 169240, loss = 2.61 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 08:21:19.433636: step 169250, loss = 2.77 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 08:21:24.240492: step 169260, loss = 2.67 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 08:21:28.990722: step 169270, loss = 2.67 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 08:21:33.775448: step 169280, loss = 2.61 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 08:21:38.388000: step 169290, loss = 2.87 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 08:21:43.131731: step 169300, loss = 2.84 (237.2 examples/sec; 0.540 sec/batch)
2016-07-16 08:21:50.151729: step 169310, loss = 2.86 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 08:21:55.918238: step 169320, loss = 2.78 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 08:22:00.753430: step 169330, loss = 2.55 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 08:22:05.556573: step 169340, loss = 2.71 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 08:22:11.823143: step 169350, loss = 2.57 (202.1 examples/sec; 0.633 sec/batch)
2016-07-16 08:22:17.393368: step 169360, loss = 2.70 (251.2 examples/sec; 0.510 sec/batch)
2016-07-16 08:22:22.183250: step 169370, loss = 2.91 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 08:22:27.026608: step 169380, loss = 2.81 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 08:22:31.683598: step 169390, loss = 3.00 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 08:22:36.338103: step 169400, loss = 3.07 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 08:22:41.993643: step 169410, loss = 2.65 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 08:22:46.671639: step 169420, loss = 2.74 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 08:22:52.438445: step 169430, loss = 2.92 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 08:22:57.279697: step 169440, loss = 2.75 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 08:23:02.153022: step 169450, loss = 2.68 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 08:23:06.843150: step 169460, loss = 2.86 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 08:23:11.469324: step 169470, loss = 2.62 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 08:23:16.539576: step 169480, loss = 2.75 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 08:23:21.977714: step 169490, loss = 2.77 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 08:23:26.685858: step 169500, loss = 2.79 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 08:23:32.569377: step 169510, loss = 2.70 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 08:23:37.212730: step 169520, loss = 2.75 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 08:23:41.874844: step 169530, loss = 2.83 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 08:23:46.639059: step 169540, loss = 2.65 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 08:23:51.302946: step 169550, loss = 2.85 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 08:23:55.883443: step 169560, loss = 2.81 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 08:24:00.557980: step 169570, loss = 2.76 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 08:24:05.176777: step 169580, loss = 2.67 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 08:24:09.859509: step 169590, loss = 2.59 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 08:24:15.571626: step 169600, loss = 2.77 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 08:24:21.437181: step 169610, loss = 2.96 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 08:24:26.093178: step 169620, loss = 2.91 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 08:24:30.715179: step 169630, loss = 2.68 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 08:24:36.456930: step 169640, loss = 2.87 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 08:24:41.237144: step 169650, loss = 2.75 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 08:24:46.071553: step 169660, loss = 2.95 (251.4 examples/sec; 0.509 sec/batch)
2016-07-16 08:24:50.770029: step 169670, loss = 3.03 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 08:24:55.625037: step 169680, loss = 2.78 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 08:25:00.322705: step 169690, loss = 2.81 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 08:25:05.103909: step 169700, loss = 2.89 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 08:25:10.663510: step 169710, loss = 2.84 (285.8 examples/sec; 0.448 sec/batch)
2016-07-16 08:25:15.491732: step 169720, loss = 2.79 (210.2 examples/sec; 0.609 sec/batch)
2016-07-16 08:25:21.134540: step 169730, loss = 2.78 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 08:25:25.925350: step 169740, loss = 2.59 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 08:25:30.721835: step 169750, loss = 2.59 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 08:25:35.381447: step 169760, loss = 2.66 (286.8 examples/sec; 0.446 sec/batch)
2016-07-16 08:25:40.044843: step 169770, loss = 2.79 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 08:25:45.316535: step 169780, loss = 2.90 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 08:25:50.595824: step 169790, loss = 2.90 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 08:25:55.282696: step 169800, loss = 2.79 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 08:26:02.034416: step 169810, loss = 2.69 (186.4 examples/sec; 0.687 sec/batch)
2016-07-16 08:26:08.190312: step 169820, loss = 2.91 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 08:26:13.655027: step 169830, loss = 2.84 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 08:26:18.705926: step 169840, loss = 2.47 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 08:26:23.396336: step 169850, loss = 2.59 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 08:26:27.952617: step 169860, loss = 2.86 (291.7 examples/sec; 0.439 sec/batch)
2016-07-16 08:26:32.625929: step 169870, loss = 2.92 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 08:26:37.319938: step 169880, loss = 2.85 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 08:26:41.960416: step 169890, loss = 2.72 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 08:26:47.732836: step 169900, loss = 2.66 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 08:26:53.554419: step 169910, loss = 2.68 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 08:26:58.132112: step 169920, loss = 2.77 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 08:27:02.780240: step 169930, loss = 2.72 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 08:27:07.457099: step 169940, loss = 2.71 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 08:27:12.123270: step 169950, loss = 2.60 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 08:27:17.361402: step 169960, loss = 2.91 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 08:27:22.684512: step 169970, loss = 2.62 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 08:27:27.347980: step 169980, loss = 2.85 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 08:27:32.187604: step 169990, loss = 2.88 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 08:27:36.869679: step 170000, loss = 2.70 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 08:27:42.522015: step 170010, loss = 2.83 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 08:27:48.250102: step 170020, loss = 2.71 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 08:27:53.043496: step 170030, loss = 2.87 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 08:27:57.852847: step 170040, loss = 2.98 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 08:28:02.618790: step 170050, loss = 2.78 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 08:28:07.247896: step 170060, loss = 2.72 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 08:28:12.177850: step 170070, loss = 2.99 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 08:28:17.710257: step 170080, loss = 2.85 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 08:28:22.520954: step 170090, loss = 2.79 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 08:28:27.231312: step 170100, loss = 2.98 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 08:28:33.491633: step 170110, loss = 2.82 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 08:28:38.537071: step 170120, loss = 2.85 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 08:28:43.229657: step 170130, loss = 2.72 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 08:28:49.015847: step 170140, loss = 2.76 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 08:28:53.801678: step 170150, loss = 3.13 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 08:28:58.444241: step 170160, loss = 2.92 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 08:29:03.133706: step 170170, loss = 2.78 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 08:29:07.740943: step 170180, loss = 2.92 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 08:29:12.923050: step 170190, loss = 2.77 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 08:29:18.317175: step 170200, loss = 2.74 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 08:29:24.061108: step 170210, loss = 2.95 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 08:29:29.601590: step 170220, loss = 2.84 (184.5 examples/sec; 0.694 sec/batch)
2016-07-16 08:29:35.829104: step 170230, loss = 2.64 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 08:29:40.674691: step 170240, loss = 2.62 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 08:29:45.441944: step 170250, loss = 2.62 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 08:29:50.217255: step 170260, loss = 2.79 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 08:29:55.088852: step 170270, loss = 2.71 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 08:29:59.731892: step 170280, loss = 2.82 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 08:30:04.318489: step 170290, loss = 2.73 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 08:30:08.975255: step 170300, loss = 2.68 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 08:30:14.574496: step 170310, loss = 2.65 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 08:30:20.324000: step 170320, loss = 2.75 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 08:30:25.113566: step 170330, loss = 2.70 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 08:30:29.974605: step 170340, loss = 2.58 (246.1 examples/sec; 0.520 sec/batch)
2016-07-16 08:30:36.213289: step 170350, loss = 2.57 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 08:30:41.597707: step 170360, loss = 2.91 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 08:30:46.305938: step 170370, loss = 2.62 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 08:30:50.929843: step 170380, loss = 2.85 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 08:30:55.640949: step 170390, loss = 2.78 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 08:31:00.316294: step 170400, loss = 2.63 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 08:31:05.921123: step 170410, loss = 2.82 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 08:31:10.513117: step 170420, loss = 2.71 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 08:31:15.148939: step 170430, loss = 2.66 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 08:31:20.834417: step 170440, loss = 2.68 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 08:31:25.503833: step 170450, loss = 2.86 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 08:31:30.165586: step 170460, loss = 2.95 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 08:31:34.800225: step 170470, loss = 2.61 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 08:31:39.419927: step 170480, loss = 2.77 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 08:31:44.107360: step 170490, loss = 2.79 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 08:31:48.736774: step 170500, loss = 2.89 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 08:31:54.290202: step 170510, loss = 2.74 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 08:32:00.026559: step 170520, loss = 2.69 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 08:32:05.322509: step 170530, loss = 2.81 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 08:32:10.681337: step 170540, loss = 2.87 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 08:32:15.401312: step 170550, loss = 2.87 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 08:32:20.014597: step 170560, loss = 3.15 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 08:32:24.646277: step 170570, loss = 2.66 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 08:32:29.258139: step 170580, loss = 2.55 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 08:32:33.909458: step 170590, loss = 2.78 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 08:32:38.543757: step 170600, loss = 2.67 (286.8 examples/sec; 0.446 sec/batch)
2016-07-16 08:32:44.444467: step 170610, loss = 2.72 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 08:32:50.008165: step 170620, loss = 2.75 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 08:32:54.745433: step 170630, loss = 2.60 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 08:32:59.732875: step 170640, loss = 2.52 (226.6 examples/sec; 0.565 sec/batch)
2016-07-16 08:33:06.269386: step 170650, loss = 2.84 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 08:33:11.468036: step 170660, loss = 2.74 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 08:33:16.197166: step 170670, loss = 2.74 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 08:33:21.062188: step 170680, loss = 2.93 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 08:33:25.911249: step 170690, loss = 2.83 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 08:33:30.639032: step 170700, loss = 2.74 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 08:33:36.258202: step 170710, loss = 2.78 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 08:33:41.467039: step 170720, loss = 2.83 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 08:33:46.786901: step 170730, loss = 2.68 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 08:33:51.475336: step 170740, loss = 2.62 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 08:33:56.102391: step 170750, loss = 2.63 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 08:34:01.218782: step 170760, loss = 2.75 (197.5 examples/sec; 0.648 sec/batch)
2016-07-16 08:34:06.599931: step 170770, loss = 2.76 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 08:34:12.354098: step 170780, loss = 2.97 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 08:34:17.234179: step 170790, loss = 2.80 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 08:34:22.060047: step 170800, loss = 2.85 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 08:34:27.814940: step 170810, loss = 2.64 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 08:34:32.433683: step 170820, loss = 3.01 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 08:34:37.103706: step 170830, loss = 3.00 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 08:34:41.737786: step 170840, loss = 2.70 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 08:34:46.432719: step 170850, loss = 2.83 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 08:34:52.174113: step 170860, loss = 2.87 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 08:34:57.464507: step 170870, loss = 2.66 (208.6 examples/sec; 0.614 sec/batch)
2016-07-16 08:35:02.847831: step 170880, loss = 2.64 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 08:35:07.518223: step 170890, loss = 2.85 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 08:35:12.171141: step 170900, loss = 2.98 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 08:35:17.823807: step 170910, loss = 2.69 (284.5 examples/sec; 0.450 sec/batch)
2016-07-16 08:35:22.463589: step 170920, loss = 2.79 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 08:35:28.223546: step 170930, loss = 2.73 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 08:35:33.040362: step 170940, loss = 2.82 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 08:35:37.800602: step 170950, loss = 3.04 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 08:35:42.528090: step 170960, loss = 2.66 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 08:35:47.147616: step 170970, loss = 2.80 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 08:35:51.833637: step 170980, loss = 2.86 (248.1 examples/sec; 0.516 sec/batch)
2016-07-16 08:35:57.568171: step 170990, loss = 2.60 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 08:36:02.373606: step 171000, loss = 3.01 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 08:36:08.516848: step 171010, loss = 2.54 (187.9 examples/sec; 0.681 sec/batch)
2016-07-16 08:36:15.055817: step 171020, loss = 2.75 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 08:36:20.038415: step 171030, loss = 2.83 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 08:36:24.791895: step 171040, loss = 2.88 (252.7 examples/sec; 0.506 sec/batch)
2016-07-16 08:36:29.614774: step 171050, loss = 2.67 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 08:36:34.316841: step 171060, loss = 2.57 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 08:36:39.007175: step 171070, loss = 2.68 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 08:36:44.724404: step 171080, loss = 2.96 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 08:36:49.371767: step 171090, loss = 3.05 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 08:36:54.809666: step 171100, loss = 2.87 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 08:37:00.990526: step 171110, loss = 2.67 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 08:37:05.796484: step 171120, loss = 2.83 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 08:37:10.588150: step 171130, loss = 2.56 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 08:37:15.238089: step 171140, loss = 2.94 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 08:37:19.995965: step 171150, loss = 2.87 (226.6 examples/sec; 0.565 sec/batch)
2016-07-16 08:37:25.710713: step 171160, loss = 2.64 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 08:37:30.500253: step 171170, loss = 2.97 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 08:37:35.422663: step 171180, loss = 2.90 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 08:37:40.134732: step 171190, loss = 2.61 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 08:37:44.816485: step 171200, loss = 2.88 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 08:37:50.406033: step 171210, loss = 2.77 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 08:37:55.048767: step 171220, loss = 2.61 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 08:37:59.698007: step 171230, loss = 2.97 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 08:38:04.647653: step 171240, loss = 2.89 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 08:38:10.260954: step 171250, loss = 2.84 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 08:38:15.043213: step 171260, loss = 2.89 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 08:38:19.672875: step 171270, loss = 2.80 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 08:38:24.307424: step 171280, loss = 2.74 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 08:38:29.435870: step 171290, loss = 2.65 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 08:38:34.886315: step 171300, loss = 2.89 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 08:38:40.622300: step 171310, loss = 3.02 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 08:38:46.162583: step 171320, loss = 2.93 (187.9 examples/sec; 0.681 sec/batch)
2016-07-16 08:38:51.407100: step 171330, loss = 2.85 (282.9 examples/sec; 0.452 sec/batch)
2016-07-16 08:38:56.075677: step 171340, loss = 2.90 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 08:39:00.731745: step 171350, loss = 2.64 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 08:39:06.051857: step 171360, loss = 2.71 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 08:39:11.318154: step 171370, loss = 2.80 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 08:39:17.093912: step 171380, loss = 2.71 (253.9 examples/sec; 0.504 sec/batch)
2016-07-16 08:39:22.632660: step 171390, loss = 2.84 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 08:39:27.749124: step 171400, loss = 2.66 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 08:39:33.453089: step 171410, loss = 2.80 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 08:39:39.332672: step 171420, loss = 2.85 (190.4 examples/sec; 0.672 sec/batch)
2016-07-16 08:39:45.135393: step 171430, loss = 2.70 (286.6 examples/sec; 0.447 sec/batch)
2016-07-16 08:39:49.792963: step 171440, loss = 2.85 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 08:39:55.395881: step 171450, loss = 2.69 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 08:40:00.441498: step 171460, loss = 2.66 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 08:40:05.129651: step 171470, loss = 2.75 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 08:40:09.754626: step 171480, loss = 2.97 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 08:40:15.351133: step 171490, loss = 2.88 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 08:40:20.342317: step 171500, loss = 2.73 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 08:40:25.986758: step 171510, loss = 2.58 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 08:40:30.659190: step 171520, loss = 3.13 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 08:40:36.400295: step 171530, loss = 2.69 (254.1 examples/sec; 0.504 sec/batch)
2016-07-16 08:40:41.232652: step 171540, loss = 2.81 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 08:40:46.006104: step 171550, loss = 2.71 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 08:40:52.032242: step 171560, loss = 2.63 (197.7 examples/sec; 0.648 sec/batch)
2016-07-16 08:40:57.769005: step 171570, loss = 2.54 (253.5 examples/sec; 0.505 sec/batch)
2016-07-16 08:41:02.522081: step 171580, loss = 2.78 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 08:41:07.336750: step 171590, loss = 2.76 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 08:41:12.027707: step 171600, loss = 2.68 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 08:41:17.880542: step 171610, loss = 2.76 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 08:41:22.511251: step 171620, loss = 2.86 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 08:41:27.175860: step 171630, loss = 2.85 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 08:41:32.949103: step 171640, loss = 2.87 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 08:41:37.753587: step 171650, loss = 2.68 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 08:41:42.521877: step 171660, loss = 2.75 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 08:41:47.273073: step 171670, loss = 2.85 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 08:41:52.145732: step 171680, loss = 2.71 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 08:41:56.825914: step 171690, loss = 2.87 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 08:42:01.457204: step 171700, loss = 2.69 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 08:42:07.135476: step 171710, loss = 2.98 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 08:42:11.844352: step 171720, loss = 3.05 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 08:42:16.545357: step 171730, loss = 2.58 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 08:42:21.212855: step 171740, loss = 2.59 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 08:42:25.866510: step 171750, loss = 2.87 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 08:42:30.532441: step 171760, loss = 2.66 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 08:42:35.194503: step 171770, loss = 2.76 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 08:42:40.955315: step 171780, loss = 2.95 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 08:42:45.771865: step 171790, loss = 2.86 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 08:42:50.527992: step 171800, loss = 2.85 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 08:42:56.243700: step 171810, loss = 2.76 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 08:43:00.838894: step 171820, loss = 2.71 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 08:43:05.479845: step 171830, loss = 2.75 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 08:43:10.148848: step 171840, loss = 2.83 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 08:43:15.797155: step 171850, loss = 2.81 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 08:43:20.913853: step 171860, loss = 2.59 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 08:43:26.488463: step 171870, loss = 2.88 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 08:43:31.312538: step 171880, loss = 2.75 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 08:43:36.161250: step 171890, loss = 2.83 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 08:43:40.842392: step 171900, loss = 2.93 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 08:43:46.462262: step 171910, loss = 2.71 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 08:43:52.149715: step 171920, loss = 2.85 (225.4 examples/sec; 0.568 sec/batch)
2016-07-16 08:43:57.305722: step 171930, loss = 2.88 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 08:44:02.813818: step 171940, loss = 2.95 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 08:44:08.622840: step 171950, loss = 2.86 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 08:44:13.496072: step 171960, loss = 3.02 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 08:44:18.278573: step 171970, loss = 2.79 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 08:44:22.998844: step 171980, loss = 2.82 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 08:44:27.612491: step 171990, loss = 2.80 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 08:44:32.437553: step 172000, loss = 2.74 (216.6 examples/sec; 0.591 sec/batch)
2016-07-16 08:44:39.332070: step 172010, loss = 2.83 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 08:44:45.120152: step 172020, loss = 2.92 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 08:44:49.928283: step 172030, loss = 2.73 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 08:44:54.762003: step 172040, loss = 2.59 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 08:44:59.492768: step 172050, loss = 2.82 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 08:45:04.356003: step 172060, loss = 3.04 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 08:45:09.074543: step 172070, loss = 2.77 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 08:45:13.869176: step 172080, loss = 2.66 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 08:45:18.628904: step 172090, loss = 2.66 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 08:45:23.378730: step 172100, loss = 2.78 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 08:45:29.676885: step 172110, loss = 2.84 (186.4 examples/sec; 0.687 sec/batch)
2016-07-16 08:45:36.153165: step 172120, loss = 2.76 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 08:45:41.036118: step 172130, loss = 2.76 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 08:45:45.764833: step 172140, loss = 2.85 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 08:45:51.685043: step 172150, loss = 2.72 (188.4 examples/sec; 0.679 sec/batch)
2016-07-16 08:45:57.505547: step 172160, loss = 2.86 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 08:46:02.141753: step 172170, loss = 2.95 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 08:46:07.790631: step 172180, loss = 2.72 (198.0 examples/sec; 0.646 sec/batch)
2016-07-16 08:46:12.724984: step 172190, loss = 2.98 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 08:46:17.448176: step 172200, loss = 2.58 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 08:46:23.053833: step 172210, loss = 2.72 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 08:46:27.694044: step 172220, loss = 2.50 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 08:46:32.489102: step 172230, loss = 2.70 (227.9 examples/sec; 0.562 sec/batch)
2016-07-16 08:46:38.235808: step 172240, loss = 2.94 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 08:46:43.026848: step 172250, loss = 2.98 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 08:46:47.889073: step 172260, loss = 2.80 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 08:46:54.411785: step 172270, loss = 2.65 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 08:46:59.712014: step 172280, loss = 2.92 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 08:47:04.449110: step 172290, loss = 2.78 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 08:47:09.802256: step 172300, loss = 2.62 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 08:47:17.577633: step 172310, loss = 2.79 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 08:47:22.445926: step 172320, loss = 2.75 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 08:47:27.148559: step 172330, loss = 2.95 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 08:47:31.836497: step 172340, loss = 2.85 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 08:47:37.565572: step 172350, loss = 2.73 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 08:47:42.365941: step 172360, loss = 2.85 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 08:47:47.204937: step 172370, loss = 2.77 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 08:47:52.010407: step 172380, loss = 2.70 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 08:47:56.839251: step 172390, loss = 2.72 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 08:48:01.571956: step 172400, loss = 2.74 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 08:48:07.347634: step 172410, loss = 2.77 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 08:48:12.241495: step 172420, loss = 2.82 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 08:48:16.970022: step 172430, loss = 2.86 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 08:48:21.815557: step 172440, loss = 2.93 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 08:48:26.484915: step 172450, loss = 2.79 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 08:48:31.163097: step 172460, loss = 2.89 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 08:48:35.820674: step 172470, loss = 2.79 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 08:48:40.567071: step 172480, loss = 2.80 (234.5 examples/sec; 0.546 sec/batch)
2016-07-16 08:48:46.291608: step 172490, loss = 2.62 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 08:48:51.085918: step 172500, loss = 2.83 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 08:48:57.225059: step 172510, loss = 2.68 (189.1 examples/sec; 0.677 sec/batch)
2016-07-16 08:49:03.709953: step 172520, loss = 2.79 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 08:49:08.685000: step 172530, loss = 2.85 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 08:49:13.425298: step 172540, loss = 2.58 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 08:49:18.304690: step 172550, loss = 2.84 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 08:49:23.084671: step 172560, loss = 2.74 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 08:49:29.337593: step 172570, loss = 2.69 (205.3 examples/sec; 0.624 sec/batch)
2016-07-16 08:49:34.963211: step 172580, loss = 2.82 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 08:49:39.785249: step 172590, loss = 2.99 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 08:49:44.379096: step 172600, loss = 2.96 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 08:49:50.507557: step 172610, loss = 2.67 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 08:49:55.825116: step 172620, loss = 2.53 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 08:50:00.572547: step 172630, loss = 2.73 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 08:50:05.420769: step 172640, loss = 2.88 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 08:50:10.159694: step 172650, loss = 2.78 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 08:50:16.187294: step 172660, loss = 2.85 (189.1 examples/sec; 0.677 sec/batch)
2016-07-16 08:50:22.043449: step 172670, loss = 2.79 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 08:50:26.844557: step 172680, loss = 2.82 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 08:50:31.467467: step 172690, loss = 2.54 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 08:50:36.172247: step 172700, loss = 2.72 (247.3 examples/sec; 0.518 sec/batch)
2016-07-16 08:50:43.144328: step 172710, loss = 2.77 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 08:50:48.957462: step 172720, loss = 2.69 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 08:50:53.809351: step 172730, loss = 2.72 (249.9 examples/sec; 0.512 sec/batch)
2016-07-16 08:50:58.617778: step 172740, loss = 2.75 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 08:51:04.719057: step 172750, loss = 2.69 (200.2 examples/sec; 0.640 sec/batch)
2016-07-16 08:51:10.427498: step 172760, loss = 2.67 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 08:51:15.205445: step 172770, loss = 2.64 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 08:51:20.058926: step 172780, loss = 2.73 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 08:51:24.748012: step 172790, loss = 2.81 (282.9 examples/sec; 0.453 sec/batch)
2016-07-16 08:51:29.391241: step 172800, loss = 2.67 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 08:51:35.013640: step 172810, loss = 2.69 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 08:51:39.612166: step 172820, loss = 2.79 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 08:51:44.282436: step 172830, loss = 2.92 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 08:51:50.072223: step 172840, loss = 2.72 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 08:51:55.671822: step 172850, loss = 2.88 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 08:52:00.778851: step 172860, loss = 2.64 (227.6 examples/sec; 0.562 sec/batch)
2016-07-16 08:52:06.454664: step 172870, loss = 2.94 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 08:52:11.226395: step 172880, loss = 2.57 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 08:52:16.090938: step 172890, loss = 2.96 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 08:52:20.790915: step 172900, loss = 2.56 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 08:52:26.332421: step 172910, loss = 2.96 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 08:52:31.913618: step 172920, loss = 2.78 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 08:52:36.985703: step 172930, loss = 2.69 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 08:52:41.750364: step 172940, loss = 2.93 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 08:52:46.645676: step 172950, loss = 2.78 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 08:52:51.434288: step 172960, loss = 2.87 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 08:52:56.200360: step 172970, loss = 2.79 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 08:53:01.100879: step 172980, loss = 2.62 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 08:53:05.792656: step 172990, loss = 2.74 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 08:53:10.445571: step 173000, loss = 2.73 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 08:53:16.074419: step 173010, loss = 2.97 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 08:53:20.810118: step 173020, loss = 2.51 (233.2 examples/sec; 0.549 sec/batch)
2016-07-16 08:53:26.539408: step 173030, loss = 2.84 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 08:53:31.270598: step 173040, loss = 2.74 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 08:53:35.945517: step 173050, loss = 2.79 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 08:53:40.591122: step 173060, loss = 2.85 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 08:53:46.336968: step 173070, loss = 2.79 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 08:53:51.203509: step 173080, loss = 2.62 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 08:53:56.096196: step 173090, loss = 2.78 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 08:54:00.851260: step 173100, loss = 2.78 (257.8 examples/sec; 0.496 sec/batch)
2016-07-16 08:54:07.741245: step 173110, loss = 2.74 (185.1 examples/sec; 0.691 sec/batch)
2016-07-16 08:54:13.758018: step 173120, loss = 2.95 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 08:54:18.579878: step 173130, loss = 2.81 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 08:54:23.388463: step 173140, loss = 2.85 (258.3 examples/sec; 0.495 sec/batch)
2016-07-16 08:54:28.142741: step 173150, loss = 2.78 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 08:54:32.989470: step 173160, loss = 2.72 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 08:54:37.736768: step 173170, loss = 2.67 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 08:54:43.398539: step 173180, loss = 2.74 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 08:54:49.474975: step 173190, loss = 2.85 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 08:54:54.127365: step 173200, loss = 2.82 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 08:54:59.660539: step 173210, loss = 2.82 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 08:55:04.275711: step 173220, loss = 2.86 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 08:55:09.067975: step 173230, loss = 2.79 (215.6 examples/sec; 0.594 sec/batch)
2016-07-16 08:55:14.761269: step 173240, loss = 2.76 (249.3 examples/sec; 0.513 sec/batch)
2016-07-16 08:55:19.567586: step 173250, loss = 2.72 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 08:55:24.238742: step 173260, loss = 2.74 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 08:55:29.093518: step 173270, loss = 2.80 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 08:55:34.762721: step 173280, loss = 2.89 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 08:55:40.456170: step 173290, loss = 2.84 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 08:55:45.390313: step 173300, loss = 2.74 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 08:55:51.220092: step 173310, loss = 2.68 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 08:55:55.943529: step 173320, loss = 2.84 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 08:56:00.804120: step 173330, loss = 2.86 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 08:56:05.492772: step 173340, loss = 2.82 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 08:56:10.153178: step 173350, loss = 2.99 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 08:56:15.591956: step 173360, loss = 2.67 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 08:56:20.809080: step 173370, loss = 2.58 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 08:56:25.528833: step 173380, loss = 2.85 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 08:56:31.108764: step 173390, loss = 2.80 (189.0 examples/sec; 0.677 sec/batch)
2016-07-16 08:56:37.354270: step 173400, loss = 2.75 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 08:56:43.206832: step 173410, loss = 2.78 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 08:56:48.013022: step 173420, loss = 2.81 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 08:56:52.718300: step 173430, loss = 2.64 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 08:56:57.342282: step 173440, loss = 2.99 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 08:57:02.001612: step 173450, loss = 2.78 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 08:57:06.661379: step 173460, loss = 2.78 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 08:57:12.206093: step 173470, loss = 2.72 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 08:57:17.187165: step 173480, loss = 2.83 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 08:57:21.915259: step 173490, loss = 2.74 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 08:57:26.727916: step 173500, loss = 2.75 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 08:57:32.305679: step 173510, loss = 2.73 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 08:57:37.004174: step 173520, loss = 2.98 (245.3 examples/sec; 0.522 sec/batch)
2016-07-16 08:57:42.723637: step 173530, loss = 2.84 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 08:57:47.470103: step 173540, loss = 2.59 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 08:57:52.342073: step 173550, loss = 2.74 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 08:57:58.830516: step 173560, loss = 2.69 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 08:58:04.173335: step 173570, loss = 2.70 (247.3 examples/sec; 0.517 sec/batch)
2016-07-16 08:58:08.906734: step 173580, loss = 2.87 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 08:58:13.723429: step 173590, loss = 2.82 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 08:58:18.507880: step 173600, loss = 2.63 (250.9 examples/sec; 0.510 sec/batch)
2016-07-16 08:58:25.757661: step 173610, loss = 2.72 (207.8 examples/sec; 0.616 sec/batch)
2016-07-16 08:58:31.390669: step 173620, loss = 2.86 (248.0 examples/sec; 0.516 sec/batch)
2016-07-16 08:58:37.171738: step 173630, loss = 2.80 (246.3 examples/sec; 0.520 sec/batch)
2016-07-16 08:58:42.404966: step 173640, loss = 2.67 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 08:58:47.862250: step 173650, loss = 2.54 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 08:58:52.571651: step 173660, loss = 2.75 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 08:58:57.782273: step 173670, loss = 2.98 (182.9 examples/sec; 0.700 sec/batch)
2016-07-16 08:59:04.269252: step 173680, loss = 2.74 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 08:59:09.237915: step 173690, loss = 2.92 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 08:59:13.910918: step 173700, loss = 2.85 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 08:59:19.532111: step 173710, loss = 2.80 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 08:59:25.203528: step 173720, loss = 2.65 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 08:59:30.078275: step 173730, loss = 2.62 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 08:59:34.824713: step 173740, loss = 2.82 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 08:59:39.580702: step 173750, loss = 2.90 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 08:59:44.413303: step 173760, loss = 2.95 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 08:59:50.844867: step 173770, loss = 2.99 (207.7 examples/sec; 0.616 sec/batch)
2016-07-16 08:59:56.174821: step 173780, loss = 2.90 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 09:00:00.876670: step 173790, loss = 2.67 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 09:00:05.511338: step 173800, loss = 2.68 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 09:00:11.109492: step 173810, loss = 2.81 (286.0 examples/sec; 0.448 sec/batch)
2016-07-16 09:00:15.742968: step 173820, loss = 2.97 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 09:00:21.471286: step 173830, loss = 2.74 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 09:00:26.295239: step 173840, loss = 2.93 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 09:00:31.064181: step 173850, loss = 2.42 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 09:00:35.849881: step 173860, loss = 2.72 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 09:00:40.475266: step 173870, loss = 2.77 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 09:00:45.230865: step 173880, loss = 2.57 (235.1 examples/sec; 0.545 sec/batch)
2016-07-16 09:00:50.970483: step 173890, loss = 2.66 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 09:00:55.673087: step 173900, loss = 2.59 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 09:01:01.529321: step 173910, loss = 2.61 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 09:01:06.179191: step 173920, loss = 2.65 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 09:01:10.774967: step 173930, loss = 2.71 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 09:01:15.407382: step 173940, loss = 2.71 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 09:01:21.121188: step 173950, loss = 2.82 (222.2 examples/sec; 0.576 sec/batch)
2016-07-16 09:01:26.321484: step 173960, loss = 2.71 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 09:01:31.797381: step 173970, loss = 2.71 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 09:01:36.544807: step 173980, loss = 2.58 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 09:01:41.413884: step 173990, loss = 2.62 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 09:01:46.162330: step 174000, loss = 2.68 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 09:01:53.302452: step 174010, loss = 2.94 (196.9 examples/sec; 0.650 sec/batch)
2016-07-16 09:01:59.052255: step 174020, loss = 2.62 (255.1 examples/sec; 0.502 sec/batch)
2016-07-16 09:02:03.828917: step 174030, loss = 3.02 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 09:02:08.740233: step 174040, loss = 2.87 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 09:02:15.241201: step 174050, loss = 2.91 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 09:02:20.566805: step 174060, loss = 2.56 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 09:02:25.292293: step 174070, loss = 2.91 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 09:02:30.103411: step 174080, loss = 2.96 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 09:02:34.717745: step 174090, loss = 2.78 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 09:02:39.412141: step 174100, loss = 2.89 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 09:02:46.145577: step 174110, loss = 2.72 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 09:02:50.931672: step 174120, loss = 2.81 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 09:02:55.737809: step 174130, loss = 2.86 (253.3 examples/sec; 0.505 sec/batch)
2016-07-16 09:03:00.449721: step 174140, loss = 2.64 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 09:03:05.389944: step 174150, loss = 2.85 (223.2 examples/sec; 0.573 sec/batch)
2016-07-16 09:03:11.924914: step 174160, loss = 2.66 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 09:03:17.059849: step 174170, loss = 2.51 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 09:03:21.724138: step 174180, loss = 2.85 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 09:03:27.166414: step 174190, loss = 2.71 (186.5 examples/sec; 0.686 sec/batch)
2016-07-16 09:03:33.459743: step 174200, loss = 2.84 (253.3 examples/sec; 0.505 sec/batch)
2016-07-16 09:03:39.242807: step 174210, loss = 2.77 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 09:03:43.855672: step 174220, loss = 2.88 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 09:03:48.517560: step 174230, loss = 2.70 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 09:03:54.283717: step 174240, loss = 2.73 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 09:03:59.114652: step 174250, loss = 2.85 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 09:04:03.935173: step 174260, loss = 2.89 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 09:04:08.658203: step 174270, loss = 2.85 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 09:04:13.257347: step 174280, loss = 2.68 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 09:04:18.190027: step 174290, loss = 2.74 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 09:04:23.693675: step 174300, loss = 2.84 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 09:04:30.530596: step 174310, loss = 2.77 (249.5 examples/sec; 0.513 sec/batch)
2016-07-16 09:04:35.338654: step 174320, loss = 2.61 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 09:04:40.182591: step 174330, loss = 2.76 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 09:04:44.909348: step 174340, loss = 2.89 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 09:04:49.818069: step 174350, loss = 2.75 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 09:04:54.437055: step 174360, loss = 2.73 (282.9 examples/sec; 0.453 sec/batch)
2016-07-16 09:04:59.095900: step 174370, loss = 2.94 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 09:05:04.736167: step 174380, loss = 2.84 (200.1 examples/sec; 0.640 sec/batch)
2016-07-16 09:05:09.874295: step 174390, loss = 2.70 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 09:05:15.484553: step 174400, loss = 3.06 (258.3 examples/sec; 0.495 sec/batch)
2016-07-16 09:05:21.232367: step 174410, loss = 2.87 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 09:05:26.049729: step 174420, loss = 2.84 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 09:05:30.835895: step 174430, loss = 2.86 (257.8 examples/sec; 0.496 sec/batch)
2016-07-16 09:05:35.600559: step 174440, loss = 2.65 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 09:05:40.477219: step 174450, loss = 2.81 (245.1 examples/sec; 0.522 sec/batch)
2016-07-16 09:05:46.854972: step 174460, loss = 2.65 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 09:05:52.269693: step 174470, loss = 2.67 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 09:05:57.011493: step 174480, loss = 3.06 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 09:06:02.192648: step 174490, loss = 2.75 (187.5 examples/sec; 0.683 sec/batch)
2016-07-16 09:06:08.669797: step 174500, loss = 2.65 (207.7 examples/sec; 0.616 sec/batch)
2016-07-16 09:06:14.608547: step 174510, loss = 3.04 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 09:06:19.407668: step 174520, loss = 2.64 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 09:06:25.469515: step 174530, loss = 2.72 (194.1 examples/sec; 0.660 sec/batch)
2016-07-16 09:06:31.031306: step 174540, loss = 2.74 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 09:06:35.645403: step 174550, loss = 2.83 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 09:06:41.341045: step 174560, loss = 2.80 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 09:06:46.136991: step 174570, loss = 2.87 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 09:06:50.899054: step 174580, loss = 2.78 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 09:06:55.733355: step 174590, loss = 2.86 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 09:07:00.392427: step 174600, loss = 2.60 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 09:07:06.263516: step 174610, loss = 2.78 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 09:07:11.815449: step 174620, loss = 2.82 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 09:07:16.583518: step 174630, loss = 2.78 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 09:07:21.471423: step 174640, loss = 2.80 (233.4 examples/sec; 0.548 sec/batch)
2016-07-16 09:07:28.013913: step 174650, loss = 2.72 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 09:07:33.014027: step 174660, loss = 2.70 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 09:07:37.932646: step 174670, loss = 2.62 (202.0 examples/sec; 0.634 sec/batch)
2016-07-16 09:07:43.561875: step 174680, loss = 2.58 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 09:07:48.291582: step 174690, loss = 2.77 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 09:07:52.926065: step 174700, loss = 2.81 (284.5 examples/sec; 0.450 sec/batch)
2016-07-16 09:07:58.617845: step 174710, loss = 2.76 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 09:08:03.226129: step 174720, loss = 2.80 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 09:08:08.773467: step 174730, loss = 2.61 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 09:08:13.776772: step 174740, loss = 2.86 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 09:08:18.527904: step 174750, loss = 2.86 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 09:08:23.328296: step 174760, loss = 2.58 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 09:08:28.136819: step 174770, loss = 2.74 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 09:08:34.372586: step 174780, loss = 3.04 (204.3 examples/sec; 0.627 sec/batch)
2016-07-16 09:08:39.952193: step 174790, loss = 2.79 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 09:08:45.685678: step 174800, loss = 2.68 (249.4 examples/sec; 0.513 sec/batch)
2016-07-16 09:08:51.553928: step 174810, loss = 2.60 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 09:08:56.352153: step 174820, loss = 2.75 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 09:09:01.062251: step 174830, loss = 2.94 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 09:09:06.149047: step 174840, loss = 2.77 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 09:09:12.676483: step 174850, loss = 2.95 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 09:09:17.685727: step 174860, loss = 2.84 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 09:09:22.375624: step 174870, loss = 2.75 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 09:09:26.985240: step 174880, loss = 2.71 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 09:09:32.441629: step 174890, loss = 3.00 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 09:09:37.556366: step 174900, loss = 2.77 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 09:09:43.158276: step 174910, loss = 2.88 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 09:09:47.834340: step 174920, loss = 2.99 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 09:09:53.535037: step 174930, loss = 3.00 (210.9 examples/sec; 0.607 sec/batch)
2016-07-16 09:09:58.770923: step 174940, loss = 2.82 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 09:10:04.228316: step 174950, loss = 2.70 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 09:10:10.007758: step 174960, loss = 2.74 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 09:10:14.897974: step 174970, loss = 2.65 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 09:10:19.708563: step 174980, loss = 2.69 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 09:10:25.792109: step 174990, loss = 2.84 (197.8 examples/sec; 0.647 sec/batch)
2016-07-16 09:10:31.503357: step 175000, loss = 2.81 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 09:10:37.383699: step 175010, loss = 2.44 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 09:10:42.015750: step 175020, loss = 2.76 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 09:10:47.147282: step 175030, loss = 2.75 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 09:10:52.551166: step 175040, loss = 2.60 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 09:10:58.321234: step 175050, loss = 2.63 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 09:11:03.678520: step 175060, loss = 2.70 (208.6 examples/sec; 0.614 sec/batch)
2016-07-16 09:11:09.008749: step 175070, loss = 2.93 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 09:11:13.744869: step 175080, loss = 2.59 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 09:11:19.173322: step 175090, loss = 2.83 (187.1 examples/sec; 0.684 sec/batch)
2016-07-16 09:11:25.627708: step 175100, loss = 2.85 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 09:11:31.461769: step 175110, loss = 2.77 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 09:11:36.210791: step 175120, loss = 2.78 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 09:11:40.916932: step 175130, loss = 2.89 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 09:11:45.504603: step 175140, loss = 2.69 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 09:11:50.115121: step 175150, loss = 2.70 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 09:11:55.222892: step 175160, loss = 2.59 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 09:12:00.660536: step 175170, loss = 2.66 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 09:12:05.386053: step 175180, loss = 2.79 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 09:12:10.529387: step 175190, loss = 2.87 (188.4 examples/sec; 0.680 sec/batch)
2016-07-16 09:12:17.030999: step 175200, loss = 2.67 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 09:12:23.083664: step 175210, loss = 2.88 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 09:12:27.899192: step 175220, loss = 2.49 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 09:12:34.023311: step 175230, loss = 2.87 (193.9 examples/sec; 0.660 sec/batch)
2016-07-16 09:12:39.722913: step 175240, loss = 2.95 (250.9 examples/sec; 0.510 sec/batch)
2016-07-16 09:12:44.507014: step 175250, loss = 2.71 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 09:12:49.365391: step 175260, loss = 3.10 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 09:12:54.063560: step 175270, loss = 2.88 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 09:12:58.676978: step 175280, loss = 2.66 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 09:13:03.444641: step 175290, loss = 2.86 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 09:13:08.144685: step 175300, loss = 2.76 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 09:13:13.750750: step 175310, loss = 2.86 (287.4 examples/sec; 0.445 sec/batch)
2016-07-16 09:13:18.787436: step 175320, loss = 2.72 (199.9 examples/sec; 0.640 sec/batch)
2016-07-16 09:13:24.333384: step 175330, loss = 2.55 (247.7 examples/sec; 0.517 sec/batch)
2016-07-16 09:13:30.144401: step 175340, loss = 2.90 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 09:13:35.008561: step 175350, loss = 2.63 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 09:13:39.776914: step 175360, loss = 2.74 (255.8 examples/sec; 0.500 sec/batch)
2016-07-16 09:13:44.528356: step 175370, loss = 2.90 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 09:13:49.360296: step 175380, loss = 2.85 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 09:13:55.861358: step 175390, loss = 2.76 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 09:14:01.181296: step 175400, loss = 2.74 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 09:14:07.976322: step 175410, loss = 2.80 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 09:14:12.632782: step 175420, loss = 2.78 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 09:14:17.287245: step 175430, loss = 2.79 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 09:14:22.091234: step 175440, loss = 2.64 (208.4 examples/sec; 0.614 sec/batch)
2016-07-16 09:14:27.776883: step 175450, loss = 2.72 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 09:14:33.548805: step 175460, loss = 2.81 (202.4 examples/sec; 0.632 sec/batch)
2016-07-16 09:14:38.454058: step 175470, loss = 2.78 (243.9 examples/sec; 0.525 sec/batch)
2016-07-16 09:14:43.208037: step 175480, loss = 2.84 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 09:14:48.015260: step 175490, loss = 2.71 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 09:14:52.851163: step 175500, loss = 2.80 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 09:14:58.560462: step 175510, loss = 2.74 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 09:15:03.204856: step 175520, loss = 2.65 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 09:15:08.651489: step 175530, loss = 2.81 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 09:15:13.788507: step 175540, loss = 2.89 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 09:15:19.552980: step 175550, loss = 2.74 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 09:15:24.339388: step 175560, loss = 2.87 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 09:15:28.957627: step 175570, loss = 2.60 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 09:15:33.621690: step 175580, loss = 2.57 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 09:15:39.382919: step 175590, loss = 2.91 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 09:15:44.190827: step 175600, loss = 3.00 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 09:15:49.818718: step 175610, loss = 2.89 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 09:15:54.531055: step 175620, loss = 2.74 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 09:15:59.166328: step 175630, loss = 2.72 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 09:16:04.584098: step 175640, loss = 2.86 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 09:16:09.811989: step 175650, loss = 2.87 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 09:16:15.546069: step 175660, loss = 2.69 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 09:16:20.344909: step 175670, loss = 3.00 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 09:16:24.929752: step 175680, loss = 2.65 (285.4 examples/sec; 0.449 sec/batch)
2016-07-16 09:16:29.626226: step 175690, loss = 2.56 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 09:16:35.412937: step 175700, loss = 2.77 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 09:16:42.205387: step 175710, loss = 2.74 (236.4 examples/sec; 0.541 sec/batch)
2016-07-16 09:16:47.081904: step 175720, loss = 2.90 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 09:16:51.827842: step 175730, loss = 2.89 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 09:16:56.620332: step 175740, loss = 2.74 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 09:17:01.534179: step 175750, loss = 2.82 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 09:17:08.064626: step 175760, loss = 2.65 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 09:17:13.378040: step 175770, loss = 2.90 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 09:17:19.109516: step 175780, loss = 2.69 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 09:17:23.932487: step 175790, loss = 2.56 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 09:17:28.569204: step 175800, loss = 2.80 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 09:17:34.207156: step 175810, loss = 2.82 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 09:17:38.794292: step 175820, loss = 2.66 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 09:17:44.011102: step 175830, loss = 2.66 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 09:17:49.335780: step 175840, loss = 2.70 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 09:17:54.041808: step 175850, loss = 2.67 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 09:17:58.884962: step 175860, loss = 2.70 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 09:18:03.541659: step 175870, loss = 2.72 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 09:18:08.203192: step 175880, loss = 2.82 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 09:18:12.836144: step 175890, loss = 2.88 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 09:18:18.689169: step 175900, loss = 2.71 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 09:18:24.525845: step 175910, loss = 2.92 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 09:18:29.358758: step 175920, loss = 2.52 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 09:18:34.135617: step 175930, loss = 2.72 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 09:18:38.987387: step 175940, loss = 2.98 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 09:18:43.669680: step 175950, loss = 2.74 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 09:18:48.281401: step 175960, loss = 2.71 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 09:18:53.972866: step 175970, loss = 2.52 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 09:18:58.826529: step 175980, loss = 3.17 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 09:19:03.584333: step 175990, loss = 2.70 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 09:19:08.384802: step 176000, loss = 2.84 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 09:19:14.009770: step 176010, loss = 2.77 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 09:19:18.682850: step 176020, loss = 2.76 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 09:19:23.243234: step 176030, loss = 2.83 (284.3 examples/sec; 0.450 sec/batch)
2016-07-16 09:19:27.860567: step 176040, loss = 2.76 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 09:19:33.439945: step 176050, loss = 2.98 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 09:19:38.607163: step 176060, loss = 2.75 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 09:19:44.237881: step 176070, loss = 2.74 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 09:19:49.057219: step 176080, loss = 3.09 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 09:19:53.914317: step 176090, loss = 2.86 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 09:19:58.628131: step 176100, loss = 2.86 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 09:20:04.464360: step 176110, loss = 2.69 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 09:20:09.304637: step 176120, loss = 2.81 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 09:20:13.992954: step 176130, loss = 2.70 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 09:20:18.826738: step 176140, loss = 2.87 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 09:20:23.549549: step 176150, loss = 2.89 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 09:20:28.321880: step 176160, loss = 2.77 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 09:20:33.204476: step 176170, loss = 2.81 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 09:20:39.574843: step 176180, loss = 2.68 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 09:20:45.042701: step 176190, loss = 2.61 (265.8 examples/sec; 0.481 sec/batch)
2016-07-16 09:20:49.806812: step 176200, loss = 2.99 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 09:20:55.705710: step 176210, loss = 2.54 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 09:21:00.537017: step 176220, loss = 2.77 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 09:21:05.275154: step 176230, loss = 2.95 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 09:21:10.138092: step 176240, loss = 2.50 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 09:21:14.853898: step 176250, loss = 2.72 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 09:21:19.541434: step 176260, loss = 2.73 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 09:21:24.977834: step 176270, loss = 2.75 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 09:21:30.129964: step 176280, loss = 2.76 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 09:21:34.843581: step 176290, loss = 2.73 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 09:21:40.348947: step 176300, loss = 2.66 (187.2 examples/sec; 0.684 sec/batch)
2016-07-16 09:21:46.729474: step 176310, loss = 2.73 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 09:21:51.396199: step 176320, loss = 2.64 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 09:21:56.069480: step 176330, loss = 2.85 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 09:22:00.775899: step 176340, loss = 2.61 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 09:22:06.544967: step 176350, loss = 2.78 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 09:22:11.415810: step 176360, loss = 2.69 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 09:22:16.119893: step 176370, loss = 2.70 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 09:22:20.811463: step 176380, loss = 2.77 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 09:22:26.512648: step 176390, loss = 2.79 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 09:22:31.361579: step 176400, loss = 2.88 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 09:22:37.174797: step 176410, loss = 2.67 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 09:22:43.520712: step 176420, loss = 2.96 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 09:22:49.019272: step 176430, loss = 2.87 (248.9 examples/sec; 0.514 sec/batch)
2016-07-16 09:22:53.777609: step 176440, loss = 2.82 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 09:22:58.924581: step 176450, loss = 2.78 (186.0 examples/sec; 0.688 sec/batch)
2016-07-16 09:23:05.472566: step 176460, loss = 2.79 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 09:23:10.506748: step 176470, loss = 2.76 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 09:23:15.245946: step 176480, loss = 2.82 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 09:23:20.932406: step 176490, loss = 2.86 (188.1 examples/sec; 0.680 sec/batch)
2016-07-16 09:23:26.004005: step 176500, loss = 3.17 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 09:23:32.263256: step 176510, loss = 2.71 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 09:23:37.436242: step 176520, loss = 2.64 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 09:23:42.088531: step 176530, loss = 2.75 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 09:23:46.694150: step 176540, loss = 2.92 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 09:23:51.788239: step 176550, loss = 2.81 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 09:23:57.217071: step 176560, loss = 2.72 (253.0 examples/sec; 0.506 sec/batch)
2016-07-16 09:24:01.958307: step 176570, loss = 2.67 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 09:24:06.864999: step 176580, loss = 2.71 (250.9 examples/sec; 0.510 sec/batch)
2016-07-16 09:24:11.568978: step 176590, loss = 2.71 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 09:24:16.197883: step 176600, loss = 2.66 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 09:24:22.888495: step 176610, loss = 2.67 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 09:24:27.726488: step 176620, loss = 2.96 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 09:24:32.514332: step 176630, loss = 2.80 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 09:24:38.592999: step 176640, loss = 2.83 (200.7 examples/sec; 0.638 sec/batch)
2016-07-16 09:24:44.282876: step 176650, loss = 2.83 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 09:24:49.076034: step 176660, loss = 2.76 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 09:24:53.962477: step 176670, loss = 2.75 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 09:25:00.500650: step 176680, loss = 2.71 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 09:25:05.852316: step 176690, loss = 3.04 (252.2 examples/sec; 0.508 sec/batch)
2016-07-16 09:25:10.578531: step 176700, loss = 2.56 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 09:25:16.221411: step 176710, loss = 2.75 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 09:25:20.891980: step 176720, loss = 2.92 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 09:25:25.597671: step 176730, loss = 2.61 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 09:25:30.243261: step 176740, loss = 2.60 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 09:25:34.861885: step 176750, loss = 2.85 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 09:25:39.513672: step 176760, loss = 3.04 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 09:25:44.223604: step 176770, loss = 2.79 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 09:25:48.904273: step 176780, loss = 2.81 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 09:25:53.597779: step 176790, loss = 2.62 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 09:25:59.399948: step 176800, loss = 2.69 (253.2 examples/sec; 0.505 sec/batch)
2016-07-16 09:26:05.260628: step 176810, loss = 2.81 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 09:26:09.994521: step 176820, loss = 2.72 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 09:26:14.697136: step 176830, loss = 2.86 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 09:26:19.400187: step 176840, loss = 2.58 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 09:26:24.718774: step 176850, loss = 2.60 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 09:26:30.015980: step 176860, loss = 2.92 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 09:26:34.747587: step 176870, loss = 2.84 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 09:26:39.424091: step 176880, loss = 2.74 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 09:26:44.100989: step 176890, loss = 2.79 (282.9 examples/sec; 0.452 sec/batch)
2016-07-16 09:26:48.714480: step 176900, loss = 2.75 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 09:26:55.406086: step 176910, loss = 2.82 (253.6 examples/sec; 0.505 sec/batch)
2016-07-16 09:27:00.291645: step 176920, loss = 2.70 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 09:27:05.081920: step 176930, loss = 2.86 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 09:27:11.268316: step 176940, loss = 2.64 (205.6 examples/sec; 0.622 sec/batch)
2016-07-16 09:27:16.892777: step 176950, loss = 2.53 (252.5 examples/sec; 0.507 sec/batch)
2016-07-16 09:27:21.682020: step 176960, loss = 2.83 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 09:27:26.675997: step 176970, loss = 2.71 (216.9 examples/sec; 0.590 sec/batch)
2016-07-16 09:27:33.238382: step 176980, loss = 2.78 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 09:27:38.373043: step 176990, loss = 2.81 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 09:27:43.066082: step 177000, loss = 2.65 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 09:27:48.881857: step 177010, loss = 2.65 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 09:27:53.555100: step 177020, loss = 2.64 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 09:27:58.273436: step 177030, loss = 2.71 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 09:28:04.085715: step 177040, loss = 2.74 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 09:28:09.749692: step 177050, loss = 2.79 (202.4 examples/sec; 0.632 sec/batch)
2016-07-16 09:28:14.529957: step 177060, loss = 2.70 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 09:28:19.478651: step 177070, loss = 2.78 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 09:28:25.009181: step 177080, loss = 2.88 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 09:28:29.704577: step 177090, loss = 2.61 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 09:28:34.556757: step 177100, loss = 2.85 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 09:28:40.182483: step 177110, loss = 2.72 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 09:28:44.838573: step 177120, loss = 2.78 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 09:28:50.618810: step 177130, loss = 2.56 (209.1 examples/sec; 0.612 sec/batch)
2016-07-16 09:28:55.510254: step 177140, loss = 3.02 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 09:29:00.252878: step 177150, loss = 2.62 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 09:29:06.237203: step 177160, loss = 2.89 (187.8 examples/sec; 0.682 sec/batch)
2016-07-16 09:29:12.059645: step 177170, loss = 2.85 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 09:29:16.801334: step 177180, loss = 2.64 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 09:29:21.625510: step 177190, loss = 2.81 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 09:29:26.324635: step 177200, loss = 2.82 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 09:29:32.159876: step 177210, loss = 2.88 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 09:29:36.973050: step 177220, loss = 2.79 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 09:29:41.702165: step 177230, loss = 2.90 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 09:29:46.594984: step 177240, loss = 3.18 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 09:29:51.318077: step 177250, loss = 2.83 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 09:29:56.213291: step 177260, loss = 2.92 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 09:30:01.045428: step 177270, loss = 2.68 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 09:30:05.810905: step 177280, loss = 2.68 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 09:30:10.710775: step 177290, loss = 2.77 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 09:30:15.428385: step 177300, loss = 2.64 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 09:30:20.941464: step 177310, loss = 2.74 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 09:30:26.664839: step 177320, loss = 2.83 (220.6 examples/sec; 0.580 sec/batch)
2016-07-16 09:30:31.499144: step 177330, loss = 2.78 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 09:30:36.347294: step 177340, loss = 2.66 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 09:30:41.114033: step 177350, loss = 2.63 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 09:30:45.725272: step 177360, loss = 2.69 (284.3 examples/sec; 0.450 sec/batch)
2016-07-16 09:30:50.372126: step 177370, loss = 2.67 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 09:30:56.165893: step 177380, loss = 2.70 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 09:31:01.757770: step 177390, loss = 2.98 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 09:31:06.609427: step 177400, loss = 2.68 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 09:31:12.908970: step 177410, loss = 2.73 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 09:31:18.160421: step 177420, loss = 2.82 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 09:31:23.929877: step 177430, loss = 2.85 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 09:31:28.706056: step 177440, loss = 2.75 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 09:31:33.897660: step 177450, loss = 2.72 (242.0 examples/sec; 0.529 sec/batch)
2016-07-16 09:31:38.609371: step 177460, loss = 2.69 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 09:31:43.443897: step 177470, loss = 2.86 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 09:31:48.197905: step 177480, loss = 2.76 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 09:31:52.943513: step 177490, loss = 2.75 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 09:31:57.772957: step 177500, loss = 2.81 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 09:32:03.473857: step 177510, loss = 2.79 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 09:32:08.110374: step 177520, loss = 2.67 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 09:32:12.810131: step 177530, loss = 2.80 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 09:32:17.493841: step 177540, loss = 2.68 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 09:32:23.246779: step 177550, loss = 2.78 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 09:32:28.580471: step 177560, loss = 2.91 (202.0 examples/sec; 0.634 sec/batch)
2016-07-16 09:32:33.916685: step 177570, loss = 2.91 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 09:32:38.629820: step 177580, loss = 2.71 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 09:32:43.222611: step 177590, loss = 2.80 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 09:32:48.325280: step 177600, loss = 2.67 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 09:32:54.908398: step 177610, loss = 2.79 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 09:32:59.580252: step 177620, loss = 2.87 (284.0 examples/sec; 0.451 sec/batch)
2016-07-16 09:33:04.195043: step 177630, loss = 2.75 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 09:33:08.828483: step 177640, loss = 2.89 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 09:33:13.570952: step 177650, loss = 2.73 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 09:33:19.277765: step 177660, loss = 3.10 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 09:33:24.126263: step 177670, loss = 2.85 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 09:33:28.833372: step 177680, loss = 2.99 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 09:33:34.884400: step 177690, loss = 2.98 (190.7 examples/sec; 0.671 sec/batch)
2016-07-16 09:33:40.555471: step 177700, loss = 3.09 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 09:33:46.300318: step 177710, loss = 2.85 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 09:33:51.194593: step 177720, loss = 2.70 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 09:33:55.927802: step 177730, loss = 2.70 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 09:34:00.567638: step 177740, loss = 2.70 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 09:34:05.187281: step 177750, loss = 2.64 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 09:34:10.845454: step 177760, loss = 2.69 (220.9 examples/sec; 0.579 sec/batch)
2016-07-16 09:34:15.704076: step 177770, loss = 2.51 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 09:34:20.489095: step 177780, loss = 2.77 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 09:34:26.437514: step 177790, loss = 2.65 (186.2 examples/sec; 0.687 sec/batch)
2016-07-16 09:34:32.328469: step 177800, loss = 2.62 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 09:34:39.083766: step 177810, loss = 2.74 (242.5 examples/sec; 0.528 sec/batch)
2016-07-16 09:34:44.296278: step 177820, loss = 2.83 (208.4 examples/sec; 0.614 sec/batch)
2016-07-16 09:34:49.699645: step 177830, loss = 2.81 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 09:34:54.435007: step 177840, loss = 2.75 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 09:34:59.487216: step 177850, loss = 2.92 (188.4 examples/sec; 0.680 sec/batch)
2016-07-16 09:35:06.026301: step 177860, loss = 2.68 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 09:35:11.049256: step 177870, loss = 2.88 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 09:35:15.828658: step 177880, loss = 2.82 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 09:35:20.606004: step 177890, loss = 2.76 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 09:35:25.389905: step 177900, loss = 2.56 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 09:35:32.311251: step 177910, loss = 2.72 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 09:35:36.959841: step 177920, loss = 3.07 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 09:35:42.609360: step 177930, loss = 2.88 (198.7 examples/sec; 0.644 sec/batch)
2016-07-16 09:35:47.764001: step 177940, loss = 2.72 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 09:35:53.367192: step 177950, loss = 2.63 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 09:35:58.118466: step 177960, loss = 2.76 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 09:36:02.957308: step 177970, loss = 2.73 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 09:36:07.660113: step 177980, loss = 2.75 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 09:36:12.280361: step 177990, loss = 2.71 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 09:36:17.700347: step 178000, loss = 2.72 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 09:36:23.986411: step 178010, loss = 2.66 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 09:36:28.599892: step 178020, loss = 3.06 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 09:36:33.304332: step 178030, loss = 2.84 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 09:36:38.105363: step 178040, loss = 2.86 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 09:36:43.601052: step 178050, loss = 2.82 (120.0 examples/sec; 1.067 sec/batch)
2016-07-16 09:36:49.269119: step 178060, loss = 2.77 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 09:36:54.058600: step 178070, loss = 2.94 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 09:36:58.911470: step 178080, loss = 2.81 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 09:37:03.666213: step 178090, loss = 2.80 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 09:37:09.223705: step 178100, loss = 2.82 (183.7 examples/sec; 0.697 sec/batch)
2016-07-16 09:37:15.641754: step 178110, loss = 2.61 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 09:37:20.250487: step 178120, loss = 2.90 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 09:37:24.896083: step 178130, loss = 2.68 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 09:37:29.514200: step 178140, loss = 2.72 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 09:37:34.108739: step 178150, loss = 2.70 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 09:37:38.783005: step 178160, loss = 2.61 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 09:37:43.381276: step 178170, loss = 2.85 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 09:37:48.548286: step 178180, loss = 2.80 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 09:37:53.997443: step 178190, loss = 2.88 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 09:37:59.712796: step 178200, loss = 2.80 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 09:38:05.427056: step 178210, loss = 2.67 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 09:38:10.073813: step 178220, loss = 2.65 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 09:38:14.765800: step 178230, loss = 2.74 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 09:38:20.539226: step 178240, loss = 2.79 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 09:38:25.394422: step 178250, loss = 2.70 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 09:38:30.207876: step 178260, loss = 2.76 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 09:38:36.565898: step 178270, loss = 2.73 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 09:38:42.025254: step 178280, loss = 2.67 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 09:38:46.713853: step 178290, loss = 2.74 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 09:38:51.568033: step 178300, loss = 2.92 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 09:38:57.151215: step 178310, loss = 2.71 (280.4 examples/sec; 0.456 sec/batch)
2016-07-16 09:39:01.782313: step 178320, loss = 2.70 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 09:39:07.507685: step 178330, loss = 2.84 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 09:39:12.300920: step 178340, loss = 2.75 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 09:39:16.953339: step 178350, loss = 2.74 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 09:39:21.648188: step 178360, loss = 2.80 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 09:39:27.312890: step 178370, loss = 2.77 (245.8 examples/sec; 0.521 sec/batch)
2016-07-16 09:39:32.563594: step 178380, loss = 2.71 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 09:39:38.054889: step 178390, loss = 2.68 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 09:39:42.776248: step 178400, loss = 2.67 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 09:39:49.439542: step 178410, loss = 2.89 (183.3 examples/sec; 0.698 sec/batch)
2016-07-16 09:39:55.654132: step 178420, loss = 2.92 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 09:40:00.478980: step 178430, loss = 2.55 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 09:40:05.275273: step 178440, loss = 2.68 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 09:40:11.342607: step 178450, loss = 2.84 (194.5 examples/sec; 0.658 sec/batch)
2016-07-16 09:40:17.046953: step 178460, loss = 2.62 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 09:40:21.815012: step 178470, loss = 2.75 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 09:40:26.686667: step 178480, loss = 2.75 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 09:40:33.206534: step 178490, loss = 2.77 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 09:40:38.274325: step 178500, loss = 2.52 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 09:40:43.778664: step 178510, loss = 2.62 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 09:40:48.461740: step 178520, loss = 2.72 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 09:40:53.089944: step 178530, loss = 2.71 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 09:40:58.686990: step 178540, loss = 2.59 (209.5 examples/sec; 0.611 sec/batch)
2016-07-16 09:41:03.690846: step 178550, loss = 2.94 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 09:41:08.323617: step 178560, loss = 2.74 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 09:41:12.977382: step 178570, loss = 2.80 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 09:41:18.521607: step 178580, loss = 2.90 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 09:41:23.520947: step 178590, loss = 2.64 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 09:41:28.222988: step 178600, loss = 2.77 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 09:41:33.990215: step 178610, loss = 2.69 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 09:41:38.794108: step 178620, loss = 2.52 (255.6 examples/sec; 0.501 sec/batch)
2016-07-16 09:41:43.512549: step 178630, loss = 2.74 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 09:41:48.139078: step 178640, loss = 2.66 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 09:41:53.415083: step 178650, loss = 2.84 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 09:41:58.706878: step 178660, loss = 2.91 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 09:42:04.470115: step 178670, loss = 2.62 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 09:42:09.262371: step 178680, loss = 2.64 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 09:42:13.914665: step 178690, loss = 2.63 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 09:42:18.595333: step 178700, loss = 2.75 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 09:42:25.471334: step 178710, loss = 2.89 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 09:42:30.230213: step 178720, loss = 2.86 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 09:42:35.085322: step 178730, loss = 2.95 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 09:42:39.806116: step 178740, loss = 2.77 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 09:42:45.310257: step 178750, loss = 2.82 (187.7 examples/sec; 0.682 sec/batch)
2016-07-16 09:42:51.589804: step 178760, loss = 2.64 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 09:42:56.415619: step 178770, loss = 2.77 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 09:43:01.198118: step 178780, loss = 2.67 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 09:43:05.982055: step 178790, loss = 2.84 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 09:43:10.817306: step 178800, loss = 2.68 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 09:43:18.829871: step 178810, loss = 2.69 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 09:43:23.803709: step 178820, loss = 3.07 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 09:43:28.497822: step 178830, loss = 2.58 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 09:43:33.120434: step 178840, loss = 2.96 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 09:43:38.612520: step 178850, loss = 2.89 (207.8 examples/sec; 0.616 sec/batch)
2016-07-16 09:43:43.615136: step 178860, loss = 2.98 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 09:43:48.386662: step 178870, loss = 2.82 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 09:43:54.089103: step 178880, loss = 2.68 (187.8 examples/sec; 0.682 sec/batch)
2016-07-16 09:44:00.112317: step 178890, loss = 2.54 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 09:44:04.747650: step 178900, loss = 2.61 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 09:44:11.410372: step 178910, loss = 2.61 (220.9 examples/sec; 0.579 sec/batch)
2016-07-16 09:44:16.615517: step 178920, loss = 2.71 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 09:44:21.918447: step 178930, loss = 2.50 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 09:44:26.525963: step 178940, loss = 2.82 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 09:44:32.262955: step 178950, loss = 2.76 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 09:44:37.039243: step 178960, loss = 2.71 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 09:44:41.866682: step 178970, loss = 2.92 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 09:44:46.603946: step 178980, loss = 2.86 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 09:44:51.449710: step 178990, loss = 2.54 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 09:44:56.167873: step 179000, loss = 2.53 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 09:45:01.978078: step 179010, loss = 2.95 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 09:45:06.758484: step 179020, loss = 2.59 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 09:45:11.474304: step 179030, loss = 2.73 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 09:45:16.321974: step 179040, loss = 2.97 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 09:45:20.990862: step 179050, loss = 2.76 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 09:45:25.646936: step 179060, loss = 2.65 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 09:45:30.320245: step 179070, loss = 3.06 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 09:45:34.895564: step 179080, loss = 2.80 (284.2 examples/sec; 0.450 sec/batch)
2016-07-16 09:45:39.691684: step 179090, loss = 2.71 (223.0 examples/sec; 0.574 sec/batch)
2016-07-16 09:45:45.439075: step 179100, loss = 2.74 (240.0 examples/sec; 0.533 sec/batch)
2016-07-16 09:45:52.188908: step 179110, loss = 2.63 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 09:45:57.521739: step 179120, loss = 2.78 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 09:46:02.792944: step 179130, loss = 2.75 (263.1 examples/sec; 0.486 sec/batch)
2016-07-16 09:46:07.546775: step 179140, loss = 2.64 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 09:46:12.392734: step 179150, loss = 2.80 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 09:46:17.031742: step 179160, loss = 2.74 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 09:46:21.631181: step 179170, loss = 2.67 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 09:46:26.276928: step 179180, loss = 2.68 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 09:46:32.010934: step 179190, loss = 2.96 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 09:46:37.331615: step 179200, loss = 2.61 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 09:46:43.893093: step 179210, loss = 2.77 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 09:46:48.669509: step 179220, loss = 2.77 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 09:46:54.419849: step 179230, loss = 2.80 (193.0 examples/sec; 0.663 sec/batch)
2016-07-16 09:47:00.463694: step 179240, loss = 2.80 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 09:47:05.282248: step 179250, loss = 2.82 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 09:47:10.060872: step 179260, loss = 2.98 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 09:47:16.277650: step 179270, loss = 2.77 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 09:47:21.773823: step 179280, loss = 2.89 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 09:47:26.432202: step 179290, loss = 2.97 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 09:47:31.043364: step 179300, loss = 2.80 (281.6 examples/sec; 0.454 sec/batch)
2016-07-16 09:47:36.676994: step 179310, loss = 2.71 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 09:47:41.300472: step 179320, loss = 2.62 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 09:47:46.919447: step 179330, loss = 2.78 (198.3 examples/sec; 0.646 sec/batch)
2016-07-16 09:47:51.965808: step 179340, loss = 2.80 (254.3 examples/sec; 0.503 sec/batch)
2016-07-16 09:47:56.703119: step 179350, loss = 2.68 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 09:48:01.494618: step 179360, loss = 2.80 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 09:48:06.146299: step 179370, loss = 2.78 (285.4 examples/sec; 0.449 sec/batch)
2016-07-16 09:48:10.797850: step 179380, loss = 2.89 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 09:48:16.587624: step 179390, loss = 2.93 (244.1 examples/sec; 0.524 sec/batch)
2016-07-16 09:48:22.028133: step 179400, loss = 2.78 (199.9 examples/sec; 0.640 sec/batch)
2016-07-16 09:48:28.392789: step 179410, loss = 2.75 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 09:48:33.171354: step 179420, loss = 2.88 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 09:48:37.951957: step 179430, loss = 2.75 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 09:48:42.610816: step 179440, loss = 2.66 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 09:48:47.264398: step 179450, loss = 2.66 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 09:48:53.041083: step 179460, loss = 2.81 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 09:48:57.885241: step 179470, loss = 2.79 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 09:49:02.752857: step 179480, loss = 2.83 (248.6 examples/sec; 0.515 sec/batch)
2016-07-16 09:49:09.128979: step 179490, loss = 2.93 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 09:49:14.518165: step 179500, loss = 2.68 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 09:49:20.235351: step 179510, loss = 2.73 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 09:49:25.912915: step 179520, loss = 2.81 (186.8 examples/sec; 0.685 sec/batch)
2016-07-16 09:49:32.146369: step 179530, loss = 2.94 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 09:49:36.954940: step 179540, loss = 2.55 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 09:49:41.706696: step 179550, loss = 2.72 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 09:49:46.523240: step 179560, loss = 2.80 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 09:49:51.176107: step 179570, loss = 2.61 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 09:49:55.843962: step 179580, loss = 2.79 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 09:50:00.440861: step 179590, loss = 2.60 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 09:50:05.096909: step 179600, loss = 2.66 (249.4 examples/sec; 0.513 sec/batch)
2016-07-16 09:50:10.818383: step 179610, loss = 2.89 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 09:50:15.448174: step 179620, loss = 2.69 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 09:50:20.062677: step 179630, loss = 2.88 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 09:50:25.144272: step 179640, loss = 2.77 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 09:50:30.567041: step 179650, loss = 2.87 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 09:50:35.276598: step 179660, loss = 2.53 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 09:50:40.109015: step 179670, loss = 2.75 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 09:50:44.845644: step 179680, loss = 3.16 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 09:50:50.576584: step 179690, loss = 2.74 (187.9 examples/sec; 0.681 sec/batch)
2016-07-16 09:50:56.606528: step 179700, loss = 2.67 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 09:51:02.398705: step 179710, loss = 2.89 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 09:51:07.224430: step 179720, loss = 2.71 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 09:51:13.680087: step 179730, loss = 2.72 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 09:51:18.978724: step 179740, loss = 2.86 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 09:51:23.722378: step 179750, loss = 2.66 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 09:51:28.407184: step 179760, loss = 2.81 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 09:51:33.666836: step 179770, loss = 2.77 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 09:51:38.952150: step 179780, loss = 2.69 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 09:51:43.678594: step 179790, loss = 3.02 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 09:51:48.588720: step 179800, loss = 2.75 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 09:51:54.337963: step 179810, loss = 2.72 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 09:51:59.136329: step 179820, loss = 2.89 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 09:52:03.796265: step 179830, loss = 2.61 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 09:52:08.483914: step 179840, loss = 2.49 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 09:52:13.115159: step 179850, loss = 2.59 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 09:52:18.590046: step 179860, loss = 2.82 (210.9 examples/sec; 0.607 sec/batch)
2016-07-16 09:52:23.699226: step 179870, loss = 2.74 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 09:52:28.413068: step 179880, loss = 2.94 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 09:52:33.224071: step 179890, loss = 2.83 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 09:52:38.032116: step 179900, loss = 2.65 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 09:52:43.779433: step 179910, loss = 2.76 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 09:52:48.652062: step 179920, loss = 2.67 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 09:52:53.404307: step 179930, loss = 2.74 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 09:52:58.165058: step 179940, loss = 2.64 (279.2 examples/sec; 0.459 sec/batch)
2016-07-16 09:53:02.985613: step 179950, loss = 2.80 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 09:53:09.291288: step 179960, loss = 2.77 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 09:53:14.745998: step 179970, loss = 2.89 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 09:53:19.479159: step 179980, loss = 2.70 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 09:53:24.307633: step 179990, loss = 2.76 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 09:53:29.018303: step 180000, loss = 2.76 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 09:53:36.110055: step 180010, loss = 2.96 (199.2 examples/sec; 0.643 sec/batch)
2016-07-16 09:53:41.803800: step 180020, loss = 2.96 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 09:53:47.579125: step 180030, loss = 2.80 (199.7 examples/sec; 0.641 sec/batch)
2016-07-16 09:53:52.695309: step 180040, loss = 2.73 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 09:53:58.252900: step 180050, loss = 2.61 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 09:54:03.035937: step 180060, loss = 2.51 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 09:54:07.871708: step 180070, loss = 2.69 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 09:54:12.621649: step 180080, loss = 2.84 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 09:54:17.512257: step 180090, loss = 2.76 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 09:54:22.321969: step 180100, loss = 2.77 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 09:54:28.100263: step 180110, loss = 2.67 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 09:54:32.746234: step 180120, loss = 2.66 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 09:54:38.020687: step 180130, loss = 2.75 (200.7 examples/sec; 0.638 sec/batch)
2016-07-16 09:54:43.294014: step 180140, loss = 2.88 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 09:54:47.985293: step 180150, loss = 2.55 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 09:54:53.240107: step 180160, loss = 2.52 (191.3 examples/sec; 0.669 sec/batch)
2016-07-16 09:54:59.688794: step 180170, loss = 2.93 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 09:55:04.528340: step 180180, loss = 2.63 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 09:55:09.306955: step 180190, loss = 2.86 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 09:55:14.107640: step 180200, loss = 2.60 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 09:55:19.916387: step 180210, loss = 2.70 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 09:55:26.464918: step 180220, loss = 2.59 (201.7 examples/sec; 0.635 sec/batch)
2016-07-16 09:55:31.638552: step 180230, loss = 2.92 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 09:55:36.363961: step 180240, loss = 2.94 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 09:55:41.213127: step 180250, loss = 2.74 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 09:55:46.012013: step 180260, loss = 2.78 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 09:55:50.774289: step 180270, loss = 2.85 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 09:55:55.618126: step 180280, loss = 2.63 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 09:56:02.118545: step 180290, loss = 2.81 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 09:56:07.389718: step 180300, loss = 2.74 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 09:56:14.198878: step 180310, loss = 2.89 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 09:56:18.913492: step 180320, loss = 2.82 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 09:56:23.583006: step 180330, loss = 2.68 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 09:56:28.261850: step 180340, loss = 2.82 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 09:56:32.892499: step 180350, loss = 2.97 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 09:56:37.611058: step 180360, loss = 2.58 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 09:56:42.271939: step 180370, loss = 2.81 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 09:56:48.000822: step 180380, loss = 2.62 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 09:56:52.805200: step 180390, loss = 2.84 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 09:56:57.581284: step 180400, loss = 2.80 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 09:57:04.898009: step 180410, loss = 2.88 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 09:57:10.532865: step 180420, loss = 2.91 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 09:57:16.291882: step 180430, loss = 2.66 (244.5 examples/sec; 0.523 sec/batch)
2016-07-16 09:57:21.498077: step 180440, loss = 2.90 (210.5 examples/sec; 0.608 sec/batch)
2016-07-16 09:57:26.985262: step 180450, loss = 2.99 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 09:57:31.710307: step 180460, loss = 2.92 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 09:57:36.561253: step 180470, loss = 2.95 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 09:57:41.276737: step 180480, loss = 2.81 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 09:57:47.048429: step 180490, loss = 3.01 (188.9 examples/sec; 0.678 sec/batch)
2016-07-16 09:57:53.074734: step 180500, loss = 3.00 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 09:57:58.917722: step 180510, loss = 2.70 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 09:58:03.764127: step 180520, loss = 2.79 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 09:58:08.502653: step 180530, loss = 2.91 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 09:58:13.346348: step 180540, loss = 2.89 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 09:58:18.116185: step 180550, loss = 2.84 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 09:58:24.163350: step 180560, loss = 2.86 (190.2 examples/sec; 0.673 sec/batch)
2016-07-16 09:58:29.865116: step 180570, loss = 2.58 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 09:58:34.629130: step 180580, loss = 2.65 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 09:58:39.514278: step 180590, loss = 2.64 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 09:58:44.177688: step 180600, loss = 2.75 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 09:58:50.039929: step 180610, loss = 2.61 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 09:58:54.821940: step 180620, loss = 2.96 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 09:58:59.584077: step 180630, loss = 2.81 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 09:59:04.457200: step 180640, loss = 2.84 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 09:59:09.177498: step 180650, loss = 3.00 (255.4 examples/sec; 0.501 sec/batch)
2016-07-16 09:59:14.925285: step 180660, loss = 2.76 (189.0 examples/sec; 0.677 sec/batch)
2016-07-16 09:59:21.016755: step 180670, loss = 2.55 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 09:59:25.788696: step 180680, loss = 2.59 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 09:59:30.564678: step 180690, loss = 2.58 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 09:59:35.274828: step 180700, loss = 2.67 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 09:59:40.808529: step 180710, loss = 2.67 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 09:59:45.457337: step 180720, loss = 2.83 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 09:59:50.102977: step 180730, loss = 2.65 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 09:59:54.778573: step 180740, loss = 2.81 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 09:59:59.456765: step 180750, loss = 2.74 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 10:00:05.302837: step 180760, loss = 2.61 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 10:00:10.102307: step 180770, loss = 2.65 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 10:00:14.723308: step 180780, loss = 2.68 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 10:00:19.409429: step 180790, loss = 2.68 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 10:00:25.129126: step 180800, loss = 2.51 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 10:00:30.929996: step 180810, loss = 2.82 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 10:00:35.802041: step 180820, loss = 2.60 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 10:00:40.529816: step 180830, loss = 2.67 (251.3 examples/sec; 0.509 sec/batch)
2016-07-16 10:00:45.363250: step 180840, loss = 3.17 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 10:00:50.195899: step 180850, loss = 2.74 (247.6 examples/sec; 0.517 sec/batch)
2016-07-16 10:00:56.224680: step 180860, loss = 2.74 (188.0 examples/sec; 0.681 sec/batch)
2016-07-16 10:01:01.977531: step 180870, loss = 2.97 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 10:01:06.752639: step 180880, loss = 2.57 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 10:01:11.376369: step 180890, loss = 2.80 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 10:01:16.064326: step 180900, loss = 2.98 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 10:01:23.139026: step 180910, loss = 2.70 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 10:01:27.895465: step 180920, loss = 2.67 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 10:01:32.751340: step 180930, loss = 2.79 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 10:01:37.426099: step 180940, loss = 2.67 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 10:01:42.062886: step 180950, loss = 2.77 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 10:01:47.580655: step 180960, loss = 2.81 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 10:01:52.613957: step 180970, loss = 2.98 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 10:01:57.316572: step 180980, loss = 2.95 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 10:02:03.008355: step 180990, loss = 2.90 (187.9 examples/sec; 0.681 sec/batch)
2016-07-16 10:02:09.102617: step 181000, loss = 2.92 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 10:02:14.833548: step 181010, loss = 2.59 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 10:02:19.610082: step 181020, loss = 2.71 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 10:02:24.256919: step 181030, loss = 2.77 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 10:02:29.085872: step 181040, loss = 2.90 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 10:02:33.834299: step 181050, loss = 2.81 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 10:02:39.651466: step 181060, loss = 2.60 (189.4 examples/sec; 0.676 sec/batch)
2016-07-16 10:02:45.540254: step 181070, loss = 2.87 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 10:02:51.036541: step 181080, loss = 2.63 (200.6 examples/sec; 0.638 sec/batch)
2016-07-16 10:02:56.223149: step 181090, loss = 2.71 (245.6 examples/sec; 0.521 sec/batch)
2016-07-16 10:03:00.902495: step 181100, loss = 2.93 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 10:03:07.918205: step 181110, loss = 2.85 (190.8 examples/sec; 0.671 sec/batch)
2016-07-16 10:03:13.801992: step 181120, loss = 2.80 (247.2 examples/sec; 0.518 sec/batch)
2016-07-16 10:03:18.620573: step 181130, loss = 2.91 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 10:03:23.458845: step 181140, loss = 2.92 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 10:03:29.795207: step 181150, loss = 2.76 (198.5 examples/sec; 0.645 sec/batch)
2016-07-16 10:03:35.257719: step 181160, loss = 2.85 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 10:03:40.001480: step 181170, loss = 2.68 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 10:03:44.879875: step 181180, loss = 2.79 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 10:03:49.549189: step 181190, loss = 3.11 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 10:03:54.192049: step 181200, loss = 2.82 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 10:04:00.902165: step 181210, loss = 2.63 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 10:04:06.263911: step 181220, loss = 2.52 (200.9 examples/sec; 0.637 sec/batch)
2016-07-16 10:04:11.594972: step 181230, loss = 2.64 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 10:04:17.342316: step 181240, loss = 2.91 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 10:04:22.152210: step 181250, loss = 2.61 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 10:04:26.966548: step 181260, loss = 2.77 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 10:04:33.133714: step 181270, loss = 2.92 (208.5 examples/sec; 0.614 sec/batch)
2016-07-16 10:04:38.708947: step 181280, loss = 2.71 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 10:04:43.520268: step 181290, loss = 2.81 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 10:04:48.405743: step 181300, loss = 2.74 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 10:04:55.485021: step 181310, loss = 2.56 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 10:05:00.141533: step 181320, loss = 2.72 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 10:05:05.961499: step 181330, loss = 3.11 (252.5 examples/sec; 0.507 sec/batch)
2016-07-16 10:05:11.514718: step 181340, loss = 2.68 (211.2 examples/sec; 0.606 sec/batch)
2016-07-16 10:05:16.618772: step 181350, loss = 2.94 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 10:05:21.354491: step 181360, loss = 2.72 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 10:05:26.157315: step 181370, loss = 2.74 (282.2 examples/sec; 0.454 sec/batch)
2016-07-16 10:05:30.979236: step 181380, loss = 2.82 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 10:05:37.155872: step 181390, loss = 3.00 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 10:05:42.759342: step 181400, loss = 2.54 (246.3 examples/sec; 0.520 sec/batch)
2016-07-16 10:05:48.478175: step 181410, loss = 2.83 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 10:05:53.796768: step 181420, loss = 2.69 (190.1 examples/sec; 0.673 sec/batch)
2016-07-16 10:06:00.248736: step 181430, loss = 2.82 (202.0 examples/sec; 0.634 sec/batch)
2016-07-16 10:06:05.366128: step 181440, loss = 2.77 (201.8 examples/sec; 0.634 sec/batch)
2016-07-16 10:06:10.929093: step 181450, loss = 2.70 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 10:06:15.668471: step 181460, loss = 2.86 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 10:06:20.545012: step 181470, loss = 2.65 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 10:06:25.266547: step 181480, loss = 2.65 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 10:06:30.082080: step 181490, loss = 2.76 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 10:06:34.730876: step 181500, loss = 2.58 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 10:06:40.343504: step 181510, loss = 3.07 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 10:06:44.981406: step 181520, loss = 2.60 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 10:06:49.644544: step 181530, loss = 2.82 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 10:06:54.277647: step 181540, loss = 2.58 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 10:06:58.917705: step 181550, loss = 2.92 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 10:07:04.726661: step 181560, loss = 2.63 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 10:07:09.595231: step 181570, loss = 2.72 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 10:07:14.242727: step 181580, loss = 2.75 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 10:07:18.981794: step 181590, loss = 2.96 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 10:07:24.717985: step 181600, loss = 2.78 (221.5 examples/sec; 0.578 sec/batch)
2016-07-16 10:07:30.491019: step 181610, loss = 3.00 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 10:07:35.178923: step 181620, loss = 2.91 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 10:07:39.990807: step 181630, loss = 2.83 (254.8 examples/sec; 0.502 sec/batch)
2016-07-16 10:07:45.790757: step 181640, loss = 2.75 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 10:07:51.326362: step 181650, loss = 3.10 (202.1 examples/sec; 0.633 sec/batch)
2016-07-16 10:07:56.501395: step 181660, loss = 2.86 (245.3 examples/sec; 0.522 sec/batch)
2016-07-16 10:08:01.200701: step 181670, loss = 3.08 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 10:08:06.653450: step 181680, loss = 2.84 (184.9 examples/sec; 0.692 sec/batch)
2016-07-16 10:08:12.910380: step 181690, loss = 2.77 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 10:08:17.775029: step 181700, loss = 2.68 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 10:08:23.518058: step 181710, loss = 2.91 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 10:08:28.235187: step 181720, loss = 2.85 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 10:08:33.331979: step 181730, loss = 2.52 (186.8 examples/sec; 0.685 sec/batch)
2016-07-16 10:08:39.855064: step 181740, loss = 2.66 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 10:08:44.838958: step 181750, loss = 2.67 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 10:08:49.608764: step 181760, loss = 2.68 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 10:08:54.405726: step 181770, loss = 2.83 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 10:08:59.208645: step 181780, loss = 2.89 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 10:09:04.013922: step 181790, loss = 2.74 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 10:09:08.631168: step 181800, loss = 2.62 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 10:09:14.848717: step 181810, loss = 2.79 (211.2 examples/sec; 0.606 sec/batch)
2016-07-16 10:09:20.165777: step 181820, loss = 2.64 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 10:09:25.902057: step 181830, loss = 2.70 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 10:09:31.427694: step 181840, loss = 2.58 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 10:09:36.614168: step 181850, loss = 2.58 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 10:09:41.323959: step 181860, loss = 2.64 (255.1 examples/sec; 0.502 sec/batch)
2016-07-16 10:09:46.156687: step 181870, loss = 2.89 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 10:09:50.928926: step 181880, loss = 2.44 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 10:09:57.066760: step 181890, loss = 2.56 (197.9 examples/sec; 0.647 sec/batch)
2016-07-16 10:10:02.808334: step 181900, loss = 2.73 (256.2 examples/sec; 0.500 sec/batch)
2016-07-16 10:10:08.561629: step 181910, loss = 2.58 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 10:10:13.186361: step 181920, loss = 2.55 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 10:10:18.332331: step 181930, loss = 2.69 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 10:10:23.739020: step 181940, loss = 2.70 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 10:10:29.434812: step 181950, loss = 2.58 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 10:10:34.763360: step 181960, loss = 2.61 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 10:10:40.054475: step 181970, loss = 2.85 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 10:10:44.761342: step 181980, loss = 2.59 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 10:10:49.724200: step 181990, loss = 2.80 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 10:10:54.478888: step 182000, loss = 2.96 (265.8 examples/sec; 0.481 sec/batch)
2016-07-16 10:11:00.224241: step 182010, loss = 2.76 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 10:11:05.160660: step 182020, loss = 2.60 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 10:11:09.897480: step 182030, loss = 2.97 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 10:11:15.653847: step 182040, loss = 2.55 (186.0 examples/sec; 0.688 sec/batch)
2016-07-16 10:11:21.703011: step 182050, loss = 2.77 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 10:11:26.521366: step 182060, loss = 2.87 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 10:11:31.326635: step 182070, loss = 2.79 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 10:11:37.530344: step 182080, loss = 2.76 (200.7 examples/sec; 0.638 sec/batch)
2016-07-16 10:11:42.976552: step 182090, loss = 2.61 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 10:11:47.614170: step 182100, loss = 2.81 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 10:11:53.212816: step 182110, loss = 2.93 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 10:11:57.945054: step 182120, loss = 2.69 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 10:12:02.563347: step 182130, loss = 2.86 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 10:12:08.068145: step 182140, loss = 2.93 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 10:12:13.094153: step 182150, loss = 2.77 (248.4 examples/sec; 0.515 sec/batch)
2016-07-16 10:12:17.845955: step 182160, loss = 3.00 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 10:12:22.742804: step 182170, loss = 2.68 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 10:12:27.546752: step 182180, loss = 2.55 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 10:12:32.287429: step 182190, loss = 2.75 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 10:12:37.145832: step 182200, loss = 2.87 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 10:12:42.822649: step 182210, loss = 2.85 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 10:12:47.592016: step 182220, loss = 2.68 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 10:12:52.420758: step 182230, loss = 2.84 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 10:12:57.142183: step 182240, loss = 2.71 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 10:13:01.986029: step 182250, loss = 2.72 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 10:13:06.782345: step 182260, loss = 2.92 (254.1 examples/sec; 0.504 sec/batch)
2016-07-16 10:13:11.525244: step 182270, loss = 2.68 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 10:13:16.345384: step 182280, loss = 2.70 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 10:13:22.855661: step 182290, loss = 2.73 (202.4 examples/sec; 0.633 sec/batch)
2016-07-16 10:13:28.160289: step 182300, loss = 2.66 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 10:13:33.845768: step 182310, loss = 2.74 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 10:13:38.610502: step 182320, loss = 2.74 (276.2 examples/sec; 0.464 sec/batch)
2016-07-16 10:13:43.229841: step 182330, loss = 2.62 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 10:13:47.960817: step 182340, loss = 2.77 (252.6 examples/sec; 0.507 sec/batch)
2016-07-16 10:13:53.653858: step 182350, loss = 3.13 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 10:13:59.031859: step 182360, loss = 2.67 (200.7 examples/sec; 0.638 sec/batch)
2016-07-16 10:14:04.342425: step 182370, loss = 2.61 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 10:14:09.100008: step 182380, loss = 2.85 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 10:14:14.435855: step 182390, loss = 2.60 (189.9 examples/sec; 0.674 sec/batch)
2016-07-16 10:14:20.948874: step 182400, loss = 2.84 (208.3 examples/sec; 0.614 sec/batch)
2016-07-16 10:14:27.391867: step 182410, loss = 2.79 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 10:14:32.570893: step 182420, loss = 2.76 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 10:14:38.320798: step 182430, loss = 2.85 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 10:14:43.115525: step 182440, loss = 2.71 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 10:14:47.887885: step 182450, loss = 2.84 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 10:14:52.611422: step 182460, loss = 2.81 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 10:14:57.821254: step 182470, loss = 2.85 (183.5 examples/sec; 0.698 sec/batch)
2016-07-16 10:15:04.283311: step 182480, loss = 2.58 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 10:15:09.283253: step 182490, loss = 2.69 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 10:15:14.055094: step 182500, loss = 2.77 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 10:15:19.857928: step 182510, loss = 2.90 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 10:15:24.666026: step 182520, loss = 2.68 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 10:15:31.217593: step 182530, loss = 2.74 (199.2 examples/sec; 0.643 sec/batch)
2016-07-16 10:15:36.485078: step 182540, loss = 2.81 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 10:15:42.237789: step 182550, loss = 2.98 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 10:15:47.114960: step 182560, loss = 2.59 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 10:15:52.023504: step 182570, loss = 2.74 (246.0 examples/sec; 0.520 sec/batch)
2016-07-16 10:15:58.371121: step 182580, loss = 2.86 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 10:16:03.656283: step 182590, loss = 2.64 (275.6 examples/sec; 0.465 sec/batch)
2016-07-16 10:16:08.303051: step 182600, loss = 2.74 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 10:16:13.841960: step 182610, loss = 2.45 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 10:16:18.519695: step 182620, loss = 2.87 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 10:16:23.217783: step 182630, loss = 2.65 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 10:16:27.908514: step 182640, loss = 2.74 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 10:16:33.637909: step 182650, loss = 2.67 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 10:16:38.481485: step 182660, loss = 2.71 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 10:16:43.230829: step 182670, loss = 2.72 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 10:16:49.300989: step 182680, loss = 2.85 (195.1 examples/sec; 0.656 sec/batch)
2016-07-16 10:16:54.976155: step 182690, loss = 2.51 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 10:16:59.764371: step 182700, loss = 2.89 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 10:17:05.630111: step 182710, loss = 2.94 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 10:17:10.317954: step 182720, loss = 2.74 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 10:17:16.092442: step 182730, loss = 2.79 (188.3 examples/sec; 0.680 sec/batch)
2016-07-16 10:17:22.114547: step 182740, loss = 2.81 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 10:17:26.890440: step 182750, loss = 2.89 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 10:17:31.693937: step 182760, loss = 2.79 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 10:17:36.480969: step 182770, loss = 2.75 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 10:17:41.102465: step 182780, loss = 2.67 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 10:17:46.028308: step 182790, loss = 3.05 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 10:17:51.611040: step 182800, loss = 2.73 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 10:17:58.392162: step 182810, loss = 3.15 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 10:18:03.185388: step 182820, loss = 2.71 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 10:18:07.813393: step 182830, loss = 3.04 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 10:18:12.478088: step 182840, loss = 2.93 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 10:18:17.064214: step 182850, loss = 2.79 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 10:18:21.950795: step 182860, loss = 2.79 (199.8 examples/sec; 0.641 sec/batch)
2016-07-16 10:18:27.546238: step 182870, loss = 2.74 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 10:18:32.277134: step 182880, loss = 2.72 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 10:18:37.146307: step 182890, loss = 2.68 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 10:18:43.711171: step 182900, loss = 2.66 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 10:18:49.883616: step 182910, loss = 2.75 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 10:18:55.061168: step 182920, loss = 2.90 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 10:19:00.364135: step 182930, loss = 2.81 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 10:19:06.117668: step 182940, loss = 2.88 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 10:19:10.926832: step 182950, loss = 2.69 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 10:19:15.719126: step 182960, loss = 2.61 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 10:19:21.888180: step 182970, loss = 2.89 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 10:19:27.484739: step 182980, loss = 2.75 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 10:19:32.217783: step 182990, loss = 2.52 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 10:19:37.071277: step 183000, loss = 2.79 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 10:19:42.862147: step 183010, loss = 2.74 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 10:19:47.644567: step 183020, loss = 2.67 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 10:19:52.312481: step 183030, loss = 2.90 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 10:19:57.007838: step 183040, loss = 2.40 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 10:20:02.771710: step 183050, loss = 2.53 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 10:20:07.538655: step 183060, loss = 2.71 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 10:20:12.372407: step 183070, loss = 2.92 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 10:20:18.695191: step 183080, loss = 2.68 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 10:20:24.118831: step 183090, loss = 2.73 (255.7 examples/sec; 0.501 sec/batch)
2016-07-16 10:20:28.828693: step 183100, loss = 2.58 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 10:20:34.665181: step 183110, loss = 2.68 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 10:20:39.409957: step 183120, loss = 2.71 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 10:20:45.495261: step 183130, loss = 2.86 (195.8 examples/sec; 0.654 sec/batch)
2016-07-16 10:20:51.215504: step 183140, loss = 2.56 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 10:20:55.986068: step 183150, loss = 2.82 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 10:21:00.856015: step 183160, loss = 2.81 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 10:21:05.584570: step 183170, loss = 2.95 (284.6 examples/sec; 0.450 sec/batch)
2016-07-16 10:21:10.214708: step 183180, loss = 2.68 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 10:21:15.474926: step 183190, loss = 2.81 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 10:21:20.744874: step 183200, loss = 2.81 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 10:21:27.621249: step 183210, loss = 2.94 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 10:21:32.371508: step 183220, loss = 2.97 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 10:21:37.188558: step 183230, loss = 2.73 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 10:21:41.915368: step 183240, loss = 2.72 (245.5 examples/sec; 0.521 sec/batch)
2016-07-16 10:21:46.776683: step 183250, loss = 2.66 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 10:21:51.591350: step 183260, loss = 2.79 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 10:21:56.357066: step 183270, loss = 3.03 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 10:22:01.220572: step 183280, loss = 2.79 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 10:22:05.880153: step 183290, loss = 2.92 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 10:22:11.391969: step 183300, loss = 3.11 (187.3 examples/sec; 0.684 sec/batch)
2016-07-16 10:22:17.852602: step 183310, loss = 2.76 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 10:22:22.987568: step 183320, loss = 2.75 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 10:22:28.389204: step 183330, loss = 2.71 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 10:22:34.130948: step 183340, loss = 2.50 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 10:22:38.994845: step 183350, loss = 2.71 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 10:22:43.755488: step 183360, loss = 2.90 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 10:22:48.480629: step 183370, loss = 2.61 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 10:22:53.305440: step 183380, loss = 2.68 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 10:22:57.992004: step 183390, loss = 2.55 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 10:23:03.522714: step 183400, loss = 2.69 (192.7 examples/sec; 0.664 sec/batch)
2016-07-16 10:23:09.917335: step 183410, loss = 2.66 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 10:23:14.570329: step 183420, loss = 2.87 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 10:23:19.842986: step 183430, loss = 2.70 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 10:23:25.125405: step 183440, loss = 2.74 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 10:23:29.831537: step 183450, loss = 2.67 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 10:23:34.564226: step 183460, loss = 2.71 (253.0 examples/sec; 0.506 sec/batch)
2016-07-16 10:23:39.885635: step 183470, loss = 2.63 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 10:23:45.119064: step 183480, loss = 2.84 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 10:23:50.919766: step 183490, loss = 2.76 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 10:23:55.672249: step 183500, loss = 2.45 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 10:24:01.545178: step 183510, loss = 3.08 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 10:24:06.237520: step 183520, loss = 2.70 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 10:24:11.836585: step 183530, loss = 2.56 (190.1 examples/sec; 0.673 sec/batch)
2016-07-16 10:24:18.043130: step 183540, loss = 2.77 (254.1 examples/sec; 0.504 sec/batch)
2016-07-16 10:24:23.342746: step 183550, loss = 2.60 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 10:24:28.627492: step 183560, loss = 2.72 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 10:24:33.375896: step 183570, loss = 2.61 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 10:24:37.954941: step 183580, loss = 2.62 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 10:24:42.693847: step 183590, loss = 2.67 (252.4 examples/sec; 0.507 sec/batch)
2016-07-16 10:24:47.348184: step 183600, loss = 2.87 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 10:24:53.047810: step 183610, loss = 2.73 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 10:24:57.839424: step 183620, loss = 2.83 (217.2 examples/sec; 0.589 sec/batch)
2016-07-16 10:25:03.543717: step 183630, loss = 2.76 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 10:25:08.272595: step 183640, loss = 2.48 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 10:25:13.142120: step 183650, loss = 2.64 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 10:25:17.879921: step 183660, loss = 2.62 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 10:25:22.513470: step 183670, loss = 2.65 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 10:25:27.770844: step 183680, loss = 2.76 (208.0 examples/sec; 0.616 sec/batch)
2016-07-16 10:25:33.265237: step 183690, loss = 2.87 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 10:25:39.199734: step 183700, loss = 2.92 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 10:25:45.993535: step 183710, loss = 3.04 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 10:25:50.785729: step 183720, loss = 2.65 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 10:25:55.511781: step 183730, loss = 2.76 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 10:26:00.316710: step 183740, loss = 2.80 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 10:26:05.148243: step 183750, loss = 2.68 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 10:26:11.655557: step 183760, loss = 2.66 (201.1 examples/sec; 0.637 sec/batch)
2016-07-16 10:26:16.968527: step 183770, loss = 2.86 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 10:26:21.672481: step 183780, loss = 2.63 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 10:26:26.318201: step 183790, loss = 2.63 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 10:26:31.042578: step 183800, loss = 2.81 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 10:26:36.543848: step 183810, loss = 2.80 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 10:26:41.126764: step 183820, loss = 2.69 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 10:26:45.902617: step 183830, loss = 2.73 (223.9 examples/sec; 0.572 sec/batch)
2016-07-16 10:26:51.610309: step 183840, loss = 2.77 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 10:26:56.358732: step 183850, loss = 2.49 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 10:27:01.221356: step 183860, loss = 2.80 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 10:27:05.908977: step 183870, loss = 2.65 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 10:27:10.779144: step 183880, loss = 2.77 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 10:27:15.391112: step 183890, loss = 2.66 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 10:27:20.045549: step 183900, loss = 2.56 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 10:27:25.720093: step 183910, loss = 2.79 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 10:27:30.653006: step 183920, loss = 2.77 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 10:27:36.174607: step 183930, loss = 2.77 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 10:27:41.909732: step 183940, loss = 2.69 (239.2 examples/sec; 0.535 sec/batch)
2016-07-16 10:27:47.187573: step 183950, loss = 2.87 (205.3 examples/sec; 0.624 sec/batch)
2016-07-16 10:27:52.610200: step 183960, loss = 2.64 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 10:27:57.298155: step 183970, loss = 2.87 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 10:28:02.425089: step 183980, loss = 3.04 (188.5 examples/sec; 0.679 sec/batch)
2016-07-16 10:28:08.874458: step 183990, loss = 2.71 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 10:28:13.952380: step 184000, loss = 2.77 (227.2 examples/sec; 0.563 sec/batch)
2016-07-16 10:28:20.877519: step 184010, loss = 2.86 (246.5 examples/sec; 0.519 sec/batch)
2016-07-16 10:28:26.671984: step 184020, loss = 2.92 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 10:28:31.501399: step 184030, loss = 2.74 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 10:28:36.274962: step 184040, loss = 2.87 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 10:28:42.253073: step 184050, loss = 2.66 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 10:28:46.918962: step 184060, loss = 2.63 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 10:28:52.210686: step 184070, loss = 2.59 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 10:28:57.379824: step 184080, loss = 2.71 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 10:29:02.061930: step 184090, loss = 2.82 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 10:29:06.678911: step 184100, loss = 2.87 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 10:29:13.141621: step 184110, loss = 2.84 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 10:29:18.170092: step 184120, loss = 2.61 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 10:29:22.892791: step 184130, loss = 2.81 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 10:29:28.599198: step 184140, loss = 2.63 (189.7 examples/sec; 0.675 sec/batch)
2016-07-16 10:29:34.692803: step 184150, loss = 2.52 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 10:29:39.477624: step 184160, loss = 2.55 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 10:29:44.086833: step 184170, loss = 2.73 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 10:29:48.722634: step 184180, loss = 2.81 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 10:29:53.377273: step 184190, loss = 2.75 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 10:29:58.008485: step 184200, loss = 2.80 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 10:30:04.287523: step 184210, loss = 2.52 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 10:30:09.504023: step 184220, loss = 2.77 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 10:30:15.282228: step 184230, loss = 2.86 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 10:30:20.098366: step 184240, loss = 2.63 (278.0 examples/sec; 0.461 sec/batch)
2016-07-16 10:30:24.902810: step 184250, loss = 2.67 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 10:30:31.244450: step 184260, loss = 2.62 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 10:30:36.782250: step 184270, loss = 2.85 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 10:30:41.684548: step 184280, loss = 2.67 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 10:30:47.004457: step 184290, loss = 3.01 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 10:30:52.320345: step 184300, loss = 2.91 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 10:30:58.721830: step 184310, loss = 2.86 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 10:31:03.516051: step 184320, loss = 2.79 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 10:31:09.511970: step 184330, loss = 2.83 (189.6 examples/sec; 0.675 sec/batch)
2016-07-16 10:31:15.377102: step 184340, loss = 2.64 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 10:31:20.168890: step 184350, loss = 2.84 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 10:31:25.064555: step 184360, loss = 2.76 (241.5 examples/sec; 0.530 sec/batch)
2016-07-16 10:31:31.423550: step 184370, loss = 2.55 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 10:31:36.861615: step 184380, loss = 2.93 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 10:31:41.579365: step 184390, loss = 2.77 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 10:31:46.457721: step 184400, loss = 2.79 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 10:31:52.081679: step 184410, loss = 2.66 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 10:31:56.722828: step 184420, loss = 2.78 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 10:32:01.366941: step 184430, loss = 2.85 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 10:32:05.966483: step 184440, loss = 2.67 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 10:32:10.603658: step 184450, loss = 2.88 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 10:32:15.779033: step 184460, loss = 3.03 (199.8 examples/sec; 0.641 sec/batch)
2016-07-16 10:32:21.128992: step 184470, loss = 2.87 (257.3 examples/sec; 0.498 sec/batch)
2016-07-16 10:32:26.950090: step 184480, loss = 2.85 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 10:32:32.446263: step 184490, loss = 2.77 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 10:32:37.655644: step 184500, loss = 2.90 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 10:32:44.620628: step 184510, loss = 2.76 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 10:32:49.417196: step 184520, loss = 2.88 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 10:32:54.024690: step 184530, loss = 2.70 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 10:32:58.744221: step 184540, loss = 2.59 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 10:33:03.398550: step 184550, loss = 2.62 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 10:33:08.069267: step 184560, loss = 2.55 (269.2 examples/sec; 0.476 sec/batch)
2016-07-16 10:33:12.804054: step 184570, loss = 2.79 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 10:33:18.590995: step 184580, loss = 2.76 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 10:33:23.410379: step 184590, loss = 2.82 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 10:33:28.197546: step 184600, loss = 2.84 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 10:33:33.984611: step 184610, loss = 2.75 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 10:33:38.834010: step 184620, loss = 2.58 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 10:33:43.619569: step 184630, loss = 2.70 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 10:33:48.463703: step 184640, loss = 2.85 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 10:33:53.088415: step 184650, loss = 2.61 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 10:33:57.764446: step 184660, loss = 2.46 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 10:34:03.539156: step 184670, loss = 2.96 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 10:34:09.154247: step 184680, loss = 2.58 (209.1 examples/sec; 0.612 sec/batch)
2016-07-16 10:34:14.228272: step 184690, loss = 2.87 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 10:34:18.969156: step 184700, loss = 2.74 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 10:34:24.731633: step 184710, loss = 2.64 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 10:34:29.619145: step 184720, loss = 2.67 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 10:34:34.304828: step 184730, loss = 2.63 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 10:34:39.694026: step 184740, loss = 2.60 (187.1 examples/sec; 0.684 sec/batch)
2016-07-16 10:34:46.109111: step 184750, loss = 2.58 (219.0 examples/sec; 0.585 sec/batch)
2016-07-16 10:34:50.984724: step 184760, loss = 2.79 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 10:34:55.715086: step 184770, loss = 2.83 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 10:35:01.682397: step 184780, loss = 2.66 (186.9 examples/sec; 0.685 sec/batch)
2016-07-16 10:35:07.539831: step 184790, loss = 2.66 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 10:35:12.339687: step 184800, loss = 2.81 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 10:35:18.190954: step 184810, loss = 2.69 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 10:35:22.854679: step 184820, loss = 2.66 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 10:35:27.581573: step 184830, loss = 2.67 (241.9 examples/sec; 0.529 sec/batch)
2016-07-16 10:35:32.220596: step 184840, loss = 2.81 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 10:35:36.881482: step 184850, loss = 2.74 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 10:35:41.612692: step 184860, loss = 2.99 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 10:35:46.371861: step 184870, loss = 2.93 (226.0 examples/sec; 0.566 sec/batch)
2016-07-16 10:35:52.109202: step 184880, loss = 2.82 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 10:35:56.867980: step 184890, loss = 2.50 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 10:36:01.745929: step 184900, loss = 2.74 (263.1 examples/sec; 0.486 sec/batch)
2016-07-16 10:36:09.933544: step 184910, loss = 2.80 (202.7 examples/sec; 0.632 sec/batch)
2016-07-16 10:36:14.829124: step 184920, loss = 2.67 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 10:36:19.566204: step 184930, loss = 2.91 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 10:36:24.387431: step 184940, loss = 2.68 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 10:36:29.215152: step 184950, loss = 2.64 (242.8 examples/sec; 0.527 sec/batch)
2016-07-16 10:36:33.968602: step 184960, loss = 2.85 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 10:36:38.626746: step 184970, loss = 2.63 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 10:36:43.769319: step 184980, loss = 2.81 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 10:36:48.999096: step 184990, loss = 2.73 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 10:36:53.758172: step 185000, loss = 2.79 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 10:37:00.725526: step 185010, loss = 2.62 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 10:37:06.446331: step 185020, loss = 2.80 (247.7 examples/sec; 0.517 sec/batch)
2016-07-16 10:37:11.296840: step 185030, loss = 2.76 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 10:37:15.934853: step 185040, loss = 2.71 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 10:37:20.556002: step 185050, loss = 2.71 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 10:37:25.206807: step 185060, loss = 2.57 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 10:37:29.899773: step 185070, loss = 2.78 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 10:37:35.676229: step 185080, loss = 2.77 (253.9 examples/sec; 0.504 sec/batch)
2016-07-16 10:37:40.491282: step 185090, loss = 2.90 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 10:37:45.361262: step 185100, loss = 2.81 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 10:37:51.215522: step 185110, loss = 2.97 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 10:37:56.012445: step 185120, loss = 2.76 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 10:38:00.832213: step 185130, loss = 2.97 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 10:38:05.586292: step 185140, loss = 2.97 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 10:38:10.457476: step 185150, loss = 2.86 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 10:38:15.162923: step 185160, loss = 2.74 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 10:38:19.982801: step 185170, loss = 2.91 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 10:38:24.779869: step 185180, loss = 2.73 (255.3 examples/sec; 0.501 sec/batch)
2016-07-16 10:38:30.976137: step 185190, loss = 2.72 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 10:38:36.579231: step 185200, loss = 2.70 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 10:38:42.464768: step 185210, loss = 2.57 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 10:38:47.318018: step 185220, loss = 2.58 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 10:38:51.982751: step 185230, loss = 2.60 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 10:38:56.625057: step 185240, loss = 2.78 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 10:39:02.379885: step 185250, loss = 2.82 (223.7 examples/sec; 0.572 sec/batch)
2016-07-16 10:39:07.579073: step 185260, loss = 2.79 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 10:39:13.014487: step 185270, loss = 2.74 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 10:39:17.717505: step 185280, loss = 2.79 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 10:39:22.332877: step 185290, loss = 2.73 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 10:39:27.275964: step 185300, loss = 2.57 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 10:39:34.016781: step 185310, loss = 2.45 (256.8 examples/sec; 0.498 sec/batch)
2016-07-16 10:39:38.772384: step 185320, loss = 2.55 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 10:39:43.621991: step 185330, loss = 2.81 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 10:39:48.401721: step 185340, loss = 2.83 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 10:39:54.479558: step 185350, loss = 2.71 (195.5 examples/sec; 0.655 sec/batch)
2016-07-16 10:40:00.189663: step 185360, loss = 2.71 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 10:40:04.992976: step 185370, loss = 2.46 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 10:40:09.854881: step 185380, loss = 2.66 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 10:40:16.306934: step 185390, loss = 2.97 (207.9 examples/sec; 0.616 sec/batch)
2016-07-16 10:40:21.601224: step 185400, loss = 3.06 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 10:40:27.303180: step 185410, loss = 2.73 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 10:40:31.957314: step 185420, loss = 2.63 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 10:40:36.565842: step 185430, loss = 2.82 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 10:40:41.242117: step 185440, loss = 2.57 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 10:40:46.968692: step 185450, loss = 2.97 (253.9 examples/sec; 0.504 sec/batch)
2016-07-16 10:40:52.255176: step 185460, loss = 2.85 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 10:40:57.598691: step 185470, loss = 2.64 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 10:41:03.384817: step 185480, loss = 3.00 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 10:41:08.230346: step 185490, loss = 2.85 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 10:41:13.037116: step 185500, loss = 2.63 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 10:41:18.780565: step 185510, loss = 2.94 (279.2 examples/sec; 0.459 sec/batch)
2016-07-16 10:41:23.429297: step 185520, loss = 2.70 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 10:41:28.047831: step 185530, loss = 2.64 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 10:41:32.688676: step 185540, loss = 2.82 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 10:41:38.374483: step 185550, loss = 2.87 (199.3 examples/sec; 0.642 sec/batch)
2016-07-16 10:41:43.308422: step 185560, loss = 2.81 (250.9 examples/sec; 0.510 sec/batch)
2016-07-16 10:41:48.070182: step 185570, loss = 3.09 (253.6 examples/sec; 0.505 sec/batch)
2016-07-16 10:41:52.853953: step 185580, loss = 2.94 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 10:41:57.508205: step 185590, loss = 2.71 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 10:42:02.250654: step 185600, loss = 2.72 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 10:42:07.804445: step 185610, loss = 2.79 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 10:42:13.270044: step 185620, loss = 3.11 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 10:42:18.403092: step 185630, loss = 2.87 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 10:42:23.101782: step 185640, loss = 2.82 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 10:42:28.682243: step 185650, loss = 2.81 (186.3 examples/sec; 0.687 sec/batch)
2016-07-16 10:42:34.956704: step 185660, loss = 2.77 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 10:42:39.785365: step 185670, loss = 2.70 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 10:42:44.572251: step 185680, loss = 2.81 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 10:42:49.320033: step 185690, loss = 2.62 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 10:42:53.940252: step 185700, loss = 2.79 (286.5 examples/sec; 0.447 sec/batch)
2016-07-16 10:42:59.936290: step 185710, loss = 2.63 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 10:43:05.388613: step 185720, loss = 2.59 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 10:43:10.171753: step 185730, loss = 2.75 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 10:43:15.041776: step 185740, loss = 2.68 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 10:43:19.768160: step 185750, loss = 2.80 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 10:43:25.543839: step 185760, loss = 2.60 (192.9 examples/sec; 0.664 sec/batch)
2016-07-16 10:43:31.565176: step 185770, loss = 2.73 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 10:43:37.099199: step 185780, loss = 2.70 (198.3 examples/sec; 0.645 sec/batch)
2016-07-16 10:43:42.269564: step 185790, loss = 2.89 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 10:43:46.982147: step 185800, loss = 2.84 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 10:43:52.878606: step 185810, loss = 2.54 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 10:43:57.715671: step 185820, loss = 2.55 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 10:44:04.135888: step 185830, loss = 3.00 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 10:44:09.518481: step 185840, loss = 2.65 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 10:44:14.265780: step 185850, loss = 2.78 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 10:44:19.086029: step 185860, loss = 2.77 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 10:44:23.733799: step 185870, loss = 2.66 (285.3 examples/sec; 0.449 sec/batch)
2016-07-16 10:44:28.458268: step 185880, loss = 2.58 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 10:44:34.116585: step 185890, loss = 2.84 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 10:44:39.077302: step 185900, loss = 2.86 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 10:44:44.685192: step 185910, loss = 2.60 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 10:44:49.327704: step 185920, loss = 2.76 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 10:44:55.120966: step 185930, loss = 2.96 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 10:44:59.939773: step 185940, loss = 2.62 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 10:45:04.763999: step 185950, loss = 2.65 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 10:45:09.528513: step 185960, loss = 2.69 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 10:45:14.147731: step 185970, loss = 2.87 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 10:45:18.970647: step 185980, loss = 2.85 (211.8 examples/sec; 0.604 sec/batch)
2016-07-16 10:45:24.648960: step 185990, loss = 2.64 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 10:45:30.377881: step 186000, loss = 2.81 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 10:45:36.258973: step 186010, loss = 2.85 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 10:45:41.060537: step 186020, loss = 2.93 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 10:45:45.815075: step 186030, loss = 3.01 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 10:45:50.736307: step 186040, loss = 2.66 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 10:45:55.439907: step 186050, loss = 2.47 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 10:46:00.079862: step 186060, loss = 2.77 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 10:46:05.586203: step 186070, loss = 2.61 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 10:46:10.787129: step 186080, loss = 2.69 (228.6 examples/sec; 0.560 sec/batch)
2016-07-16 10:46:16.571666: step 186090, loss = 2.92 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 10:46:21.307167: step 186100, loss = 2.90 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 10:46:27.415683: step 186110, loss = 2.54 (188.4 examples/sec; 0.680 sec/batch)
2016-07-16 10:46:33.960714: step 186120, loss = 2.78 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 10:46:38.804429: step 186130, loss = 2.63 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 10:46:43.519105: step 186140, loss = 2.71 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 10:46:48.143550: step 186150, loss = 2.89 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 10:46:53.593002: step 186160, loss = 2.79 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 10:46:58.722293: step 186170, loss = 2.69 (240.5 examples/sec; 0.532 sec/batch)
2016-07-16 10:47:04.445849: step 186180, loss = 2.86 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 10:47:10.070066: step 186190, loss = 2.64 (205.8 examples/sec; 0.622 sec/batch)
2016-07-16 10:47:15.068420: step 186200, loss = 2.69 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 10:47:20.745192: step 186210, loss = 2.64 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 10:47:26.758823: step 186220, loss = 2.61 (190.5 examples/sec; 0.672 sec/batch)
2016-07-16 10:47:32.450427: step 186230, loss = 2.66 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 10:47:38.108865: step 186240, loss = 2.77 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 10:47:43.158066: step 186250, loss = 2.91 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 10:47:47.920412: step 186260, loss = 2.71 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 10:47:52.735338: step 186270, loss = 2.85 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 10:47:57.399612: step 186280, loss = 2.65 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 10:48:02.094755: step 186290, loss = 2.87 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 10:48:07.883531: step 186300, loss = 2.69 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 10:48:13.711722: step 186310, loss = 2.76 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 10:48:18.320136: step 186320, loss = 2.78 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 10:48:23.168690: step 186330, loss = 2.84 (209.9 examples/sec; 0.610 sec/batch)
2016-07-16 10:48:28.860281: step 186340, loss = 2.74 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 10:48:33.632724: step 186350, loss = 2.60 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 10:48:38.530141: step 186360, loss = 2.82 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 10:48:43.302718: step 186370, loss = 2.94 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 10:48:48.155580: step 186380, loss = 2.79 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 10:48:52.965424: step 186390, loss = 2.63 (261.5 examples/sec; 0.490 sec/batch)
2016-07-16 10:48:59.050995: step 186400, loss = 2.63 (194.7 examples/sec; 0.657 sec/batch)
2016-07-16 10:49:06.051663: step 186410, loss = 2.87 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 10:49:10.762541: step 186420, loss = 2.85 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 10:49:15.406594: step 186430, loss = 2.68 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 10:49:20.527150: step 186440, loss = 2.76 (198.9 examples/sec; 0.644 sec/batch)
2016-07-16 10:49:25.989517: step 186450, loss = 2.74 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 10:49:30.709810: step 186460, loss = 2.62 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 10:49:35.350780: step 186470, loss = 2.94 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 10:49:40.428039: step 186480, loss = 2.85 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 10:49:45.852433: step 186490, loss = 2.79 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 10:49:51.618063: step 186500, loss = 2.91 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 10:49:57.463357: step 186510, loss = 2.75 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 10:50:02.294275: step 186520, loss = 2.61 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 10:50:07.084353: step 186530, loss = 2.53 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 10:50:11.735727: step 186540, loss = 2.70 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 10:50:16.387963: step 186550, loss = 2.92 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 10:50:21.069568: step 186560, loss = 3.01 (267.5 examples/sec; 0.478 sec/batch)
2016-07-16 10:50:25.713562: step 186570, loss = 2.87 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 10:50:30.383875: step 186580, loss = 2.73 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 10:50:35.263869: step 186590, loss = 2.86 (212.1 examples/sec; 0.603 sec/batch)
2016-07-16 10:50:41.026047: step 186600, loss = 2.73 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 10:50:47.825269: step 186610, loss = 2.91 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 10:50:52.624488: step 186620, loss = 2.78 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 10:50:57.448073: step 186630, loss = 2.59 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 10:51:03.641917: step 186640, loss = 2.97 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 10:51:09.240363: step 186650, loss = 2.80 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 10:51:14.056231: step 186660, loss = 2.62 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 10:51:18.979752: step 186670, loss = 2.78 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 10:51:23.659776: step 186680, loss = 2.72 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 10:51:28.282461: step 186690, loss = 2.61 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 10:51:33.697408: step 186700, loss = 2.63 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 10:51:39.968692: step 186710, loss = 2.81 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 10:51:44.631890: step 186720, loss = 2.78 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 10:51:49.375242: step 186730, loss = 2.85 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 10:51:54.061521: step 186740, loss = 2.85 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 10:51:58.731002: step 186750, loss = 2.72 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 10:52:04.552905: step 186760, loss = 2.70 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 10:52:09.344744: step 186770, loss = 2.88 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 10:52:14.204585: step 186780, loss = 2.70 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 10:52:18.920250: step 186790, loss = 2.82 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 10:52:23.486626: step 186800, loss = 2.54 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 10:52:29.838799: step 186810, loss = 2.78 (205.3 examples/sec; 0.624 sec/batch)
2016-07-16 10:52:34.961798: step 186820, loss = 2.62 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 10:52:39.684770: step 186830, loss = 2.76 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 10:52:45.223187: step 186840, loss = 2.75 (188.2 examples/sec; 0.680 sec/batch)
2016-07-16 10:52:51.499103: step 186850, loss = 2.65 (257.8 examples/sec; 0.497 sec/batch)
2016-07-16 10:52:56.321909: step 186860, loss = 2.82 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 10:53:01.131164: step 186870, loss = 2.68 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 10:53:05.928450: step 186880, loss = 2.69 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 10:53:10.529407: step 186890, loss = 2.74 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 10:53:15.296805: step 186900, loss = 2.98 (230.8 examples/sec; 0.555 sec/batch)
2016-07-16 10:53:22.239318: step 186910, loss = 2.81 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 10:53:28.046641: step 186920, loss = 2.78 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 10:53:32.848312: step 186930, loss = 2.71 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 10:53:37.663407: step 186940, loss = 2.75 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 10:53:42.388307: step 186950, loss = 2.92 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 10:53:46.962709: step 186960, loss = 2.79 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 10:53:51.551004: step 186970, loss = 2.87 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 10:53:56.650387: step 186980, loss = 2.72 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 10:54:02.074820: step 186990, loss = 2.81 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 10:54:06.853856: step 187000, loss = 2.57 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 10:54:12.745504: step 187010, loss = 2.74 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 10:54:17.571705: step 187020, loss = 2.70 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 10:54:22.340564: step 187030, loss = 2.59 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 10:54:27.307232: step 187040, loss = 2.49 (228.0 examples/sec; 0.561 sec/batch)
2016-07-16 10:54:33.855970: step 187050, loss = 2.81 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 10:54:39.034571: step 187060, loss = 2.72 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 10:54:43.747956: step 187070, loss = 2.64 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 10:54:48.555673: step 187080, loss = 2.71 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 10:54:53.197846: step 187090, loss = 2.66 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 10:54:57.926904: step 187100, loss = 2.69 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 10:55:03.545097: step 187110, loss = 2.81 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 10:55:08.585059: step 187120, loss = 2.74 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 10:55:14.018431: step 187130, loss = 2.81 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 10:55:18.774752: step 187140, loss = 2.84 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 10:55:23.667682: step 187150, loss = 2.61 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 10:55:28.411478: step 187160, loss = 2.87 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 10:55:34.178587: step 187170, loss = 2.65 (191.7 examples/sec; 0.668 sec/batch)
2016-07-16 10:55:40.208506: step 187180, loss = 2.96 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 10:55:45.055044: step 187190, loss = 2.63 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 10:55:49.920464: step 187200, loss = 2.81 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 10:55:55.631172: step 187210, loss = 2.50 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 10:56:00.483449: step 187220, loss = 2.83 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 10:56:05.219075: step 187230, loss = 2.68 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 10:56:11.283786: step 187240, loss = 2.81 (193.5 examples/sec; 0.662 sec/batch)
2016-07-16 10:56:16.972022: step 187250, loss = 2.93 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 10:56:21.798086: step 187260, loss = 2.88 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 10:56:26.710359: step 187270, loss = 2.89 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 10:56:32.717301: step 187280, loss = 2.82 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 10:56:37.361078: step 187290, loss = 2.83 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 10:56:42.982674: step 187300, loss = 2.73 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 10:56:48.854338: step 187310, loss = 2.81 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 10:56:53.665135: step 187320, loss = 2.82 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 10:56:58.413012: step 187330, loss = 2.60 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 10:57:03.248890: step 187340, loss = 2.80 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 10:57:07.916497: step 187350, loss = 2.69 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 10:57:12.737249: step 187360, loss = 2.70 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 10:57:17.354206: step 187370, loss = 2.94 (284.7 examples/sec; 0.450 sec/batch)
2016-07-16 10:57:22.028387: step 187380, loss = 2.75 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 10:57:27.781407: step 187390, loss = 2.60 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 10:57:32.587600: step 187400, loss = 2.66 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 10:57:38.231017: step 187410, loss = 3.00 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 10:57:42.896936: step 187420, loss = 2.84 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 10:57:47.593062: step 187430, loss = 2.85 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 10:57:52.258566: step 187440, loss = 2.84 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 10:57:57.596195: step 187450, loss = 2.90 (193.2 examples/sec; 0.663 sec/batch)
2016-07-16 10:58:02.823977: step 187460, loss = 2.74 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 10:58:08.559224: step 187470, loss = 2.70 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 10:58:13.402538: step 187480, loss = 2.61 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 10:58:18.190859: step 187490, loss = 2.80 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 10:58:22.952838: step 187500, loss = 2.89 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 10:58:29.441450: step 187510, loss = 2.57 (185.5 examples/sec; 0.690 sec/batch)
2016-07-16 10:58:35.766002: step 187520, loss = 2.87 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 10:58:40.607847: step 187530, loss = 2.97 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 10:58:45.399819: step 187540, loss = 2.74 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 10:58:51.472504: step 187550, loss = 2.62 (193.7 examples/sec; 0.661 sec/batch)
2016-07-16 10:58:57.069480: step 187560, loss = 2.62 (284.6 examples/sec; 0.450 sec/batch)
2016-07-16 10:59:01.756401: step 187570, loss = 2.73 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 10:59:06.378439: step 187580, loss = 2.81 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 10:59:11.054041: step 187590, loss = 2.85 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 10:59:16.826737: step 187600, loss = 2.73 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 10:59:22.700000: step 187610, loss = 2.82 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 10:59:27.298950: step 187620, loss = 2.67 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 10:59:32.238740: step 187630, loss = 2.94 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 10:59:37.813266: step 187640, loss = 2.94 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 10:59:42.574461: step 187650, loss = 2.72 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 10:59:47.439934: step 187660, loss = 2.91 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 10:59:52.171218: step 187670, loss = 2.62 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 10:59:57.015955: step 187680, loss = 2.65 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 11:00:01.795496: step 187690, loss = 2.83 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 11:00:06.559808: step 187700, loss = 2.71 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 11:00:12.266150: step 187710, loss = 2.94 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 11:00:16.923118: step 187720, loss = 2.90 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 11:00:21.628855: step 187730, loss = 2.74 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 11:00:26.272928: step 187740, loss = 2.69 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 11:00:30.921653: step 187750, loss = 2.83 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 11:00:35.696066: step 187760, loss = 2.67 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 11:00:40.301685: step 187770, loss = 2.68 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 11:00:45.616927: step 187780, loss = 2.70 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 11:00:50.903012: step 187790, loss = 2.79 (245.2 examples/sec; 0.522 sec/batch)
2016-07-16 11:00:56.657965: step 187800, loss = 2.60 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 11:01:03.422174: step 187810, loss = 2.73 (207.6 examples/sec; 0.617 sec/batch)
2016-07-16 11:01:08.285055: step 187820, loss = 2.63 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 11:01:13.013179: step 187830, loss = 2.73 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 11:01:18.891419: step 187840, loss = 2.69 (187.7 examples/sec; 0.682 sec/batch)
2016-07-16 11:01:24.783930: step 187850, loss = 2.66 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 11:01:29.587100: step 187860, loss = 2.70 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 11:01:34.395186: step 187870, loss = 2.62 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 11:01:39.133639: step 187880, loss = 2.74 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 11:01:43.997579: step 187890, loss = 2.83 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 11:01:48.751123: step 187900, loss = 2.79 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 11:01:54.575122: step 187910, loss = 2.74 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 11:01:59.438145: step 187920, loss = 2.72 (252.2 examples/sec; 0.507 sec/batch)
2016-07-16 11:02:04.167813: step 187930, loss = 2.90 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 11:02:08.773899: step 187940, loss = 2.76 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 11:02:13.449662: step 187950, loss = 2.79 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 11:02:18.157364: step 187960, loss = 2.49 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 11:02:22.864946: step 187970, loss = 2.86 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 11:02:27.518300: step 187980, loss = 2.84 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 11:02:32.506423: step 187990, loss = 2.88 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 11:02:38.016437: step 188000, loss = 2.86 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 11:02:44.795799: step 188010, loss = 2.50 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 11:02:49.452859: step 188020, loss = 2.84 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 11:02:54.055350: step 188030, loss = 2.73 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 11:02:58.737385: step 188040, loss = 2.72 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 11:03:04.555977: step 188050, loss = 2.74 (251.3 examples/sec; 0.509 sec/batch)
2016-07-16 11:03:10.024337: step 188060, loss = 2.98 (209.0 examples/sec; 0.612 sec/batch)
2016-07-16 11:03:15.239553: step 188070, loss = 2.94 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 11:03:19.941171: step 188080, loss = 2.96 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 11:03:24.600862: step 188090, loss = 2.92 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 11:03:29.248651: step 188100, loss = 2.64 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 11:03:34.817516: step 188110, loss = 2.95 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 11:03:40.609171: step 188120, loss = 2.75 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 11:03:45.434646: step 188130, loss = 2.87 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 11:03:50.291001: step 188140, loss = 2.69 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 11:03:56.551076: step 188150, loss = 2.90 (208.3 examples/sec; 0.615 sec/batch)
2016-07-16 11:04:02.090861: step 188160, loss = 2.49 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 11:04:06.815718: step 188170, loss = 2.63 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 11:04:11.669111: step 188180, loss = 2.66 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 11:04:16.356099: step 188190, loss = 2.54 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 11:04:20.969447: step 188200, loss = 2.56 (278.6 examples/sec; 0.460 sec/batch)
2016-07-16 11:04:27.669541: step 188210, loss = 2.68 (220.2 examples/sec; 0.581 sec/batch)
2016-07-16 11:04:32.918134: step 188220, loss = 2.66 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 11:04:38.333281: step 188230, loss = 2.57 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 11:04:44.147748: step 188240, loss = 2.67 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 11:04:48.970240: step 188250, loss = 2.70 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 11:04:53.717915: step 188260, loss = 2.76 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 11:04:58.496065: step 188270, loss = 2.68 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 11:05:03.194353: step 188280, loss = 2.78 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 11:05:07.917351: step 188290, loss = 2.68 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 11:05:12.560817: step 188300, loss = 2.56 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 11:05:18.223787: step 188310, loss = 2.77 (283.5 examples/sec; 0.452 sec/batch)
2016-07-16 11:05:22.853179: step 188320, loss = 2.86 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 11:05:28.593105: step 188330, loss = 2.61 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 11:05:33.425786: step 188340, loss = 2.90 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 11:05:38.259895: step 188350, loss = 2.59 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 11:05:42.985464: step 188360, loss = 2.88 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 11:05:47.878233: step 188370, loss = 2.50 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 11:05:52.543134: step 188380, loss = 2.75 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 11:05:57.173321: step 188390, loss = 2.82 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 11:06:01.826576: step 188400, loss = 2.84 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 11:06:07.396959: step 188410, loss = 2.86 (251.8 examples/sec; 0.508 sec/batch)
2016-07-16 11:06:13.214767: step 188420, loss = 2.83 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 11:06:18.021823: step 188430, loss = 2.77 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 11:06:22.688150: step 188440, loss = 2.71 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 11:06:27.459103: step 188450, loss = 2.67 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 11:06:33.265498: step 188460, loss = 2.73 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 11:06:38.805396: step 188470, loss = 2.58 (209.9 examples/sec; 0.610 sec/batch)
2016-07-16 11:06:43.935549: step 188480, loss = 2.56 (227.1 examples/sec; 0.564 sec/batch)
2016-07-16 11:06:49.695185: step 188490, loss = 2.91 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 11:06:54.512757: step 188500, loss = 2.72 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 11:07:00.055122: step 188510, loss = 2.71 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 11:07:04.696406: step 188520, loss = 2.75 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 11:07:09.979349: step 188530, loss = 2.84 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 11:07:15.233385: step 188540, loss = 2.72 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 11:07:21.045350: step 188550, loss = 2.63 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 11:07:25.861263: step 188560, loss = 2.74 (247.4 examples/sec; 0.517 sec/batch)
2016-07-16 11:07:30.647200: step 188570, loss = 2.79 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 11:07:35.371684: step 188580, loss = 2.76 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 11:07:40.322027: step 188590, loss = 2.60 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 11:07:45.021243: step 188600, loss = 2.59 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 11:07:50.631410: step 188610, loss = 2.85 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 11:07:56.387275: step 188620, loss = 2.74 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 11:08:01.847035: step 188630, loss = 2.64 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 11:08:07.165061: step 188640, loss = 2.68 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 11:08:11.866670: step 188650, loss = 2.67 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 11:08:16.716456: step 188660, loss = 2.86 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 11:08:21.452662: step 188670, loss = 2.75 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 11:08:27.433199: step 188680, loss = 2.95 (184.7 examples/sec; 0.693 sec/batch)
2016-07-16 11:08:33.268705: step 188690, loss = 2.69 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 11:08:38.069491: step 188700, loss = 2.71 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 11:08:43.875658: step 188710, loss = 2.62 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 11:08:48.559603: step 188720, loss = 2.72 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 11:08:53.195015: step 188730, loss = 2.76 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 11:08:58.616663: step 188740, loss = 2.69 (204.3 examples/sec; 0.627 sec/batch)
2016-07-16 11:09:03.800946: step 188750, loss = 2.81 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 11:09:09.569936: step 188760, loss = 2.85 (253.4 examples/sec; 0.505 sec/batch)
2016-07-16 11:09:15.229373: step 188770, loss = 2.71 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 11:09:20.254655: step 188780, loss = 2.67 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 11:09:25.001074: step 188790, loss = 2.88 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 11:09:29.813793: step 188800, loss = 2.66 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 11:09:35.425686: step 188810, loss = 2.72 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 11:09:40.124490: step 188820, loss = 2.95 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 11:09:44.750678: step 188830, loss = 2.51 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 11:09:50.017324: step 188840, loss = 2.60 (207.8 examples/sec; 0.616 sec/batch)
2016-07-16 11:09:55.333272: step 188850, loss = 2.81 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 11:10:00.035856: step 188860, loss = 2.77 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 11:10:05.415331: step 188870, loss = 2.73 (178.3 examples/sec; 0.718 sec/batch)
2016-07-16 11:10:11.866443: step 188880, loss = 2.90 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 11:10:16.694668: step 188890, loss = 2.77 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 11:10:21.457196: step 188900, loss = 2.84 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 11:10:28.753137: step 188910, loss = 2.96 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 11:10:34.338066: step 188920, loss = 2.80 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 11:10:39.105428: step 188930, loss = 2.71 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 11:10:44.026154: step 188940, loss = 2.68 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 11:10:48.735587: step 188950, loss = 2.83 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 11:10:53.370560: step 188960, loss = 2.59 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 11:10:58.766891: step 188970, loss = 2.61 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 11:11:03.904642: step 188980, loss = 2.66 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 11:11:08.605976: step 188990, loss = 2.57 (257.3 examples/sec; 0.498 sec/batch)
2016-07-16 11:11:14.116759: step 189000, loss = 2.78 (190.3 examples/sec; 0.673 sec/batch)
2016-07-16 11:11:20.489078: step 189010, loss = 2.53 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 11:11:25.142810: step 189020, loss = 2.72 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 11:11:29.767132: step 189030, loss = 2.62 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 11:11:35.335647: step 189040, loss = 2.69 (201.4 examples/sec; 0.636 sec/batch)
2016-07-16 11:11:40.350722: step 189050, loss = 2.81 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 11:11:45.143418: step 189060, loss = 2.80 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 11:11:49.965333: step 189070, loss = 2.60 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 11:11:54.618511: step 189080, loss = 2.68 (285.6 examples/sec; 0.448 sec/batch)
2016-07-16 11:11:59.339188: step 189090, loss = 2.88 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 11:12:05.112208: step 189100, loss = 2.75 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 11:12:11.934957: step 189110, loss = 2.69 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 11:12:16.803763: step 189120, loss = 2.54 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 11:12:21.489853: step 189130, loss = 2.79 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 11:12:26.153327: step 189140, loss = 3.00 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 11:12:31.832878: step 189150, loss = 2.76 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 11:12:36.695657: step 189160, loss = 2.87 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 11:12:41.451916: step 189170, loss = 2.83 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 11:12:46.198189: step 189180, loss = 2.61 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 11:12:51.030469: step 189190, loss = 2.64 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 11:12:57.349628: step 189200, loss = 2.92 (201.8 examples/sec; 0.634 sec/batch)
2016-07-16 11:13:04.067648: step 189210, loss = 2.83 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 11:13:08.721788: step 189220, loss = 2.91 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 11:13:13.365664: step 189230, loss = 2.90 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 11:13:18.753779: step 189240, loss = 2.96 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 11:13:23.957769: step 189250, loss = 2.89 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 11:13:28.641861: step 189260, loss = 2.63 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 11:13:34.100322: step 189270, loss = 2.61 (188.7 examples/sec; 0.678 sec/batch)
2016-07-16 11:13:40.396980: step 189280, loss = 2.68 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 11:13:45.024522: step 189290, loss = 2.69 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 11:13:50.318704: step 189300, loss = 2.71 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 11:13:56.799333: step 189310, loss = 2.91 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 11:14:01.546581: step 189320, loss = 2.43 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 11:14:06.352234: step 189330, loss = 2.65 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 11:14:11.161607: step 189340, loss = 2.70 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 11:14:17.546926: step 189350, loss = 2.47 (197.3 examples/sec; 0.649 sec/batch)
2016-07-16 11:14:22.962771: step 189360, loss = 2.59 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 11:14:27.704655: step 189370, loss = 2.56 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 11:14:32.794404: step 189380, loss = 2.78 (188.7 examples/sec; 0.678 sec/batch)
2016-07-16 11:14:39.380726: step 189390, loss = 2.61 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 11:14:44.208765: step 189400, loss = 2.75 (267.5 examples/sec; 0.478 sec/batch)
2016-07-16 11:14:49.760095: step 189410, loss = 2.69 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 11:14:54.424908: step 189420, loss = 2.80 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 11:15:00.096789: step 189430, loss = 2.66 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 11:15:04.941326: step 189440, loss = 2.67 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 11:15:09.693810: step 189450, loss = 2.58 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 11:15:15.679700: step 189460, loss = 2.58 (193.1 examples/sec; 0.663 sec/batch)
2016-07-16 11:15:21.607091: step 189470, loss = 2.63 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 11:15:26.416068: step 189480, loss = 2.69 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 11:15:31.227205: step 189490, loss = 2.82 (254.5 examples/sec; 0.503 sec/batch)
2016-07-16 11:15:37.626465: step 189500, loss = 2.81 (198.6 examples/sec; 0.644 sec/batch)
2016-07-16 11:15:44.274781: step 189510, loss = 2.87 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 11:15:48.967117: step 189520, loss = 2.92 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 11:15:53.633995: step 189530, loss = 2.49 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 11:15:59.099626: step 189540, loss = 2.81 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 11:16:04.186436: step 189550, loss = 2.86 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 11:16:09.939039: step 189560, loss = 2.74 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 11:16:15.590865: step 189570, loss = 2.88 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 11:16:20.616353: step 189580, loss = 2.88 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 11:16:25.355063: step 189590, loss = 2.69 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 11:16:30.157862: step 189600, loss = 2.83 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 11:16:35.887069: step 189610, loss = 2.64 (254.7 examples/sec; 0.502 sec/batch)
2016-07-16 11:16:42.399560: step 189620, loss = 2.76 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 11:16:47.681201: step 189630, loss = 2.61 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 11:16:52.449598: step 189640, loss = 2.52 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 11:16:57.840986: step 189650, loss = 2.89 (186.8 examples/sec; 0.685 sec/batch)
2016-07-16 11:17:04.304510: step 189660, loss = 2.69 (217.9 examples/sec; 0.587 sec/batch)
2016-07-16 11:17:09.152325: step 189670, loss = 2.81 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 11:17:13.961129: step 189680, loss = 2.73 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 11:17:19.916972: step 189690, loss = 2.81 (188.8 examples/sec; 0.678 sec/batch)
2016-07-16 11:17:25.784488: step 189700, loss = 2.71 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 11:17:31.679025: step 189710, loss = 2.75 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 11:17:36.311489: step 189720, loss = 2.79 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 11:17:41.235522: step 189730, loss = 2.82 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 11:17:46.793729: step 189740, loss = 2.76 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 11:17:51.586209: step 189750, loss = 2.70 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 11:17:56.497396: step 189760, loss = 2.59 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 11:18:01.276925: step 189770, loss = 2.91 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 11:18:06.134473: step 189780, loss = 2.50 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 11:18:10.949041: step 189790, loss = 2.68 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 11:18:15.671596: step 189800, loss = 2.67 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 11:18:21.554220: step 189810, loss = 2.91 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 11:18:26.382139: step 189820, loss = 2.77 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 11:18:32.423150: step 189830, loss = 2.67 (186.7 examples/sec; 0.685 sec/batch)
2016-07-16 11:18:38.099486: step 189840, loss = 2.85 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 11:18:43.757202: step 189850, loss = 2.81 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 11:18:48.747317: step 189860, loss = 2.81 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 11:18:53.438996: step 189870, loss = 2.93 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 11:18:58.300854: step 189880, loss = 2.57 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 11:19:02.941731: step 189890, loss = 2.61 (288.8 examples/sec; 0.443 sec/batch)
2016-07-16 11:19:07.609595: step 189900, loss = 2.91 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 11:19:13.108132: step 189910, loss = 2.67 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 11:19:17.750520: step 189920, loss = 2.76 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 11:19:23.266889: step 189930, loss = 2.63 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 11:19:28.360256: step 189940, loss = 2.63 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 11:19:33.098395: step 189950, loss = 3.00 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 11:19:38.767290: step 189960, loss = 2.71 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 11:19:44.943274: step 189970, loss = 2.68 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 11:19:50.233250: step 189980, loss = 2.86 (205.6 examples/sec; 0.622 sec/batch)
2016-07-16 11:19:55.593504: step 189990, loss = 2.62 (261.5 examples/sec; 0.489 sec/batch)
2016-07-16 11:20:01.343115: step 190000, loss = 2.63 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 11:20:07.191413: step 190010, loss = 2.71 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 11:20:12.080890: step 190020, loss = 2.79 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 11:20:18.592311: step 190030, loss = 2.83 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 11:20:23.876937: step 190040, loss = 2.85 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 11:20:28.586265: step 190050, loss = 2.65 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 11:20:33.435454: step 190060, loss = 2.77 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 11:20:38.185099: step 190070, loss = 2.61 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 11:20:42.999022: step 190080, loss = 2.86 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 11:20:47.830549: step 190090, loss = 2.76 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 11:20:52.572884: step 190100, loss = 2.64 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 11:20:58.423167: step 190110, loss = 2.83 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 11:21:03.160199: step 190120, loss = 2.76 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 11:21:07.838660: step 190130, loss = 2.63 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 11:21:12.491757: step 190140, loss = 2.60 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 11:21:17.485784: step 190150, loss = 2.59 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 11:21:23.081002: step 190160, loss = 2.73 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 11:21:27.774233: step 190170, loss = 2.88 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 11:21:32.830076: step 190180, loss = 2.65 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 11:21:39.366889: step 190190, loss = 2.85 (206.3 examples/sec; 0.620 sec/batch)
2016-07-16 11:21:44.532045: step 190200, loss = 2.77 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 11:21:50.181439: step 190210, loss = 2.81 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 11:21:54.871115: step 190220, loss = 2.81 (250.7 examples/sec; 0.510 sec/batch)
2016-07-16 11:21:59.564751: step 190230, loss = 2.57 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 11:22:04.267725: step 190240, loss = 2.72 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 11:22:10.032981: step 190250, loss = 2.94 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 11:22:15.749990: step 190260, loss = 2.79 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 11:22:20.784515: step 190270, loss = 2.69 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 11:22:25.467318: step 190280, loss = 2.58 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 11:22:30.288369: step 190290, loss = 3.02 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 11:22:35.095405: step 190300, loss = 2.80 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 11:22:40.845698: step 190310, loss = 2.83 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 11:22:45.486425: step 190320, loss = 2.92 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 11:22:50.151711: step 190330, loss = 2.72 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 11:22:54.795114: step 190340, loss = 2.77 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 11:22:59.540575: step 190350, loss = 2.68 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 11:23:04.201509: step 190360, loss = 2.96 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 11:23:09.005539: step 190370, loss = 2.75 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 11:23:14.724511: step 190380, loss = 2.77 (231.8 examples/sec; 0.552 sec/batch)
2016-07-16 11:23:20.484186: step 190390, loss = 3.06 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 11:23:25.304235: step 190400, loss = 2.76 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 11:23:31.156842: step 190410, loss = 2.88 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 11:23:37.311159: step 190420, loss = 2.68 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 11:23:42.890665: step 190430, loss = 2.64 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 11:23:47.602934: step 190440, loss = 2.66 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 11:23:52.245731: step 190450, loss = 2.65 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 11:23:56.987170: step 190460, loss = 2.75 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 11:24:01.587812: step 190470, loss = 2.70 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 11:24:06.869428: step 190480, loss = 2.83 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 11:24:12.173479: step 190490, loss = 2.70 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 11:24:17.938212: step 190500, loss = 2.60 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 11:24:23.811350: step 190510, loss = 2.54 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 11:24:28.782334: step 190520, loss = 2.53 (230.2 examples/sec; 0.556 sec/batch)
2016-07-16 11:24:35.390803: step 190530, loss = 2.73 (195.8 examples/sec; 0.654 sec/batch)
2016-07-16 11:24:40.561314: step 190540, loss = 2.91 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 11:24:45.288290: step 190550, loss = 2.56 (245.5 examples/sec; 0.521 sec/batch)
2016-07-16 11:24:50.819365: step 190560, loss = 2.70 (188.4 examples/sec; 0.679 sec/batch)
2016-07-16 11:24:57.156124: step 190570, loss = 2.74 (255.3 examples/sec; 0.501 sec/batch)
2016-07-16 11:25:01.973096: step 190580, loss = 2.61 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 11:25:06.782267: step 190590, loss = 2.74 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 11:25:11.577510: step 190600, loss = 2.73 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 11:25:17.101084: step 190610, loss = 2.62 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 11:25:22.145501: step 190620, loss = 2.60 (207.6 examples/sec; 0.617 sec/batch)
2016-07-16 11:25:27.567721: step 190630, loss = 2.53 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 11:25:32.294424: step 190640, loss = 2.67 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 11:25:36.919311: step 190650, loss = 2.81 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 11:25:41.930045: step 190660, loss = 2.82 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 11:25:47.421313: step 190670, loss = 2.59 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 11:25:53.254439: step 190680, loss = 2.62 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 11:25:58.097188: step 190690, loss = 2.76 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 11:26:02.879759: step 190700, loss = 2.65 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 11:26:10.343726: step 190710, loss = 2.93 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 11:26:15.744631: step 190720, loss = 2.64 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 11:26:20.505220: step 190730, loss = 2.95 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 11:26:25.695939: step 190740, loss = 2.80 (187.6 examples/sec; 0.682 sec/batch)
2016-07-16 11:26:32.193909: step 190750, loss = 2.69 (204.6 examples/sec; 0.625 sec/batch)
2016-07-16 11:26:37.196354: step 190760, loss = 2.79 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 11:26:41.938953: step 190770, loss = 2.71 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 11:26:46.736174: step 190780, loss = 2.72 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 11:26:51.392727: step 190790, loss = 2.86 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 11:26:56.041823: step 190800, loss = 2.86 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 11:27:01.646050: step 190810, loss = 2.74 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 11:27:06.359394: step 190820, loss = 2.62 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 11:27:10.984617: step 190830, loss = 2.73 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 11:27:15.682424: step 190840, loss = 2.88 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 11:27:21.481897: step 190850, loss = 2.73 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 11:27:26.840918: step 190860, loss = 2.77 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 11:27:32.128889: step 190870, loss = 2.66 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 11:27:36.874353: step 190880, loss = 2.65 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 11:27:41.705844: step 190890, loss = 2.59 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 11:27:46.489839: step 190900, loss = 2.66 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 11:27:53.767791: step 190910, loss = 2.67 (201.4 examples/sec; 0.636 sec/batch)
2016-07-16 11:27:59.303259: step 190920, loss = 2.68 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 11:28:04.082362: step 190930, loss = 2.87 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 11:28:08.935690: step 190940, loss = 2.75 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 11:28:13.627898: step 190950, loss = 2.77 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 11:28:18.213916: step 190960, loss = 2.72 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 11:28:23.698669: step 190970, loss = 2.77 (199.1 examples/sec; 0.643 sec/batch)
2016-07-16 11:28:28.856630: step 190980, loss = 2.72 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 11:28:34.601239: step 190990, loss = 2.76 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 11:28:40.293662: step 191000, loss = 3.09 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 11:28:46.265054: step 191010, loss = 2.97 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 11:28:51.063992: step 191020, loss = 2.71 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 11:28:55.869607: step 191030, loss = 2.86 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 11:29:00.511545: step 191040, loss = 2.61 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 11:29:05.306132: step 191050, loss = 2.52 (210.2 examples/sec; 0.609 sec/batch)
2016-07-16 11:29:11.024537: step 191060, loss = 2.90 (262.0 examples/sec; 0.488 sec/batch)
2016-07-16 11:29:15.823944: step 191070, loss = 3.01 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 11:29:20.472484: step 191080, loss = 2.88 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 11:29:25.223814: step 191090, loss = 2.74 (224.7 examples/sec; 0.570 sec/batch)
2016-07-16 11:29:30.925565: step 191100, loss = 2.73 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 11:29:37.735316: step 191110, loss = 2.83 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 11:29:42.576067: step 191120, loss = 2.83 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 11:29:47.400149: step 191130, loss = 2.60 (249.4 examples/sec; 0.513 sec/batch)
2016-07-16 11:29:52.143818: step 191140, loss = 2.97 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 11:29:57.005798: step 191150, loss = 2.65 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 11:30:01.740551: step 191160, loss = 2.53 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 11:30:07.320498: step 191170, loss = 2.80 (186.0 examples/sec; 0.688 sec/batch)
2016-07-16 11:30:13.536947: step 191180, loss = 2.88 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 11:30:18.844637: step 191190, loss = 2.64 (208.3 examples/sec; 0.615 sec/batch)
2016-07-16 11:30:24.202367: step 191200, loss = 2.86 (252.9 examples/sec; 0.506 sec/batch)
2016-07-16 11:30:29.803979: step 191210, loss = 2.66 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 11:30:34.476306: step 191220, loss = 2.77 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 11:30:39.209182: step 191230, loss = 2.65 (285.1 examples/sec; 0.449 sec/batch)
2016-07-16 11:30:43.874060: step 191240, loss = 2.83 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 11:30:49.623675: step 191250, loss = 2.83 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 11:30:54.995873: step 191260, loss = 2.53 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 11:31:00.092735: step 191270, loss = 2.78 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 11:31:04.785988: step 191280, loss = 2.76 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 11:31:10.564951: step 191290, loss = 2.89 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 11:31:16.237420: step 191300, loss = 2.86 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 11:31:22.400920: step 191310, loss = 2.68 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 11:31:27.237508: step 191320, loss = 2.75 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 11:31:33.383906: step 191330, loss = 2.67 (200.5 examples/sec; 0.638 sec/batch)
2016-07-16 11:31:39.078098: step 191340, loss = 2.73 (263.1 examples/sec; 0.486 sec/batch)
2016-07-16 11:31:44.816073: step 191350, loss = 2.75 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 11:31:49.747825: step 191360, loss = 2.80 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 11:31:54.527677: step 191370, loss = 2.70 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 11:31:59.340321: step 191380, loss = 2.82 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 11:32:04.167527: step 191390, loss = 2.86 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 11:32:08.886278: step 191400, loss = 2.63 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 11:32:14.745328: step 191410, loss = 2.95 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 11:32:19.570736: step 191420, loss = 2.75 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 11:32:24.329235: step 191430, loss = 2.67 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 11:32:29.255336: step 191440, loss = 2.52 (229.8 examples/sec; 0.557 sec/batch)
2016-07-16 11:32:35.807804: step 191450, loss = 3.16 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 11:32:40.946708: step 191460, loss = 2.68 (259.7 examples/sec; 0.493 sec/batch)
2016-07-16 11:32:45.650874: step 191470, loss = 2.77 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 11:32:50.406748: step 191480, loss = 2.79 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 11:32:55.136798: step 191490, loss = 2.71 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 11:32:59.975800: step 191500, loss = 2.75 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 11:33:06.121390: step 191510, loss = 2.79 (188.2 examples/sec; 0.680 sec/batch)
2016-07-16 11:33:12.648670: step 191520, loss = 2.95 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 11:33:17.819760: step 191530, loss = 2.74 (199.8 examples/sec; 0.641 sec/batch)
2016-07-16 11:33:23.444512: step 191540, loss = 2.77 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 11:33:28.214314: step 191550, loss = 3.06 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 11:33:33.141194: step 191560, loss = 2.83 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 11:33:39.754965: step 191570, loss = 2.90 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 11:33:45.000258: step 191580, loss = 2.69 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 11:33:49.694902: step 191590, loss = 2.57 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 11:33:54.351841: step 191600, loss = 2.69 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 11:34:00.999395: step 191610, loss = 2.85 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 11:34:06.120004: step 191620, loss = 2.84 (200.5 examples/sec; 0.638 sec/batch)
2016-07-16 11:34:11.687365: step 191630, loss = 2.74 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 11:34:16.418807: step 191640, loss = 2.73 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 11:34:21.254245: step 191650, loss = 2.66 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 11:34:25.932833: step 191660, loss = 2.55 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 11:34:30.574144: step 191670, loss = 2.76 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 11:34:36.022685: step 191680, loss = 2.76 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 11:34:41.249639: step 191690, loss = 2.68 (252.5 examples/sec; 0.507 sec/batch)
2016-07-16 11:34:46.965386: step 191700, loss = 2.76 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 11:34:52.732546: step 191710, loss = 2.46 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 11:34:57.586289: step 191720, loss = 2.80 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 11:35:02.363972: step 191730, loss = 2.74 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 11:35:08.056231: step 191740, loss = 2.70 (188.3 examples/sec; 0.680 sec/batch)
2016-07-16 11:35:14.212704: step 191750, loss = 2.62 (246.7 examples/sec; 0.519 sec/batch)
2016-07-16 11:35:19.042640: step 191760, loss = 2.69 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 11:35:23.864335: step 191770, loss = 2.75 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 11:35:28.645393: step 191780, loss = 2.77 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 11:35:33.281080: step 191790, loss = 2.73 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 11:35:38.243829: step 191800, loss = 2.74 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 11:35:45.004212: step 191810, loss = 2.67 (253.1 examples/sec; 0.506 sec/batch)
2016-07-16 11:35:49.760051: step 191820, loss = 2.67 (256.8 examples/sec; 0.498 sec/batch)
2016-07-16 11:35:54.585073: step 191830, loss = 2.61 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 11:35:59.383291: step 191840, loss = 2.67 (253.1 examples/sec; 0.506 sec/batch)
2016-07-16 11:36:04.151718: step 191850, loss = 2.77 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 11:36:08.804432: step 191860, loss = 2.78 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 11:36:13.590191: step 191870, loss = 2.80 (229.1 examples/sec; 0.559 sec/batch)
2016-07-16 11:36:19.317507: step 191880, loss = 2.86 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 11:36:25.095366: step 191890, loss = 2.73 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 11:36:30.018780: step 191900, loss = 2.82 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 11:36:35.745133: step 191910, loss = 2.72 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 11:36:40.485208: step 191920, loss = 2.68 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 11:36:45.328951: step 191930, loss = 2.73 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 11:36:51.908542: step 191940, loss = 2.60 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 11:36:57.135041: step 191950, loss = 2.84 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 11:37:01.927906: step 191960, loss = 2.86 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 11:37:07.385565: step 191970, loss = 3.17 (188.2 examples/sec; 0.680 sec/batch)
2016-07-16 11:37:13.648155: step 191980, loss = 2.79 (252.3 examples/sec; 0.507 sec/batch)
2016-07-16 11:37:18.871795: step 191990, loss = 2.90 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 11:37:24.324674: step 192000, loss = 2.93 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 11:37:30.119750: step 192010, loss = 2.99 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 11:37:35.660791: step 192020, loss = 2.82 (188.1 examples/sec; 0.680 sec/batch)
2016-07-16 11:37:41.911782: step 192030, loss = 2.77 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 11:37:46.738197: step 192040, loss = 2.64 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 11:37:51.535847: step 192050, loss = 2.69 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 11:37:57.639541: step 192060, loss = 2.62 (192.2 examples/sec; 0.666 sec/batch)
2016-07-16 11:38:03.352033: step 192070, loss = 2.62 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 11:38:08.091169: step 192080, loss = 2.59 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 11:38:12.945748: step 192090, loss = 2.79 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 11:38:18.902540: step 192100, loss = 2.78 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 11:38:24.413327: step 192110, loss = 2.73 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 11:38:29.043210: step 192120, loss = 2.75 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 11:38:33.854050: step 192130, loss = 2.77 (211.0 examples/sec; 0.607 sec/batch)
2016-07-16 11:38:39.546211: step 192140, loss = 2.72 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 11:38:44.346350: step 192150, loss = 2.74 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 11:38:48.931792: step 192160, loss = 3.00 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 11:38:53.811084: step 192170, loss = 2.57 (213.9 examples/sec; 0.598 sec/batch)
2016-07-16 11:38:59.434087: step 192180, loss = 2.96 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 11:39:04.182311: step 192190, loss = 2.68 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 11:39:09.015321: step 192200, loss = 2.62 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 11:39:17.024058: step 192210, loss = 2.61 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 11:39:22.169281: step 192220, loss = 2.85 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 11:39:27.774805: step 192230, loss = 2.98 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 11:39:32.520959: step 192240, loss = 2.82 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 11:39:37.409857: step 192250, loss = 2.82 (255.8 examples/sec; 0.500 sec/batch)
2016-07-16 11:39:42.164762: step 192260, loss = 2.63 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 11:39:46.948957: step 192270, loss = 2.72 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 11:39:51.744795: step 192280, loss = 2.62 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 11:39:57.883048: step 192290, loss = 2.75 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 11:40:03.530019: step 192300, loss = 2.63 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 11:40:09.298412: step 192310, loss = 2.54 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 11:40:14.483709: step 192320, loss = 2.52 (187.2 examples/sec; 0.684 sec/batch)
2016-07-16 11:40:20.974563: step 192330, loss = 2.91 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 11:40:26.013564: step 192340, loss = 2.80 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 11:40:30.829160: step 192350, loss = 2.67 (246.7 examples/sec; 0.519 sec/batch)
2016-07-16 11:40:35.575461: step 192360, loss = 2.74 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 11:40:40.225489: step 192370, loss = 2.64 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 11:40:44.930660: step 192380, loss = 2.89 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 11:40:50.705227: step 192390, loss = 2.95 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 11:40:55.564024: step 192400, loss = 2.68 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 11:41:01.105842: step 192410, loss = 2.70 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 11:41:05.929021: step 192420, loss = 2.50 (215.1 examples/sec; 0.595 sec/batch)
2016-07-16 11:41:11.631685: step 192430, loss = 2.83 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 11:41:17.356634: step 192440, loss = 2.70 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 11:41:22.448348: step 192450, loss = 2.74 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 11:41:27.999101: step 192460, loss = 2.68 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 11:41:32.713099: step 192470, loss = 2.60 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 11:41:37.680221: step 192480, loss = 2.89 (224.1 examples/sec; 0.571 sec/batch)
2016-07-16 11:41:44.236216: step 192490, loss = 2.78 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 11:41:49.380434: step 192500, loss = 2.91 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 11:41:55.135973: step 192510, loss = 2.77 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 11:42:01.010262: step 192520, loss = 2.79 (182.9 examples/sec; 0.700 sec/batch)
2016-07-16 11:42:06.938191: step 192530, loss = 2.69 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 11:42:11.721280: step 192540, loss = 2.71 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 11:42:16.562254: step 192550, loss = 2.79 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 11:42:21.290255: step 192560, loss = 2.62 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 11:42:26.208597: step 192570, loss = 2.77 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 11:42:30.883813: step 192580, loss = 2.73 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 11:42:35.531673: step 192590, loss = 2.66 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 11:42:41.149011: step 192600, loss = 2.73 (200.9 examples/sec; 0.637 sec/batch)
2016-07-16 11:42:47.115325: step 192610, loss = 2.94 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 11:42:51.898158: step 192620, loss = 2.65 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 11:42:58.002331: step 192630, loss = 2.83 (197.8 examples/sec; 0.647 sec/batch)
2016-07-16 11:43:03.695860: step 192640, loss = 2.81 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 11:43:08.456134: step 192650, loss = 2.60 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 11:43:13.315602: step 192660, loss = 2.80 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 11:43:18.019972: step 192670, loss = 2.75 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 11:43:22.659640: step 192680, loss = 2.75 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 11:43:27.361154: step 192690, loss = 2.65 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 11:43:32.040690: step 192700, loss = 2.73 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 11:43:37.664187: step 192710, loss = 3.15 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 11:43:42.629596: step 192720, loss = 2.84 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 11:43:48.222375: step 192730, loss = 2.86 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 11:43:53.973811: step 192740, loss = 2.65 (248.2 examples/sec; 0.516 sec/batch)
2016-07-16 11:43:59.205571: step 192750, loss = 2.62 (205.6 examples/sec; 0.622 sec/batch)
2016-07-16 11:44:04.646757: step 192760, loss = 2.78 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 11:44:09.369688: step 192770, loss = 2.59 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 11:44:14.416668: step 192780, loss = 2.73 (187.4 examples/sec; 0.683 sec/batch)
2016-07-16 11:44:20.922986: step 192790, loss = 2.76 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 11:44:25.945891: step 192800, loss = 2.74 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 11:44:31.533825: step 192810, loss = 2.68 (286.5 examples/sec; 0.447 sec/batch)
2016-07-16 11:44:36.183855: step 192820, loss = 2.66 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 11:44:40.799605: step 192830, loss = 2.73 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 11:44:45.488605: step 192840, loss = 2.71 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 11:44:51.284911: step 192850, loss = 2.70 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 11:44:56.043901: step 192860, loss = 2.93 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 11:45:00.883132: step 192870, loss = 2.71 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 11:45:05.677404: step 192880, loss = 2.92 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 11:45:10.311703: step 192890, loss = 2.76 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 11:45:14.924749: step 192900, loss = 2.62 (279.7 examples/sec; 0.458 sec/batch)
2016-07-16 11:45:20.501857: step 192910, loss = 2.68 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 11:45:26.175178: step 192920, loss = 2.69 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 11:45:31.044317: step 192930, loss = 2.66 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 11:45:35.776393: step 192940, loss = 2.71 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 11:45:40.602657: step 192950, loss = 2.73 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 11:45:45.434178: step 192960, loss = 2.73 (259.9 examples/sec; 0.493 sec/batch)
2016-07-16 11:45:50.094020: step 192970, loss = 2.83 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 11:45:54.748979: step 192980, loss = 2.64 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 11:46:00.080200: step 192990, loss = 2.64 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 11:46:05.383032: step 193000, loss = 2.53 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 11:46:12.279880: step 193010, loss = 2.73 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 11:46:17.006645: step 193020, loss = 2.74 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 11:46:21.875609: step 193030, loss = 2.83 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 11:46:26.618615: step 193040, loss = 2.89 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 11:46:32.265620: step 193050, loss = 2.62 (186.2 examples/sec; 0.687 sec/batch)
2016-07-16 11:46:38.487369: step 193060, loss = 2.81 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 11:46:43.331737: step 193070, loss = 2.60 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 11:46:47.982894: step 193080, loss = 2.68 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 11:46:52.669222: step 193090, loss = 2.52 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 11:46:58.418563: step 193100, loss = 2.85 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 11:47:05.071198: step 193110, loss = 2.85 (197.3 examples/sec; 0.649 sec/batch)
2016-07-16 11:47:10.089754: step 193120, loss = 2.75 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 11:47:14.744470: step 193130, loss = 2.98 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 11:47:19.426678: step 193140, loss = 2.72 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 11:47:24.079676: step 193150, loss = 2.93 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 11:47:28.803320: step 193160, loss = 2.96 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 11:47:34.564800: step 193170, loss = 2.98 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 11:47:39.373895: step 193180, loss = 2.99 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 11:47:44.063885: step 193190, loss = 2.62 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 11:47:48.700008: step 193200, loss = 2.58 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 11:47:54.297740: step 193210, loss = 2.71 (284.2 examples/sec; 0.450 sec/batch)
2016-07-16 11:47:58.932411: step 193220, loss = 2.66 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 11:48:04.202066: step 193230, loss = 2.61 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 11:48:09.495724: step 193240, loss = 2.44 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 11:48:15.270403: step 193250, loss = 2.77 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 11:48:20.040856: step 193260, loss = 2.69 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 11:48:24.886021: step 193270, loss = 2.62 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 11:48:29.659946: step 193280, loss = 2.88 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 11:48:34.747396: step 193290, loss = 2.72 (187.8 examples/sec; 0.681 sec/batch)
2016-07-16 11:48:41.257560: step 193300, loss = 2.55 (209.7 examples/sec; 0.611 sec/batch)
2016-07-16 11:48:47.302805: step 193310, loss = 2.83 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 11:48:52.073689: step 193320, loss = 2.98 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 11:48:56.821068: step 193330, loss = 2.59 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 11:49:01.692988: step 193340, loss = 2.91 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 11:49:08.241616: step 193350, loss = 2.73 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 11:49:13.510505: step 193360, loss = 2.77 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 11:49:19.254408: step 193370, loss = 2.59 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 11:49:24.108618: step 193380, loss = 2.91 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 11:49:28.949127: step 193390, loss = 2.69 (249.0 examples/sec; 0.514 sec/batch)
2016-07-16 11:49:33.693126: step 193400, loss = 2.71 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 11:49:40.397214: step 193410, loss = 2.73 (188.9 examples/sec; 0.678 sec/batch)
2016-07-16 11:49:46.633174: step 193420, loss = 2.60 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 11:49:51.477802: step 193430, loss = 2.71 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 11:49:56.123130: step 193440, loss = 2.67 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 11:50:00.801933: step 193450, loss = 2.70 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 11:50:06.565869: step 193460, loss = 2.69 (254.0 examples/sec; 0.504 sec/batch)
2016-07-16 11:50:11.415749: step 193470, loss = 2.71 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 11:50:16.256824: step 193480, loss = 2.64 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 11:50:22.382015: step 193490, loss = 2.58 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 11:50:28.094708: step 193500, loss = 2.87 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 11:50:33.895444: step 193510, loss = 2.84 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 11:50:38.809115: step 193520, loss = 2.80 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 11:50:43.571560: step 193530, loss = 2.79 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 11:50:49.428063: step 193540, loss = 2.98 (188.7 examples/sec; 0.678 sec/batch)
2016-07-16 11:50:55.354301: step 193550, loss = 2.76 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 11:51:00.172716: step 193560, loss = 2.90 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 11:51:05.021149: step 193570, loss = 2.75 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 11:51:09.751729: step 193580, loss = 2.69 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 11:51:14.932061: step 193590, loss = 2.96 (190.7 examples/sec; 0.671 sec/batch)
2016-07-16 11:51:21.441825: step 193600, loss = 2.89 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 11:51:27.463554: step 193610, loss = 2.69 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 11:51:32.257917: step 193620, loss = 2.71 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 11:51:37.030139: step 193630, loss = 2.58 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 11:51:41.656876: step 193640, loss = 2.75 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 11:51:46.447911: step 193650, loss = 2.77 (222.8 examples/sec; 0.574 sec/batch)
2016-07-16 11:51:52.128932: step 193660, loss = 2.79 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 11:51:56.900463: step 193670, loss = 2.71 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 11:52:01.730729: step 193680, loss = 2.79 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 11:52:06.423959: step 193690, loss = 2.79 (264.7 examples/sec; 0.483 sec/batch)
2016-07-16 11:52:11.837132: step 193700, loss = 2.88 (186.5 examples/sec; 0.686 sec/batch)
2016-07-16 11:52:19.596753: step 193710, loss = 2.77 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 11:52:24.372132: step 193720, loss = 2.72 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 11:52:29.185648: step 193730, loss = 2.79 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 11:52:33.921985: step 193740, loss = 2.70 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 11:52:38.565586: step 193750, loss = 2.64 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 11:52:43.698765: step 193760, loss = 2.66 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 11:52:49.140515: step 193770, loss = 2.88 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 11:52:53.849215: step 193780, loss = 2.68 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 11:52:58.957770: step 193790, loss = 2.71 (188.3 examples/sec; 0.680 sec/batch)
2016-07-16 11:53:05.491252: step 193800, loss = 2.98 (208.3 examples/sec; 0.614 sec/batch)
2016-07-16 11:53:11.571083: step 193810, loss = 2.72 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 11:53:16.420801: step 193820, loss = 2.69 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 11:53:22.484468: step 193830, loss = 2.78 (197.7 examples/sec; 0.648 sec/batch)
2016-07-16 11:53:28.250443: step 193840, loss = 2.76 (259.4 examples/sec; 0.494 sec/batch)
2016-07-16 11:53:33.957797: step 193850, loss = 2.64 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 11:53:39.060650: step 193860, loss = 2.64 (201.0 examples/sec; 0.637 sec/batch)
2016-07-16 11:53:44.685020: step 193870, loss = 2.63 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 11:53:49.419338: step 193880, loss = 2.66 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 11:53:54.285385: step 193890, loss = 2.72 (246.1 examples/sec; 0.520 sec/batch)
2016-07-16 11:54:00.850461: step 193900, loss = 2.65 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 11:54:07.027508: step 193910, loss = 2.59 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 11:54:11.674823: step 193920, loss = 2.71 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 11:54:17.132703: step 193930, loss = 2.57 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 11:54:22.281691: step 193940, loss = 2.74 (255.3 examples/sec; 0.501 sec/batch)
2016-07-16 11:54:28.032250: step 193950, loss = 2.76 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 11:54:32.836069: step 193960, loss = 2.73 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 11:54:37.696383: step 193970, loss = 2.41 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 11:54:42.437708: step 193980, loss = 2.85 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 11:54:47.340577: step 193990, loss = 3.02 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 11:54:52.027456: step 194000, loss = 2.72 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 11:54:57.613197: step 194010, loss = 2.56 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 11:55:03.380342: step 194020, loss = 2.90 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 11:55:08.875315: step 194030, loss = 2.71 (208.0 examples/sec; 0.616 sec/batch)
2016-07-16 11:55:14.072146: step 194040, loss = 2.80 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 11:55:18.787584: step 194050, loss = 2.89 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 11:55:23.718709: step 194060, loss = 2.62 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 11:55:28.510762: step 194070, loss = 2.66 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 11:55:34.654416: step 194080, loss = 2.60 (193.7 examples/sec; 0.661 sec/batch)
2016-07-16 11:55:40.395830: step 194090, loss = 2.67 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 11:55:46.141528: step 194100, loss = 2.80 (209.2 examples/sec; 0.612 sec/batch)
2016-07-16 11:55:51.991698: step 194110, loss = 2.70 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 11:55:56.765860: step 194120, loss = 2.82 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 11:56:01.531950: step 194130, loss = 2.81 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 11:56:06.394402: step 194140, loss = 2.67 (252.1 examples/sec; 0.508 sec/batch)
2016-07-16 11:56:12.986757: step 194150, loss = 2.71 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 11:56:18.171332: step 194160, loss = 2.55 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 11:56:22.888515: step 194170, loss = 2.77 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 11:56:27.713655: step 194180, loss = 2.79 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 11:56:32.517160: step 194190, loss = 2.86 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 11:56:37.266212: step 194200, loss = 2.90 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 11:56:42.825027: step 194210, loss = 2.66 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 11:56:48.024182: step 194220, loss = 2.69 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 11:56:53.487615: step 194230, loss = 2.61 (249.2 examples/sec; 0.514 sec/batch)
2016-07-16 11:56:59.295908: step 194240, loss = 2.76 (244.3 examples/sec; 0.524 sec/batch)
2016-07-16 11:57:04.092824: step 194250, loss = 2.70 (278.0 examples/sec; 0.461 sec/batch)
2016-07-16 11:57:08.935186: step 194260, loss = 2.76 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 11:57:13.704788: step 194270, loss = 2.76 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 11:57:18.641291: step 194280, loss = 3.01 (224.8 examples/sec; 0.569 sec/batch)
2016-07-16 11:57:25.183573: step 194290, loss = 2.63 (208.0 examples/sec; 0.615 sec/batch)
2016-07-16 11:57:30.320113: step 194300, loss = 2.70 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 11:57:36.012904: step 194310, loss = 2.79 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 11:57:40.844956: step 194320, loss = 2.94 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 11:57:45.466163: step 194330, loss = 2.98 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 11:57:50.134276: step 194340, loss = 2.86 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 11:57:55.885740: step 194350, loss = 2.57 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 11:58:00.661090: step 194360, loss = 2.48 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 11:58:05.504659: step 194370, loss = 2.54 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 11:58:11.843292: step 194380, loss = 2.63 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 11:58:17.268949: step 194390, loss = 2.87 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 11:58:22.048229: step 194400, loss = 2.76 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 11:58:27.951252: step 194410, loss = 2.80 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 11:58:32.579089: step 194420, loss = 2.76 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 11:58:37.308276: step 194430, loss = 2.74 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 11:58:43.031120: step 194440, loss = 2.77 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 11:58:47.851039: step 194450, loss = 2.87 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 11:58:52.626005: step 194460, loss = 2.53 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 11:58:57.444092: step 194470, loss = 2.56 (250.7 examples/sec; 0.511 sec/batch)
2016-07-16 11:59:02.303310: step 194480, loss = 2.79 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 11:59:06.986124: step 194490, loss = 2.58 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 11:59:11.794356: step 194500, loss = 2.88 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 11:59:17.612855: step 194510, loss = 2.93 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 11:59:22.351637: step 194520, loss = 2.77 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 11:59:26.968132: step 194530, loss = 2.61 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 11:59:32.126830: step 194540, loss = 2.89 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 11:59:37.589184: step 194550, loss = 2.68 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 11:59:43.350478: step 194560, loss = 2.74 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 11:59:48.157298: step 194570, loss = 2.69 (276.2 examples/sec; 0.464 sec/batch)
2016-07-16 11:59:52.785594: step 194580, loss = 2.90 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 11:59:57.476147: step 194590, loss = 2.67 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 12:00:03.270987: step 194600, loss = 2.60 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 12:00:09.920348: step 194610, loss = 2.71 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 12:00:14.937552: step 194620, loss = 2.76 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 12:00:19.707912: step 194630, loss = 2.72 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 12:00:25.482849: step 194640, loss = 2.61 (185.2 examples/sec; 0.691 sec/batch)
2016-07-16 12:00:31.566902: step 194650, loss = 2.60 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 12:00:36.391774: step 194660, loss = 2.98 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 12:00:41.221302: step 194670, loss = 2.54 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 12:00:47.419987: step 194680, loss = 2.82 (207.6 examples/sec; 0.617 sec/batch)
2016-07-16 12:00:52.952449: step 194690, loss = 2.55 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 12:00:57.734327: step 194700, loss = 2.77 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 12:01:03.350566: step 194710, loss = 2.72 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 12:01:08.484622: step 194720, loss = 2.73 (200.5 examples/sec; 0.639 sec/batch)
2016-07-16 12:01:13.825491: step 194730, loss = 2.94 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 12:01:19.618392: step 194740, loss = 2.78 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 12:01:24.457711: step 194750, loss = 2.88 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 12:01:29.303778: step 194760, loss = 2.90 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 12:01:35.509608: step 194770, loss = 2.85 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 12:01:41.095221: step 194780, loss = 2.86 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 12:01:45.847114: step 194790, loss = 2.62 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 12:01:50.781418: step 194800, loss = 3.10 (234.0 examples/sec; 0.547 sec/batch)
2016-07-16 12:01:58.967873: step 194810, loss = 2.71 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 12:02:04.280855: step 194820, loss = 2.72 (196.3 examples/sec; 0.652 sec/batch)
2016-07-16 12:02:09.719355: step 194830, loss = 2.72 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 12:02:15.470732: step 194840, loss = 2.69 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 12:02:20.820214: step 194850, loss = 2.96 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 12:02:26.162445: step 194860, loss = 2.74 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 12:02:30.890268: step 194870, loss = 2.76 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 12:02:36.263453: step 194880, loss = 2.52 (188.0 examples/sec; 0.681 sec/batch)
2016-07-16 12:02:42.739032: step 194890, loss = 2.60 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 12:02:47.572041: step 194900, loss = 2.54 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 12:02:53.249523: step 194910, loss = 2.61 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 12:02:59.304558: step 194920, loss = 2.65 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 12:03:03.944339: step 194930, loss = 2.82 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 12:03:09.327936: step 194940, loss = 2.72 (207.0 examples/sec; 0.618 sec/batch)
2016-07-16 12:03:14.544350: step 194950, loss = 2.81 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 12:03:19.316733: step 194960, loss = 2.83 (249.4 examples/sec; 0.513 sec/batch)
2016-07-16 12:03:24.112179: step 194970, loss = 2.69 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 12:03:28.936541: step 194980, loss = 2.55 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 12:03:34.995899: step 194990, loss = 2.48 (199.1 examples/sec; 0.643 sec/batch)
2016-07-16 12:03:40.740205: step 195000, loss = 2.72 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 12:03:46.508145: step 195010, loss = 2.53 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 12:03:51.803863: step 195020, loss = 2.76 (188.6 examples/sec; 0.679 sec/batch)
2016-07-16 12:03:58.266196: step 195030, loss = 2.67 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 12:04:03.242408: step 195040, loss = 2.67 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 12:04:08.013573: step 195050, loss = 2.75 (259.4 examples/sec; 0.494 sec/batch)
2016-07-16 12:04:13.746188: step 195060, loss = 2.60 (186.9 examples/sec; 0.685 sec/batch)
2016-07-16 12:04:19.846261: step 195070, loss = 2.66 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 12:04:24.667840: step 195080, loss = 2.58 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 12:04:29.565893: step 195090, loss = 2.70 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 12:04:35.811558: step 195100, loss = 2.73 (206.3 examples/sec; 0.621 sec/batch)
2016-07-16 12:04:42.620103: step 195110, loss = 2.77 (251.6 examples/sec; 0.509 sec/batch)
2016-07-16 12:04:47.393641: step 195120, loss = 2.85 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 12:04:52.260095: step 195130, loss = 2.44 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 12:04:57.033423: step 195140, loss = 2.78 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 12:05:03.129761: step 195150, loss = 3.16 (199.0 examples/sec; 0.643 sec/batch)
2016-07-16 12:05:08.856688: step 195160, loss = 2.66 (247.9 examples/sec; 0.516 sec/batch)
2016-07-16 12:05:13.674435: step 195170, loss = 2.69 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 12:05:18.532385: step 195180, loss = 2.94 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 12:05:25.052806: step 195190, loss = 2.61 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 12:05:30.410181: step 195200, loss = 2.74 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 12:05:37.318283: step 195210, loss = 2.81 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 12:05:42.061446: step 195220, loss = 2.64 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 12:05:46.877131: step 195230, loss = 2.80 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 12:05:53.465199: step 195240, loss = 2.60 (202.2 examples/sec; 0.633 sec/batch)
2016-07-16 12:05:58.688383: step 195250, loss = 2.72 (257.3 examples/sec; 0.498 sec/batch)
2016-07-16 12:06:03.424395: step 195260, loss = 2.89 (255.3 examples/sec; 0.501 sec/batch)
2016-07-16 12:06:08.271846: step 195270, loss = 2.61 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 12:06:13.125900: step 195280, loss = 2.81 (254.5 examples/sec; 0.503 sec/batch)
2016-07-16 12:06:19.283051: step 195290, loss = 2.94 (197.9 examples/sec; 0.647 sec/batch)
2016-07-16 12:06:24.992695: step 195300, loss = 2.95 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 12:06:30.769013: step 195310, loss = 2.48 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 12:06:35.367175: step 195320, loss = 2.66 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 12:06:40.380674: step 195330, loss = 2.85 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 12:06:45.813318: step 195340, loss = 2.79 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 12:06:50.591759: step 195350, loss = 2.91 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 12:06:55.829813: step 195360, loss = 2.78 (189.9 examples/sec; 0.674 sec/batch)
2016-07-16 12:07:02.342591: step 195370, loss = 2.56 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 12:07:07.352583: step 195380, loss = 2.74 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 12:07:12.084721: step 195390, loss = 2.67 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 12:07:16.911596: step 195400, loss = 2.73 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 12:07:22.541201: step 195410, loss = 2.58 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 12:07:27.353784: step 195420, loss = 2.75 (212.7 examples/sec; 0.602 sec/batch)
2016-07-16 12:07:33.050255: step 195430, loss = 2.72 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 12:07:39.053965: step 195440, loss = 2.91 (207.3 examples/sec; 0.618 sec/batch)
2016-07-16 12:07:44.244502: step 195450, loss = 2.73 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 12:07:49.731030: step 195460, loss = 2.68 (256.9 examples/sec; 0.498 sec/batch)
2016-07-16 12:07:55.585432: step 195470, loss = 2.79 (237.5 examples/sec; 0.539 sec/batch)
2016-07-16 12:08:00.382269: step 195480, loss = 2.52 (281.6 examples/sec; 0.455 sec/batch)
2016-07-16 12:08:05.159232: step 195490, loss = 2.66 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 12:08:11.196464: step 195500, loss = 3.01 (196.5 examples/sec; 0.651 sec/batch)
2016-07-16 12:08:18.058290: step 195510, loss = 2.67 (266.4 examples/sec; 0.481 sec/batch)
2016-07-16 12:08:23.825415: step 195520, loss = 2.66 (253.4 examples/sec; 0.505 sec/batch)
2016-07-16 12:08:29.222822: step 195530, loss = 2.45 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 12:08:34.516299: step 195540, loss = 2.60 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 12:08:39.256828: step 195550, loss = 2.79 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 12:08:44.103426: step 195560, loss = 2.75 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 12:08:48.787584: step 195570, loss = 2.83 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 12:08:53.471298: step 195580, loss = 2.80 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 12:08:59.164392: step 195590, loss = 2.73 (221.8 examples/sec; 0.577 sec/batch)
2016-07-16 12:09:04.382134: step 195600, loss = 2.93 (197.8 examples/sec; 0.647 sec/batch)
2016-07-16 12:09:11.158890: step 195610, loss = 2.84 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 12:09:15.786378: step 195620, loss = 2.44 (285.2 examples/sec; 0.449 sec/batch)
2016-07-16 12:09:20.459766: step 195630, loss = 2.50 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 12:09:25.855275: step 195640, loss = 2.86 (200.5 examples/sec; 0.638 sec/batch)
2016-07-16 12:09:31.048056: step 195650, loss = 2.59 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 12:09:35.779128: step 195660, loss = 2.73 (253.1 examples/sec; 0.506 sec/batch)
2016-07-16 12:09:40.563497: step 195670, loss = 2.56 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 12:09:45.401381: step 195680, loss = 2.67 (250.7 examples/sec; 0.511 sec/batch)
2016-07-16 12:09:51.536229: step 195690, loss = 2.78 (200.9 examples/sec; 0.637 sec/batch)
2016-07-16 12:09:57.228177: step 195700, loss = 2.94 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 12:10:03.052900: step 195710, loss = 2.87 (276.8 examples/sec; 0.463 sec/batch)
2016-07-16 12:10:07.657579: step 195720, loss = 2.74 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 12:10:12.835301: step 195730, loss = 2.59 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 12:10:18.199240: step 195740, loss = 2.84 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 12:10:23.955500: step 195750, loss = 2.56 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 12:10:29.369656: step 195760, loss = 2.67 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 12:10:34.820490: step 195770, loss = 2.74 (200.8 examples/sec; 0.637 sec/batch)
2016-07-16 12:10:40.669459: step 195780, loss = 2.56 (236.2 examples/sec; 0.542 sec/batch)
2016-07-16 12:10:46.311420: step 195790, loss = 2.57 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 12:10:51.408914: step 195800, loss = 2.69 (229.7 examples/sec; 0.557 sec/batch)
2016-07-16 12:10:58.317986: step 195810, loss = 2.81 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 12:11:02.996677: step 195820, loss = 2.92 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 12:11:08.147971: step 195830, loss = 2.67 (190.1 examples/sec; 0.673 sec/batch)
2016-07-16 12:11:14.682384: step 195840, loss = 2.83 (199.9 examples/sec; 0.640 sec/batch)
2016-07-16 12:11:19.653768: step 195850, loss = 2.61 (275.6 examples/sec; 0.465 sec/batch)
2016-07-16 12:11:24.384213: step 195860, loss = 2.85 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 12:11:29.194227: step 195870, loss = 2.73 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 12:11:33.841042: step 195880, loss = 2.61 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 12:11:38.509030: step 195890, loss = 2.72 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 12:11:44.303882: step 195900, loss = 2.80 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 12:11:51.022200: step 195910, loss = 2.86 (207.7 examples/sec; 0.616 sec/batch)
2016-07-16 12:11:56.122691: step 195920, loss = 2.98 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 12:12:01.703844: step 195930, loss = 2.83 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 12:12:06.512328: step 195940, loss = 2.84 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 12:12:11.454637: step 195950, loss = 2.91 (233.1 examples/sec; 0.549 sec/batch)
2016-07-16 12:12:18.011602: step 195960, loss = 3.03 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 12:12:23.164318: step 195970, loss = 2.92 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 12:12:27.898483: step 195980, loss = 3.00 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 12:12:33.339186: step 195990, loss = 2.86 (189.5 examples/sec; 0.675 sec/batch)
2016-07-16 12:12:39.666592: step 196000, loss = 2.71 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 12:12:45.503344: step 196010, loss = 2.90 (244.5 examples/sec; 0.523 sec/batch)
2016-07-16 12:12:50.161480: step 196020, loss = 2.77 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 12:12:54.893063: step 196030, loss = 2.79 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 12:13:00.628412: step 196040, loss = 2.52 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 12:13:05.486640: step 196050, loss = 2.57 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 12:13:10.265776: step 196060, loss = 2.62 (258.3 examples/sec; 0.496 sec/batch)
2016-07-16 12:13:14.943040: step 196070, loss = 2.70 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 12:13:20.015103: step 196080, loss = 2.81 (192.3 examples/sec; 0.666 sec/batch)
2016-07-16 12:13:26.490434: step 196090, loss = 2.94 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 12:13:31.659463: step 196100, loss = 2.77 (217.5 examples/sec; 0.588 sec/batch)
2016-07-16 12:13:38.561555: step 196110, loss = 2.76 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 12:13:44.390856: step 196120, loss = 2.84 (257.8 examples/sec; 0.496 sec/batch)
2016-07-16 12:13:49.828372: step 196130, loss = 2.77 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 12:13:55.045348: step 196140, loss = 2.57 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 12:14:00.803942: step 196150, loss = 2.49 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 12:14:05.597822: step 196160, loss = 2.61 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 12:14:10.380915: step 196170, loss = 2.64 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 12:14:16.707193: step 196180, loss = 2.92 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 12:14:22.134007: step 196190, loss = 2.52 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 12:14:26.839478: step 196200, loss = 2.46 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 12:14:32.715636: step 196210, loss = 2.74 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 12:14:37.449376: step 196220, loss = 2.63 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 12:14:43.456764: step 196230, loss = 2.54 (192.9 examples/sec; 0.663 sec/batch)
2016-07-16 12:14:49.200188: step 196240, loss = 2.63 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 12:14:54.076256: step 196250, loss = 2.71 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 12:14:58.680257: step 196260, loss = 2.68 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 12:15:03.344469: step 196270, loss = 2.50 (267.5 examples/sec; 0.478 sec/batch)
2016-07-16 12:15:07.927356: step 196280, loss = 2.89 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 12:15:12.638868: step 196290, loss = 2.69 (283.7 examples/sec; 0.451 sec/batch)
2016-07-16 12:15:17.256377: step 196300, loss = 2.81 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 12:15:23.976646: step 196310, loss = 3.09 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 12:15:28.763390: step 196320, loss = 2.69 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 12:15:33.536732: step 196330, loss = 2.68 (243.8 examples/sec; 0.525 sec/batch)
2016-07-16 12:15:38.306333: step 196340, loss = 3.06 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 12:15:43.264856: step 196350, loss = 2.48 (253.1 examples/sec; 0.506 sec/batch)
2016-07-16 12:15:48.157378: step 196360, loss = 2.77 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 12:15:54.608304: step 196370, loss = 2.83 (188.7 examples/sec; 0.678 sec/batch)
2016-07-16 12:16:00.621254: step 196380, loss = 2.63 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 12:16:05.419346: step 196390, loss = 2.83 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 12:16:10.123903: step 196400, loss = 2.57 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 12:16:15.825625: step 196410, loss = 2.66 (214.0 examples/sec; 0.598 sec/batch)
2016-07-16 12:16:21.547506: step 196420, loss = 2.88 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 12:16:27.290927: step 196430, loss = 2.72 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 12:16:32.188970: step 196440, loss = 2.85 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 12:16:36.961987: step 196450, loss = 2.86 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 12:16:41.770374: step 196460, loss = 2.89 (250.6 examples/sec; 0.511 sec/batch)
2016-07-16 12:16:46.630070: step 196470, loss = 2.87 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 12:16:51.339708: step 196480, loss = 2.84 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 12:16:56.017039: step 196490, loss = 2.46 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 12:17:01.146015: step 196500, loss = 2.76 (201.7 examples/sec; 0.635 sec/batch)
2016-07-16 12:17:07.749979: step 196510, loss = 2.92 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 12:17:12.494526: step 196520, loss = 2.81 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 12:17:17.333330: step 196530, loss = 2.63 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 12:17:22.218689: step 196540, loss = 2.78 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 12:17:27.039269: step 196550, loss = 2.70 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 12:17:31.903618: step 196560, loss = 2.67 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 12:17:36.654801: step 196570, loss = 2.85 (257.3 examples/sec; 0.498 sec/batch)
2016-07-16 12:17:42.379439: step 196580, loss = 2.68 (186.1 examples/sec; 0.688 sec/batch)
2016-07-16 12:17:48.504008: step 196590, loss = 2.59 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 12:17:53.320419: step 196600, loss = 2.61 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 12:17:59.201575: step 196610, loss = 2.85 (259.4 examples/sec; 0.494 sec/batch)
2016-07-16 12:18:03.871318: step 196620, loss = 2.68 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 12:18:08.482568: step 196630, loss = 2.75 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 12:18:13.693923: step 196640, loss = 2.62 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 12:18:19.005818: step 196650, loss = 2.68 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 12:18:23.686893: step 196660, loss = 3.00 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 12:18:29.032682: step 196670, loss = 2.83 (191.0 examples/sec; 0.670 sec/batch)
2016-07-16 12:18:35.474916: step 196680, loss = 2.53 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 12:18:40.311608: step 196690, loss = 2.69 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 12:18:45.061348: step 196700, loss = 2.64 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 12:18:50.846926: step 196710, loss = 2.90 (254.2 examples/sec; 0.504 sec/batch)
2016-07-16 12:18:55.745663: step 196720, loss = 2.57 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 12:19:00.428590: step 196730, loss = 2.70 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 12:19:05.059135: step 196740, loss = 2.89 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 12:19:10.528930: step 196750, loss = 2.67 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 12:19:15.667807: step 196760, loss = 2.87 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 12:19:20.398225: step 196770, loss = 2.74 (255.8 examples/sec; 0.500 sec/batch)
2016-07-16 12:19:25.969117: step 196780, loss = 2.70 (192.7 examples/sec; 0.664 sec/batch)
2016-07-16 12:19:32.196937: step 196790, loss = 2.66 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 12:19:37.035173: step 196800, loss = 2.69 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 12:19:42.875748: step 196810, loss = 2.66 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 12:19:49.302820: step 196820, loss = 2.82 (199.2 examples/sec; 0.642 sec/batch)
2016-07-16 12:19:54.668838: step 196830, loss = 2.68 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 12:19:59.372277: step 196840, loss = 2.64 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 12:20:04.241867: step 196850, loss = 2.83 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 12:20:08.934155: step 196860, loss = 2.97 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 12:20:13.595213: step 196870, loss = 2.57 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 12:20:19.163079: step 196880, loss = 2.67 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 12:20:24.130280: step 196890, loss = 2.86 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 12:20:28.831829: step 196900, loss = 2.53 (266.4 examples/sec; 0.480 sec/batch)
2016-07-16 12:20:36.011323: step 196910, loss = 2.66 (198.7 examples/sec; 0.644 sec/batch)
2016-07-16 12:20:41.718362: step 196920, loss = 2.61 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 12:20:47.441550: step 196930, loss = 2.79 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 12:20:52.558331: step 196940, loss = 2.48 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 12:20:58.000557: step 196950, loss = 2.58 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 12:21:02.677083: step 196960, loss = 2.66 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 12:21:07.328542: step 196970, loss = 2.71 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 12:21:12.189821: step 196980, loss = 2.81 (208.0 examples/sec; 0.615 sec/batch)
2016-07-16 12:21:17.861808: step 196990, loss = 2.86 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 12:21:22.605620: step 197000, loss = 2.42 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 12:21:28.236191: step 197010, loss = 2.55 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 12:21:32.911959: step 197020, loss = 2.81 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 12:21:37.540429: step 197030, loss = 2.68 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 12:21:42.190635: step 197040, loss = 2.60 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 12:21:46.844285: step 197050, loss = 2.71 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 12:21:51.525041: step 197060, loss = 2.88 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 12:21:56.153760: step 197070, loss = 2.77 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 12:22:01.198331: step 197080, loss = 2.81 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 12:22:06.609763: step 197090, loss = 2.61 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 12:22:11.350007: step 197100, loss = 2.98 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 12:22:17.170202: step 197110, loss = 2.57 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 12:22:21.829995: step 197120, loss = 3.04 (284.9 examples/sec; 0.449 sec/batch)
2016-07-16 12:22:26.480331: step 197130, loss = 2.87 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 12:22:32.195698: step 197140, loss = 2.92 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 12:22:37.036755: step 197150, loss = 2.82 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 12:22:41.899870: step 197160, loss = 2.65 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 12:22:47.945710: step 197170, loss = 2.72 (195.0 examples/sec; 0.657 sec/batch)
2016-07-16 12:22:53.698398: step 197180, loss = 2.64 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 12:22:58.503136: step 197190, loss = 2.75 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 12:23:03.332687: step 197200, loss = 2.71 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 12:23:11.350702: step 197210, loss = 2.63 (202.0 examples/sec; 0.634 sec/batch)
2016-07-16 12:23:16.481361: step 197220, loss = 2.69 (207.3 examples/sec; 0.618 sec/batch)
2016-07-16 12:23:22.065908: step 197230, loss = 2.71 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 12:23:27.849802: step 197240, loss = 2.78 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 12:23:32.744423: step 197250, loss = 2.90 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 12:23:37.541543: step 197260, loss = 2.73 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 12:23:43.556548: step 197270, loss = 2.65 (192.3 examples/sec; 0.666 sec/batch)
2016-07-16 12:23:49.439849: step 197280, loss = 2.97 (244.0 examples/sec; 0.525 sec/batch)
2016-07-16 12:23:55.011371: step 197290, loss = 2.59 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 12:24:00.090285: step 197300, loss = 2.67 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 12:24:05.842066: step 197310, loss = 2.56 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 12:24:10.643682: step 197320, loss = 2.60 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 12:24:15.510390: step 197330, loss = 2.78 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 12:24:20.209841: step 197340, loss = 2.81 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 12:24:24.855845: step 197350, loss = 2.45 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 12:24:30.096708: step 197360, loss = 2.51 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 12:24:35.393813: step 197370, loss = 2.63 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 12:24:40.162478: step 197380, loss = 2.88 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 12:24:45.543157: step 197390, loss = 2.76 (190.0 examples/sec; 0.674 sec/batch)
2016-07-16 12:24:51.967418: step 197400, loss = 2.80 (209.9 examples/sec; 0.610 sec/batch)
2016-07-16 12:24:57.815625: step 197410, loss = 2.71 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 12:25:02.613132: step 197420, loss = 2.59 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 12:25:08.852696: step 197430, loss = 2.57 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 12:25:14.438529: step 197440, loss = 2.80 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 12:25:20.644636: step 197450, loss = 2.51 (246.8 examples/sec; 0.519 sec/batch)
2016-07-16 12:25:26.001929: step 197460, loss = 2.77 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 12:25:31.133822: step 197470, loss = 2.86 (285.7 examples/sec; 0.448 sec/batch)
2016-07-16 12:25:35.834915: step 197480, loss = 2.91 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 12:25:41.595234: step 197490, loss = 2.61 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 12:25:46.400536: step 197500, loss = 2.88 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 12:25:51.969982: step 197510, loss = 2.57 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 12:25:56.860429: step 197520, loss = 2.76 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 12:26:02.454846: step 197530, loss = 2.82 (263.1 examples/sec; 0.486 sec/batch)
2016-07-16 12:26:07.205446: step 197540, loss = 2.69 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 12:26:12.079706: step 197550, loss = 2.75 (244.6 examples/sec; 0.523 sec/batch)
2016-07-16 12:26:18.606989: step 197560, loss = 2.62 (204.9 examples/sec; 0.625 sec/batch)
2016-07-16 12:26:23.727328: step 197570, loss = 2.83 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 12:26:28.417519: step 197580, loss = 2.91 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 12:26:33.239565: step 197590, loss = 2.72 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 12:26:38.002833: step 197600, loss = 2.79 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 12:26:45.372856: step 197610, loss = 2.95 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 12:26:50.818373: step 197620, loss = 2.77 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 12:26:55.551648: step 197630, loss = 2.87 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 12:27:00.219412: step 197640, loss = 2.77 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 12:27:04.849231: step 197650, loss = 2.60 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 12:27:09.467815: step 197660, loss = 2.77 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 12:27:14.856522: step 197670, loss = 2.92 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 12:27:19.968456: step 197680, loss = 2.63 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 12:27:24.607024: step 197690, loss = 2.67 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 12:27:29.242221: step 197700, loss = 2.36 (270.5 examples/sec; 0.473 sec/batch)
2016-07-16 12:27:34.774521: step 197710, loss = 2.63 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 12:27:39.416159: step 197720, loss = 2.66 (278.6 examples/sec; 0.460 sec/batch)
2016-07-16 12:27:45.160932: step 197730, loss = 2.86 (256.2 examples/sec; 0.500 sec/batch)
2016-07-16 12:27:50.582171: step 197740, loss = 2.69 (208.3 examples/sec; 0.614 sec/batch)
2016-07-16 12:27:55.780628: step 197750, loss = 2.63 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 12:28:01.582697: step 197760, loss = 2.66 (255.4 examples/sec; 0.501 sec/batch)
2016-07-16 12:28:06.315537: step 197770, loss = 2.57 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 12:28:10.961622: step 197780, loss = 2.65 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 12:28:15.619056: step 197790, loss = 2.75 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 12:28:21.335074: step 197800, loss = 2.57 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 12:28:28.202397: step 197810, loss = 2.76 (212.4 examples/sec; 0.603 sec/batch)
2016-07-16 12:28:33.046599: step 197820, loss = 3.07 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 12:28:37.785306: step 197830, loss = 2.72 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 12:28:43.704345: step 197840, loss = 2.61 (186.5 examples/sec; 0.686 sec/batch)
2016-07-16 12:28:49.600580: step 197850, loss = 2.71 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 12:28:54.380470: step 197860, loss = 2.55 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 12:28:59.191189: step 197870, loss = 2.77 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 12:29:05.537380: step 197880, loss = 2.61 (207.2 examples/sec; 0.618 sec/batch)
2016-07-16 12:29:10.795591: step 197890, loss = 2.57 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 12:29:15.457723: step 197900, loss = 2.53 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 12:29:21.073980: step 197910, loss = 2.74 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 12:29:26.342943: step 197920, loss = 2.82 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 12:29:31.606261: step 197930, loss = 2.76 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 12:29:36.309659: step 197940, loss = 2.86 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 12:29:41.545428: step 197950, loss = 2.55 (191.6 examples/sec; 0.668 sec/batch)
2016-07-16 12:29:47.990104: step 197960, loss = 2.63 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 12:29:52.885386: step 197970, loss = 2.85 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 12:29:57.615327: step 197980, loss = 2.63 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 12:30:02.376488: step 197990, loss = 2.73 (280.8 examples/sec; 0.456 sec/batch)
2016-07-16 12:30:07.180169: step 198000, loss = 2.76 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 12:30:12.861217: step 198010, loss = 2.81 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 12:30:17.680722: step 198020, loss = 2.95 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 12:30:22.454005: step 198030, loss = 2.64 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 12:30:27.316640: step 198040, loss = 2.65 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 12:30:33.525573: step 198050, loss = 2.85 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 12:30:38.328823: step 198060, loss = 2.54 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 12:30:43.132373: step 198070, loss = 2.69 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 12:30:47.898757: step 198080, loss = 2.85 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 12:30:52.781957: step 198090, loss = 2.60 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 12:30:57.539512: step 198100, loss = 2.61 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 12:31:03.325781: step 198110, loss = 2.61 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 12:31:08.178866: step 198120, loss = 2.62 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 12:31:12.924489: step 198130, loss = 2.60 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 12:31:17.752959: step 198140, loss = 2.76 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 12:31:22.463043: step 198150, loss = 2.82 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 12:31:27.099286: step 198160, loss = 2.57 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 12:31:32.860490: step 198170, loss = 3.00 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 12:31:38.283782: step 198180, loss = 2.60 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 12:31:43.696190: step 198190, loss = 2.71 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 12:31:48.422555: step 198200, loss = 2.61 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 12:31:54.269364: step 198210, loss = 2.70 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 12:31:58.941460: step 198220, loss = 2.88 (281.6 examples/sec; 0.454 sec/batch)
2016-07-16 12:32:03.593026: step 198230, loss = 2.73 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 12:32:09.371261: step 198240, loss = 2.81 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 12:32:14.791078: step 198250, loss = 2.63 (207.7 examples/sec; 0.616 sec/batch)
2016-07-16 12:32:19.998669: step 198260, loss = 2.89 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 12:32:25.807577: step 198270, loss = 2.68 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 12:32:30.577526: step 198280, loss = 2.81 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 12:32:35.451541: step 198290, loss = 2.86 (240.8 examples/sec; 0.532 sec/batch)
2016-07-16 12:32:41.799972: step 198300, loss = 2.59 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 12:32:48.466572: step 198310, loss = 2.77 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 12:32:53.112827: step 198320, loss = 2.81 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 12:32:57.742562: step 198330, loss = 2.82 (280.4 examples/sec; 0.457 sec/batch)
2016-07-16 12:33:03.222895: step 198340, loss = 2.70 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 12:33:08.329166: step 198350, loss = 2.77 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 12:33:13.041946: step 198360, loss = 2.54 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 12:33:17.824282: step 198370, loss = 2.61 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 12:33:22.473582: step 198380, loss = 2.53 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 12:33:27.104408: step 198390, loss = 2.64 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 12:33:32.888170: step 198400, loss = 2.86 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 12:33:39.621189: step 198410, loss = 2.83 (207.8 examples/sec; 0.616 sec/batch)
2016-07-16 12:33:44.449631: step 198420, loss = 2.70 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 12:33:49.211083: step 198430, loss = 2.52 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 12:33:53.822688: step 198440, loss = 2.79 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 12:33:59.288114: step 198450, loss = 2.62 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 12:34:04.417870: step 198460, loss = 2.60 (257.2 examples/sec; 0.498 sec/batch)
2016-07-16 12:34:10.156197: step 198470, loss = 2.68 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 12:34:15.801912: step 198480, loss = 2.80 (206.6 examples/sec; 0.619 sec/batch)
2016-07-16 12:34:20.815771: step 198490, loss = 2.85 (270.3 examples/sec; 0.474 sec/batch)
2016-07-16 12:34:25.505792: step 198500, loss = 2.64 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 12:34:32.668186: step 198510, loss = 2.40 (195.5 examples/sec; 0.655 sec/batch)
2016-07-16 12:34:38.364900: step 198520, loss = 2.56 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 12:34:43.173111: step 198530, loss = 2.62 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 12:34:48.032478: step 198540, loss = 2.79 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 12:34:52.788046: step 198550, loss = 2.68 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 12:34:57.366302: step 198560, loss = 2.82 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 12:35:02.576847: step 198570, loss = 2.57 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 12:35:07.909042: step 198580, loss = 2.87 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 12:35:12.625194: step 198590, loss = 2.58 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 12:35:17.513830: step 198600, loss = 2.84 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 12:35:23.077538: step 198610, loss = 2.57 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 12:35:27.727900: step 198620, loss = 2.80 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 12:35:33.483986: step 198630, loss = 2.78 (252.7 examples/sec; 0.507 sec/batch)
2016-07-16 12:35:38.280766: step 198640, loss = 2.92 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 12:35:43.072153: step 198650, loss = 2.68 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 12:35:49.280863: step 198660, loss = 2.71 (197.8 examples/sec; 0.647 sec/batch)
2016-07-16 12:35:54.736684: step 198670, loss = 2.86 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 12:35:59.368574: step 198680, loss = 2.46 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 12:36:05.099435: step 198690, loss = 2.78 (254.4 examples/sec; 0.503 sec/batch)
2016-07-16 12:36:09.980740: step 198700, loss = 2.75 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 12:36:15.610441: step 198710, loss = 2.69 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 12:36:20.300914: step 198720, loss = 2.58 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 12:36:26.078048: step 198730, loss = 2.76 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 12:36:31.731856: step 198740, loss = 2.97 (200.8 examples/sec; 0.637 sec/batch)
2016-07-16 12:36:36.786451: step 198750, loss = 2.83 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 12:36:41.502534: step 198760, loss = 2.85 (285.3 examples/sec; 0.449 sec/batch)
2016-07-16 12:36:46.201111: step 198770, loss = 2.99 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 12:36:51.663912: step 198780, loss = 2.76 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 12:36:56.843429: step 198790, loss = 2.67 (222.0 examples/sec; 0.576 sec/batch)
2016-07-16 12:37:02.572812: step 198800, loss = 2.85 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 12:37:08.394001: step 198810, loss = 2.44 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 12:37:13.013212: step 198820, loss = 2.71 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 12:37:18.154342: step 198830, loss = 2.85 (203.1 examples/sec; 0.630 sec/batch)
2016-07-16 12:37:23.632402: step 198840, loss = 2.60 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 12:37:29.346254: step 198850, loss = 2.61 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 12:37:34.728059: step 198860, loss = 2.49 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 12:37:40.075536: step 198870, loss = 2.64 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 12:37:44.784409: step 198880, loss = 2.74 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 12:37:49.624367: step 198890, loss = 2.68 (267.5 examples/sec; 0.478 sec/batch)
2016-07-16 12:37:54.316443: step 198900, loss = 2.98 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 12:37:59.886145: step 198910, loss = 2.73 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 12:38:05.678619: step 198920, loss = 2.70 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 12:38:10.536424: step 198930, loss = 2.70 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 12:38:15.148048: step 198940, loss = 2.65 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 12:38:19.775516: step 198950, loss = 2.87 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 12:38:24.375659: step 198960, loss = 2.55 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 12:38:29.054253: step 198970, loss = 2.46 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 12:38:33.670263: step 198980, loss = 2.87 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 12:38:38.973259: step 198990, loss = 2.53 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 12:38:44.319841: step 199000, loss = 2.91 (260.4 examples/sec; 0.492 sec/batch)
2016-07-16 12:38:51.344482: step 199010, loss = 2.82 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 12:38:56.112837: step 199020, loss = 2.58 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 12:39:00.737658: step 199030, loss = 2.65 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 12:39:05.438773: step 199040, loss = 2.50 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 12:39:10.101877: step 199050, loss = 2.55 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 12:39:15.532231: step 199060, loss = 2.55 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 12:39:20.475225: step 199070, loss = 2.78 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 12:39:25.316101: step 199080, loss = 2.79 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 12:39:30.916722: step 199090, loss = 2.59 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 12:39:36.637174: step 199100, loss = 2.54 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 12:39:42.248122: step 199110, loss = 2.48 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 12:39:46.931021: step 199120, loss = 2.60 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 12:39:51.547365: step 199130, loss = 2.71 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 12:39:57.296926: step 199140, loss = 2.56 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 12:40:02.643234: step 199150, loss = 2.80 (204.1 examples/sec; 0.627 sec/batch)
2016-07-16 12:40:07.963469: step 199160, loss = 2.76 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 12:40:13.754523: step 199170, loss = 2.97 (282.9 examples/sec; 0.452 sec/batch)
2016-07-16 12:40:18.393076: step 199180, loss = 2.87 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 12:40:23.881524: step 199190, loss = 2.76 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 12:40:28.983664: step 199200, loss = 2.45 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 12:40:34.621736: step 199210, loss = 2.80 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 12:40:39.309790: step 199220, loss = 2.64 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 12:40:43.961174: step 199230, loss = 2.61 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 12:40:48.662487: step 199240, loss = 2.89 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 12:40:53.320360: step 199250, loss = 2.57 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 12:40:58.407897: step 199260, loss = 2.62 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 12:41:03.869999: step 199270, loss = 2.81 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 12:41:08.625674: step 199280, loss = 2.56 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 12:41:13.271884: step 199290, loss = 2.78 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 12:41:18.419771: step 199300, loss = 2.82 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 12:41:25.060523: step 199310, loss = 2.67 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 12:41:29.784766: step 199320, loss = 2.50 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 12:41:34.612300: step 199330, loss = 2.80 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 12:41:39.430862: step 199340, loss = 2.64 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 12:41:45.756132: step 199350, loss = 2.79 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 12:41:51.325022: step 199360, loss = 2.65 (252.1 examples/sec; 0.508 sec/batch)
2016-07-16 12:41:56.074796: step 199370, loss = 2.55 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 12:42:01.060839: step 199380, loss = 2.73 (227.7 examples/sec; 0.562 sec/batch)
2016-07-16 12:42:07.630529: step 199390, loss = 2.69 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 12:42:12.812776: step 199400, loss = 2.97 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 12:42:18.466471: step 199410, loss = 2.65 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 12:42:23.083036: step 199420, loss = 2.88 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 12:42:27.748831: step 199430, loss = 2.62 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 12:42:33.477342: step 199440, loss = 2.89 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 12:42:38.302210: step 199450, loss = 3.04 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 12:42:43.125909: step 199460, loss = 2.60 (257.3 examples/sec; 0.497 sec/batch)
2016-07-16 12:42:47.885659: step 199470, loss = 2.54 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 12:42:52.513783: step 199480, loss = 2.81 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 12:42:57.373642: step 199490, loss = 2.63 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 12:43:03.040886: step 199500, loss = 2.58 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 12:43:09.857346: step 199510, loss = 2.76 (253.8 examples/sec; 0.504 sec/batch)
2016-07-16 12:43:14.744826: step 199520, loss = 2.60 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 12:43:19.553184: step 199530, loss = 2.58 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 12:43:24.285865: step 199540, loss = 2.60 (266.8 examples/sec; 0.480 sec/batch)
2016-07-16 12:43:28.915099: step 199550, loss = 2.69 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 12:43:33.932559: step 199560, loss = 2.79 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 12:43:39.487773: step 199570, loss = 2.87 (265.8 examples/sec; 0.481 sec/batch)
2016-07-16 12:43:44.304556: step 199580, loss = 2.65 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 12:43:48.905022: step 199590, loss = 2.79 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 12:43:53.779654: step 199600, loss = 2.70 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 12:44:00.647024: step 199610, loss = 2.68 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 12:44:05.321215: step 199620, loss = 2.51 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 12:44:09.980813: step 199630, loss = 2.61 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 12:44:14.657979: step 199640, loss = 2.67 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 12:44:19.347707: step 199650, loss = 2.90 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 12:44:25.039143: step 199660, loss = 2.71 (212.5 examples/sec; 0.602 sec/batch)
2016-07-16 12:44:30.254073: step 199670, loss = 2.67 (196.9 examples/sec; 0.650 sec/batch)
2016-07-16 12:44:35.744649: step 199680, loss = 2.50 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 12:44:40.508182: step 199690, loss = 2.69 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 12:44:45.662105: step 199700, loss = 2.76 (189.1 examples/sec; 0.677 sec/batch)
2016-07-16 12:44:52.561443: step 199710, loss = 2.53 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 12:44:57.257523: step 199720, loss = 2.52 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 12:45:01.925041: step 199730, loss = 2.58 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 12:45:06.624030: step 199740, loss = 2.91 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 12:45:11.228855: step 199750, loss = 2.69 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 12:45:15.879713: step 199760, loss = 2.87 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 12:45:20.517222: step 199770, loss = 2.82 (282.2 examples/sec; 0.454 sec/batch)
2016-07-16 12:45:25.267411: step 199780, loss = 2.65 (229.3 examples/sec; 0.558 sec/batch)
2016-07-16 12:45:31.076660: step 199790, loss = 2.87 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 12:45:36.807947: step 199800, loss = 2.80 (206.3 examples/sec; 0.620 sec/batch)
2016-07-16 12:45:42.698171: step 199810, loss = 2.73 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 12:45:47.536015: step 199820, loss = 2.78 (252.8 examples/sec; 0.506 sec/batch)
2016-07-16 12:45:52.236569: step 199830, loss = 2.69 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 12:45:57.110770: step 199840, loss = 2.61 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 12:46:01.814401: step 199850, loss = 2.86 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 12:46:06.500772: step 199860, loss = 2.78 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 12:46:11.132988: step 199870, loss = 2.80 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 12:46:15.774028: step 199880, loss = 2.85 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 12:46:21.498113: step 199890, loss = 2.88 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 12:46:26.334365: step 199900, loss = 2.66 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 12:46:32.210943: step 199910, loss = 2.61 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 12:46:36.975405: step 199920, loss = 2.83 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 12:46:41.595221: step 199930, loss = 3.09 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 12:46:46.728561: step 199940, loss = 2.88 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 12:46:52.167705: step 199950, loss = 2.69 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 12:46:57.864034: step 199960, loss = 2.77 (283.4 examples/sec; 0.452 sec/batch)
2016-07-16 12:47:02.533864: step 199970, loss = 2.74 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 12:47:07.155715: step 199980, loss = 2.80 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 12:47:11.765909: step 199990, loss = 2.84 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 12:47:16.442396: step 200000, loss = 2.73 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 12:47:23.292745: step 200010, loss = 2.60 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 12:47:29.094675: step 200020, loss = 2.55 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 12:47:33.937567: step 200030, loss = 2.58 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 12:47:38.704658: step 200040, loss = 2.82 (257.8 examples/sec; 0.496 sec/batch)
2016-07-16 12:47:44.539728: step 200050, loss = 2.71 (188.0 examples/sec; 0.681 sec/batch)
2016-07-16 12:47:50.434313: step 200060, loss = 2.81 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 12:47:55.935925: step 200070, loss = 2.76 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 12:48:01.088857: step 200080, loss = 2.66 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 12:48:05.760706: step 200090, loss = 2.63 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 12:48:10.591822: step 200100, loss = 2.70 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 12:48:16.337021: step 200110, loss = 2.80 (255.1 examples/sec; 0.502 sec/batch)
2016-07-16 12:48:21.086034: step 200120, loss = 2.62 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 12:48:25.693021: step 200130, loss = 2.48 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 12:48:30.375363: step 200140, loss = 2.83 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 12:48:35.071202: step 200150, loss = 2.62 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 12:48:40.652378: step 200160, loss = 2.83 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 12:48:45.664145: step 200170, loss = 2.75 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 12:48:50.403350: step 200180, loss = 2.55 (262.1 examples/sec; 0.488 sec/batch)
2016-07-16 12:48:55.192482: step 200190, loss = 2.58 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 12:48:59.825257: step 200200, loss = 2.69 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 12:49:05.576099: step 200210, loss = 2.63 (214.2 examples/sec; 0.598 sec/batch)
2016-07-16 12:49:11.292338: step 200220, loss = 2.58 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 12:49:16.076791: step 200230, loss = 2.78 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 12:49:20.702081: step 200240, loss = 2.77 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 12:49:25.401703: step 200250, loss = 2.57 (248.6 examples/sec; 0.515 sec/batch)
2016-07-16 12:49:31.175872: step 200260, loss = 2.60 (258.1 examples/sec; 0.496 sec/batch)
2016-07-16 12:49:35.947277: step 200270, loss = 2.75 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 12:49:40.762286: step 200280, loss = 2.74 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 12:49:47.176544: step 200290, loss = 2.70 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 12:49:52.544065: step 200300, loss = 2.56 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 12:49:58.259199: step 200310, loss = 2.84 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 12:50:02.930925: step 200320, loss = 2.79 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 12:50:08.502323: step 200330, loss = 2.99 (205.6 examples/sec; 0.622 sec/batch)
2016-07-16 12:50:13.309569: step 200340, loss = 2.91 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 12:50:17.982754: step 200350, loss = 2.76 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 12:50:22.619408: step 200360, loss = 2.38 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 12:50:28.055559: step 200370, loss = 2.72 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 12:50:33.256792: step 200380, loss = 2.68 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 12:50:37.948490: step 200390, loss = 2.62 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 12:50:42.809985: step 200400, loss = 2.90 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 12:50:48.439789: step 200410, loss = 2.54 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 12:50:53.131753: step 200420, loss = 2.66 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 12:50:57.795116: step 200430, loss = 2.74 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 12:51:02.934775: step 200440, loss = 2.68 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 12:51:08.326346: step 200450, loss = 2.77 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 12:51:14.072414: step 200460, loss = 2.75 (255.7 examples/sec; 0.501 sec/batch)
2016-07-16 12:51:18.907898: step 200470, loss = 2.69 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 12:51:23.590234: step 200480, loss = 2.77 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 12:51:28.265502: step 200490, loss = 2.85 (284.4 examples/sec; 0.450 sec/batch)
2016-07-16 12:51:32.906775: step 200500, loss = 2.67 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 12:51:38.488304: step 200510, loss = 2.70 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 12:51:43.129635: step 200520, loss = 2.59 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 12:51:48.676083: step 200530, loss = 2.61 (206.8 examples/sec; 0.619 sec/batch)
2016-07-16 12:51:53.693039: step 200540, loss = 2.71 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 12:51:58.423795: step 200550, loss = 2.77 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 12:52:04.151974: step 200560, loss = 2.95 (190.5 examples/sec; 0.672 sec/batch)
2016-07-16 12:52:10.277807: step 200570, loss = 2.85 (247.3 examples/sec; 0.518 sec/batch)
2016-07-16 12:52:15.088450: step 200580, loss = 2.78 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 12:52:19.907561: step 200590, loss = 2.75 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 12:52:24.577397: step 200600, loss = 2.94 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 12:52:30.953002: step 200610, loss = 2.96 (189.6 examples/sec; 0.675 sec/batch)
2016-07-16 12:52:37.440550: step 200620, loss = 2.76 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 12:52:42.284156: step 200630, loss = 2.80 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 12:52:47.103264: step 200640, loss = 2.81 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 12:52:51.872075: step 200650, loss = 2.66 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 12:52:56.536872: step 200660, loss = 2.63 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 12:53:01.271862: step 200670, loss = 2.60 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 12:53:05.882727: step 200680, loss = 3.10 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 12:53:11.029199: step 200690, loss = 2.73 (198.6 examples/sec; 0.644 sec/batch)
2016-07-16 12:53:16.465584: step 200700, loss = 2.65 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 12:53:22.240638: step 200710, loss = 2.63 (269.6 examples/sec; 0.475 sec/batch)
2016-07-16 12:53:26.847163: step 200720, loss = 2.81 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 12:53:32.319851: step 200730, loss = 2.78 (203.9 examples/sec; 0.628 sec/batch)
2016-07-16 12:53:37.439292: step 200740, loss = 2.72 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 12:53:42.144724: step 200750, loss = 2.77 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 12:53:46.955771: step 200760, loss = 2.78 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 12:53:51.714382: step 200770, loss = 2.47 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 12:53:56.467821: step 200780, loss = 2.96 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 12:54:01.158404: step 200790, loss = 2.80 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 12:54:05.826045: step 200800, loss = 2.64 (283.1 examples/sec; 0.452 sec/batch)
2016-07-16 12:54:11.392364: step 200810, loss = 2.74 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 12:54:16.954394: step 200820, loss = 2.67 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 12:54:21.939484: step 200830, loss = 2.59 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 12:54:26.716118: step 200840, loss = 2.58 (254.2 examples/sec; 0.504 sec/batch)
2016-07-16 12:54:32.464064: step 200850, loss = 2.66 (189.6 examples/sec; 0.675 sec/batch)
2016-07-16 12:54:38.492485: step 200860, loss = 3.04 (287.7 examples/sec; 0.445 sec/batch)
2016-07-16 12:54:43.142820: step 200870, loss = 3.02 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 12:54:47.759516: step 200880, loss = 2.85 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 12:54:52.356050: step 200890, loss = 2.82 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 12:54:57.027959: step 200900, loss = 2.78 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 12:55:03.908609: step 200910, loss = 2.61 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 12:55:08.645793: step 200920, loss = 2.92 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 12:55:13.557481: step 200930, loss = 2.83 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 12:55:18.248111: step 200940, loss = 2.65 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 12:55:22.902545: step 200950, loss = 3.00 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 12:55:27.563292: step 200960, loss = 2.85 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 12:55:32.294791: step 200970, loss = 2.70 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 12:55:37.023794: step 200980, loss = 2.78 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 12:55:41.664016: step 200990, loss = 2.72 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 12:55:46.692782: step 201000, loss = 2.55 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 12:55:53.561376: step 201010, loss = 2.81 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 12:55:59.318029: step 201020, loss = 2.91 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 12:56:04.967102: step 201030, loss = 2.79 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 12:56:09.980002: step 201040, loss = 2.96 (283.7 examples/sec; 0.451 sec/batch)
2016-07-16 12:56:14.696490: step 201050, loss = 2.93 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 12:56:20.384812: step 201060, loss = 2.68 (187.3 examples/sec; 0.683 sec/batch)
2016-07-16 12:56:26.430479: step 201070, loss = 2.72 (281.4 examples/sec; 0.455 sec/batch)
2016-07-16 12:56:31.118247: step 201080, loss = 3.01 (262.8 examples/sec; 0.487 sec/batch)
2016-07-16 12:56:35.795437: step 201090, loss = 2.72 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 12:56:40.511207: step 201100, loss = 2.66 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 12:56:47.357789: step 201110, loss = 2.68 (258.0 examples/sec; 0.496 sec/batch)
2016-07-16 12:56:53.034469: step 201120, loss = 2.82 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 12:56:58.020542: step 201130, loss = 2.76 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 12:57:02.746134: step 201140, loss = 2.69 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 12:57:07.552768: step 201150, loss = 2.78 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 12:57:12.375231: step 201160, loss = 2.62 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 12:57:18.611723: step 201170, loss = 2.70 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 12:57:24.165225: step 201180, loss = 2.65 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 12:57:28.916748: step 201190, loss = 2.76 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 12:57:33.751631: step 201200, loss = 2.85 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 12:57:39.454425: step 201210, loss = 2.63 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 12:57:44.121601: step 201220, loss = 2.74 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 12:57:49.881913: step 201230, loss = 2.61 (224.4 examples/sec; 0.570 sec/batch)
2016-07-16 12:57:54.743783: step 201240, loss = 2.66 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 12:57:59.419878: step 201250, loss = 2.67 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 12:58:04.056418: step 201260, loss = 2.79 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 12:58:09.799748: step 201270, loss = 2.79 (223.8 examples/sec; 0.572 sec/batch)
2016-07-16 12:58:14.699450: step 201280, loss = 2.79 (246.0 examples/sec; 0.520 sec/batch)
2016-07-16 12:58:19.396406: step 201290, loss = 2.74 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 12:58:24.076431: step 201300, loss = 2.79 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 12:58:29.765288: step 201310, loss = 2.53 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 12:58:34.472065: step 201320, loss = 2.80 (288.6 examples/sec; 0.444 sec/batch)
2016-07-16 12:58:39.084347: step 201330, loss = 2.64 (282.3 examples/sec; 0.453 sec/batch)
2016-07-16 12:58:44.449091: step 201340, loss = 2.57 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 12:58:49.630226: step 201350, loss = 2.62 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 12:58:54.307652: step 201360, loss = 2.78 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 12:58:59.156058: step 201370, loss = 2.83 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 12:59:03.917582: step 201380, loss = 2.61 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 12:59:08.744289: step 201390, loss = 2.80 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 12:59:13.363162: step 201400, loss = 2.59 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 12:59:19.395548: step 201410, loss = 2.66 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 12:59:24.636944: step 201420, loss = 2.76 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 12:59:29.295008: step 201430, loss = 2.66 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 12:59:35.070917: step 201440, loss = 2.80 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 12:59:39.888702: step 201450, loss = 2.79 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 12:59:44.539208: step 201460, loss = 2.61 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 12:59:49.222267: step 201470, loss = 2.80 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 12:59:53.877253: step 201480, loss = 2.56 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 12:59:58.895195: step 201490, loss = 2.78 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 13:00:04.477476: step 201500, loss = 2.60 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 13:00:10.200262: step 201510, loss = 2.60 (283.3 examples/sec; 0.452 sec/batch)
2016-07-16 13:00:14.840291: step 201520, loss = 2.47 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 13:00:19.490513: step 201530, loss = 2.77 (285.0 examples/sec; 0.449 sec/batch)
2016-07-16 13:00:24.130145: step 201540, loss = 2.58 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 13:00:29.834622: step 201550, loss = 2.71 (206.6 examples/sec; 0.619 sec/batch)
2016-07-16 13:00:34.683787: step 201560, loss = 2.98 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 13:00:39.384089: step 201570, loss = 2.87 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 13:00:45.281162: step 201580, loss = 2.87 (186.4 examples/sec; 0.687 sec/batch)
2016-07-16 13:00:51.187184: step 201590, loss = 2.62 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 13:00:55.979398: step 201600, loss = 2.61 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 13:01:01.834913: step 201610, loss = 2.70 (256.5 examples/sec; 0.499 sec/batch)
2016-07-16 13:01:06.613275: step 201620, loss = 2.83 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 13:01:11.471927: step 201630, loss = 2.82 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 13:01:16.133385: step 201640, loss = 2.66 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 13:01:20.823036: step 201650, loss = 2.81 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 13:01:26.569068: step 201660, loss = 2.63 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 13:01:31.491038: step 201670, loss = 2.62 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 13:01:36.132872: step 201680, loss = 2.84 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 13:01:40.801711: step 201690, loss = 2.97 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 13:01:45.443515: step 201700, loss = 2.66 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 13:01:51.573480: step 201710, loss = 2.82 (203.4 examples/sec; 0.629 sec/batch)
2016-07-16 13:01:56.990879: step 201720, loss = 2.88 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 13:02:01.731007: step 201730, loss = 2.72 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 13:02:06.891632: step 201740, loss = 2.81 (190.1 examples/sec; 0.673 sec/batch)
2016-07-16 13:02:13.386983: step 201750, loss = 2.77 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 13:02:18.379144: step 201760, loss = 2.88 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 13:02:23.124697: step 201770, loss = 2.65 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 13:02:27.944351: step 201780, loss = 2.67 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 13:02:32.630242: step 201790, loss = 2.53 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 13:02:37.299522: step 201800, loss = 2.72 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 13:02:44.090664: step 201810, loss = 2.71 (254.7 examples/sec; 0.503 sec/batch)
2016-07-16 13:02:48.829068: step 201820, loss = 2.54 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 13:02:53.430094: step 201830, loss = 2.66 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 13:02:58.070004: step 201840, loss = 2.94 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 13:03:02.762185: step 201850, loss = 2.79 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 13:03:07.410951: step 201860, loss = 2.59 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:03:12.083738: step 201870, loss = 2.66 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 13:03:16.787795: step 201880, loss = 2.65 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 13:03:22.499110: step 201890, loss = 2.64 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 13:03:27.338331: step 201900, loss = 2.96 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 13:03:33.123164: step 201910, loss = 2.76 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 13:03:37.816124: step 201920, loss = 2.76 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 13:03:42.706968: step 201930, loss = 2.66 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 13:03:47.343161: step 201940, loss = 2.91 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 13:03:52.014233: step 201950, loss = 2.81 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 13:03:56.656908: step 201960, loss = 2.78 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 13:04:01.394466: step 201970, loss = 2.87 (251.5 examples/sec; 0.509 sec/batch)
2016-07-16 13:04:05.990373: step 201980, loss = 2.60 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 13:04:10.855039: step 201990, loss = 2.65 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 13:04:16.459416: step 202000, loss = 2.90 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 13:04:22.126950: step 202010, loss = 2.70 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 13:04:27.428437: step 202020, loss = 2.91 (187.0 examples/sec; 0.685 sec/batch)
2016-07-16 13:04:33.923856: step 202030, loss = 3.00 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 13:04:38.793917: step 202040, loss = 2.78 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 13:04:43.420145: step 202050, loss = 2.79 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 13:04:48.111366: step 202060, loss = 2.87 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 13:04:52.761693: step 202070, loss = 2.73 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 13:04:57.453937: step 202080, loss = 2.59 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 13:05:03.231901: step 202090, loss = 2.61 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 13:05:08.028559: step 202100, loss = 2.80 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 13:05:13.660878: step 202110, loss = 2.89 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 13:05:18.425790: step 202120, loss = 2.81 (223.9 examples/sec; 0.572 sec/batch)
2016-07-16 13:05:24.116382: step 202130, loss = 2.57 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 13:05:28.853441: step 202140, loss = 2.79 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 13:05:33.682685: step 202150, loss = 2.65 (260.1 examples/sec; 0.492 sec/batch)
2016-07-16 13:05:38.384943: step 202160, loss = 2.61 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 13:05:43.743915: step 202170, loss = 2.60 (188.7 examples/sec; 0.678 sec/batch)
2016-07-16 13:05:50.182875: step 202180, loss = 2.63 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 13:05:55.060910: step 202190, loss = 2.49 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 13:05:59.791780: step 202200, loss = 2.73 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 13:06:05.418994: step 202210, loss = 2.77 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 13:06:11.203960: step 202220, loss = 2.73 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 13:06:16.079826: step 202230, loss = 2.70 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 13:06:20.730037: step 202240, loss = 2.88 (283.8 examples/sec; 0.451 sec/batch)
2016-07-16 13:06:25.414682: step 202250, loss = 2.81 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 13:06:31.207587: step 202260, loss = 2.70 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 13:06:36.007782: step 202270, loss = 2.47 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 13:06:40.785402: step 202280, loss = 2.84 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 13:06:45.523759: step 202290, loss = 2.82 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 13:06:50.161240: step 202300, loss = 2.78 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 13:06:56.337813: step 202310, loss = 2.78 (204.0 examples/sec; 0.627 sec/batch)
2016-07-16 13:07:01.655664: step 202320, loss = 2.62 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 13:07:06.414601: step 202330, loss = 2.72 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 13:07:11.761704: step 202340, loss = 2.59 (188.3 examples/sec; 0.680 sec/batch)
2016-07-16 13:07:18.190659: step 202350, loss = 2.70 (204.6 examples/sec; 0.626 sec/batch)
2016-07-16 13:07:23.063030: step 202360, loss = 2.77 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 13:07:27.818778: step 202370, loss = 3.04 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 13:07:33.722546: step 202380, loss = 2.87 (187.6 examples/sec; 0.682 sec/batch)
2016-07-16 13:07:39.892736: step 202390, loss = 2.98 (244.4 examples/sec; 0.524 sec/batch)
2016-07-16 13:07:44.706399: step 202400, loss = 2.88 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 13:07:50.263853: step 202410, loss = 2.71 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 13:07:55.242458: step 202420, loss = 2.84 (200.4 examples/sec; 0.639 sec/batch)
2016-07-16 13:08:00.758315: step 202430, loss = 2.78 (261.2 examples/sec; 0.490 sec/batch)
2016-07-16 13:08:06.501553: step 202440, loss = 2.70 (245.0 examples/sec; 0.522 sec/batch)
2016-07-16 13:08:11.326654: step 202450, loss = 2.75 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 13:08:15.962246: step 202460, loss = 2.75 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 13:08:20.614519: step 202470, loss = 2.49 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 13:08:26.310370: step 202480, loss = 2.65 (209.1 examples/sec; 0.612 sec/batch)
2016-07-16 13:08:31.187654: step 202490, loss = 2.72 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 13:08:35.954001: step 202500, loss = 2.52 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 13:08:41.758330: step 202510, loss = 2.50 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:08:46.363775: step 202520, loss = 2.67 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 13:08:51.058312: step 202530, loss = 2.72 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 13:08:56.273285: step 202540, loss = 2.59 (204.5 examples/sec; 0.626 sec/batch)
2016-07-16 13:09:01.593884: step 202550, loss = 2.69 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 13:09:06.271722: step 202560, loss = 2.66 (281.3 examples/sec; 0.455 sec/batch)
2016-07-16 13:09:11.004387: step 202570, loss = 2.70 (252.3 examples/sec; 0.507 sec/batch)
2016-07-16 13:09:16.184114: step 202580, loss = 2.54 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 13:09:21.489021: step 202590, loss = 2.65 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 13:09:27.281901: step 202600, loss = 2.63 (257.6 examples/sec; 0.497 sec/batch)
2016-07-16 13:09:33.032004: step 202610, loss = 2.87 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 13:09:37.860226: step 202620, loss = 2.66 (269.2 examples/sec; 0.475 sec/batch)
2016-07-16 13:09:44.299827: step 202630, loss = 2.83 (206.9 examples/sec; 0.619 sec/batch)
2016-07-16 13:09:49.404471: step 202640, loss = 2.62 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 13:09:54.083425: step 202650, loss = 2.93 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 13:09:58.697335: step 202660, loss = 2.49 (286.8 examples/sec; 0.446 sec/batch)
2016-07-16 13:10:03.316604: step 202670, loss = 2.81 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 13:10:08.584242: step 202680, loss = 2.58 (202.3 examples/sec; 0.633 sec/batch)
2016-07-16 13:10:13.856874: step 202690, loss = 2.85 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 13:10:18.615673: step 202700, loss = 2.88 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 13:10:24.170044: step 202710, loss = 2.79 (283.4 examples/sec; 0.452 sec/batch)
2016-07-16 13:10:28.840127: step 202720, loss = 2.88 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 13:10:33.471038: step 202730, loss = 2.83 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 13:10:38.163133: step 202740, loss = 2.82 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 13:10:42.748582: step 202750, loss = 2.74 (284.9 examples/sec; 0.449 sec/batch)
2016-07-16 13:10:47.413347: step 202760, loss = 2.66 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 13:10:52.062169: step 202770, loss = 2.79 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 13:10:56.701832: step 202780, loss = 2.64 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 13:11:02.392253: step 202790, loss = 2.66 (204.4 examples/sec; 0.626 sec/batch)
2016-07-16 13:11:07.265348: step 202800, loss = 2.93 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 13:11:13.054046: step 202810, loss = 2.87 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 13:11:17.819859: step 202820, loss = 2.69 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 13:11:22.486776: step 202830, loss = 2.70 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 13:11:27.128226: step 202840, loss = 2.82 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 13:11:32.257753: step 202850, loss = 2.74 (209.1 examples/sec; 0.612 sec/batch)
2016-07-16 13:11:37.635486: step 202860, loss = 2.58 (263.1 examples/sec; 0.487 sec/batch)
2016-07-16 13:11:43.377858: step 202870, loss = 2.66 (267.2 examples/sec; 0.479 sec/batch)
2016-07-16 13:11:48.244935: step 202880, loss = 2.78 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 13:11:53.075147: step 202890, loss = 2.51 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 13:11:57.800295: step 202900, loss = 2.55 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 13:12:03.617345: step 202910, loss = 2.70 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 13:12:08.247629: step 202920, loss = 2.73 (285.4 examples/sec; 0.448 sec/batch)
2016-07-16 13:12:12.928593: step 202930, loss = 2.74 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 13:12:17.548478: step 202940, loss = 2.68 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 13:12:22.291476: step 202950, loss = 2.62 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 13:12:28.081889: step 202960, loss = 2.57 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 13:12:32.843179: step 202970, loss = 2.85 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 13:12:37.483624: step 202980, loss = 2.78 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 13:12:42.104822: step 202990, loss = 2.90 (268.9 examples/sec; 0.476 sec/batch)
2016-07-16 13:12:46.768273: step 203000, loss = 2.65 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 13:12:52.364663: step 203010, loss = 2.86 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 13:12:57.946367: step 203020, loss = 2.65 (203.0 examples/sec; 0.631 sec/batch)
2016-07-16 13:13:02.995874: step 203030, loss = 2.64 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 13:13:07.766003: step 203040, loss = 2.63 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 13:13:12.596633: step 203050, loss = 2.78 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 13:13:17.422937: step 203060, loss = 2.92 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 13:13:22.112977: step 203070, loss = 2.74 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 13:13:26.970084: step 203080, loss = 2.69 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 13:13:31.728451: step 203090, loss = 2.50 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 13:13:37.524970: step 203100, loss = 2.79 (184.2 examples/sec; 0.695 sec/batch)
2016-07-16 13:13:43.724057: step 203110, loss = 2.97 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 13:13:48.328402: step 203120, loss = 2.63 (285.2 examples/sec; 0.449 sec/batch)
2016-07-16 13:13:52.980888: step 203130, loss = 2.92 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 13:13:58.707135: step 203140, loss = 2.53 (219.2 examples/sec; 0.584 sec/batch)
2016-07-16 13:14:03.535283: step 203150, loss = 2.52 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 13:14:08.289114: step 203160, loss = 2.75 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 13:14:13.128681: step 203170, loss = 2.84 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 13:14:18.015336: step 203180, loss = 2.78 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 13:14:22.721371: step 203190, loss = 2.79 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 13:14:27.543479: step 203200, loss = 2.76 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 13:14:33.144029: step 203210, loss = 2.64 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 13:14:37.816165: step 203220, loss = 2.92 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 13:14:43.615343: step 203230, loss = 2.61 (259.4 examples/sec; 0.494 sec/batch)
2016-07-16 13:14:48.445821: step 203240, loss = 2.69 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 13:14:53.222723: step 203250, loss = 2.68 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 13:14:57.966882: step 203260, loss = 2.79 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 13:15:02.610864: step 203270, loss = 2.57 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 13:15:07.555917: step 203280, loss = 2.63 (210.1 examples/sec; 0.609 sec/batch)
2016-07-16 13:15:13.165760: step 203290, loss = 2.58 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 13:15:17.928732: step 203300, loss = 2.52 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:15:23.572770: step 203310, loss = 2.62 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 13:15:28.943693: step 203320, loss = 2.64 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 13:15:34.202548: step 203330, loss = 2.68 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 13:15:38.938449: step 203340, loss = 2.61 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 13:15:43.572652: step 203350, loss = 2.64 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 13:15:48.863634: step 203360, loss = 2.73 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 13:15:54.152843: step 203370, loss = 2.63 (252.1 examples/sec; 0.508 sec/batch)
2016-07-16 13:15:58.908884: step 203380, loss = 2.55 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 13:16:03.780591: step 203390, loss = 2.71 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 13:16:08.453488: step 203400, loss = 2.80 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 13:16:14.050754: step 203410, loss = 2.74 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 13:16:18.734717: step 203420, loss = 2.73 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 13:16:23.355449: step 203430, loss = 2.51 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 13:16:28.021031: step 203440, loss = 2.77 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 13:16:32.668495: step 203450, loss = 2.84 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 13:16:37.373034: step 203460, loss = 2.75 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 13:16:42.096529: step 203470, loss = 2.83 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 13:16:46.728583: step 203480, loss = 2.83 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 13:16:51.595165: step 203490, loss = 2.65 (202.5 examples/sec; 0.632 sec/batch)
2016-07-16 13:16:57.231137: step 203500, loss = 2.74 (251.8 examples/sec; 0.508 sec/batch)
2016-07-16 13:17:02.967562: step 203510, loss = 2.59 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 13:17:07.790610: step 203520, loss = 2.89 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 13:17:12.449849: step 203530, loss = 2.85 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 13:17:17.070930: step 203540, loss = 2.83 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 13:17:21.755929: step 203550, loss = 2.99 (277.3 examples/sec; 0.462 sec/batch)
2016-07-16 13:17:26.373278: step 203560, loss = 2.89 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 13:17:31.063140: step 203570, loss = 2.47 (269.8 examples/sec; 0.475 sec/batch)
2016-07-16 13:17:35.717387: step 203580, loss = 2.59 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 13:17:41.049816: step 203590, loss = 2.71 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 13:17:46.308242: step 203600, loss = 2.67 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 13:17:53.208901: step 203610, loss = 2.67 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 13:17:57.877909: step 203620, loss = 2.84 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 13:18:03.573585: step 203630, loss = 2.45 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 13:18:08.239092: step 203640, loss = 2.60 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 13:18:12.938816: step 203650, loss = 2.62 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 13:18:17.561297: step 203660, loss = 2.54 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 13:18:23.366815: step 203670, loss = 2.69 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 13:18:28.699903: step 203680, loss = 2.61 (201.3 examples/sec; 0.636 sec/batch)
2016-07-16 13:18:34.007269: step 203690, loss = 2.81 (258.8 examples/sec; 0.495 sec/batch)
2016-07-16 13:18:38.713590: step 203700, loss = 2.54 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 13:18:44.391788: step 203710, loss = 2.58 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 13:18:49.093088: step 203720, loss = 2.63 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 13:18:53.779224: step 203730, loss = 2.72 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 13:18:58.391977: step 203740, loss = 2.55 (281.0 examples/sec; 0.455 sec/batch)
2016-07-16 13:19:03.075258: step 203750, loss = 2.74 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 13:19:07.660717: step 203760, loss = 2.61 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 13:19:12.274359: step 203770, loss = 2.45 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 13:19:16.962534: step 203780, loss = 2.41 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 13:19:21.603177: step 203790, loss = 2.55 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 13:19:27.348271: step 203800, loss = 2.68 (258.5 examples/sec; 0.495 sec/batch)
2016-07-16 13:19:33.180048: step 203810, loss = 2.92 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 13:19:38.029436: step 203820, loss = 2.61 (263.8 examples/sec; 0.485 sec/batch)
2016-07-16 13:19:42.703357: step 203830, loss = 2.87 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 13:19:47.349596: step 203840, loss = 2.47 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 13:19:52.521197: step 203850, loss = 2.81 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 13:19:57.906910: step 203860, loss = 2.72 (266.7 examples/sec; 0.480 sec/batch)
2016-07-16 13:20:03.666015: step 203870, loss = 2.84 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 13:20:08.500910: step 203880, loss = 2.79 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 13:20:13.203175: step 203890, loss = 2.74 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 13:20:17.846339: step 203900, loss = 2.57 (270.9 examples/sec; 0.472 sec/batch)
2016-07-16 13:20:23.406317: step 203910, loss = 2.78 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 13:20:28.062346: step 203920, loss = 2.60 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 13:20:32.672668: step 203930, loss = 2.74 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 13:20:37.423365: step 203940, loss = 2.64 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 13:20:42.123479: step 203950, loss = 2.57 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 13:20:46.771318: step 203960, loss = 2.87 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 13:20:51.409400: step 203970, loss = 2.71 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 13:20:56.507957: step 203980, loss = 2.76 (207.6 examples/sec; 0.616 sec/batch)
2016-07-16 13:21:01.938457: step 203990, loss = 2.81 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 13:21:06.678945: step 204000, loss = 2.77 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 13:21:12.544648: step 204010, loss = 2.60 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 13:21:17.336748: step 204020, loss = 2.55 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 13:21:22.106334: step 204030, loss = 2.79 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 13:21:26.949149: step 204040, loss = 2.76 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 13:21:33.521684: step 204050, loss = 2.75 (205.0 examples/sec; 0.624 sec/batch)
2016-07-16 13:21:38.683582: step 204060, loss = 2.53 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 13:21:43.379562: step 204070, loss = 2.51 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 13:21:48.029581: step 204080, loss = 2.89 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 13:21:52.676191: step 204090, loss = 2.44 (280.2 examples/sec; 0.457 sec/batch)
2016-07-16 13:21:57.345034: step 204100, loss = 2.62 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 13:22:02.933646: step 204110, loss = 2.66 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 13:22:07.609478: step 204120, loss = 2.71 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 13:22:12.304601: step 204130, loss = 2.76 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 13:22:16.915527: step 204140, loss = 2.74 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 13:22:21.560125: step 204150, loss = 2.89 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 13:22:27.257564: step 204160, loss = 2.74 (238.6 examples/sec; 0.536 sec/batch)
2016-07-16 13:22:32.088905: step 204170, loss = 2.74 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 13:22:36.764944: step 204180, loss = 2.95 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 13:22:41.427288: step 204190, loss = 2.54 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 13:22:46.075237: step 204200, loss = 2.53 (284.0 examples/sec; 0.451 sec/batch)
2016-07-16 13:22:51.832173: step 204210, loss = 2.76 (202.9 examples/sec; 0.631 sec/batch)
2016-07-16 13:22:57.426827: step 204220, loss = 2.71 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 13:23:02.164612: step 204230, loss = 2.73 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 13:23:06.775789: step 204240, loss = 2.68 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 13:23:11.441836: step 204250, loss = 2.61 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 13:23:16.080935: step 204260, loss = 2.80 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 13:23:20.738345: step 204270, loss = 2.70 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 13:23:25.365955: step 204280, loss = 2.44 (279.9 examples/sec; 0.457 sec/batch)
2016-07-16 13:23:31.112509: step 204290, loss = 2.86 (206.3 examples/sec; 0.620 sec/batch)
2016-07-16 13:23:36.304462: step 204300, loss = 2.57 (204.0 examples/sec; 0.628 sec/batch)
2016-07-16 13:23:43.063256: step 204310, loss = 2.54 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 13:23:47.813562: step 204320, loss = 2.84 (256.6 examples/sec; 0.499 sec/batch)
2016-07-16 13:23:53.562508: step 204330, loss = 2.61 (189.3 examples/sec; 0.676 sec/batch)
2016-07-16 13:23:58.638531: step 204340, loss = 2.67 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 13:24:03.320622: step 204350, loss = 2.94 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 13:24:07.982101: step 204360, loss = 2.70 (271.5 examples/sec; 0.472 sec/batch)
2016-07-16 13:24:13.194002: step 204370, loss = 2.85 (200.0 examples/sec; 0.640 sec/batch)
2016-07-16 13:24:18.528826: step 204380, loss = 2.75 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 13:24:23.197091: step 204390, loss = 2.77 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 13:24:28.084131: step 204400, loss = 2.78 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 13:24:33.749721: step 204410, loss = 2.71 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 13:24:38.446716: step 204420, loss = 2.84 (254.9 examples/sec; 0.502 sec/batch)
2016-07-16 13:24:43.055672: step 204430, loss = 2.62 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 13:24:47.711668: step 204440, loss = 2.65 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 13:24:52.405987: step 204450, loss = 2.66 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 13:24:57.088741: step 204460, loss = 2.58 (265.2 examples/sec; 0.483 sec/batch)
2016-07-16 13:25:02.697868: step 204470, loss = 2.75 (208.2 examples/sec; 0.615 sec/batch)
2016-07-16 13:25:07.650831: step 204480, loss = 2.94 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 13:25:12.385381: step 204490, loss = 2.64 (254.7 examples/sec; 0.502 sec/batch)
2016-07-16 13:25:17.156156: step 204500, loss = 2.80 (277.8 examples/sec; 0.461 sec/batch)
2016-07-16 13:25:22.685840: step 204510, loss = 2.63 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 13:25:27.492866: step 204520, loss = 2.73 (219.9 examples/sec; 0.582 sec/batch)
2016-07-16 13:25:33.194522: step 204530, loss = 2.71 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 13:25:38.944580: step 204540, loss = 2.59 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 13:25:43.833165: step 204550, loss = 2.63 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 13:25:48.511391: step 204560, loss = 2.94 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:25:53.197239: step 204570, loss = 2.80 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 13:25:57.859509: step 204580, loss = 2.57 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 13:26:02.544372: step 204590, loss = 2.52 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 13:26:07.237241: step 204600, loss = 3.05 (278.2 examples/sec; 0.460 sec/batch)
2016-07-16 13:26:12.840246: step 204610, loss = 2.99 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 13:26:17.504513: step 204620, loss = 2.72 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 13:26:22.177465: step 204630, loss = 2.94 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 13:26:26.889119: step 204640, loss = 2.69 (282.6 examples/sec; 0.453 sec/batch)
2016-07-16 13:26:31.577963: step 204650, loss = 2.58 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 13:26:36.200402: step 204660, loss = 2.62 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 13:26:41.522509: step 204670, loss = 2.89 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 13:26:46.729160: step 204680, loss = 2.69 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 13:26:51.469399: step 204690, loss = 2.65 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 13:26:56.295635: step 204700, loss = 2.91 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 13:27:01.914146: step 204710, loss = 2.90 (280.1 examples/sec; 0.457 sec/batch)
2016-07-16 13:27:06.639836: step 204720, loss = 2.87 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 13:27:11.247060: step 204730, loss = 2.63 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 13:27:16.212241: step 204740, loss = 2.53 (207.6 examples/sec; 0.617 sec/batch)
2016-07-16 13:27:21.798597: step 204750, loss = 2.63 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 13:27:26.583161: step 204760, loss = 2.54 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 13:27:31.526955: step 204770, loss = 2.65 (226.7 examples/sec; 0.565 sec/batch)
2016-07-16 13:27:38.096083: step 204780, loss = 2.77 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 13:27:43.311715: step 204790, loss = 2.84 (256.0 examples/sec; 0.500 sec/batch)
2016-07-16 13:27:48.000194: step 204800, loss = 2.67 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 13:27:53.797610: step 204810, loss = 2.61 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 13:27:58.413785: step 204820, loss = 2.46 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 13:28:03.066554: step 204830, loss = 2.67 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 13:28:08.855171: step 204840, loss = 2.59 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 13:28:13.661303: step 204850, loss = 2.79 (280.9 examples/sec; 0.456 sec/batch)
2016-07-16 13:28:18.335303: step 204860, loss = 2.46 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 13:28:22.972847: step 204870, loss = 2.78 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 13:28:27.768201: step 204880, loss = 2.66 (213.5 examples/sec; 0.600 sec/batch)
2016-07-16 13:28:33.433334: step 204890, loss = 2.60 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 13:28:39.152253: step 204900, loss = 2.67 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 13:28:45.041941: step 204910, loss = 2.62 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 13:28:49.807983: step 204920, loss = 2.51 (268.1 examples/sec; 0.477 sec/batch)
2016-07-16 13:28:54.597303: step 204930, loss = 2.57 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 13:28:59.496840: step 204940, loss = 2.64 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 13:29:04.152138: step 204950, loss = 2.66 (284.5 examples/sec; 0.450 sec/batch)
2016-07-16 13:29:08.836592: step 204960, loss = 2.66 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 13:29:14.291294: step 204970, loss = 2.68 (203.6 examples/sec; 0.629 sec/batch)
2016-07-16 13:29:19.391690: step 204980, loss = 2.61 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 13:29:25.144749: step 204990, loss = 2.57 (258.2 examples/sec; 0.496 sec/batch)
2016-07-16 13:29:30.791828: step 205000, loss = 2.78 (208.1 examples/sec; 0.615 sec/batch)
2016-07-16 13:29:36.824156: step 205010, loss = 2.82 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 13:29:41.484080: step 205020, loss = 2.78 (285.3 examples/sec; 0.449 sec/batch)
2016-07-16 13:29:46.104234: step 205030, loss = 2.78 (287.0 examples/sec; 0.446 sec/batch)
2016-07-16 13:29:50.722755: step 205040, loss = 2.79 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 13:29:56.484818: step 205050, loss = 2.79 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 13:30:01.288219: step 205060, loss = 2.60 (272.4 examples/sec; 0.470 sec/batch)
2016-07-16 13:30:06.066426: step 205070, loss = 2.76 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 13:30:10.775607: step 205080, loss = 2.57 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 13:30:15.583847: step 205090, loss = 2.76 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 13:30:20.404109: step 205100, loss = 2.80 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 13:30:26.233408: step 205110, loss = 2.59 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 13:30:31.097121: step 205120, loss = 2.80 (243.1 examples/sec; 0.527 sec/batch)
2016-07-16 13:30:35.851314: step 205130, loss = 2.65 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 13:30:40.467015: step 205140, loss = 2.79 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 13:30:45.756696: step 205150, loss = 2.74 (206.3 examples/sec; 0.620 sec/batch)
2016-07-16 13:30:50.980270: step 205160, loss = 2.79 (263.7 examples/sec; 0.485 sec/batch)
2016-07-16 13:30:56.677130: step 205170, loss = 2.67 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 13:31:01.337111: step 205180, loss = 2.71 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 13:31:06.886826: step 205190, loss = 2.61 (207.5 examples/sec; 0.617 sec/batch)
2016-07-16 13:31:11.863792: step 205200, loss = 2.52 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 13:31:17.602607: step 205210, loss = 2.74 (268.5 examples/sec; 0.477 sec/batch)
2016-07-16 13:31:22.416364: step 205220, loss = 2.65 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 13:31:27.044047: step 205230, loss = 2.66 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 13:31:31.898560: step 205240, loss = 2.82 (208.9 examples/sec; 0.613 sec/batch)
2016-07-16 13:31:37.573258: step 205250, loss = 2.53 (256.1 examples/sec; 0.500 sec/batch)
2016-07-16 13:31:42.406464: step 205260, loss = 2.64 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 13:31:47.082988: step 205270, loss = 2.63 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 13:31:51.775265: step 205280, loss = 2.67 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 13:31:56.448606: step 205290, loss = 2.67 (282.5 examples/sec; 0.453 sec/batch)
2016-07-16 13:32:01.651241: step 205300, loss = 2.83 (202.7 examples/sec; 0.631 sec/batch)
2016-07-16 13:32:07.914755: step 205310, loss = 2.81 (285.6 examples/sec; 0.448 sec/batch)
2016-07-16 13:32:12.585507: step 205320, loss = 2.75 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 13:32:17.234612: step 205330, loss = 2.80 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 13:32:21.860112: step 205340, loss = 2.93 (273.2 examples/sec; 0.468 sec/batch)
2016-07-16 13:32:27.567875: step 205350, loss = 2.53 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 13:32:32.475741: step 205360, loss = 2.56 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 13:32:37.229469: step 205370, loss = 2.81 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 13:32:42.041570: step 205380, loss = 3.01 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 13:32:46.637972: step 205390, loss = 2.70 (286.8 examples/sec; 0.446 sec/batch)
2016-07-16 13:32:51.310322: step 205400, loss = 2.73 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 13:32:58.218008: step 205410, loss = 2.62 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 13:33:03.050440: step 205420, loss = 2.88 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 13:33:07.681211: step 205430, loss = 2.61 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 13:33:12.328563: step 205440, loss = 2.52 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 13:33:17.522830: step 205450, loss = 2.90 (202.4 examples/sec; 0.632 sec/batch)
2016-07-16 13:33:22.839970: step 205460, loss = 2.59 (262.0 examples/sec; 0.489 sec/batch)
2016-07-16 13:33:27.660385: step 205470, loss = 2.55 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 13:33:32.523213: step 205480, loss = 2.76 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 13:33:37.311723: step 205490, loss = 2.83 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 13:33:42.232327: step 205500, loss = 2.72 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 13:33:48.100035: step 205510, loss = 2.79 (260.9 examples/sec; 0.491 sec/batch)
2016-07-16 13:33:52.843254: step 205520, loss = 2.59 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 13:33:57.704673: step 205530, loss = 2.71 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 13:34:02.324615: step 205540, loss = 2.63 (276.6 examples/sec; 0.463 sec/batch)
2016-07-16 13:34:07.032431: step 205550, loss = 2.78 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 13:34:11.650644: step 205560, loss = 2.80 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 13:34:16.598997: step 205570, loss = 2.63 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 13:34:22.171314: step 205580, loss = 2.89 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 13:34:26.896169: step 205590, loss = 2.78 (275.7 examples/sec; 0.464 sec/batch)
2016-07-16 13:34:31.793032: step 205600, loss = 2.85 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 13:34:37.442628: step 205610, loss = 2.54 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 13:34:42.098489: step 205620, loss = 2.50 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 13:34:46.765201: step 205630, loss = 2.70 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 13:34:51.402590: step 205640, loss = 2.79 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 13:34:57.190533: step 205650, loss = 2.74 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 13:35:02.007259: step 205660, loss = 2.92 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 13:35:06.800820: step 205670, loss = 2.73 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 13:35:11.525550: step 205680, loss = 2.72 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 13:35:16.155997: step 205690, loss = 2.83 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 13:35:21.190046: step 205700, loss = 2.74 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 13:35:27.823639: step 205710, loss = 2.63 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 13:35:32.488399: step 205720, loss = 2.92 (284.9 examples/sec; 0.449 sec/batch)
2016-07-16 13:35:37.135267: step 205730, loss = 2.73 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 13:35:42.592019: step 205740, loss = 2.61 (194.8 examples/sec; 0.657 sec/batch)
2016-07-16 13:35:47.762981: step 205750, loss = 2.53 (251.6 examples/sec; 0.509 sec/batch)
2016-07-16 13:35:53.511774: step 205760, loss = 3.07 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 13:35:58.336822: step 205770, loss = 2.70 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 13:36:03.039871: step 205780, loss = 2.72 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 13:36:07.700327: step 205790, loss = 2.85 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 13:36:12.366247: step 205800, loss = 2.69 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 13:36:18.065438: step 205810, loss = 2.69 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 13:36:22.667673: step 205820, loss = 2.67 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 13:36:28.409995: step 205830, loss = 2.92 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 13:36:33.249131: step 205840, loss = 2.69 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 13:36:37.924948: step 205850, loss = 2.66 (268.6 examples/sec; 0.476 sec/batch)
2016-07-16 13:36:42.575263: step 205860, loss = 2.92 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 13:36:48.309772: step 205870, loss = 2.80 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 13:36:53.153308: step 205880, loss = 2.78 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 13:36:57.950224: step 205890, loss = 2.60 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 13:37:02.700304: step 205900, loss = 2.62 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 13:37:08.604535: step 205910, loss = 2.56 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 13:37:13.357531: step 205920, loss = 2.65 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 13:37:19.235576: step 205930, loss = 2.69 (187.9 examples/sec; 0.681 sec/batch)
2016-07-16 13:37:25.079131: step 205940, loss = 2.69 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 13:37:29.742274: step 205950, loss = 2.74 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 13:37:35.345734: step 205960, loss = 2.63 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 13:37:40.495557: step 205970, loss = 2.57 (205.3 examples/sec; 0.623 sec/batch)
2016-07-16 13:37:46.109437: step 205980, loss = 2.89 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 13:37:50.874414: step 205990, loss = 2.77 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 13:37:55.520223: step 206000, loss = 2.62 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 13:38:01.152613: step 206010, loss = 2.89 (281.9 examples/sec; 0.454 sec/batch)
2016-07-16 13:38:05.774772: step 206020, loss = 2.59 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:38:11.407346: step 206030, loss = 2.70 (203.0 examples/sec; 0.630 sec/batch)
2016-07-16 13:38:16.564383: step 206040, loss = 2.69 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 13:38:22.173817: step 206050, loss = 2.69 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 13:38:26.940755: step 206060, loss = 2.63 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 13:38:31.791452: step 206070, loss = 2.71 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 13:38:36.481799: step 206080, loss = 2.72 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 13:38:41.953404: step 206090, loss = 2.67 (192.1 examples/sec; 0.666 sec/batch)
2016-07-16 13:38:48.240193: step 206100, loss = 2.66 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 13:38:53.841748: step 206110, loss = 2.66 (268.3 examples/sec; 0.477 sec/batch)
2016-07-16 13:38:58.497453: step 206120, loss = 2.66 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 13:39:03.163104: step 206130, loss = 2.89 (253.4 examples/sec; 0.505 sec/batch)
2016-07-16 13:39:08.951287: step 206140, loss = 2.57 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 13:39:13.765539: step 206150, loss = 2.64 (278.9 examples/sec; 0.459 sec/batch)
2016-07-16 13:39:18.614597: step 206160, loss = 2.71 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 13:39:24.865305: step 206170, loss = 2.73 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 13:39:30.420614: step 206180, loss = 2.72 (264.2 examples/sec; 0.484 sec/batch)
2016-07-16 13:39:35.191290: step 206190, loss = 2.89 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 13:39:40.079603: step 206200, loss = 2.71 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 13:39:45.726107: step 206210, loss = 2.83 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 13:39:50.407430: step 206220, loss = 2.66 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 13:39:56.135967: step 206230, loss = 2.78 (222.8 examples/sec; 0.574 sec/batch)
2016-07-16 13:40:00.988508: step 206240, loss = 2.67 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 13:40:05.645754: step 206250, loss = 2.71 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 13:40:10.327700: step 206260, loss = 2.73 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 13:40:16.067198: step 206270, loss = 2.81 (221.7 examples/sec; 0.577 sec/batch)
2016-07-16 13:40:21.236316: step 206280, loss = 3.05 (205.4 examples/sec; 0.623 sec/batch)
2016-07-16 13:40:26.707111: step 206290, loss = 2.63 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 13:40:31.519641: step 206300, loss = 2.71 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 13:40:37.389605: step 206310, loss = 2.85 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 13:40:42.177821: step 206320, loss = 2.71 (257.1 examples/sec; 0.498 sec/batch)
2016-07-16 13:40:47.026033: step 206330, loss = 2.73 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 13:40:51.621742: step 206340, loss = 2.63 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 13:40:56.415459: step 206350, loss = 2.61 (211.7 examples/sec; 0.605 sec/batch)
2016-07-16 13:41:02.147387: step 206360, loss = 2.54 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 13:41:06.885041: step 206370, loss = 2.71 (282.7 examples/sec; 0.453 sec/batch)
2016-07-16 13:41:11.530164: step 206380, loss = 2.81 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 13:41:16.295015: step 206390, loss = 2.63 (224.5 examples/sec; 0.570 sec/batch)
2016-07-16 13:41:22.004654: step 206400, loss = 2.82 (262.6 examples/sec; 0.487 sec/batch)
2016-07-16 13:41:27.792390: step 206410, loss = 2.60 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 13:41:32.727459: step 206420, loss = 2.69 (251.4 examples/sec; 0.509 sec/batch)
2016-07-16 13:41:37.453274: step 206430, loss = 2.66 (265.4 examples/sec; 0.482 sec/batch)
2016-07-16 13:41:42.235350: step 206440, loss = 2.72 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 13:41:47.035410: step 206450, loss = 2.70 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 13:41:51.819735: step 206460, loss = 2.77 (271.3 examples/sec; 0.472 sec/batch)
2016-07-16 13:41:56.428582: step 206470, loss = 2.94 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:42:01.111493: step 206480, loss = 2.62 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 13:42:05.791881: step 206490, loss = 2.87 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 13:42:10.447870: step 206500, loss = 2.68 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 13:42:16.078858: step 206510, loss = 2.59 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 13:42:21.155555: step 206520, loss = 2.62 (199.8 examples/sec; 0.641 sec/batch)
2016-07-16 13:42:26.689937: step 206530, loss = 2.81 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 13:42:31.434392: step 206540, loss = 2.97 (276.4 examples/sec; 0.463 sec/batch)
2016-07-16 13:42:36.041209: step 206550, loss = 2.83 (279.3 examples/sec; 0.458 sec/batch)
2016-07-16 13:42:41.058299: step 206560, loss = 2.71 (203.8 examples/sec; 0.628 sec/batch)
2016-07-16 13:42:46.604978: step 206570, loss = 2.64 (267.0 examples/sec; 0.479 sec/batch)
2016-07-16 13:42:51.363794: step 206580, loss = 2.77 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 13:42:56.210127: step 206590, loss = 2.47 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 13:43:00.830048: step 206600, loss = 2.71 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 13:43:06.326361: step 206610, loss = 3.01 (279.6 examples/sec; 0.458 sec/batch)
2016-07-16 13:43:11.010992: step 206620, loss = 2.66 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 13:43:15.612923: step 206630, loss = 2.53 (282.0 examples/sec; 0.454 sec/batch)
2016-07-16 13:43:20.352646: step 206640, loss = 2.69 (239.8 examples/sec; 0.534 sec/batch)
2016-07-16 13:43:26.110110: step 206650, loss = 2.94 (253.7 examples/sec; 0.504 sec/batch)
2016-07-16 13:43:30.871977: step 206660, loss = 2.70 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 13:43:35.684984: step 206670, loss = 2.82 (257.7 examples/sec; 0.497 sec/batch)
2016-07-16 13:43:40.409346: step 206680, loss = 2.66 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 13:43:45.813776: step 206690, loss = 2.76 (186.4 examples/sec; 0.687 sec/batch)
2016-07-16 13:43:52.218379: step 206700, loss = 2.78 (209.1 examples/sec; 0.612 sec/batch)
2016-07-16 13:43:58.059024: step 206710, loss = 2.80 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 13:44:02.765955: step 206720, loss = 2.72 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 13:44:07.417143: step 206730, loss = 2.52 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 13:44:13.202072: step 206740, loss = 2.79 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 13:44:18.006029: step 206750, loss = 2.68 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 13:44:22.783721: step 206760, loss = 2.78 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 13:44:27.513326: step 206770, loss = 2.88 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 13:44:32.188342: step 206780, loss = 2.58 (265.7 examples/sec; 0.482 sec/batch)
2016-07-16 13:44:36.926442: step 206790, loss = 2.87 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 13:44:41.626422: step 206800, loss = 2.70 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 13:44:48.329355: step 206810, loss = 2.66 (222.5 examples/sec; 0.575 sec/batch)
2016-07-16 13:44:53.209958: step 206820, loss = 2.86 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 13:44:57.841947: step 206830, loss = 2.80 (286.2 examples/sec; 0.447 sec/batch)
2016-07-16 13:45:02.445356: step 206840, loss = 2.64 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 13:45:07.095968: step 206850, loss = 2.71 (284.3 examples/sec; 0.450 sec/batch)
2016-07-16 13:45:11.679314: step 206860, loss = 2.80 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 13:45:16.229087: step 206870, loss = 2.63 (287.7 examples/sec; 0.445 sec/batch)
2016-07-16 13:45:21.140814: step 206880, loss = 2.72 (209.2 examples/sec; 0.612 sec/batch)
2016-07-16 13:45:26.716703: step 206890, loss = 2.60 (265.3 examples/sec; 0.483 sec/batch)
2016-07-16 13:45:31.462107: step 206900, loss = 2.66 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 13:45:37.270706: step 206910, loss = 2.52 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 13:45:41.963472: step 206920, loss = 3.09 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 13:45:46.634739: step 206930, loss = 2.65 (268.2 examples/sec; 0.477 sec/batch)
2016-07-16 13:45:52.359368: step 206940, loss = 2.70 (224.1 examples/sec; 0.571 sec/batch)
2016-07-16 13:45:57.543031: step 206950, loss = 2.83 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 13:46:02.984469: step 206960, loss = 2.65 (266.0 examples/sec; 0.481 sec/batch)
2016-07-16 13:46:07.718832: step 206970, loss = 2.74 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 13:46:12.349185: step 206980, loss = 2.68 (271.1 examples/sec; 0.472 sec/batch)
2016-07-16 13:46:17.102897: step 206990, loss = 2.63 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 13:46:21.741663: step 207000, loss = 2.67 (268.8 examples/sec; 0.476 sec/batch)
2016-07-16 13:46:27.292371: step 207010, loss = 2.75 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 13:46:31.913548: step 207020, loss = 2.95 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 13:46:36.601524: step 207030, loss = 2.82 (278.3 examples/sec; 0.460 sec/batch)
2016-07-16 13:46:41.236787: step 207040, loss = 2.79 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 13:46:46.001886: step 207050, loss = 2.63 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 13:46:50.649658: step 207060, loss = 2.59 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 13:46:56.349179: step 207070, loss = 2.77 (206.5 examples/sec; 0.620 sec/batch)
2016-07-16 13:47:01.193447: step 207080, loss = 2.62 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 13:47:05.999830: step 207090, loss = 2.61 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 13:47:10.771345: step 207100, loss = 2.77 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:47:16.343191: step 207110, loss = 2.66 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 13:47:21.305036: step 207120, loss = 2.70 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 13:47:26.857521: step 207130, loss = 2.58 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 13:47:32.630223: step 207140, loss = 2.84 (255.1 examples/sec; 0.502 sec/batch)
2016-07-16 13:47:37.501718: step 207150, loss = 2.71 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 13:47:42.291717: step 207160, loss = 2.63 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 13:47:47.094276: step 207170, loss = 2.71 (273.8 examples/sec; 0.468 sec/batch)
2016-07-16 13:47:51.952499: step 207180, loss = 2.61 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 13:47:56.635513: step 207190, loss = 2.77 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 13:48:01.238807: step 207200, loss = 2.81 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:48:06.793960: step 207210, loss = 2.77 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 13:48:12.478993: step 207220, loss = 2.70 (224.2 examples/sec; 0.571 sec/batch)
2016-07-16 13:48:17.343736: step 207230, loss = 2.65 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 13:48:22.106807: step 207240, loss = 2.66 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 13:48:26.859067: step 207250, loss = 2.76 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 13:48:31.497649: step 207260, loss = 2.68 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 13:48:36.228059: step 207270, loss = 2.72 (247.6 examples/sec; 0.517 sec/batch)
2016-07-16 13:48:40.863365: step 207280, loss = 2.77 (274.0 examples/sec; 0.467 sec/batch)
2016-07-16 13:48:45.567029: step 207290, loss = 2.62 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 13:48:50.185270: step 207300, loss = 2.75 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 13:48:55.827331: step 207310, loss = 2.66 (273.1 examples/sec; 0.469 sec/batch)
2016-07-16 13:49:00.453416: step 207320, loss = 2.76 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 13:49:05.403676: step 207330, loss = 2.76 (209.8 examples/sec; 0.610 sec/batch)
2016-07-16 13:49:10.926047: step 207340, loss = 2.83 (265.3 examples/sec; 0.482 sec/batch)
2016-07-16 13:49:15.677098: step 207350, loss = 2.91 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 13:49:20.535803: step 207360, loss = 2.71 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 13:49:25.253072: step 207370, loss = 2.86 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 13:49:29.858145: step 207380, loss = 2.54 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 13:49:35.334787: step 207390, loss = 2.60 (207.6 examples/sec; 0.617 sec/batch)
2016-07-16 13:49:40.446140: step 207400, loss = 2.69 (266.1 examples/sec; 0.481 sec/batch)
2016-07-16 13:49:46.081876: step 207410, loss = 2.61 (283.2 examples/sec; 0.452 sec/batch)
2016-07-16 13:49:50.678720: step 207420, loss = 2.79 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 13:49:55.362479: step 207430, loss = 2.76 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 13:50:01.127989: step 207440, loss = 3.05 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 13:50:06.020907: step 207450, loss = 2.74 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 13:50:10.756766: step 207460, loss = 2.89 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 13:50:15.466269: step 207470, loss = 2.96 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 13:50:20.100701: step 207480, loss = 2.86 (280.3 examples/sec; 0.457 sec/batch)
2016-07-16 13:50:24.907349: step 207490, loss = 3.01 (212.7 examples/sec; 0.602 sec/batch)
2016-07-16 13:50:30.584314: step 207500, loss = 2.55 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 13:50:36.422116: step 207510, loss = 2.62 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 13:50:41.041687: step 207520, loss = 2.60 (271.0 examples/sec; 0.472 sec/batch)
2016-07-16 13:50:46.168127: step 207530, loss = 2.57 (207.4 examples/sec; 0.617 sec/batch)
2016-07-16 13:50:51.585007: step 207540, loss = 2.65 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 13:50:57.336568: step 207550, loss = 2.55 (259.5 examples/sec; 0.493 sec/batch)
2016-07-16 13:51:02.221226: step 207560, loss = 2.78 (268.6 examples/sec; 0.477 sec/batch)
2016-07-16 13:51:06.996736: step 207570, loss = 2.70 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 13:51:11.746235: step 207580, loss = 2.54 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 13:51:16.673389: step 207590, loss = 2.65 (235.2 examples/sec; 0.544 sec/batch)
2016-07-16 13:51:23.268765: step 207600, loss = 2.88 (199.1 examples/sec; 0.643 sec/batch)
2016-07-16 13:51:29.546047: step 207610, loss = 2.73 (278.1 examples/sec; 0.460 sec/batch)
2016-07-16 13:51:34.222010: step 207620, loss = 2.75 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 13:51:38.871575: step 207630, loss = 2.95 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 13:51:44.608237: step 207640, loss = 2.49 (205.9 examples/sec; 0.622 sec/batch)
2016-07-16 13:51:49.449466: step 207650, loss = 3.00 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 13:51:54.257853: step 207660, loss = 2.59 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 13:52:00.184391: step 207670, loss = 2.69 (186.9 examples/sec; 0.685 sec/batch)
2016-07-16 13:52:06.084567: step 207680, loss = 2.61 (259.8 examples/sec; 0.493 sec/batch)
2016-07-16 13:52:10.902494: step 207690, loss = 2.85 (276.8 examples/sec; 0.462 sec/batch)
2016-07-16 13:52:15.736767: step 207700, loss = 2.62 (255.9 examples/sec; 0.500 sec/batch)
2016-07-16 13:52:21.467635: step 207710, loss = 2.77 (277.7 examples/sec; 0.461 sec/batch)
2016-07-16 13:52:26.101321: step 207720, loss = 2.74 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 13:52:31.542843: step 207730, loss = 2.53 (204.8 examples/sec; 0.625 sec/batch)
2016-07-16 13:52:36.779446: step 207740, loss = 2.68 (239.9 examples/sec; 0.533 sec/batch)
2016-07-16 13:52:42.386528: step 207750, loss = 2.50 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 13:52:47.061550: step 207760, loss = 2.79 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 13:52:51.657620: step 207770, loss = 2.62 (283.8 examples/sec; 0.451 sec/batch)
2016-07-16 13:52:56.294774: step 207780, loss = 2.67 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 13:53:02.082437: step 207790, loss = 2.59 (262.9 examples/sec; 0.487 sec/batch)
2016-07-16 13:53:06.900637: step 207800, loss = 2.61 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 13:53:12.787819: step 207810, loss = 2.71 (270.7 examples/sec; 0.473 sec/batch)
2016-07-16 13:53:17.452938: step 207820, loss = 2.60 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 13:53:22.082204: step 207830, loss = 2.74 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 13:53:27.474456: step 207840, loss = 2.60 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 13:53:32.661745: step 207850, loss = 2.82 (260.6 examples/sec; 0.491 sec/batch)
2016-07-16 13:53:37.381769: step 207860, loss = 2.77 (265.8 examples/sec; 0.482 sec/batch)
2016-07-16 13:53:42.229653: step 207870, loss = 2.57 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 13:53:46.906936: step 207880, loss = 2.79 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 13:53:51.601047: step 207890, loss = 2.78 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 13:53:57.374516: step 207900, loss = 2.83 (260.2 examples/sec; 0.492 sec/batch)
2016-07-16 13:54:03.177123: step 207910, loss = 2.60 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 13:54:07.826833: step 207920, loss = 2.74 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 13:54:12.517255: step 207930, loss = 2.71 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 13:54:17.152166: step 207940, loss = 2.65 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 13:54:22.343250: step 207950, loss = 2.94 (187.5 examples/sec; 0.683 sec/batch)
2016-07-16 13:54:27.858248: step 207960, loss = 2.75 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 13:54:32.659517: step 207970, loss = 2.80 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 13:54:37.295837: step 207980, loss = 2.70 (279.2 examples/sec; 0.458 sec/batch)
2016-07-16 13:54:41.968926: step 207990, loss = 2.62 (283.0 examples/sec; 0.452 sec/batch)
2016-07-16 13:54:46.663076: step 208000, loss = 2.85 (267.5 examples/sec; 0.479 sec/batch)
2016-07-16 13:54:53.397103: step 208010, loss = 2.72 (254.1 examples/sec; 0.504 sec/batch)
2016-07-16 13:54:58.736330: step 208020, loss = 2.86 (206.1 examples/sec; 0.621 sec/batch)
2016-07-16 13:55:04.052463: step 208030, loss = 2.59 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 13:55:08.760824: step 208040, loss = 2.80 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 13:55:13.635475: step 208050, loss = 2.73 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 13:55:18.408205: step 208060, loss = 2.72 (259.1 examples/sec; 0.494 sec/batch)
2016-07-16 13:55:23.219819: step 208070, loss = 2.77 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 13:55:28.046732: step 208080, loss = 2.84 (262.3 examples/sec; 0.488 sec/batch)
2016-07-16 13:55:32.792111: step 208090, loss = 2.68 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 13:55:37.668107: step 208100, loss = 2.59 (280.5 examples/sec; 0.456 sec/batch)
2016-07-16 13:55:43.440746: step 208110, loss = 2.74 (262.2 examples/sec; 0.488 sec/batch)
2016-07-16 13:55:48.228980: step 208120, loss = 2.51 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 13:55:52.854580: step 208130, loss = 2.77 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 13:55:57.518482: step 208140, loss = 2.75 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 13:56:02.116439: step 208150, loss = 2.74 (281.5 examples/sec; 0.455 sec/batch)
2016-07-16 13:56:07.493521: step 208160, loss = 2.65 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 13:56:12.637352: step 208170, loss = 2.71 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 13:56:17.288535: step 208180, loss = 2.45 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 13:56:22.071589: step 208190, loss = 2.51 (272.6 examples/sec; 0.470 sec/batch)
2016-07-16 13:56:26.857929: step 208200, loss = 2.87 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 13:56:33.867683: step 208210, loss = 2.63 (283.5 examples/sec; 0.451 sec/batch)
2016-07-16 13:56:38.477585: step 208220, loss = 2.63 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 13:56:43.080136: step 208230, loss = 2.74 (284.9 examples/sec; 0.449 sec/batch)
2016-07-16 13:56:47.711416: step 208240, loss = 2.56 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 13:56:53.420913: step 208250, loss = 2.75 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 13:56:58.733104: step 208260, loss = 2.77 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 13:57:04.086514: step 208270, loss = 2.71 (263.0 examples/sec; 0.487 sec/batch)
2016-07-16 13:57:08.833096: step 208280, loss = 2.50 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 13:57:13.456565: step 208290, loss = 2.87 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 13:57:18.167127: step 208300, loss = 2.86 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 13:57:23.774870: step 208310, loss = 2.63 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 13:57:29.570931: step 208320, loss = 2.63 (247.6 examples/sec; 0.517 sec/batch)
2016-07-16 13:57:34.431052: step 208330, loss = 2.86 (270.9 examples/sec; 0.473 sec/batch)
2016-07-16 13:57:39.064378: step 208340, loss = 2.67 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 13:57:43.695333: step 208350, loss = 2.92 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 13:57:49.453745: step 208360, loss = 2.76 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 13:57:54.276742: step 208370, loss = 2.61 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 13:57:59.083297: step 208380, loss = 2.69 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 13:58:03.863030: step 208390, loss = 2.46 (275.2 examples/sec; 0.465 sec/batch)
2016-07-16 13:58:08.512382: step 208400, loss = 2.76 (273.6 examples/sec; 0.468 sec/batch)
2016-07-16 13:58:14.144237: step 208410, loss = 2.64 (279.8 examples/sec; 0.458 sec/batch)
2016-07-16 13:58:18.795191: step 208420, loss = 2.75 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 13:58:24.389532: step 208430, loss = 2.68 (207.1 examples/sec; 0.618 sec/batch)
2016-07-16 13:58:29.326911: step 208440, loss = 2.80 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 13:58:34.087509: step 208450, loss = 2.58 (255.5 examples/sec; 0.501 sec/batch)
2016-07-16 13:58:39.746852: step 208460, loss = 2.67 (183.6 examples/sec; 0.697 sec/batch)
2016-07-16 13:58:45.773462: step 208470, loss = 2.46 (283.4 examples/sec; 0.452 sec/batch)
2016-07-16 13:58:50.419919: step 208480, loss = 2.58 (275.4 examples/sec; 0.465 sec/batch)
2016-07-16 13:58:55.052199: step 208490, loss = 2.76 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 13:59:00.715679: step 208500, loss = 2.84 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 13:59:06.573977: step 208510, loss = 2.82 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 13:59:11.432102: step 208520, loss = 2.61 (248.8 examples/sec; 0.514 sec/batch)
2016-07-16 13:59:16.167348: step 208530, loss = 2.79 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 13:59:21.004415: step 208540, loss = 2.55 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 13:59:27.658773: step 208550, loss = 2.76 (205.2 examples/sec; 0.624 sec/batch)
2016-07-16 13:59:32.614152: step 208560, loss = 2.60 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 13:59:37.268688: step 208570, loss = 2.84 (273.7 examples/sec; 0.468 sec/batch)
2016-07-16 13:59:41.832303: step 208580, loss = 2.62 (278.0 examples/sec; 0.460 sec/batch)
2016-07-16 13:59:46.487736: step 208590, loss = 2.74 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 13:59:51.109001: step 208600, loss = 2.54 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 13:59:56.762706: step 208610, loss = 2.75 (275.8 examples/sec; 0.464 sec/batch)
2016-07-16 14:00:02.576764: step 208620, loss = 2.91 (256.1 examples/sec; 0.500 sec/batch)
2016-07-16 14:00:07.340060: step 208630, loss = 2.95 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 14:00:12.160693: step 208640, loss = 3.01 (260.8 examples/sec; 0.491 sec/batch)
2016-07-16 14:00:16.912590: step 208650, loss = 2.91 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 14:00:21.590924: step 208660, loss = 2.71 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 14:00:26.254968: step 208670, loss = 2.55 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 14:00:30.855680: step 208680, loss = 2.52 (279.8 examples/sec; 0.458 sec/batch)
2016-07-16 14:00:35.495554: step 208690, loss = 2.63 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 14:00:40.077170: step 208700, loss = 2.76 (278.6 examples/sec; 0.460 sec/batch)
2016-07-16 14:00:45.939629: step 208710, loss = 2.48 (203.2 examples/sec; 0.630 sec/batch)
2016-07-16 14:00:51.504364: step 208720, loss = 2.70 (259.0 examples/sec; 0.494 sec/batch)
2016-07-16 14:00:56.248070: step 208730, loss = 2.62 (272.6 examples/sec; 0.469 sec/batch)
2016-07-16 14:01:01.168117: step 208740, loss = 2.67 (261.3 examples/sec; 0.490 sec/batch)
2016-07-16 14:01:05.885710: step 208750, loss = 2.82 (262.0 examples/sec; 0.488 sec/batch)
2016-07-16 14:01:10.764702: step 208760, loss = 2.67 (279.8 examples/sec; 0.457 sec/batch)
2016-07-16 14:01:15.557877: step 208770, loss = 2.50 (257.5 examples/sec; 0.497 sec/batch)
2016-07-16 14:01:20.329289: step 208780, loss = 2.49 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 14:01:25.172964: step 208790, loss = 2.70 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 14:01:29.888631: step 208800, loss = 2.53 (284.1 examples/sec; 0.451 sec/batch)
2016-07-16 14:01:35.512181: step 208810, loss = 2.79 (273.5 examples/sec; 0.468 sec/batch)
2016-07-16 14:01:41.224395: step 208820, loss = 2.80 (223.6 examples/sec; 0.573 sec/batch)
2016-07-16 14:01:46.173269: step 208830, loss = 2.72 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 14:01:50.841110: step 208840, loss = 2.49 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 14:01:55.501363: step 208850, loss = 2.88 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 14:02:00.172384: step 208860, loss = 2.64 (273.0 examples/sec; 0.469 sec/batch)
2016-07-16 14:02:04.868074: step 208870, loss = 2.61 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 14:02:10.646162: step 208880, loss = 2.75 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 14:02:15.450477: step 208890, loss = 2.74 (271.6 examples/sec; 0.471 sec/batch)
2016-07-16 14:02:20.283310: step 208900, loss = 2.60 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 14:02:25.959623: step 208910, loss = 2.61 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 14:02:30.643533: step 208920, loss = 2.79 (273.4 examples/sec; 0.468 sec/batch)
2016-07-16 14:02:35.362992: step 208930, loss = 2.58 (272.1 examples/sec; 0.470 sec/batch)
2016-07-16 14:02:40.037910: step 208940, loss = 2.69 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 14:02:44.707010: step 208950, loss = 2.68 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 14:02:49.370024: step 208960, loss = 2.54 (243.6 examples/sec; 0.525 sec/batch)
2016-07-16 14:02:55.102503: step 208970, loss = 2.87 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 14:02:59.882506: step 208980, loss = 2.63 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 14:03:04.704234: step 208990, loss = 2.82 (264.4 examples/sec; 0.484 sec/batch)
2016-07-16 14:03:11.093664: step 209000, loss = 2.64 (208.1 examples/sec; 0.615 sec/batch)
2016-07-16 14:03:17.758841: step 209010, loss = 2.86 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 14:03:22.420393: step 209020, loss = 2.87 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 14:03:27.065851: step 209030, loss = 2.66 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 14:03:32.643597: step 209040, loss = 2.78 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 14:03:37.649725: step 209050, loss = 2.65 (266.5 examples/sec; 0.480 sec/batch)
2016-07-16 14:03:42.391060: step 209060, loss = 2.86 (261.1 examples/sec; 0.490 sec/batch)
2016-07-16 14:03:47.171703: step 209070, loss = 2.85 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 14:03:51.834286: step 209080, loss = 2.70 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 14:03:56.555490: step 209090, loss = 2.78 (277.2 examples/sec; 0.462 sec/batch)
2016-07-16 14:04:01.242163: step 209100, loss = 2.74 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 14:04:06.938197: step 209110, loss = 2.79 (286.7 examples/sec; 0.446 sec/batch)
2016-07-16 14:04:11.604197: step 209120, loss = 2.81 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 14:04:17.344546: step 209130, loss = 2.71 (214.0 examples/sec; 0.598 sec/batch)
2016-07-16 14:04:22.543357: step 209140, loss = 2.74 (205.5 examples/sec; 0.623 sec/batch)
2016-07-16 14:04:28.027115: step 209150, loss = 2.88 (263.3 examples/sec; 0.486 sec/batch)
2016-07-16 14:04:32.768138: step 209160, loss = 2.74 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 14:04:37.409899: step 209170, loss = 2.74 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 14:04:42.112441: step 209180, loss = 2.67 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 14:04:46.747193: step 209190, loss = 2.52 (275.6 examples/sec; 0.464 sec/batch)
2016-07-16 14:04:51.419926: step 209200, loss = 2.66 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 14:04:57.001605: step 209210, loss = 2.63 (271.5 examples/sec; 0.471 sec/batch)
2016-07-16 14:05:02.820604: step 209220, loss = 2.55 (260.5 examples/sec; 0.491 sec/batch)
2016-07-16 14:05:07.633240: step 209230, loss = 2.70 (281.1 examples/sec; 0.455 sec/batch)
2016-07-16 14:05:12.471930: step 209240, loss = 2.86 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 14:05:18.862329: step 209250, loss = 2.74 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 14:05:24.360492: step 209260, loss = 2.79 (256.4 examples/sec; 0.499 sec/batch)
2016-07-16 14:05:29.144588: step 209270, loss = 2.90 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 14:05:34.013621: step 209280, loss = 2.60 (270.8 examples/sec; 0.473 sec/batch)
2016-07-16 14:05:38.677656: step 209290, loss = 2.46 (279.1 examples/sec; 0.459 sec/batch)
2016-07-16 14:05:43.311416: step 209300, loss = 2.41 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 14:05:50.080147: step 209310, loss = 2.85 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 14:05:54.875713: step 209320, loss = 2.55 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 14:05:59.566810: step 209330, loss = 2.65 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 14:06:04.239932: step 209340, loss = 2.63 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 14:06:08.894867: step 209350, loss = 2.61 (282.9 examples/sec; 0.453 sec/batch)
2016-07-16 14:06:13.701008: step 209360, loss = 2.71 (220.9 examples/sec; 0.580 sec/batch)
2016-07-16 14:06:19.400291: step 209370, loss = 2.65 (267.9 examples/sec; 0.478 sec/batch)
2016-07-16 14:06:25.118864: step 209380, loss = 2.69 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 14:06:30.273189: step 209390, loss = 2.50 (202.6 examples/sec; 0.632 sec/batch)
2016-07-16 14:06:35.874584: step 209400, loss = 2.66 (267.3 examples/sec; 0.479 sec/batch)
2016-07-16 14:06:41.587841: step 209410, loss = 2.71 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 14:06:46.427924: step 209420, loss = 2.83 (277.6 examples/sec; 0.461 sec/batch)
2016-07-16 14:06:51.115264: step 209430, loss = 2.76 (271.4 examples/sec; 0.472 sec/batch)
2016-07-16 14:06:55.758426: step 209440, loss = 2.44 (271.9 examples/sec; 0.471 sec/batch)
2016-07-16 14:07:01.498252: step 209450, loss = 2.77 (222.8 examples/sec; 0.574 sec/batch)
2016-07-16 14:07:06.708309: step 209460, loss = 2.96 (204.7 examples/sec; 0.625 sec/batch)
2016-07-16 14:07:12.184141: step 209470, loss = 2.71 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 14:07:16.898841: step 209480, loss = 2.59 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 14:07:21.592831: step 209490, loss = 2.96 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 14:07:26.275295: step 209500, loss = 2.62 (276.5 examples/sec; 0.463 sec/batch)
2016-07-16 14:07:31.866009: step 209510, loss = 2.73 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 14:07:37.773276: step 209520, loss = 2.83 (219.9 examples/sec; 0.582 sec/batch)
2016-07-16 14:07:42.614663: step 209530, loss = 2.59 (276.1 examples/sec; 0.464 sec/batch)
2016-07-16 14:07:47.358441: step 209540, loss = 2.74 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 14:07:52.124608: step 209550, loss = 2.54 (272.3 examples/sec; 0.470 sec/batch)
2016-07-16 14:07:56.992116: step 209560, loss = 2.75 (258.4 examples/sec; 0.495 sec/batch)
2016-07-16 14:08:02.952703: step 209570, loss = 2.82 (281.8 examples/sec; 0.454 sec/batch)
2016-07-16 14:08:07.626405: step 209580, loss = 2.63 (270.0 examples/sec; 0.474 sec/batch)
2016-07-16 14:08:13.306586: step 209590, loss = 2.74 (203.7 examples/sec; 0.628 sec/batch)
2016-07-16 14:08:18.165268: step 209600, loss = 2.91 (277.9 examples/sec; 0.461 sec/batch)
2016-07-16 14:08:23.894357: step 209610, loss = 2.57 (261.4 examples/sec; 0.490 sec/batch)
2016-07-16 14:08:28.633748: step 209620, loss = 2.58 (264.9 examples/sec; 0.483 sec/batch)
2016-07-16 14:08:33.538020: step 209630, loss = 2.55 (250.3 examples/sec; 0.511 sec/batch)
2016-07-16 14:08:39.464686: step 209640, loss = 2.70 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 14:08:44.124414: step 209650, loss = 2.64 (269.7 examples/sec; 0.475 sec/batch)
2016-07-16 14:08:48.782392: step 209660, loss = 2.86 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 14:08:54.615394: step 209670, loss = 2.85 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 14:08:59.458328: step 209680, loss = 2.56 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 14:09:04.275327: step 209690, loss = 2.71 (267.1 examples/sec; 0.479 sec/batch)
2016-07-16 14:09:10.258980: step 209700, loss = 2.65 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 14:09:15.785617: step 209710, loss = 2.77 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 14:09:20.530526: step 209720, loss = 2.62 (253.8 examples/sec; 0.504 sec/batch)
2016-07-16 14:09:25.173307: step 209730, loss = 2.78 (273.2 examples/sec; 0.469 sec/batch)
2016-07-16 14:09:29.920805: step 209740, loss = 2.64 (271.7 examples/sec; 0.471 sec/batch)
2016-07-16 14:09:34.582322: step 209750, loss = 2.95 (271.5 examples/sec; 0.472 sec/batch)
2016-07-16 14:09:39.901689: step 209760, loss = 2.66 (201.5 examples/sec; 0.635 sec/batch)
2016-07-16 14:09:45.143702: step 209770, loss = 2.75 (266.2 examples/sec; 0.481 sec/batch)
2016-07-16 14:09:49.856423: step 209780, loss = 2.61 (280.6 examples/sec; 0.456 sec/batch)
2016-07-16 14:09:54.477560: step 209790, loss = 2.52 (273.3 examples/sec; 0.468 sec/batch)
2016-07-16 14:09:59.750090: step 209800, loss = 2.47 (204.2 examples/sec; 0.627 sec/batch)
2016-07-16 14:10:06.337015: step 209810, loss = 2.82 (252.9 examples/sec; 0.506 sec/batch)
2016-07-16 14:10:11.071717: step 209820, loss = 2.59 (272.0 examples/sec; 0.471 sec/batch)
2016-07-16 14:10:15.752343: step 209830, loss = 2.40 (263.5 examples/sec; 0.486 sec/batch)
2016-07-16 14:10:21.389686: step 209840, loss = 2.57 (206.7 examples/sec; 0.619 sec/batch)
2016-07-16 14:10:26.282445: step 209850, loss = 2.58 (272.7 examples/sec; 0.469 sec/batch)
2016-07-16 14:10:30.919305: step 209860, loss = 2.72 (275.5 examples/sec; 0.465 sec/batch)
2016-07-16 14:10:35.573538: step 209870, loss = 2.73 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 14:10:40.205738: step 209880, loss = 2.63 (278.4 examples/sec; 0.460 sec/batch)
2016-07-16 14:10:44.891038: step 209890, loss = 2.60 (275.9 examples/sec; 0.464 sec/batch)
2016-07-16 14:10:50.687231: step 209900, loss = 2.67 (254.5 examples/sec; 0.503 sec/batch)
2016-07-16 14:10:56.481780: step 209910, loss = 2.80 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 14:11:01.358211: step 209920, loss = 2.81 (262.5 examples/sec; 0.488 sec/batch)
2016-07-16 14:11:07.883148: step 209930, loss = 2.58 (201.1 examples/sec; 0.636 sec/batch)
2016-07-16 14:11:13.218965: step 209940, loss = 2.56 (249.9 examples/sec; 0.512 sec/batch)
2016-07-16 14:11:17.925450: step 209950, loss = 2.71 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 14:11:22.558435: step 209960, loss = 2.56 (274.3 examples/sec; 0.467 sec/batch)
2016-07-16 14:11:27.833529: step 209970, loss = 2.89 (198.9 examples/sec; 0.644 sec/batch)
2016-07-16 14:11:33.144569: step 209980, loss = 2.66 (258.6 examples/sec; 0.495 sec/batch)
2016-07-16 14:11:37.824068: step 209990, loss = 2.75 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 14:11:43.161563: step 210000, loss = 2.86 (189.2 examples/sec; 0.677 sec/batch)
2016-07-16 14:11:49.784283: step 210010, loss = 2.92 (277.5 examples/sec; 0.461 sec/batch)
2016-07-16 14:11:54.508079: step 210020, loss = 2.63 (276.9 examples/sec; 0.462 sec/batch)
2016-07-16 14:11:59.196936: step 210030, loss = 2.65 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 14:12:03.816510: step 210040, loss = 2.69 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 14:12:08.516883: step 210050, loss = 2.79 (268.7 examples/sec; 0.476 sec/batch)
2016-07-16 14:12:14.290633: step 210060, loss = 2.62 (265.1 examples/sec; 0.483 sec/batch)
2016-07-16 14:12:19.612363: step 210070, loss = 2.69 (206.0 examples/sec; 0.621 sec/batch)
2016-07-16 14:12:25.032322: step 210080, loss = 2.63 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 14:12:29.747412: step 210090, loss = 2.51 (270.6 examples/sec; 0.473 sec/batch)
2016-07-16 14:12:34.693401: step 210100, loss = 2.62 (266.6 examples/sec; 0.480 sec/batch)
2016-07-16 14:12:40.392507: step 210110, loss = 2.61 (274.5 examples/sec; 0.466 sec/batch)
2016-07-16 14:12:45.062723: step 210120, loss = 2.53 (259.9 examples/sec; 0.492 sec/batch)
2016-07-16 14:12:49.724130: step 210130, loss = 2.86 (270.4 examples/sec; 0.473 sec/batch)
2016-07-16 14:12:54.680705: step 210140, loss = 2.55 (203.3 examples/sec; 0.630 sec/batch)
2016-07-16 14:13:00.317190: step 210150, loss = 2.72 (264.5 examples/sec; 0.484 sec/batch)
2016-07-16 14:13:05.048472: step 210160, loss = 2.64 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 14:13:09.642334: step 210170, loss = 2.77 (276.0 examples/sec; 0.464 sec/batch)
2016-07-16 14:13:14.291425: step 210180, loss = 2.64 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 14:13:18.937574: step 210190, loss = 2.66 (282.1 examples/sec; 0.454 sec/batch)
2016-07-16 14:13:23.609781: step 210200, loss = 2.62 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 14:13:29.254807: step 210210, loss = 2.71 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 14:13:34.125376: step 210220, loss = 2.77 (217.7 examples/sec; 0.588 sec/batch)
2016-07-16 14:13:39.810183: step 210230, loss = 2.86 (257.9 examples/sec; 0.496 sec/batch)
2016-07-16 14:13:45.567002: step 210240, loss = 2.77 (200.1 examples/sec; 0.640 sec/batch)
2016-07-16 14:13:50.728853: step 210250, loss = 2.84 (203.5 examples/sec; 0.629 sec/batch)
2016-07-16 14:13:56.295234: step 210260, loss = 2.68 (264.1 examples/sec; 0.485 sec/batch)
2016-07-16 14:14:01.108640: step 210270, loss = 3.07 (271.8 examples/sec; 0.471 sec/batch)
2016-07-16 14:14:06.076473: step 210280, loss = 2.89 (221.3 examples/sec; 0.578 sec/batch)
2016-07-16 14:14:12.661953: step 210290, loss = 2.86 (199.6 examples/sec; 0.641 sec/batch)
2016-07-16 14:14:17.812681: step 210300, loss = 2.76 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 14:14:24.819204: step 210310, loss = 2.69 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 14:14:29.554209: step 210320, loss = 2.79 (274.1 examples/sec; 0.467 sec/batch)
2016-07-16 14:14:34.370027: step 210330, loss = 2.83 (278.8 examples/sec; 0.459 sec/batch)
2016-07-16 14:14:39.095501: step 210340, loss = 2.72 (265.5 examples/sec; 0.482 sec/batch)
2016-07-16 14:14:44.854262: step 210350, loss = 2.53 (191.2 examples/sec; 0.669 sec/batch)
2016-07-16 14:14:50.910567: step 210360, loss = 2.57 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 14:14:56.331061: step 210370, loss = 2.59 (205.7 examples/sec; 0.622 sec/batch)
2016-07-16 14:15:01.563676: step 210380, loss = 2.77 (256.2 examples/sec; 0.500 sec/batch)
2016-07-16 14:15:06.243777: step 210390, loss = 2.74 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 14:15:10.880907: step 210400, loss = 2.98 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 14:15:16.480000: step 210410, loss = 2.71 (278.5 examples/sec; 0.460 sec/batch)
2016-07-16 14:15:21.148317: step 210420, loss = 2.77 (268.4 examples/sec; 0.477 sec/batch)
2016-07-16 14:15:26.916978: step 210430, loss = 2.87 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 14:15:31.689923: step 210440, loss = 2.66 (280.7 examples/sec; 0.456 sec/batch)
2016-07-16 14:15:36.537203: step 210450, loss = 2.50 (255.1 examples/sec; 0.502 sec/batch)
2016-07-16 14:15:41.305578: step 210460, loss = 2.44 (272.2 examples/sec; 0.470 sec/batch)
2016-07-16 14:15:45.902815: step 210470, loss = 2.79 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 14:15:50.609642: step 210480, loss = 2.51 (269.1 examples/sec; 0.476 sec/batch)
2016-07-16 14:15:55.205739: step 210490, loss = 2.79 (281.7 examples/sec; 0.454 sec/batch)
2016-07-16 14:15:59.913241: step 210500, loss = 2.57 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 14:16:05.534102: step 210510, loss = 2.62 (282.8 examples/sec; 0.453 sec/batch)
2016-07-16 14:16:10.343860: step 210520, loss = 2.76 (207.4 examples/sec; 0.617 sec/batch)
2016-07-16 14:16:16.069414: step 210530, loss = 2.58 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 14:16:20.898905: step 210540, loss = 2.61 (278.6 examples/sec; 0.459 sec/batch)
2016-07-16 14:16:25.557975: step 210550, loss = 2.58 (276.2 examples/sec; 0.463 sec/batch)
2016-07-16 14:16:30.268165: step 210560, loss = 2.59 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 14:16:34.913320: step 210570, loss = 2.72 (274.4 examples/sec; 0.466 sec/batch)
2016-07-16 14:16:39.598252: step 210580, loss = 2.37 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 14:16:44.241206: step 210590, loss = 2.87 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 14:16:49.997566: step 210600, loss = 2.66 (207.6 examples/sec; 0.617 sec/batch)
2016-07-16 14:16:55.883314: step 210610, loss = 2.83 (276.7 examples/sec; 0.463 sec/batch)
2016-07-16 14:17:00.704250: step 210620, loss = 3.02 (255.2 examples/sec; 0.502 sec/batch)
2016-07-16 14:17:05.432495: step 210630, loss = 2.59 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 14:17:10.053517: step 210640, loss = 2.63 (274.9 examples/sec; 0.466 sec/batch)
2016-07-16 14:17:14.755894: step 210650, loss = 2.88 (281.0 examples/sec; 0.456 sec/batch)
2016-07-16 14:17:19.403518: step 210660, loss = 2.65 (274.6 examples/sec; 0.466 sec/batch)
2016-07-16 14:17:24.849956: step 210670, loss = 2.65 (194.3 examples/sec; 0.659 sec/batch)
2016-07-16 14:17:30.035201: step 210680, loss = 2.83 (265.0 examples/sec; 0.483 sec/batch)
2016-07-16 14:17:34.711275: step 210690, loss = 2.74 (264.2 examples/sec; 0.485 sec/batch)
2016-07-16 14:17:40.265452: step 210700, loss = 2.65 (187.3 examples/sec; 0.683 sec/batch)
2016-07-16 14:17:46.645259: step 210710, loss = 2.70 (275.1 examples/sec; 0.465 sec/batch)
2016-07-16 14:17:51.865936: step 210720, loss = 2.63 (206.3 examples/sec; 0.620 sec/batch)
2016-07-16 14:17:57.212117: step 210730, loss = 2.82 (265.9 examples/sec; 0.481 sec/batch)
2016-07-16 14:18:01.945611: step 210740, loss = 2.70 (269.9 examples/sec; 0.474 sec/batch)
2016-07-16 14:18:06.581066: step 210750, loss = 2.80 (272.5 examples/sec; 0.470 sec/batch)
2016-07-16 14:18:11.251806: step 210760, loss = 2.88 (274.2 examples/sec; 0.467 sec/batch)
2016-07-16 14:18:15.900066: step 210770, loss = 2.90 (279.5 examples/sec; 0.458 sec/batch)
2016-07-16 14:18:21.561709: step 210780, loss = 2.88 (203.7 examples/sec; 0.629 sec/batch)
2016-07-16 14:18:26.812529: step 210790, loss = 2.79 (270.3 examples/sec; 0.473 sec/batch)
2016-07-16 14:18:31.475539: step 210800, loss = 2.50 (278.7 examples/sec; 0.459 sec/batch)
2016-07-16 14:18:37.167039: step 210810, loss = 2.78 (280.0 examples/sec; 0.457 sec/batch)
2016-07-16 14:18:41.907930: step 210820, loss = 2.87 (281.2 examples/sec; 0.455 sec/batch)
2016-07-16 14:18:46.632481: step 210830, loss = 2.76 (266.9 examples/sec; 0.480 sec/batch)
2016-07-16 14:18:51.296243: step 210840, loss = 2.56 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 14:18:55.976371: step 210850, loss = 2.67 (251.0 examples/sec; 0.510 sec/batch)
2016-07-16 14:19:00.642401: step 210860, loss = 2.70 (282.4 examples/sec; 0.453 sec/batch)
2016-07-16 14:19:05.323880: step 210870, loss = 2.75 (267.7 examples/sec; 0.478 sec/batch)
2016-07-16 14:19:11.092545: step 210880, loss = 2.90 (264.6 examples/sec; 0.484 sec/batch)
2016-07-16 14:19:15.926755: step 210890, loss = 2.71 (279.4 examples/sec; 0.458 sec/batch)
2016-07-16 14:19:20.697077: step 210900, loss = 2.59 (256.7 examples/sec; 0.499 sec/batch)
2016-07-16 14:19:26.447604: step 210910, loss = 2.79 (277.0 examples/sec; 0.462 sec/batch)
2016-07-16 14:19:31.049338: step 210920, loss = 2.68 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 14:19:36.276996: step 210930, loss = 2.71 (208.0 examples/sec; 0.615 sec/batch)
2016-07-16 14:19:41.573330: step 210940, loss = 2.68 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 14:19:46.301394: step 210950, loss = 2.48 (267.8 examples/sec; 0.478 sec/batch)
2016-07-16 14:19:51.140355: step 210960, loss = 2.52 (264.7 examples/sec; 0.483 sec/batch)
2016-07-16 14:19:55.768212: step 210970, loss = 2.88 (285.1 examples/sec; 0.449 sec/batch)
2016-07-16 14:20:00.375751: step 210980, loss = 2.76 (269.0 examples/sec; 0.476 sec/batch)
2016-07-16 14:20:05.020802: step 210990, loss = 2.58 (269.4 examples/sec; 0.475 sec/batch)
2016-07-16 14:20:10.756980: step 211000, loss = 2.77 (261.8 examples/sec; 0.489 sec/batch)
2016-07-16 14:20:16.537913: step 211010, loss = 2.56 (274.8 examples/sec; 0.466 sec/batch)
2016-07-16 14:20:21.373419: step 211020, loss = 2.69 (253.3 examples/sec; 0.505 sec/batch)
2016-07-16 14:20:27.744988: step 211030, loss = 2.70 (208.7 examples/sec; 0.613 sec/batch)
2016-07-16 14:20:33.168990: step 211040, loss = 2.73 (260.3 examples/sec; 0.492 sec/batch)
2016-07-16 14:20:38.853842: step 211050, loss = 2.89 (275.0 examples/sec; 0.465 sec/batch)
2016-07-16 14:20:43.523147: step 211060, loss = 2.70 (279.0 examples/sec; 0.459 sec/batch)
2016-07-16 14:20:48.236788: step 211070, loss = 2.80 (272.9 examples/sec; 0.469 sec/batch)
2016-07-16 14:20:52.908005: step 211080, loss = 2.62 (283.6 examples/sec; 0.451 sec/batch)
2016-07-16 14:20:57.647801: step 211090, loss = 2.67 (275.3 examples/sec; 0.465 sec/batch)
2016-07-16 14:21:03.455347: step 211100, loss = 2.68 (264.8 examples/sec; 0.483 sec/batch)
2016-07-16 14:21:09.244647: step 211110, loss = 2.86 (273.9 examples/sec; 0.467 sec/batch)
2016-07-16 14:21:14.173726: step 211120, loss = 2.52 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 14:21:20.801225: step 211130, loss = 2.80 (206.2 examples/sec; 0.621 sec/batch)
2016-07-16 14:21:25.802714: step 211140, loss = 2.54 (283.9 examples/sec; 0.451 sec/batch)
2016-07-16 14:21:30.575879: step 211150, loss = 2.63 (270.2 examples/sec; 0.474 sec/batch)
2016-07-16 14:21:35.344604: step 211160, loss = 2.77 (267.6 examples/sec; 0.478 sec/batch)
2016-07-16 14:21:40.953803: step 211170, loss = 2.95 (192.4 examples/sec; 0.665 sec/batch)
2016-07-16 14:21:46.301562: step 211180, loss = 2.56 (234.8 examples/sec; 0.545 sec/batch)
2016-07-16 14:21:52.307828: step 211190, loss = 2.64 (253.2 examples/sec; 0.506 sec/batch)
2016-07-16 14:21:58.309483: step 211200, loss = 2.92 (216.3 examples/sec; 0.592 sec/batch)
2016-07-16 14:22:05.252730: step 211210, loss = 2.85 (186.2 examples/sec; 0.687 sec/batch)
2016-07-16 14:22:10.553797: step 211220, loss = 2.57 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 14:22:15.477438: step 211230, loss = 2.57 (251.9 examples/sec; 0.508 sec/batch)
2016-07-16 14:22:20.478116: step 211240, loss = 2.70 (261.0 examples/sec; 0.490 sec/batch)
2016-07-16 14:22:25.504229: step 211250, loss = 2.42 (249.9 examples/sec; 0.512 sec/batch)
2016-07-16 14:22:30.390438: step 211260, loss = 2.60 (257.4 examples/sec; 0.497 sec/batch)
2016-07-16 14:22:35.430738: step 211270, loss = 2.63 (270.1 examples/sec; 0.474 sec/batch)
2016-07-16 14:22:40.386452: step 211280, loss = 2.59 (249.3 examples/sec; 0.513 sec/batch)
2016-07-16 14:22:46.841656: step 211290, loss = 2.75 (188.4 examples/sec; 0.679 sec/batch)
2016-07-16 14:22:52.785461: step 211300, loss = 2.92 (248.4 examples/sec; 0.515 sec/batch)
2016-07-16 14:22:58.734929: step 211310, loss = 2.75 (251.7 examples/sec; 0.509 sec/batch)
2016-07-16 14:23:04.474345: step 211320, loss = 2.60 (177.6 examples/sec; 0.721 sec/batch)
2016-07-16 14:23:11.066812: step 211330, loss = 2.86 (251.2 examples/sec; 0.510 sec/batch)
2016-07-16 14:23:16.086797: step 211340, loss = 2.69 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 14:23:21.118987: step 211350, loss = 2.60 (254.3 examples/sec; 0.503 sec/batch)
2016-07-16 14:23:26.020297: step 211360, loss = 2.65 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 14:23:31.116673: step 211370, loss = 2.77 (258.7 examples/sec; 0.495 sec/batch)
2016-07-16 14:23:36.090413: step 211380, loss = 2.67 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 14:23:41.032237: step 211390, loss = 2.83 (264.7 examples/sec; 0.484 sec/batch)
2016-07-16 14:23:46.091888: step 211400, loss = 2.59 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 14:23:52.089006: step 211410, loss = 2.84 (250.0 examples/sec; 0.512 sec/batch)
2016-07-16 14:23:57.061049: step 211420, loss = 2.73 (260.0 examples/sec; 0.492 sec/batch)
2016-07-16 14:24:02.113400: step 211430, loss = 3.00 (255.7 examples/sec; 0.500 sec/batch)
2016-07-16 14:24:07.022917: step 211440, loss = 2.95 (252.3 examples/sec; 0.507 sec/batch)
2016-07-16 14:24:12.037412: step 211450, loss = 2.98 (269.5 examples/sec; 0.475 sec/batch)
2016-07-16 14:24:17.015976: step 211460, loss = 2.77 (253.6 examples/sec; 0.505 sec/batch)
2016-07-16 14:24:23.677521: step 211470, loss = 2.67 (186.7 examples/sec; 0.686 sec/batch)
2016-07-16 14:24:29.363200: step 211480, loss = 2.60 (247.5 examples/sec; 0.517 sec/batch)
2016-07-16 14:24:35.500923: step 211490, loss = 2.52 (251.4 examples/sec; 0.509 sec/batch)
2016-07-16 14:24:41.291873: step 211500, loss = 2.77 (198.7 examples/sec; 0.644 sec/batch)
2016-07-16 14:24:47.747162: step 211510, loss = 2.51 (261.6 examples/sec; 0.489 sec/batch)
2016-07-16 14:24:52.694313: step 211520, loss = 2.58 (253.0 examples/sec; 0.506 sec/batch)
2016-07-16 14:24:57.598929: step 211530, loss = 2.63 (269.3 examples/sec; 0.475 sec/batch)
2016-07-16 14:25:02.664263: step 211540, loss = 2.63 (262.4 examples/sec; 0.488 sec/batch)
2016-07-16 14:25:07.569679: step 211550, loss = 2.70 (252.0 examples/sec; 0.508 sec/batch)
2016-07-16 14:25:13.684509: step 211560, loss = 2.85 (175.9 examples/sec; 0.728 sec/batch)
2016-07-16 14:25:19.948511: step 211570, loss = 2.64 (249.9 examples/sec; 0.512 sec/batch)
2016-07-16 14:25:25.895428: step 211580, loss = 2.67 (194.0 examples/sec; 0.660 sec/batch)
2016-07-16 14:25:31.274761: step 211590, loss = 2.86 (194.2 examples/sec; 0.659 sec/batch)
2016-07-16 14:25:37.119030: step 211600, loss = 2.59 (251.5 examples/sec; 0.509 sec/batch)
2016-07-16 14:25:43.059785: step 211610, loss = 2.50 (254.6 examples/sec; 0.503 sec/batch)
2016-07-16 14:25:48.904514: step 211620, loss = 2.46 (176.4 examples/sec; 0.726 sec/batch)
2016-07-16 14:25:55.363978: step 211630, loss = 2.66 (256.3 examples/sec; 0.499 sec/batch)
2016-07-16 14:26:01.120841: step 211640, loss = 2.57 (192.5 examples/sec; 0.665 sec/batch)
2016-07-16 14:26:06.641525: step 211650, loss = 2.89 (225.9 examples/sec; 0.567 sec/batch)
2016-07-16 14:26:12.534817: step 211660, loss = 2.44 (255.0 examples/sec; 0.502 sec/batch)
2016-07-16 14:26:17.429479: step 211670, loss = 2.46 (267.4 examples/sec; 0.479 sec/batch)
2016-07-16 14:26:22.454201: step 211680, loss = 2.50 (251.6 examples/sec; 0.509 sec/batch)
2016-07-16 14:26:29.284957: step 211690, loss = 2.76 (205.1 examples/sec; 0.624 sec/batch)
2016-07-16 14:26:34.449059: step 211700, loss = 2.63 (258.9 examples/sec; 0.494 sec/batch)
2016-07-16 14:26:41.520021: step 211710, loss = 2.57 (259.2 examples/sec; 0.494 sec/batch)
2016-07-16 14:26:47.263642: step 211720, loss = 2.71 (260.7 examples/sec; 0.491 sec/batch)
2016-07-16 14:26:52.666453: step 211730, loss = 2.71 (200.0 examples/sec; 0.640 sec/batch)
2016-07-16 14:26:57.963925: step 211740, loss = 2.42 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 14:27:03.790774: step 211750, loss = 2.84 (263.2 examples/sec; 0.486 sec/batch)
2016-07-16 14:27:08.657714: step 211760, loss = 2.58 (277.4 examples/sec; 0.461 sec/batch)
2016-07-16 14:27:13.521099: step 211770, loss = 2.71 (261.7 examples/sec; 0.489 sec/batch)
2016-07-16 14:27:20.683026: step 211780, loss = 2.82 (172.8 examples/sec; 0.741 sec/batch)
2016-07-16 14:27:26.510978: step 211790, loss = 2.73 (226.6 examples/sec; 0.565 sec/batch)
2016-07-16 14:27:32.587498: step 211800, loss = 2.55 (266.3 examples/sec; 0.481 sec/batch)
2016-07-16 14:27:38.467654: step 211810, loss = 2.94 (274.4 examples/sec; 0.467 sec/batch)
2016-07-16 14:27:43.329109: step 211820, loss = 2.53 (274.7 examples/sec; 0.466 sec/batch)
2016-07-16 14:27:48.115776: step 211830, loss = 2.66 (259.4 examples/sec; 0.493 sec/batch)
2016-07-16 14:27:52.867959: step 211840, loss = 2.82 (277.1 examples/sec; 0.462 sec/batch)
2016-07-16 14:27:57.690728: step 211850, loss = 2.64 (268.0 examples/sec; 0.478 sec/batch)
2016-07-16 14:28:04.060456: step 211860, loss = 2.70 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 14:28:09.547224: step 211870, loss = 2.55 (252.8 examples/sec; 0.506 sec/batch)
2016-07-16 14:28:15.317814: step 211880, loss = 2.87 (261.9 examples/sec; 0.489 sec/batch)
2016-07-16 14:28:20.172950: step 211890, loss = 2.82 (272.8 examples/sec; 0.469 sec/batch)
2016-07-16 14:28:24.958793: step 211900, loss = 2.63 (264.0 examples/sec; 0.485 sec/batch)
2016-07-16 14:28:30.649635: step 211910, loss = 2.81 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 14:28:35.961755: step 211920, loss = 2.77 (188.3 examples/sec; 0.680 sec/batch)
2016-07-16 14:28:42.404066: step 211930, loss = 2.76 (209.0 examples/sec; 0.612 sec/batch)
2016-07-16 14:28:47.487733: step 211940, loss = 2.85 (201.9 examples/sec; 0.634 sec/batch)
2016-07-16 14:28:53.053036: step 211950, loss = 2.72 (265.6 examples/sec; 0.482 sec/batch)
2016-07-16 14:28:57.780316: step 211960, loss = 2.47 (273.8 examples/sec; 0.467 sec/batch)
2016-07-16 14:29:02.661621: step 211970, loss = 2.70 (246.2 examples/sec; 0.520 sec/batch)
2016-07-16 14:29:09.261676: step 211980, loss = 2.73 (205.6 examples/sec; 0.623 sec/batch)
2016-07-16 14:29:14.455494: step 211990, loss = 2.64 (263.6 examples/sec; 0.486 sec/batch)
2016-07-16 14:29:19.249063: step 212000, loss = 2.65 (247.4 examples/sec; 0.517 sec/batch)
2016-07-16 14:29:26.443935: step 212010, loss = 2.77 (182.3 examples/sec; 0.702 sec/batch)
2016-07-16 14:29:32.323888: step 212020, loss = 2.70 (242.6 examples/sec; 0.528 sec/batch)
2016-07-16 14:29:38.165737: step 212030, loss = 2.70 (193.0 examples/sec; 0.663 sec/batch)
2016-07-16 14:29:43.241607: step 212040, loss = 2.75 (206.6 examples/sec; 0.620 sec/batch)
2016-07-16 14:29:48.890134: step 212050, loss = 2.76 (254.3 examples/sec; 0.503 sec/batch)
2016-07-16 14:29:54.689538: step 212060, loss = 2.63 (206.4 examples/sec; 0.620 sec/batch)
2016-07-16 14:29:59.528909: step 212070, loss = 2.57 (271.2 examples/sec; 0.472 sec/batch)
2016-07-16 14:30:04.337952: step 212080, loss = 2.64 (264.3 examples/sec; 0.484 sec/batch)
2016-07-16 14:30:10.288779: step 212090, loss = 2.69 (189.5 examples/sec; 0.676 sec/batch)
2016-07-16 14:30:16.098143: step 212100, loss = 2.71 (259.6 examples/sec; 0.493 sec/batch)
2016-07-16 14:30:21.891114: step 212110, loss = 2.61 (269.8 examples/sec; 0.474 sec/batch)
2016-07-16 14:30:26.835611: step 212120, loss = 2.67 (234.6 examples/sec; 0.546 sec/batch)
2016-07-16 14:30:33.467158: step 212130, loss = 3.04 (202.8 examples/sec; 0.631 sec/batch)
2016-07-16 14:30:38.645286: step 212140, loss = 2.73 (263.9 examples/sec; 0.485 sec/batch)
2016-07-16 14:30:43.349419: step 212150, loss = 2.53 (259.3 examples/sec; 0.494 sec/batch)
2016-07-16 14:30:48.150426: step 212160, loss = 2.68 (276.3 examples/sec; 0.463 sec/batch)
2016-07-16 14:30:52.887658: step 212170, loss = 2.65 (263.4 examples/sec; 0.486 sec/batch)
2016-07-16 14:30:58.917842: step 212180, loss = 2.62 (197.7 examples/sec; 0.647 sec/batch)
2016-07-16 14:31:04.618785: step 212190, loss = 2.72 (262.7 examples/sec; 0.487 sec/batch)
2016-07-16 14:31:09.430416: step 212200, loss = 2.51 (279.8 examples/sec; 0.457 sec/batch)

TEST TIME:

python cifar10_eval.py 
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GT 650M
major: 3 minor: 0 memoryClockRate (GHz) 0.835
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.83GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)
2016-07-16 14:41:55.304645: precision @ 1 = 0.468


